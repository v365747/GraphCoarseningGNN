{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XynMFbIsYBUz",
        "outputId": "74c879e1-3057-4093-933b-c598f17b5c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Cloning into 'nequip'...\n",
            "remote: Enumerating objects: 218, done.\u001b[K\n",
            "remote: Counting objects: 100% (218/218), done.\u001b[K\n",
            "remote: Compressing objects: 100% (205/205), done.\u001b[K\n",
            "remote: Total 218 (delta 4), reused 84 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (218/218), 360.63 KiB | 1.44 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Processing ./nequip\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (1.26.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (4.66.6)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (6.0.2)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (2.5.1+cu121)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (0.1.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase->nequip==0.6.1) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip==0.6.1) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.0.2)\n",
            "Building wheels for collected packages: nequip\n",
            "  Building wheel for nequip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nequip: filename=nequip-0.6.1-py3-none-any.whl size=175386 sha256=62879c77c48eb62523093b6fc3f46a904cf7ef1b59f07e25b8564e5daf7e6231\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-947i4qzm/wheels/11/64/44/9d30bacb0803dffa7821bb8685dbc60a0830cca339476e4e86\n",
            "Successfully built nequip\n",
            "Installing collected packages: nequip\n",
            "  Attempting uninstall: nequip\n",
            "    Found existing installation: nequip 0.6.1\n",
            "    Uninstalling nequip-0.6.1:\n",
            "      Successfully uninstalled nequip-0.6.1\n",
            "Successfully installed nequip-0.6.1\n",
            "fatal: destination path 'allegro' already exists and is not an empty directory.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nequip>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from mir-allegro==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.66.6)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (0.1.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.0.2)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27432 sha256=fe9e92c6150d1d5f525bd0d6d14a3b1f7382291384bff1536901c518e93e8dee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-74dt13ev/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: mir-allegro\n",
            "  Attempting uninstall: mir-allegro\n",
            "    Found existing installation: mir-allegro 0.2.0\n",
            "    Uninstalling mir-allegro-0.2.0:\n",
            "      Successfully uninstalled mir-allegro-0.2.0\n",
            "Successfully installed mir-allegro-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "\n",
        "# install nequip\n",
        "!rm -rf nequip\n",
        "!git clone --depth 1 \"https://github.com/mir-group/nequip.git\"\n",
        "!pip install nequip/\n",
        "\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "print(\"-----------------------------\")\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "!which nvcc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZvdhYOaOp5I",
        "outputId": "491c931f-d270-4bef-adcd-68e288e467ab"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "True\n",
            "1\n",
            "NVIDIA A100-SXM4-40GB\n",
            "-----------------------------\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Sat Dec  7 23:57:36 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0              47W / 400W |      5MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf lammps\n",
        "!git clone --depth 1 https://github.com/lammps/lammps.git\n",
        "# Stable Release (Simon )\n",
        "#!git clone -b stable_29Sep2021_update2 --depth 1 https://github.com/lammps/lammps.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJrTDEJJ8klU",
        "outputId": "c70e428a-e889-44e0-be97-16cafbdad685"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lammps'...\n",
            "remote: Enumerating objects: 14161, done.\u001b[K\n",
            "remote: Counting objects: 100% (14161/14161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10269/10269), done.\u001b[K\n",
            "remote: Total 14161 (delta 4831), reused 7870 (delta 3656), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14161/14161), 129.16 MiB | 26.36 MiB/s, done.\n",
            "Resolving deltas: 100% (4831/4831), done.\n",
            "Updating files: 100% (13507/13507), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mkl-include"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBHpSXUc8wsa",
        "outputId": "6fefbe08-bbbf-4546-fbaf-02c30bbbeca6"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mkl-include in /usr/local/lib/python3.10/dist-packages (2025.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
        "!sh ./cmake-3.23.1-linux-x86_64.sh --prefix=/usr/local --exclude-subdir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyifJb3q81U1",
        "outputId": "88b580fa-7714-4144-d580-3fff06dd5b27"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-07 23:58:08--  https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241207%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241207T235809Z&X-Amz-Expires=300&X-Amz-Signature=396441daa4a290baeb018e4694c620340492fd3e5d3bb07ecd441819e6d0e6e6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-07 23:58:09--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241207%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241207T235809Z&X-Amz-Expires=300&X-Amz-Signature=396441daa4a290baeb018e4694c620340492fd3e5d3bb07ecd441819e6d0e6e6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46004365 (44M) [application/octet-stream]\n",
            "Saving to: ‘cmake-3.23.1-linux-x86_64.sh.3’\n",
            "\n",
            "cmake-3.23.1-linux- 100%[===================>]  43.87M   106MB/s    in 0.4s    \n",
            "\n",
            "2024-12-07 23:58:10 (106 MB/s) - ‘cmake-3.23.1-linux-x86_64.sh.3’ saved [46004365/46004365]\n",
            "\n",
            "CMake Installer Version: 3.23.1, Copyright (c) Kitware\n",
            "This is a self-extracting archive.\n",
            "The archive will be extracted to: /usr/local\n",
            "\n",
            "Using target directory: /usr/local\n",
            "Extracting, please wait...\n",
            "\n",
            "Unpacking finished successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf libtorch\n",
        "!wget wget https://download.pytorch.org/libtorch/cu124/libtorch-cxx11-abi-shared-with-deps-2.5.1%2Bcu124.zip\n",
        "!unzip libtorch-cxx11-abi-shared-with-deps-2.5.1+cu124.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L0y1M-c9HNG",
        "outputId": "1262fd88-3991-4a29-f887-fa6302596df4"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/item.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/le.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/less.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/median.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/min.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/or.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/put.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/random.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/range.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/real.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/round.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/set.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/size.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/square.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/std.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/t.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/take.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/values.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/var.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/where.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_ops.h  \n",
            "   creating: libtorch/include/ATen/hip/\n",
            "   creating: libtorch/include/ATen/hip/impl/\n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPAllocatorMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPStreamMasqueradingAsCUDA.h  \n",
            "   creating: libtorch/include/ATen/mps/\n",
            "  inflating: libtorch/include/ATen/mps/EmptyTensor.h  \n",
            "  inflating: libtorch/include/ATen/mps/IndexKernels.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSAllocator.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSAllocatorInterface.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSDevice.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSEvent.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSGeneratorImpl.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSGuardImpl.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSHooks.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSProfiler.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSStream.h  \n",
            "   creating: libtorch/include/ATen/miopen/\n",
            "  inflating: libtorch/include/ATen/miopen/Descriptors.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Exceptions.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Handle.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Types.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Utils.h  \n",
            "  inflating: libtorch/include/ATen/miopen/miopen-wrapper.h  \n",
            "   creating: libtorch/include/ATen/detail/\n",
            "  inflating: libtorch/include/ATen/detail/AcceleratorHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/CUDAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/FunctionTraits.h  \n",
            "  inflating: libtorch/include/ATen/detail/HIPHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/IPUHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MAIAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MPSHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MTIAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/PrivateUse1HooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/XPUHooksInterface.h  \n",
            "   creating: libtorch/include/ATen/native/\n",
            "  inflating: libtorch/include/ATen/native/Activation.h  \n",
            "  inflating: libtorch/include/ATen/native/AdaptivePooling.h  \n",
            "  inflating: libtorch/include/ATen/native/AmpKernels.h  \n",
            "  inflating: libtorch/include/ATen/native/BatchLinearAlgebra.h  \n",
            "  inflating: libtorch/include/ATen/native/BinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/BucketizationUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/CPUBlas.h  \n",
            "  inflating: libtorch/include/ATen/native/CPUFallback.h  \n",
            "  inflating: libtorch/include/ATen/native/CanUse32BitIndexMath.h  \n",
            "  inflating: libtorch/include/ATen/native/ComplexHelper.h  \n",
            "  inflating: libtorch/include/ATen/native/CompositeRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/CompositeRandomAccessorCommon.h  \n",
            "  inflating: libtorch/include/ATen/native/ConvUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/ConvolutionMM3d.h  \n",
            "  inflating: libtorch/include/ATen/native/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/Cross.h  \n",
            "  inflating: libtorch/include/ATen/native/DilatedConvolutionUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/DispatchStub.h  \n",
            "  inflating: libtorch/include/ATen/native/Distance.h  \n",
            "  inflating: libtorch/include/ATen/native/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/Distributions.h  \n",
            "  inflating: libtorch/include/ATen/native/EmbeddingBag.h  \n",
            "  inflating: libtorch/include/ATen/native/Fill.h  \n",
            "  inflating: libtorch/include/ATen/native/ForeachUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/FractionalMaxPooling.h  \n",
            "  inflating: libtorch/include/ATen/native/FunctionOfAMatrixUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedAdagrad.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedAdam.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedSGD.h  \n",
            "  inflating: libtorch/include/ATen/native/GridSampler.h  \n",
            "  inflating: libtorch/include/ATen/native/GridSamplerUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Histogram.h  \n",
            "  inflating: libtorch/include/ATen/native/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/IndexingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Lerp.h  \n",
            "  inflating: libtorch/include/ATen/native/LinearAlgebra.h  \n",
            "  inflating: libtorch/include/ATen/native/LinearAlgebraUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/LossMulti.h  \n",
            "  inflating: libtorch/include/ATen/native/Math.h  \n",
            "  inflating: libtorch/include/ATen/native/MathBitFallThroughLists.h  \n",
            "  inflating: libtorch/include/ATen/native/MathBitsFallback.h  \n",
            "  inflating: libtorch/include/ATen/native/MaxPooling.h  \n",
            "  inflating: libtorch/include/ATen/native/NonEmptyUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/NonSymbolicBC.h  \n",
            "  inflating: libtorch/include/ATen/native/Normalization.h  \n",
            "  inflating: libtorch/include/ATen/native/Padding.h  \n",
            "  inflating: libtorch/include/ATen/native/PixelShuffle.h  \n",
            "  inflating: libtorch/include/ATen/native/PointwiseOps.h  \n",
            "  inflating: libtorch/include/ATen/native/Pool.h  \n",
            "  inflating: libtorch/include/ATen/native/Pow.h  \n",
            "  inflating: libtorch/include/ATen/native/RNN.h  \n",
            "  inflating: libtorch/include/ATen/native/RangeFactories.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceAllOps.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/ReductionType.h  \n",
            "  inflating: libtorch/include/ATen/native/Repeat.h  \n",
            "  inflating: libtorch/include/ATen/native/Resize.h  \n",
            "  inflating: libtorch/include/ATen/native/ResizeCommon.h  \n",
            "  inflating: libtorch/include/ATen/native/ScatterGatherChecks.h  \n",
            "  inflating: libtorch/include/ATen/native/SegmentReduce.h  \n",
            "  inflating: libtorch/include/ATen/native/SharedReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/SobolEngineOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Sorting.h  \n",
            "  inflating: libtorch/include/ATen/native/SortingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/SparseTensorUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/SpectralOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/StridedRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorAdvancedIndexing.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorAdvancedIndexingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorCompare.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorConversions.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorDimApply.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorFactories.h  \n",
            " extracting: libtorch/include/ATen/native/TensorIterator.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorIteratorDynamicCasting.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorProperties.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorShape.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorTransformations.h  \n",
            "  inflating: libtorch/include/ATen/native/TopKImpl.h  \n",
            "  inflating: libtorch/include/ATen/native/TransposeType.h  \n",
            "  inflating: libtorch/include/ATen/native/TriangularOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/TypeProperties.h  \n",
            "  inflating: libtorch/include/ATen/native/UnaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/Unfold2d.h  \n",
            "  inflating: libtorch/include/ATen/native/Unfold3d.h  \n",
            "  inflating: libtorch/include/ATen/native/UnfoldBackward.h  \n",
            "  inflating: libtorch/include/ATen/native/UpSample.h  \n",
            "  inflating: libtorch/include/ATen/native/batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/group_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/im2col.h  \n",
            "  inflating: libtorch/include/ATen/native/im2col_shape_check.h  \n",
            "  inflating: libtorch/include/ATen/native/layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/verbose_wrapper.h  \n",
            "  inflating: libtorch/include/ATen/native/vol2col.h  \n",
            "   creating: libtorch/include/ATen/native/cpu/\n",
            "  inflating: libtorch/include/ATen/native/cpu/AtomicAddFloat.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/CatKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/ChannelShuffleKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/CopyKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/DepthwiseConvKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/GridSamplerKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/IndexKernelUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Intrinsics.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/IsContiguous.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/LogAddExp.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Loops.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/MaxUnpoolKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/PixelShuffleKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Reduce.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/ReduceUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SampledAddmmKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SerialStackImpl.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SoftmaxKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SpmmReduceKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/StackKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/UpSampleKernelAVXAntialias.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/WeightNormKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/avx_mathfun.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/int_mm_kernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/mixed_data_type.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/moments_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/zmath.h  \n",
            "   creating: libtorch/include/ATen/native/cuda/\n",
            "  inflating: libtorch/include/ATen/native/cuda/Activation.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/BinaryInternal.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CompositeRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CuFFTPlanCache.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CuFFTUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Distributions.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/GridSampler.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/LaunchUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MiscUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Resize.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/RowwiseScaledMM.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ScanKernels.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Sort.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortStable.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Sorting.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorModeKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorTopK.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/jit_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/thread_constants.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CUDAJitLoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CUDALoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/DeviceSqrt.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/EmbeddingBackwardKernel.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ForeachFunctors.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ForeachMinMaxFunctors.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/GridSampler.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/JitLoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/KernelUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Loops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Math.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MemoryAccess.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MultiTensorApply.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Normalization.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/PersistentSoftmax.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Pow.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Randperm.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Reduce.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ScanUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortingCommon.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortingRadixSelect.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorModeKernel.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/UniqueCub.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/UpSample.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/block_reduce.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_amsgrad_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_utils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adamw_amsgrad_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adamw_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/im2col.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/reduction_template.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/vol2col.cuh  \n",
            "   creating: libtorch/include/ATen/native/mps/\n",
            "  inflating: libtorch/include/ATen/native/mps/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphSequoiaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphSonomaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphVenturaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/OperationUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/TensorFactory.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/UnaryConstants.h  \n",
            "   creating: libtorch/include/ATen/native/nested/\n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorBinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorMath.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorTransformerFunctions.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorTransformerUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorUtils.h  \n",
            "   creating: libtorch/include/ATen/native/quantized/\n",
            "  inflating: libtorch/include/ATen/native/quantized/AffineQuantizer.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/AffineQuantizerBase.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/ConvUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/FakeQuantAffine.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/PackedParams.h  \n",
            "   creating: libtorch/include/ATen/native/quantized/cpu/\n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/BinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/EmbeddingPackedParams.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/OnednnUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QnnpackUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QuantUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QuantizedOps.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/RuyUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/XnnpackUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/conv_serialization.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/fbgemm_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/init_qnnpack.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/qembeddingbag.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/qembeddingbag_prepack.h  \n",
            "   creating: libtorch/include/ATen/native/transformers/\n",
            "  inflating: libtorch/include/ATen/native/transformers/attention.h  \n",
            "  inflating: libtorch/include/ATen/native/transformers/sdp_utils_cpp.h  \n",
            "   creating: libtorch/include/ATen/native/utils/\n",
            "  inflating: libtorch/include/ATen/native/utils/Factory.h  \n",
            "  inflating: libtorch/include/ATen/native/utils/ParamUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/utils/ParamsHash.h  \n",
            "   creating: libtorch/include/ATen/quantized/\n",
            "  inflating: libtorch/include/ATen/quantized/QTensorImpl.h  \n",
            "  inflating: libtorch/include/ATen/quantized/Quantizer.h  \n",
            "   creating: libtorch/include/ATen/xpu/\n",
            "  inflating: libtorch/include/ATen/xpu/CachingHostAllocator.h  \n",
            "  inflating: libtorch/include/ATen/xpu/PinnedMemoryAllocator.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUContext.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUDevice.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUEvent.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUGeneratorImpl.h  \n",
            "   creating: libtorch/include/ATen/xpu/detail/\n",
            "  inflating: libtorch/include/ATen/xpu/detail/XPUHooks.h  \n",
            "   creating: libtorch/include/c10/\n",
            "   creating: libtorch/include/c10/xpu/\n",
            "  inflating: libtorch/include/c10/xpu/XPUCachingAllocator.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUDeviceProp.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUException.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUFunctions.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUMacros.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUStream.h  \n",
            "   creating: libtorch/include/c10/xpu/impl/\n",
            "  inflating: libtorch/include/c10/xpu/impl/XPUGuardImpl.h  \n",
            "   creating: libtorch/include/c10/macros/\n",
            "  inflating: libtorch/include/c10/macros/Export.h  \n",
            "  inflating: libtorch/include/c10/macros/Macros.h  \n",
            "  inflating: libtorch/include/c10/macros/cmake_macros.h  \n",
            "   creating: libtorch/include/c10/core/\n",
            "  inflating: libtorch/include/c10/core/Allocator.h  \n",
            "  inflating: libtorch/include/c10/core/AutogradState.h  \n",
            "  inflating: libtorch/include/c10/core/Backend.h  \n",
            "  inflating: libtorch/include/c10/core/CPUAllocator.h  \n",
            "  inflating: libtorch/include/c10/core/CachingDeviceAllocator.h  \n",
            "  inflating: libtorch/include/c10/core/CompileTimeFunctionPointer.h  \n",
            "  inflating: libtorch/include/c10/core/ConstantSymNodeImpl.h  \n",
            "  inflating: libtorch/include/c10/core/Contiguity.h  \n",
            "  inflating: libtorch/include/c10/core/CopyBytes.h  \n",
            "  inflating: libtorch/include/c10/core/DefaultDtype.h  \n",
            "  inflating: libtorch/include/c10/core/DefaultTensorOptions.h  \n",
            "  inflating: libtorch/include/c10/core/Device.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceArray.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceGuard.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceType.h  \n",
            "  inflating: libtorch/include/c10/core/DispatchKey.h  \n",
            "  inflating: libtorch/include/c10/core/DispatchKeySet.h  \n",
            "  inflating: libtorch/include/c10/core/DynamicCast.h  \n",
            "  inflating: libtorch/include/c10/core/Event.h  \n",
            "  inflating: libtorch/include/c10/core/GeneratorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/GradMode.h  \n",
            "  inflating: libtorch/include/c10/core/InferenceMode.h  \n",
            "  inflating: libtorch/include/c10/core/Layout.h  \n",
            "  inflating: libtorch/include/c10/core/MemoryFormat.h  \n",
            "  inflating: libtorch/include/c10/core/OptionalRef.h  \n",
            "  inflating: libtorch/include/c10/core/PyHandleCache.h  \n",
            "  inflating: libtorch/include/c10/core/QEngine.h  \n",
            "  inflating: libtorch/include/c10/core/QScheme.h  \n",
            "  inflating: libtorch/include/c10/core/RefcountedDeleter.h  \n",
            "  inflating: libtorch/include/c10/core/SafePyObject.h  \n",
            "  inflating: libtorch/include/c10/core/Scalar.h  \n",
            "  inflating: libtorch/include/c10/core/ScalarType.h  \n",
            "  inflating: libtorch/include/c10/core/ScalarTypeToTypeMeta.h  \n",
            "  inflating: libtorch/include/c10/core/Storage.h  \n",
            "  inflating: libtorch/include/c10/core/StorageImpl.h  \n",
            "  inflating: libtorch/include/c10/core/Stream.h  \n",
            "  inflating: libtorch/include/c10/core/StreamGuard.h  \n",
            "  inflating: libtorch/include/c10/core/SymBool.h  \n",
            "  inflating: libtorch/include/c10/core/SymFloat.h  \n",
            "  inflating: libtorch/include/c10/core/SymInt.h  \n",
            "  inflating: libtorch/include/c10/core/SymIntArrayRef.h  \n",
            "  inflating: libtorch/include/c10/core/SymNodeImpl.h  \n",
            "  inflating: libtorch/include/c10/core/SymbolicShapeMeta.h  \n",
            "  inflating: libtorch/include/c10/core/TensorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/TensorOptions.h  \n",
            "  inflating: libtorch/include/c10/core/UndefinedTensorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/WrapDimMinimal.h  \n",
            "  inflating: libtorch/include/c10/core/alignment.h  \n",
            "  inflating: libtorch/include/c10/core/thread_pool.h  \n",
            "   creating: libtorch/include/c10/core/impl/\n",
            "  inflating: libtorch/include/c10/core/impl/COW.h  \n",
            "  inflating: libtorch/include/c10/core/impl/COWDeleter.h  \n",
            "  inflating: libtorch/include/c10/core/impl/DeviceGuardImplInterface.h  \n",
            "  inflating: libtorch/include/c10/core/impl/FakeGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/core/impl/GPUTrace.h  \n",
            "  inflating: libtorch/include/c10/core/impl/HermeticPyObjectTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineDeviceGuard.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineEvent.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineStreamGuard.h  \n",
            "  inflating: libtorch/include/c10/core/impl/LocalDispatchKeySet.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PyInterpreter.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PyObjectSlot.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PythonDispatcherTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/SizesAndStrides.h  \n",
            "  inflating: libtorch/include/c10/core/impl/TorchDispatchModeTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/VirtualGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/core/impl/alloc_cpu.h  \n",
            "   creating: libtorch/include/c10/util/\n",
            "  inflating: libtorch/include/c10/util/AbortHandler.h  \n",
            "  inflating: libtorch/include/c10/util/AlignOf.h  \n",
            "  inflating: libtorch/include/c10/util/ApproximateClock.h  \n",
            "  inflating: libtorch/include/c10/util/Array.h  \n",
            "  inflating: libtorch/include/c10/util/ArrayRef.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16-inl.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16-math.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16.h  \n",
            "  inflating: libtorch/include/c10/util/Backtrace.h  \n",
            "  inflating: libtorch/include/c10/util/Bitset.h  \n",
            "  inflating: libtorch/include/c10/util/C++17.h  \n",
            "  inflating: libtorch/include/c10/util/CallOnce.h  \n",
            "  inflating: libtorch/include/c10/util/ConstexprCrc.h  \n",
            "  inflating: libtorch/include/c10/util/DeadlockDetection.h  \n",
            "  inflating: libtorch/include/c10/util/Deprecated.h  \n",
            "  inflating: libtorch/include/c10/util/DimVector.h  \n",
            "  inflating: libtorch/include/c10/util/DynamicCounter.h  \n",
            "  inflating: libtorch/include/c10/util/Exception.h  \n",
            "  inflating: libtorch/include/c10/util/ExclusivelyOwned.h  \n",
            "  inflating: libtorch/include/c10/util/ExclusivelyOwnedTensorTraits.h  \n",
            "  inflating: libtorch/include/c10/util/FbcodeMaps.h  \n",
            "  inflating: libtorch/include/c10/util/Flags.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fn-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fn.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fnuz-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fnuz.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2fnuz-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2fnuz.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_fnuz_cvt.h  \n",
            "  inflating: libtorch/include/c10/util/FunctionRef.h  \n",
            "  inflating: libtorch/include/c10/util/Gauge.h  \n",
            "  inflating: libtorch/include/c10/util/Half-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Half.h  \n",
            "  inflating: libtorch/include/c10/util/IdWrapper.h  \n",
            "  inflating: libtorch/include/c10/util/Lazy.h  \n",
            "  inflating: libtorch/include/c10/util/LeftRight.h  \n",
            "  inflating: libtorch/include/c10/util/Load.h  \n",
            "  inflating: libtorch/include/c10/util/Logging.h  \n",
            "  inflating: libtorch/include/c10/util/MathConstants.h  \n",
            "  inflating: libtorch/include/c10/util/MaybeOwned.h  \n",
            "  inflating: libtorch/include/c10/util/Metaprogramming.h  \n",
            "  inflating: libtorch/include/c10/util/NetworkFlow.h  \n",
            "  inflating: libtorch/include/c10/util/Optional.h  \n",
            "  inflating: libtorch/include/c10/util/OptionalArrayRef.h  \n",
            "  inflating: libtorch/include/c10/util/ParallelGuard.h  \n",
            "  inflating: libtorch/include/c10/util/Registry.h  \n",
            "  inflating: libtorch/include/c10/util/ScopeExit.h  \n",
            "  inflating: libtorch/include/c10/util/SmallBuffer.h  \n",
            "  inflating: libtorch/include/c10/util/SmallVector.h  \n",
            "  inflating: libtorch/include/c10/util/StringUtil.h  \n",
            "  inflating: libtorch/include/c10/util/Synchronized.h  \n",
            "  inflating: libtorch/include/c10/util/ThreadLocal.h  \n",
            "  inflating: libtorch/include/c10/util/ThreadLocalDebugInfo.h  \n",
            "  inflating: libtorch/include/c10/util/Type.h  \n",
            "  inflating: libtorch/include/c10/util/TypeCast.h  \n",
            "  inflating: libtorch/include/c10/util/TypeIndex.h  \n",
            "  inflating: libtorch/include/c10/util/TypeList.h  \n",
            "  inflating: libtorch/include/c10/util/TypeSafeSignMath.h  \n",
            "  inflating: libtorch/include/c10/util/TypeTraits.h  \n",
            "  inflating: libtorch/include/c10/util/Unicode.h  \n",
            "  inflating: libtorch/include/c10/util/UniqueVoidPtr.h  \n",
            "  inflating: libtorch/include/c10/util/Unroll.h  \n",
            "  inflating: libtorch/include/c10/util/WaitCounter.h  \n",
            "  inflating: libtorch/include/c10/util/accumulate.h  \n",
            "  inflating: libtorch/include/c10/util/bit_cast.h  \n",
            "  inflating: libtorch/include/c10/util/bits.h  \n",
            "  inflating: libtorch/include/c10/util/complex.h  \n",
            "  inflating: libtorch/include/c10/util/complex_math.h  \n",
            "  inflating: libtorch/include/c10/util/complex_utils.h  \n",
            "  inflating: libtorch/include/c10/util/copysign.h  \n",
            "  inflating: libtorch/include/c10/util/env.h  \n",
            "  inflating: libtorch/include/c10/util/flat_hash_map.h  \n",
            "  inflating: libtorch/include/c10/util/floating_point_utils.h  \n",
            "  inflating: libtorch/include/c10/util/generic_math.h  \n",
            "  inflating: libtorch/include/c10/util/hash.h  \n",
            "  inflating: libtorch/include/c10/util/int128.h  \n",
            "  inflating: libtorch/include/c10/util/intrusive_ptr.h  \n",
            "  inflating: libtorch/include/c10/util/irange.h  \n",
            "  inflating: libtorch/include/c10/util/llvmMathExtras.h  \n",
            "  inflating: libtorch/include/c10/util/logging_is_google_glog.h  \n",
            "  inflating: libtorch/include/c10/util/logging_is_not_google_glog.h  \n",
            "  inflating: libtorch/include/c10/util/numa.h  \n",
            "  inflating: libtorch/include/c10/util/order_preserving_flat_hash_map.h  \n",
            "  inflating: libtorch/include/c10/util/overloaded.h  \n",
            "  inflating: libtorch/include/c10/util/python_stub.h  \n",
            "  inflating: libtorch/include/c10/util/qint32.h  \n",
            "  inflating: libtorch/include/c10/util/qint8.h  \n",
            "  inflating: libtorch/include/c10/util/quint2x4.h  \n",
            "  inflating: libtorch/include/c10/util/quint4x2.h  \n",
            "  inflating: libtorch/include/c10/util/quint8.h  \n",
            "  inflating: libtorch/include/c10/util/safe_numerics.h  \n",
            "  inflating: libtorch/include/c10/util/signal_handler.h  \n",
            "  inflating: libtorch/include/c10/util/sparse_bitset.h  \n",
            "  inflating: libtorch/include/c10/util/ssize.h  \n",
            "  inflating: libtorch/include/c10/util/static_tracepoint.h  \n",
            "  inflating: libtorch/include/c10/util/static_tracepoint_elfx86.h  \n",
            "  inflating: libtorch/include/c10/util/strides.h  \n",
            "  inflating: libtorch/include/c10/util/string_utils.h  \n",
            "  inflating: libtorch/include/c10/util/string_view.h  \n",
            "  inflating: libtorch/include/c10/util/strong_type.h  \n",
            "  inflating: libtorch/include/c10/util/tempfile.h  \n",
            "  inflating: libtorch/include/c10/util/thread_name.h  \n",
            "  inflating: libtorch/include/c10/util/typeid.h  \n",
            "  inflating: libtorch/include/c10/util/win32-headers.h  \n",
            "   creating: libtorch/include/c10/cuda/\n",
            "  inflating: libtorch/include/c10/cuda/CUDAAllocatorConfig.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDACachingAllocator.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDADeviceAssertionHost.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAException.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAFunctions.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAGuard.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMacros.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMathCompat.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMiscFunctions.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAStream.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAAlgorithm.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDADeviceAssertion.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAGraphsC10Utils.h  \n",
            "  inflating: libtorch/include/c10/cuda/driver_api.h  \n",
            "   creating: libtorch/include/c10/cuda/impl/\n",
            "  inflating: libtorch/include/c10/cuda/impl/CUDAGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/cuda/impl/CUDATest.h  \n",
            "  inflating: libtorch/include/c10/cuda/impl/cuda_cmake_macros.h  \n",
            "   creating: libtorch/include/caffe2/\n",
            "   creating: libtorch/include/caffe2/serialize/\n",
            "  inflating: libtorch/include/caffe2/serialize/crc_alt.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/file_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/in_memory_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/inline_container.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/istream_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/read_adapter_interface.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/versions.h  \n",
            "  inflating: libtorch/include/clog.h  \n",
            "  inflating: libtorch/include/cpuinfo.h  \n",
            "  inflating: libtorch/include/dnnl_config.h  \n",
            "  inflating: libtorch/include/dnnl_debug.h  \n",
            "  inflating: libtorch/include/dnnl.h  \n",
            "  inflating: libtorch/include/dnnl_ocl.h  \n",
            "  inflating: libtorch/include/dnnl_sycl.h  \n",
            "  inflating: libtorch/include/dnnl_sycl_types.h  \n",
            "  inflating: libtorch/include/dnnl_threadpool.h  \n",
            "  inflating: libtorch/include/dnnl_types.h  \n",
            "  inflating: libtorch/include/dnnl_version.h  \n",
            "  inflating: libtorch/include/experiments-config.h  \n",
            "  inflating: libtorch/include/fp16.h  \n",
            "  inflating: libtorch/include/fxdiv.h  \n",
            "   creating: libtorch/include/kineto/\n",
            "  inflating: libtorch/include/kineto/AbstractConfig.h  \n",
            "  inflating: libtorch/include/kineto/ActivityProfilerInterface.h  \n",
            "  inflating: libtorch/include/kineto/ActivityTraceInterface.h  \n",
            "  inflating: libtorch/include/kineto/ActivityType.h  \n",
            "  inflating: libtorch/include/kineto/Config.h  \n",
            "  inflating: libtorch/include/kineto/ClientInterface.h  \n",
            "  inflating: libtorch/include/kineto/GenericTraceActivity.h  \n",
            "  inflating: libtorch/include/kineto/IActivityProfiler.h  \n",
            "  inflating: libtorch/include/kineto/ILoggerObserver.h  \n",
            "  inflating: libtorch/include/kineto/ITraceActivity.h  \n",
            "  inflating: libtorch/include/kineto/LoggingAPI.h  \n",
            "  inflating: libtorch/include/kineto/TraceSpan.h  \n",
            "  inflating: libtorch/include/kineto/ThreadUtil.h  \n",
            "  inflating: libtorch/include/kineto/libkineto.h  \n",
            "  inflating: libtorch/include/kineto/time_since_epoch.h  \n",
            "  inflating: libtorch/include/kineto/output_base.h  \n",
            "  inflating: libtorch/include/libshm.h  \n",
            "  inflating: libtorch/include/nnpack.h  \n",
            "  inflating: libtorch/include/psimd.h  \n",
            "  inflating: libtorch/include/pthreadpool.h  \n",
            "   creating: libtorch/include/pybind11/\n",
            "  inflating: libtorch/include/pybind11/attr.h  \n",
            "  inflating: libtorch/include/pybind11/buffer_info.h  \n",
            "  inflating: libtorch/include/pybind11/cast.h  \n",
            "  inflating: libtorch/include/pybind11/chrono.h  \n",
            "  inflating: libtorch/include/pybind11/common.h  \n",
            "  inflating: libtorch/include/pybind11/complex.h  \n",
            "  inflating: libtorch/include/pybind11/eigen.h  \n",
            "  inflating: libtorch/include/pybind11/embed.h  \n",
            "  inflating: libtorch/include/pybind11/eval.h  \n",
            "  inflating: libtorch/include/pybind11/functional.h  \n",
            "  inflating: libtorch/include/pybind11/gil.h  \n",
            "  inflating: libtorch/include/pybind11/gil_safe_call_once.h  \n",
            "  inflating: libtorch/include/pybind11/iostream.h  \n",
            "  inflating: libtorch/include/pybind11/numpy.h  \n",
            "  inflating: libtorch/include/pybind11/operators.h  \n",
            "  inflating: libtorch/include/pybind11/options.h  \n",
            "  inflating: libtorch/include/pybind11/pybind11.h  \n",
            "  inflating: libtorch/include/pybind11/pytypes.h  \n",
            "  inflating: libtorch/include/pybind11/stl.h  \n",
            "  inflating: libtorch/include/pybind11/stl_bind.h  \n",
            "  inflating: libtorch/include/pybind11/type_caster_pyobject_ptr.h  \n",
            "  inflating: libtorch/include/pybind11/typing.h  \n",
            "   creating: libtorch/include/pybind11/detail/\n",
            "  inflating: libtorch/include/pybind11/detail/class.h  \n",
            "  inflating: libtorch/include/pybind11/detail/common.h  \n",
            "  inflating: libtorch/include/pybind11/detail/descr.h  \n",
            "  inflating: libtorch/include/pybind11/detail/init.h  \n",
            "  inflating: libtorch/include/pybind11/detail/internals.h  \n",
            "  inflating: libtorch/include/pybind11/detail/type_caster_base.h  \n",
            "  inflating: libtorch/include/pybind11/detail/typeid.h  \n",
            "  inflating: libtorch/include/pybind11/detail/value_and_holder.h  \n",
            "   creating: libtorch/include/pybind11/eigen/\n",
            "  inflating: libtorch/include/pybind11/eigen/common.h  \n",
            "  inflating: libtorch/include/pybind11/eigen/matrix.h  \n",
            "  inflating: libtorch/include/pybind11/eigen/tensor.h  \n",
            "  inflating: libtorch/include/qnnpack_func.h  \n",
            "   creating: libtorch/include/tensorpipe/\n",
            "  inflating: libtorch/include/tensorpipe/tensorpipe.h  \n",
            "  inflating: libtorch/include/tensorpipe/config.h  \n",
            "  inflating: libtorch/include/tensorpipe/tensorpipe_cuda.h  \n",
            "  inflating: libtorch/include/tensorpipe/config_cuda.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/\n",
            "  inflating: libtorch/include/tensorpipe/channel/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/channel/error.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/basic/\n",
            "  inflating: libtorch/include/tensorpipe/channel/basic/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/cma/\n",
            "  inflating: libtorch/include/tensorpipe/channel/cma/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/mpt/\n",
            "  inflating: libtorch/include/tensorpipe/channel/mpt/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/xth/\n",
            "  inflating: libtorch/include/tensorpipe/channel/xth/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/common/\n",
            "  inflating: libtorch/include/tensorpipe/common/buffer.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/cpu_buffer.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/device.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/optional.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/cuda_buffer.h  \n",
            "   creating: libtorch/include/tensorpipe/core/\n",
            "  inflating: libtorch/include/tensorpipe/core/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/listener.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/message.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/pipe.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/\n",
            "   creating: libtorch/include/tensorpipe/transport/shm/\n",
            "  inflating: libtorch/include/tensorpipe/transport/shm/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/uv/\n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/factory.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/utility.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/error.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/ibv/\n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/factory.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/utility.h  \n",
            "   creating: libtorch/include/THC/\n",
            "  inflating: libtorch/include/THC/THCAtomics.cuh  \n",
            " extracting: libtorch/include/THC/THCDeviceUtils.cuh  \n",
            "   creating: libtorch/include/torch/\n",
            "  inflating: libtorch/include/torch/custom_class.h  \n",
            "  inflating: libtorch/include/torch/custom_class_detail.h  \n",
            "  inflating: libtorch/include/torch/library.h  \n",
            "  inflating: libtorch/include/torch/script.h  \n",
            "  inflating: libtorch/include/torch/extension.h  \n",
            "   creating: libtorch/include/torch/csrc/\n",
            "  inflating: libtorch/include/torch/csrc/Export.h  \n",
            "  inflating: libtorch/include/torch/csrc/CudaIPCTypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/DataLoader.h  \n",
            "  inflating: libtorch/include/torch/csrc/Device.h  \n",
            "  inflating: libtorch/include/torch/csrc/Dtype.h  \n",
            "  inflating: libtorch/include/torch/csrc/DynamicTypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/Exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/Generator.h  \n",
            "  inflating: libtorch/include/torch/csrc/Layout.h  \n",
            "  inflating: libtorch/include/torch/csrc/MemoryFormat.h  \n",
            "  inflating: libtorch/include/torch/csrc/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/PyInterpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/QScheme.h  \n",
            "  inflating: libtorch/include/torch/csrc/Size.h  \n",
            "  inflating: libtorch/include/torch/csrc/Storage.h  \n",
            "  inflating: libtorch/include/torch/csrc/StorageMethods.h  \n",
            "  inflating: libtorch/include/torch/csrc/StorageSharing.h  \n",
            "  inflating: libtorch/include/torch/csrc/Stream.h  \n",
            "  inflating: libtorch/include/torch/csrc/THConcat.h  \n",
            "  inflating: libtorch/include/torch/csrc/THP.h  \n",
            "  inflating: libtorch/include/torch/csrc/TypeInfo.h  \n",
            "  inflating: libtorch/include/torch/csrc/Types.h  \n",
            "  inflating: libtorch/include/torch/csrc/copy_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/itt_wrapper.h  \n",
            "  inflating: libtorch/include/torch/csrc/python_dimname.h  \n",
            "  inflating: libtorch/include/torch/csrc/python_headers.h  \n",
            "  inflating: libtorch/include/torch/csrc/serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/\n",
            "   creating: libtorch/include/torch/csrc/api/include/\n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/all.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/arg.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/cuda.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/enum.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/expanding_array.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/fft.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/imethod.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/linalg.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/mps.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nested.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/ordered_dict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/python.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/sparse.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/special.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/torch.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/xpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/version.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader_options.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/example.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/iterator.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/worker_exception.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateless.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/datasets/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/chunk.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/map.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/mnist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/shared.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/stateful.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/detail/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/data_shuttle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/queue.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/sequencers.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/samplers/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/custom_batch_request.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/distributed.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/random.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/sequential.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/stream.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/transforms/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/collate.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/lambda.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/stack.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/detail/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/detail/TensorDataContainer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/detail/static.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/cloneable.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/pimpl-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/pimpl.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/functional/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/vision.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/options/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/adaptive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/rnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformercoder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformerlayer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/vision.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/modules/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/adaptive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/common.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/rnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformercoder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformerlayer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any_module_holder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/functional.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/moduledict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/modulelist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/named_any.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/parameterdict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/parameterlist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/sequential.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/parallel/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/parallel/data_parallel.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/utils/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/clip_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/convert_parameters.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/rnn.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/optim/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adagrad.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adam.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adamw.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/lbfgs.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/optimizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/rmsprop.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/sgd.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/reduce_on_plateau_scheduler.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/step_lr.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/serialize/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/input-archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/output-archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/onnx/\n",
            "  inflating: libtorch/include/torch/csrc/onnx/back_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/onnx/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/onnx/onnx.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/api.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/collection.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/combined_traceback.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/containers.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/data_flow.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/events.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/kineto_shim.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/perf-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/perf.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/util.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/orchestration/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/python_tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/vulkan.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/standalone/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/execution_trace_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/itt_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/nvtx_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/privateuse1_observer.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/stubs/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/stubs/base.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/unwind/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/action.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/communicate.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/dwarf_enums.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/dwarf_symbolize_enums.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/eh_frame_hdr.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/fast_symbolizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/fde.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/lexer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/line_number_program.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/mem_file.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/range_table.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/sections.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwind.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwind_error.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwinder.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/python/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/combined_traceback.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/pybind.h  \n",
            "   creating: libtorch/include/torch/csrc/utils/\n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_numpy.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_new.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/byte_order.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/cpp_stacktraces.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/cuda_enabled.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/device_lazy_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/disable_torch_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/invalid_arguments.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/nested.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/numpy_stub.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/object_ptr.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/out_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pycfunction_helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pyobject_preservation.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_arg_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_dispatch.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_numbers.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_raii.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_scalars.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_strings.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_stub.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_symnode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_torch_function_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_tuples.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pythoncapi_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/schema_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/six.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/structseq.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_apply.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_dtypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_flatten.h  \n",
            " extracting: libtorch/include/torch/csrc/utils/tensor_layouts.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_memoryformats.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_qschemes.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/throughput_benchmark-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/throughput_benchmark.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/torch_dispatch_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/variadic.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/verbose.h  \n",
            "   creating: libtorch/include/torch/csrc/tensor/\n",
            "  inflating: libtorch/include/torch/csrc/tensor/python_tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/\n",
            "   creating: libtorch/include/torch/csrc/lazy/backend/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_data.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_device.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/lowering_context.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/cache.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/config.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/debug_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/dynamic_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/hash.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_builder.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_dump_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/lazy_graph_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/metrics.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/multi_wait.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/permutation_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/shape.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/shape_inference.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/thread_pool.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/trie.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/unique.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/util.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/internal_ops/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/internal_ops/ltc_ops.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/ops/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ops/arithmetic_ir_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ops/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/python/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/python/python_util.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/ts_backend/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/config.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/dynamic_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ir_builder.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/tensor_aten_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_autograd_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_backend_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_eager_fallback.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_lowering_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_node.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_node_lowering.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/FunctionsManual.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/InferenceMode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/VariableTypeUtils.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/anomaly_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/autograd_not_implemented_fallback.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/cpp_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/custom_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/edge.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/engine.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/forward_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/function_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/grad_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/graph_task.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/input_buffer.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/input_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/jit_decomp_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_kineto.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_legacy.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_anomaly_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_cpp_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_engine.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_enum_tag.h  \n",
            " extracting: libtorch/include/torch/csrc/autograd/python_fft_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_legacy_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_linalg_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_nested_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_nn_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_saved_variable_hooks.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_sparse_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_special_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_torch_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_variable_indexing.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/record_function_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/saved_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/saved_variable_hooks.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/symbolic.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/variable_info.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/functions/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/accumulate_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/basic_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/generated/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/python_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/python_return_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/VariableType.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/Functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/variable_factories.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/ViewFuncs.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/utils/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/error_messages.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/grad_layout_contract.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/lambda_post_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/python_arg_parsing.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/warnings.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/wrap_outputs.h  \n",
            "   creating: libtorch/include/torch/csrc/xpu/\n",
            "  inflating: libtorch/include/torch/csrc/xpu/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/xpu/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/xpu/Stream.h  \n",
            "   creating: libtorch/include/torch/csrc/cuda/\n",
            "  inflating: libtorch/include/torch/csrc/cuda/CUDAPluggableAllocator.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/GdsFile.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Stream.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/THCP.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/device_set.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/memory_snapshot.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/nccl.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/python_comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/python_nccl.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/\n",
            "   creating: libtorch/include/torch/csrc/distributed/c10d/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/CUDASymmetricMemory-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TraceUtils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/c10d.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/debug.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/error.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/logging.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/python_comm_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/socket.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Backend.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Backoff.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/DMAConnectivity.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/FakeProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/FileStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Functional.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/GlooDeviceFactory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/GroupRegistry.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/HashStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/NCCLUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/NanCheck.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ParamCommsUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/PrefixStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupGloo.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupMPI.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupUCC.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupWrapper.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/PyProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/RankLocal.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Store.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/SymmetricMemory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TCPStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TCPStoreBackend.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Types.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UCCTracing.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UCCUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UnixSockUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Utils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/WinSockUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Work.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/comm.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/default_comm_hooks.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/intra_node_comm.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/logger.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/reducer.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/reducer_timer.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/sequence_num.hpp  \n",
            "   creating: libtorch/include/torch/csrc/distributed/rpc/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/agent_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/message.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/py_rref.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_rpc_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback_no_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc_agent.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc_command_base.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_proto.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/tensorpipe_agent.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/tensorpipe_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/torchscript_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/unpickled_python_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/unpickled_python_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/\n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/context/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/context/container.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/context/context.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/functions/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/functions/recvrpc_backward.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/functions/sendrpc_backward.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/autograd_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rref_backward_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rref_backward_resp.h  \n",
            "   creating: libtorch/include/torch/csrc/dynamo/\n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cache_entry.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/compiled_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpp_shim.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpython_defs.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpython_includes.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/debug_macros.h  \n",
            " extracting: libtorch/include/torch/csrc/dynamo/eval_frame.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/extra_state.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/framelocals_mapping.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/guards.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/python_compiled_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/inductor_ops.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_runner/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner_cpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner_cuda.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/pybind.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_runtime/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/arrayref_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/device_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/model.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/model_container.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/scalar_to_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/thread_local.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/utils_cuda.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/mkldnn_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/oss_proxy_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/proxy_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/tensor_converter.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/c/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/c/shim.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/\n",
            "   creating: libtorch/include/torch/csrc/jit/api/\n",
            "  inflating: libtorch/include/torch/csrc/jit/api/compilation_unit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/function_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/method.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/object.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/serialization/\n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/callstack_debug_info_serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/source_range_serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/export.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/export_bytecode.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/flatbuffer_serializer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/flatbuffer_serializer_jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_read.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_source.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/mobile_bytecode_generated.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/onnx.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/pickle.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/pickler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/python_print.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/source_range_serialization_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/storage_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/type_name_uniquer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/unpickler.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/python/\n",
            "  inflating: libtorch/include/torch/csrc/jit/python/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/module_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/pybind_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_arg_flatten.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_custom_class.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_dict.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_ivalue.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_sugared_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_tree_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/script_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/update_graph_executor_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/utf8_decoding_ignore.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/mobile/\n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/code.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/file_format.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/flatbuffer_loader.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/frame.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/function.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import_data.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import_export_common.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/interpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/method.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/parse_bytecode.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/parse_operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/prim_ops_registery.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/profiler_edge.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/promoted_prim_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/quantization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/register_ops_common_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/upgrader_mobile.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/testing/\n",
            "  inflating: libtorch/include/torch/csrc/jit/testing/file_check.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/testing/hooks_for_testing.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/tensorexpr/\n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/block_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/bounds_inference.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/bounds_overlap.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cpp_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cpp_intrinsics.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cuda_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cuda_random.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/eval.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/expr.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions_core.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/fwd_decls.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/graph_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/half_support.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/hash_provider.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/intrinsic_symbols.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_cloner.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_mutator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_printer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_simplifier.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_verifier.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_visitor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/kernel.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/llvm_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/llvm_jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/loopnest.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/loopnest_randomization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/lowerings.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/mem_dependency_checker.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/reduction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/registerizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/stmt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/tensorexpr_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/unique_name_manager.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/var_substitutor.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/tensorexpr/operators/\n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/conv2d.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/matmul.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/misc.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/norm.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/pointwise.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/quantization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/reduction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/softmax.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/codegen/\n",
            "   creating: libtorch/include/torch/csrc/jit/codegen/cuda/\n",
            "  inflating: libtorch/include/torch/csrc/jit/codegen/cuda/interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/jit_log.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/jit_opt_limit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/resource_guard.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/backends/\n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_debug_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_detail.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_preprocess.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_resolver.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/add_if_then_else.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/annotate_warns.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/autocast.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/bailout_graph.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/batch_mm.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/canonicalize.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/canonicalize_graph_fuser_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/check_strict_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/clear_profiling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/clear_undefinedness.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/common_subexpression_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/concat_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/constant_pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/constant_propagation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/create_autodiff_subgraphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/create_functional_graphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/dead_code_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/decompose_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/device_type_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/dtype_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/eliminate_no_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/erase_number_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fixup_trace_scope_blocks.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fold_conv_bn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fold_linear_bn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/freeze_module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_concat_linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_conv_add_relu_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_conv_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_graph_optimizations.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_linear_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_linear_transpose.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_ops_to_mkldnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fuse_linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fuse_relu.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/graph_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/graph_rewrite_helper.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/guard_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/hoist_conv_packed_params.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_autodiff_subgraphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_fork_wait.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_forked_closures.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inliner.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inplace_check.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/insert_guards.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/integer_value_refinement.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lift_closures.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/liveness.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/loop_unrolling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_grad_of.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_graph.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_tuples.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/metal_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/mkldnn_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/mobile_optimizer_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/normalize_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/onednn_graph_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/onnx.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/pass_manager.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_alias_sensitive.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_dict_idioms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_list_idioms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_non_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/prepack_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/refine_tuple_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_expands.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_inplace_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_mutation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_redundant_profiles.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/replacement_of_old_operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/requires_grad_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/restore_mutation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/shape_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/specialize_autogradzero.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/subgraph_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_cache.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_runtime_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/tensorexpr_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/update_differentiable_graph_requires_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/value_refinement_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/variadic_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/vulkan_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/xnnpack_rewrite.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/quantization/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/dedup_module_uses.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/finalize.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/fusion_passes.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/helper.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/insert_observers.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/insert_quant_dequant.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/quantization_patterns.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/quantization_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/register_packed_params.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/utils/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/check_alias_annotation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/memory_dag.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/op_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/optimization_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/subgraph_utils.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/runtime/\n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/argument_spec.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/autodiff.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/calculate_necessary_args.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/custom_operator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/decomposition_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/decomposition_registry_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/exception_message.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_iterator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/instruction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/interpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/jit_exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/jit_trace.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/logging.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/operator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/operator_options.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/print_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/profiling_graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/profiling_record.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/register_ops_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/script_profile.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/serialized_shape_function_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/shape_function_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/simple_graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/slice_indices_adjust.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_script.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_shape_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_shape_registry_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/vararg_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/variable_tensor_list.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/ir/\n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/alias_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/attributes.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/graph_node_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/graph_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/ir_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/irparser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/named_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/node_hashing.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/scope.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/subgraph_matcher.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/type_hashing.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/frontend/\n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/source_range.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/lexer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/strtod.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parser_constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/function_schema_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parse_string_literal.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/schema_type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/error_report.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tree.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/builtin_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/canonicalize_modified_loop.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/concrete_module_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/convert_to_ssa.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/edit_distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/exit_transforms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/inline_loop_condition.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/ir_emitter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/mini_environment.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/name_mangler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/resolver.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/schema_matching.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/script_type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/source_ref.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/sugared_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tree_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/versioned_symbols.h  \n",
            "  inflating: libtorch/include/xnnpack.h  \n",
            "   creating: libtorch/share/\n",
            "   creating: libtorch/share/cmake/\n",
            "   creating: libtorch/share/cmake/ATen/\n",
            "  inflating: libtorch/share/cmake/ATen/ATenConfig.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Config.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUDAToolkit.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUSPARSELT.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUDSS.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindSYCLToolkit.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Targets.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Targets-release.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/public/\n",
            "  inflating: libtorch/share/cmake/Caffe2/public/cuda.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/xpu.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/glog.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/gflags.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/mkl.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/mkldnn.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/protobuf.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/utils.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/LoadHIP.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/FindCUDA.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/FindCUDNN.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/CMakeInitializeConfigs.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindPackageHandleStandardArgs.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindPackageMessage.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/make2cmake.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/parse_cubin.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/run_nvcc.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake  \n",
            "   creating: libtorch/share/cmake/Tensorpipe/\n",
            "  inflating: libtorch/share/cmake/Tensorpipe/TensorpipeTargets.cmake  \n",
            "  inflating: libtorch/share/cmake/Tensorpipe/TensorpipeTargets-release.cmake  \n",
            "   creating: libtorch/share/cmake/Torch/\n",
            "  inflating: libtorch/share/cmake/Torch/TorchConfigVersion.cmake  \n",
            "  inflating: libtorch/share/cmake/Torch/TorchConfig.cmake  \n",
            " extracting: libtorch/build-version  \n",
            "  inflating: libtorch/build-hash     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch;\n",
        "print(torch.utils.cmake_prefix_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaqxNAP8XG7p",
        "outputId": "9a64ea6e-75fa-4844-cb8c-fc98eab96c89"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/share/cmake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf pair_allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/pair_allegro.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoEfje93DXt3",
        "outputId": "1c706b52-5184-452e-fb3a-cb3e136d509a"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pair_allegro'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 28 (delta 0), reused 20 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (28/28), 195.32 KiB | 1.00 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "!cd pair_allegro && bash patch_lammps.sh ../lammps/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaJUONWEDi--",
        "outputId": "946bce71-a939-4983-ff4b-b00a3f6e6291"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying files...\n",
            "Updating CMakeLists.txt...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "# Use Python 3.11 Libtorch\n",
        "#!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j$(nproc)\n",
        "# Use downloaded 12.2 CUDA libtorch\n",
        "!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=/content/libtorch -DMKL_INCLUDE_DIR=`python -c \"import sysconfig;from pathlib import Path;print(Path(sysconfig.get_paths()[\\\"include\\\"]).parent)\"` && make -j$(nproc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYwb5zHP9BYo",
        "outputId": "7cdc6f47-47c6-4fe8-b55f-c00df9dd7f87"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "-- Running check for auto-generated files from make-based build system\n",
            "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n",
            "-- Found MPI: TRUE (found version \"3.1\")  \n",
            "-- Looking for C++ include omp.h\n",
            "-- Looking for C++ include omp.h - found\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\") found components: CXX \n",
            "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n",
            "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.37\") \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found GZIP: /usr/bin/gzip  \n",
            "-- Found FFMPEG: /usr/bin/ffmpeg  \n",
            "-- Looking for C++ include cmath\n",
            "-- Looking for C++ include cmath - found\n",
            "-- Generating style headers...\n",
            "-- Generating package headers...\n",
            "-- Generating lmpinstalledpkgs.h...\n",
            "-- Found Python3: /usr/local/bin/python (found version \"3.10.12\") found components: Interpreter \n",
            "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"11.0\")\n",
            "-- The following tools and libraries have been found and configured:\n",
            " * Git\n",
            " * MPI\n",
            " * OpenMP\n",
            " * JPEG\n",
            " * PNG\n",
            " * ZLIB\n",
            " * Python3\n",
            "\n",
            "-- <<< Build configuration >>>\n",
            "   LAMMPS Version:   20241119 cd16308-modified\n",
            "   Operating System: Linux Ubuntu\" 22.04\n",
            "   CMake Version:    3.23.1\n",
            "   Build type:       RelWithDebInfo\n",
            "   Install path:     /root/.local\n",
            "   Generator:        Unix Makefiles using /usr/bin/gmake\n",
            "-- Enabled packages: <None>\n",
            "-- <<< Compilers and Flags: >>>\n",
            "-- C++ Compiler:     /usr/bin/c++\n",
            "      Type:          GNU\n",
            "      Version:       11.4.0\n",
            "      C++ Standard:  17\n",
            "      C++ Flags:     -O2 -g -DNDEBUG\n",
            "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=4;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n",
            "-- <<< Linker flags: >>>\n",
            "-- Executable name:  lmp\n",
            "-- Static library flags:    \n",
            "-- <<< MPI flags >>>\n",
            "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
            "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi\n",
            "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.2\") \n",
            "-- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.2.140\") \n",
            "-- Looking for C++ include pthread.h\n",
            "-- Looking for C++ include pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Caffe2: CUDA detected: 12.2\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 12.2\n",
            "-- Found Python: /usr/local/bin/python (found version \"3.10.12\") found components: Interpreter \n",
            "\u001b[33mCMake Warning at /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:140 (message):\n",
            "  Failed to compute shorthash for libnvrtc.so\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/share/cmake-3.23/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
            "  The package name passed to `find_package_handle_standard_args` (nvtx3) does\n",
            "  not match the name of the calling package (Caffe2).  This can lead to\n",
            "  problems in calling code that expects `find_package` result variables\n",
            "  (e.g., `_FOUND`) to follow a certain pattern.\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:174 (find_package_handle_standard_args)\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could NOT find nvtx3 (missing: nvtx3_dir) \n",
            "\u001b[33mCMake Warning at /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:180 (message):\n",
            "  Cannot find NVTX3, find old NVTX instead\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- USE_CUDNN is set to 0. Compiling without cuDNN support\n",
            "-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\n",
            "-- USE_CUDSS is set to 0. Compiling without cuDSS support\n",
            "-- USE_CUFILE is set to 0. Compiling without cuFile support\n",
            "-- Autodetected CUDA architecture(s):  8.0\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_80,code=sm_80\n",
            "-- Found Torch: /content/libtorch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/lammps/build\n",
            "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/platform.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
            "[  3%] Built target memory.h\n",
            "[  3%] Built target modify.h\n",
            "[  3%] Built target neighbor.h\n",
            "[  3%] Built target neigh_list.h\n",
            "[  3%] Built target pair.h\n",
            "[  3%] Built target platform.h\n",
            "[  3%] Built target pointers.h\n",
            "[  3%] Built target region.h\n",
            "[  3%] Built target output.h\n",
            "[  3%] Built target timer.h\n",
            "[  3%] Built target universe.h\n",
            "[  3%] Built target update.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/core.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/format.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/exceptions.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
            "[  6%] Built target utils.h\n",
            "[  6%] Built target variable.h\n",
            "[  6%] Built target fmt_core.h\n",
            "[  6%] Built target fix.h\n",
            "[  6%] Built target exceptions.h\n",
            "[  6%] Built target fmt_format.h\n",
            "[  6%] Built target force.h\n",
            "[  6%] Built target group.h\n",
            "[  6%] Built target improper.h\n",
            "[  6%] Built target input.h\n",
            "[  6%] Built target info.h\n",
            "[  6%] Built target citeme.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/command.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
            "-- Git Directory: /content/lammps/.git\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
            "[  9%] Built target comm.h\n",
            "[  9%] Built target command.h\n",
            "[  9%] Built target compute.h\n",
            "[  9%] Built target dihedral.h\n",
            "[  9%] Built target domain.h\n",
            "[  9%] Built target error.h\n",
            "[  9%] Built target angle.h\n",
            "[  9%] Built target atom.h\n",
            "[  9%] Built target bond.h\n",
            "[  9%] Built target lammps.h\n",
            "[  9%] Built target kspace.h\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
            "[  9%] Built target lattice.h\n",
            "[  9%] Built target lmptype.h\n",
            "[  9%] Built target library.h\n",
            "[  9%] Built target lmppython.h\n",
            "-- Generating lmpgitversion.h...\n",
            "[  9%] Built target gitversion\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_write.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_allegro.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:535\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/fmt/base.h:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/fmt/format.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/pointers.h:33\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/atom_vec.h:17\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/atom_vec.cpp:14\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvirtual void LAMMPS_NS::AtomVec::write_data_restricted_to_general()\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/lammps/src/atom_vec.cpp:2272:21\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin_memcpy(void*, const void*, long unsigned int)\u001b[m\u001b[K’ specified bound between 18446744056529682432 and 18446744073709551592 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "   30 | \u001b[01;35m\u001b[K                                 __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
            "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_count_type.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_grid.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_write.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid_vtk.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_grid.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_bond_history.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_pair.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_langevin.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_atom.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_global.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_local.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_table.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid3d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid2d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/label_map.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_deprecated.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin_ghost.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi_old.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq_ghost.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_bin.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_nsq.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_trim.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_ghost_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi_old.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_allegro.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_molecular.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/platform.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_ellipsoid.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_id.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_image.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_mol.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:820:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  820 |         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n",
            "      |         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
            "[ 98%] Built target lammps\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
            "[100%] Built target lmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# download data\n",
        "!gdown --folder --id --no-cookies https://drive.google.com/drive/folders/1FEwF4i8IDHGmAIQ3RilA0jG9_lEX4Yk0?usp=sharing"
      ],
      "metadata": {
        "id": "eYnhz_xH-P36"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Si_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35kPXyy3-2SY",
        "outputId": "63732c7b-8559-4f70-a982-ff91694e0ad4"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sitraj.xyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edit /content/allegro/configs/tutorial.yaml removing optimizer_params (unused)\n",
        "# *Comment out lines 94-100 of /content/allegro/configs/tutorial.yaml and default_dtype: float64 (line7)*"
      ],
      "metadata": {
        "id": "7A4lfjdmA1hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./results\n",
        "!nequip-train allegro/configs/tutorial.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdlT2wo1-6C-",
        "outputId": "76d25d54-22f2-4619-d3db-546300dafa8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-mouse-177274436495688640\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241208_001121-KF3a4wVnQfdrlyLsngn27ydQ3jvQjDtqGVLqlCmu62Y\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-177274436495688640/allegro-tutorial?apiKey=c566103cdae2eb599b4277ce65a336c9cbf26cd5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-177274436495688640/allegro-tutorial/runs/KF3a4wVnQfdrlyLsngn27ydQ3jvQjDtqGVLqlCmu62Y?apiKey=c566103cdae2eb599b4277ce65a336c9cbf26cd5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_ase_dataset.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  datas = [torch.load(d) for d in datas]\n",
            "Loaded data: Batch(atomic_numbers=[7040, 1], batch=[7040], cell=[110, 3, 3], edge_cell_shift=[197238, 3], edge_index=[2, 197238], forces=[7040, 3], free_energy=[110], pbc=[110, 3], pos=[7040, 3], ptr=[111], stress=[110, 3, 3], total_energy=[110, 1])\n",
            "    processed data size: ~7.97 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(110)...\n",
            "Replace string dataset_per_atom_total_energy_mean to -129.96123880045263\n",
            "Atomic outputs are scaled by: [Si: None], shifted by [Si: -129.961239].\n",
            "Replace string dataset_forces_rms to 0.9016266007554627\n",
            "Initially outputs are globally scaled by: 0.9016266007554627, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 37352\n",
            "Number of trainable weights: 37352\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0     2         1.01        0.996       0.0137        0.718          0.9         6.75        0.105\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    2.856    0.002        0.992       0.0141         1.01        0.709        0.898         6.78        0.106\n",
            "Wall time: 2.856531350000296\n",
            "! Best model        0    1.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10        0.612         0.61      0.00226        0.571        0.704         2.74       0.0429\n",
            "      1    20        0.277        0.277     4.09e-05        0.369        0.474        0.369      0.00577\n",
            "      1    30        0.142        0.142      3.4e-05        0.268         0.34        0.336      0.00525\n",
            "      1    40       0.0946       0.0944     0.000184        0.223        0.277        0.782       0.0122\n",
            "      1    50       0.0404       0.0404     3.29e-05        0.143        0.181        0.331      0.00517\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1     2       0.0617       0.0617     8.91e-06        0.178        0.224        0.156      0.00243\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   11.804    0.002        0.291       0.0011        0.292        0.351        0.486         1.42       0.0222\n",
            "! Validation          1   11.804    0.002       0.0707     2.44e-05       0.0707        0.182         0.24        0.224       0.0035\n",
            "Wall time: 11.804958634000286\n",
            "! Best model        1    0.071\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10       0.0295       0.0295      4.7e-05        0.119        0.155        0.396      0.00618\n",
            "      2    20       0.0288       0.0288     1.64e-05        0.122        0.153        0.234      0.00366\n",
            "      2    30       0.0256       0.0256     5.35e-07        0.116        0.144       0.0422     0.000659\n",
            "      2    40       0.0381       0.0381     5.19e-06        0.143        0.176        0.131      0.00205\n",
            "      2    50       0.0213       0.0212     3.95e-05        0.106        0.131        0.363      0.00567\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2     2       0.0258       0.0257     3.79e-05        0.114        0.145        0.321      0.00501\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2   13.168    0.002       0.0315     7.53e-05       0.0316        0.124         0.16        0.396      0.00619\n",
            "! Validation          2   13.168    0.002       0.0285     3.84e-05       0.0286        0.114        0.152         0.32        0.005\n",
            "Wall time: 13.168264760999591\n",
            "! Best model        2    0.029\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10       0.0221       0.0219     0.000221        0.107        0.133        0.857       0.0134\n",
            "      3    20        0.022        0.022     6.17e-06        0.106        0.134        0.143      0.00224\n",
            "      3    30       0.0256       0.0256     1.62e-06        0.114        0.144       0.0734      0.00115\n",
            "      3    40       0.0196       0.0196     2.27e-05        0.099        0.126        0.275       0.0043\n",
            "      3    50       0.0296       0.0295     0.000103         0.12        0.155        0.584      0.00913\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3     2       0.0236       0.0236     1.99e-05        0.109        0.139        0.225      0.00351\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3   14.546    0.002       0.0241     5.73e-05       0.0242         0.11         0.14        0.349      0.00546\n",
            "! Validation          3   14.546    0.002       0.0261     1.76e-05       0.0261         0.11        0.146        0.202      0.00315\n",
            "Wall time: 14.546585030999267\n",
            "! Best model        3    0.026\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10       0.0223       0.0223     6.42e-05        0.109        0.135        0.462      0.00723\n",
            "      4    20       0.0256       0.0256     4.55e-06        0.115        0.144        0.123      0.00192\n",
            "      4    30       0.0256       0.0256     3.76e-06        0.114        0.144        0.112      0.00175\n",
            "      4    40       0.0243       0.0242     4.16e-05        0.109         0.14        0.372      0.00582\n",
            "      4    50       0.0213       0.0212     0.000167        0.103        0.131        0.746       0.0117\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4     2       0.0209       0.0209     2.75e-05        0.103         0.13        0.278      0.00435\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4   15.896    0.002       0.0215     6.37e-05       0.0216        0.104        0.132        0.388      0.00606\n",
            "! Validation          4   15.896    0.002       0.0232     2.68e-05       0.0232        0.104        0.137        0.261      0.00408\n",
            "Wall time: 15.896707060999688\n",
            "! Best model        4    0.023\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10       0.0203       0.0203     4.92e-05        0.103        0.128        0.405      0.00633\n",
            "      5    20       0.0161       0.0161      2.4e-05       0.0914        0.114        0.283      0.00442\n",
            "      5    30       0.0185       0.0185     2.67e-07       0.0978        0.123       0.0298     0.000466\n",
            "      5    40       0.0177       0.0177     8.01e-06       0.0956         0.12        0.163      0.00255\n",
            "      5    50       0.0177       0.0177     1.36e-05       0.0924         0.12        0.213      0.00333\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5     2       0.0166       0.0166     2.93e-05       0.0919        0.116        0.289      0.00451\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5   17.242    0.002       0.0175     8.24e-05       0.0176       0.0934        0.119        0.419      0.00654\n",
            "! Validation          5   17.242    0.002       0.0189     2.99e-05       0.0189       0.0948        0.124        0.281      0.00439\n",
            "Wall time: 17.24235280999892\n",
            "! Best model        5    0.019\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10       0.0172       0.0172     6.15e-05       0.0969        0.118        0.452      0.00707\n",
            "      6    20       0.0163       0.0163     3.72e-06       0.0903        0.115        0.111      0.00174\n",
            "      6    30       0.0129       0.0127     0.000208        0.081        0.102        0.831        0.013\n",
            "      6    40       0.0136       0.0136     2.19e-06       0.0838        0.105       0.0854      0.00133\n",
            "      6    50       0.0138       0.0138      4.8e-07       0.0828        0.106         0.04     0.000624\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6     2       0.0137       0.0137      2.4e-05       0.0836        0.106        0.261      0.00407\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6   18.602    0.002       0.0147     5.11e-05       0.0148        0.086        0.109         0.35      0.00547\n",
            "! Validation          6   18.602    0.002       0.0159     2.56e-05       0.0159       0.0877        0.114        0.257      0.00402\n",
            "Wall time: 18.60294589899968\n",
            "! Best model        6    0.016\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10       0.0159       0.0159     3.74e-07         0.09        0.114       0.0353     0.000551\n",
            "      7    20       0.0139       0.0136     0.000262        0.085        0.105        0.934       0.0146\n",
            "      7    30       0.0102       0.0102     4.92e-06       0.0722       0.0911        0.128        0.002\n",
            "      7    40       0.0127       0.0126     0.000101         0.08        0.101         0.58      0.00906\n",
            "      7    50       0.0159       0.0159     1.64e-05       0.0927        0.114        0.234      0.00366\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7     2       0.0125       0.0125     2.41e-05       0.0797        0.101        0.259      0.00405\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7   19.984    0.002       0.0136     5.66e-05       0.0136       0.0824        0.105        0.361      0.00564\n",
            "! Validation          7   19.984    0.002       0.0146     2.75e-05       0.0146       0.0841        0.109        0.268      0.00419\n",
            "Wall time: 19.98428717599927\n",
            "! Best model        7    0.015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10       0.0212       0.0209     0.000269       0.0918         0.13        0.946       0.0148\n",
            "      8    20       0.0128       0.0128     5.23e-07       0.0834        0.102       0.0417     0.000652\n",
            "      8    30       0.0158       0.0157     9.67e-05       0.0899        0.113        0.568      0.00887\n",
            "      8    40       0.0147       0.0147     2.82e-05       0.0832        0.109        0.306      0.00478\n",
            "      8    50       0.0126       0.0126      2.3e-05         0.08        0.101        0.277      0.00432\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8     2       0.0119       0.0119     1.66e-05       0.0776       0.0983        0.212      0.00331\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8   21.362    0.002        0.013     3.93e-05       0.0131       0.0808        0.103        0.291      0.00455\n",
            "! Validation          8   21.362    0.002       0.0138     1.92e-05       0.0138        0.082        0.106        0.216      0.00338\n",
            "Wall time: 21.363104881000254\n",
            "! Best model        8    0.014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10       0.0124       0.0123     8.53e-05       0.0814       0.0999        0.533      0.00833\n",
            "      9    20       0.0157       0.0156     6.31e-05         0.09        0.113        0.458      0.00716\n",
            "      9    30       0.0115       0.0114     4.83e-05       0.0774       0.0964        0.401      0.00626\n",
            "      9    40      0.00787      0.00787     6.55e-07       0.0648         0.08       0.0467      0.00073\n",
            "      9    50       0.0143       0.0142     3.17e-05       0.0877        0.108        0.325      0.00508\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9     2       0.0115       0.0115     1.18e-05       0.0765       0.0968        0.171      0.00268\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9   22.735    0.002       0.0127     3.63e-05       0.0127       0.0798        0.102        0.293      0.00459\n",
            "! Validation          9   22.735    0.002       0.0134     1.36e-05       0.0134       0.0807        0.104        0.173       0.0027\n",
            "Wall time: 22.73527969099996\n",
            "! Best model        9    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10       0.0154       0.0154     3.77e-05       0.0888        0.112        0.354      0.00554\n",
            "     10    20       0.0126       0.0125     6.79e-05       0.0808        0.101        0.475      0.00743\n",
            "     10    30      0.00968      0.00964     3.47e-05        0.072       0.0885         0.34      0.00531\n",
            "     10    40       0.0139       0.0139     2.23e-05       0.0867        0.106        0.272      0.00425\n",
            "     10    50       0.0112       0.0111     3.15e-05        0.079       0.0951        0.324      0.00506\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10     2       0.0112       0.0112     1.07e-05       0.0755       0.0956        0.164      0.00256\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10   24.123    0.002       0.0124     6.19e-05       0.0124       0.0787          0.1        0.378      0.00591\n",
            "! Validation         10   24.123    0.002        0.013     1.27e-05        0.013       0.0797        0.103        0.167      0.00261\n",
            "Wall time: 24.12408014599896\n",
            "! Best model       10    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10       0.0117       0.0117     7.39e-07       0.0774       0.0977       0.0496     0.000775\n",
            "     11    20     0.000161     0.000109     5.13e-05      0.00556      0.00943        0.413      0.00646\n",
            "     11    30      0.00924       0.0092     4.47e-05       0.0689       0.0865        0.386      0.00603\n",
            "     11    40       0.0118       0.0118     3.28e-06       0.0808       0.0981        0.104      0.00163\n",
            "     11    50       0.0137       0.0137     1.73e-05       0.0824        0.106         0.24      0.00375\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11     2        0.011        0.011     9.36e-06       0.0748       0.0947        0.154      0.00241\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11   25.495    0.002       0.0122     2.55e-05       0.0122       0.0779       0.0994        0.232      0.00363\n",
            "! Validation         11   25.495    0.002       0.0128      1.1e-05       0.0128       0.0788        0.102        0.159      0.00248\n",
            "Wall time: 25.495315598998786\n",
            "! Best model       11    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10        0.014       0.0138     0.000168        0.086        0.106        0.748       0.0117\n",
            "     12    20       0.0114        0.011     0.000438       0.0764       0.0945         1.21       0.0189\n",
            "     12    30       0.0138       0.0137     6.05e-05       0.0846        0.106        0.449      0.00701\n",
            "     12    40       0.0141       0.0139      0.00017       0.0794        0.106        0.751       0.0117\n",
            "     12    50      0.00878      0.00869     8.67e-05        0.066       0.0841        0.537       0.0084\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12     2       0.0108       0.0108     8.18e-06        0.074       0.0937        0.143      0.00224\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12   26.851    0.002       0.0119     0.000186       0.0121       0.0773       0.0985        0.705        0.011\n",
            "! Validation         12   26.851    0.002       0.0125     9.14e-06       0.0125        0.078        0.101         0.15      0.00234\n",
            "Wall time: 26.852011836999736\n",
            "! Best model       12    0.013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10     0.000111      0.00011     1.17e-06      0.00541      0.00946       0.0624     0.000976\n",
            "     13    20       0.0137       0.0137     2.87e-06       0.0854        0.106       0.0977      0.00153\n",
            "     13    30       0.0116       0.0115     2.79e-05       0.0764       0.0968        0.305      0.00476\n",
            "     13    40       0.0133       0.0132     3.22e-05        0.083        0.104        0.328      0.00512\n",
            "     13    50       0.0121       0.0121      8.1e-05       0.0788        0.099        0.519      0.00811\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13     2       0.0106       0.0106     8.18e-06       0.0733       0.0928        0.143      0.00224\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13   28.201    0.002       0.0117     5.09e-05       0.0118       0.0766       0.0977        0.325      0.00508\n",
            "! Validation         13   28.201    0.002       0.0123     9.24e-06       0.0123       0.0773       0.0999         0.15      0.00234\n",
            "Wall time: 28.20189191300051\n",
            "! Best model       13    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10       0.0128       0.0128     1.03e-05       0.0825        0.102        0.185      0.00289\n",
            "     14    20       0.0113       0.0113     1.68e-05       0.0771       0.0959        0.236       0.0037\n",
            "     14    30       0.0128       0.0128     2.06e-05       0.0811        0.102        0.262      0.00409\n",
            "     14    40       0.0134       0.0133     1.95e-05       0.0821        0.104        0.255      0.00398\n",
            "     14    50       0.0114       0.0114     7.72e-06       0.0784       0.0961         0.16      0.00251\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14     2       0.0104       0.0104     8.49e-06       0.0726        0.092        0.136      0.00212\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14   29.548    0.002       0.0115     5.21e-05       0.0116        0.076       0.0969        0.327      0.00511\n",
            "! Validation         14   29.548    0.002       0.0121     7.92e-06       0.0121       0.0766        0.099         0.14      0.00219\n",
            "Wall time: 29.54851647599935\n",
            "! Best model       14    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10        0.012        0.012      2.3e-05       0.0784       0.0988        0.277      0.00432\n",
            "     15    20       0.0119       0.0118     2.91e-05       0.0796       0.0981        0.311      0.00486\n",
            "     15    30       0.0196       0.0191     0.000537       0.0866        0.125         1.34       0.0209\n",
            "     15    40      0.00983      0.00981     2.23e-05        0.072       0.0893        0.273      0.00426\n",
            "     15    50       0.0133       0.0133     1.15e-05       0.0778        0.104        0.195      0.00305\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15     2       0.0102       0.0102     8.12e-06       0.0719       0.0912        0.132      0.00207\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15   30.944    0.002       0.0113     0.000114       0.0114       0.0753       0.0959         0.49      0.00766\n",
            "! Validation         15   30.944    0.002       0.0118        8e-06       0.0118       0.0759        0.098         0.14      0.00219\n",
            "Wall time: 30.94470074399942\n",
            "! Best model       15    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10      0.00972      0.00966     5.62e-05       0.0714       0.0886        0.433      0.00676\n",
            "     16    20       0.0102       0.0101     9.33e-05       0.0714       0.0905        0.557      0.00871\n",
            "     16    30       0.0131        0.013     0.000104       0.0826        0.103        0.589       0.0092\n",
            "     16    40        0.011       0.0108     0.000127       0.0757       0.0939        0.651       0.0102\n",
            "     16    50       0.0118       0.0118     2.87e-07       0.0775       0.0981       0.0309     0.000483\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16     2       0.0101       0.0101     8.66e-06       0.0713       0.0904        0.137      0.00214\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16   32.308    0.002       0.0111     6.87e-05       0.0112       0.0746       0.0951         0.39       0.0061\n",
            "! Validation         16   32.308    0.002       0.0116     7.98e-06       0.0116       0.0752       0.0971         0.14      0.00219\n",
            "Wall time: 32.30846658900009\n",
            "! Best model       16    0.012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10       0.0119       0.0119     5.63e-06       0.0795       0.0985        0.137      0.00214\n",
            "     17    20       0.0118       0.0117     4.75e-05       0.0782       0.0976        0.398      0.00621\n",
            "     17    30       0.0124       0.0123     9.24e-06       0.0785          0.1        0.175      0.00274\n",
            "     17    40       0.0104       0.0103     7.69e-05       0.0734       0.0916        0.506       0.0079\n",
            "     17    50       0.0131       0.0131      1.1e-05       0.0768        0.103        0.191      0.00299\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17     2      0.00988      0.00987     9.85e-06       0.0706       0.0896        0.141      0.00221\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17   33.666    0.002       0.0109     7.87e-05        0.011       0.0739       0.0942        0.415      0.00648\n",
            "! Validation         17   33.666    0.002       0.0113      8.4e-06       0.0114       0.0745        0.096         0.14      0.00218\n",
            "Wall time: 33.66662173799887\n",
            "! Best model       17    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10       0.0124       0.0123     5.24e-05       0.0812          0.1        0.418      0.00653\n",
            "     18    20       0.0111       0.0111     8.69e-05       0.0773       0.0948        0.538       0.0084\n",
            "     18    30       0.0126       0.0125     0.000171       0.0801        0.101        0.755       0.0118\n",
            "     18    40       0.0117       0.0117     4.88e-05       0.0763       0.0974        0.403       0.0063\n",
            "     18    50       0.0132       0.0132     4.77e-07       0.0839        0.103       0.0399     0.000623\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18     2      0.00972      0.00971     1.01e-05         0.07       0.0888        0.143      0.00223\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18   35.085    0.002       0.0108     3.35e-05       0.0108       0.0736       0.0938         0.27      0.00422\n",
            "! Validation         18   35.085    0.002       0.0111     8.52e-06       0.0111       0.0739       0.0951         0.14      0.00219\n",
            "Wall time: 35.08613348600011\n",
            "! Best model       18    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10       0.0099      0.00982     8.08e-05       0.0687       0.0894        0.519       0.0081\n",
            "     19    20       0.0119       0.0116     0.000327       0.0786        0.097         1.04       0.0163\n",
            "     19    30      0.00866      0.00845     0.000203       0.0666       0.0829        0.821       0.0128\n",
            "     19    40      0.00974      0.00962     0.000119       0.0693       0.0884        0.629      0.00982\n",
            "     19    50        0.012       0.0118     0.000203       0.0774       0.0981        0.823       0.0129\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19     2      0.00955      0.00954     1.06e-05       0.0694       0.0881        0.144      0.00224\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19   36.475    0.002       0.0107     0.000118       0.0108       0.0731       0.0931        0.507      0.00792\n",
            "! Validation         19   36.475    0.002       0.0109     8.85e-06       0.0109       0.0732       0.0941        0.141       0.0022\n",
            "Wall time: 36.475682135998795\n",
            "! Best model       19    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10       0.0115        0.011     0.000555       0.0745       0.0944         1.36       0.0212\n",
            "     20    20       0.0112       0.0111     0.000102       0.0756       0.0948        0.582       0.0091\n",
            "     20    30      0.00952      0.00944     7.81e-05       0.0683       0.0876         0.51      0.00797\n",
            "     20    40       0.0122       0.0119     0.000349       0.0797       0.0982         1.08       0.0169\n",
            "     20    50      0.00979      0.00979     8.91e-09        0.072       0.0892      0.00545     8.51e-05\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20     2      0.00939      0.00938     1.27e-05       0.0688       0.0873         0.15      0.00234\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20   37.838    0.002       0.0104      0.00015       0.0105       0.0722       0.0919        0.585      0.00914\n",
            "! Validation         20   37.838    0.002       0.0107     1.01e-05       0.0107       0.0725       0.0931        0.142      0.00223\n",
            "Wall time: 37.83870201100035\n",
            "! Best model       20    0.011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10      0.00966      0.00965     6.31e-07       0.0709       0.0886       0.0458     0.000716\n",
            "     21    20      0.00976      0.00974     1.22e-05       0.0729        0.089        0.202      0.00315\n",
            "     21    30       0.0115        0.011     0.000501       0.0762       0.0948         1.29       0.0202\n",
            "     21    40      0.00922      0.00909     0.000135       0.0688        0.086        0.671       0.0105\n",
            "     21    50       0.0109       0.0109     3.18e-05       0.0761       0.0941        0.325      0.00508\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21     2      0.00922      0.00921     9.79e-06       0.0683       0.0865         0.14      0.00219\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21   39.185    0.002       0.0103     7.96e-05       0.0104       0.0718       0.0914        0.398      0.00621\n",
            "! Validation         21   39.185    0.002       0.0104     8.44e-06       0.0104       0.0719       0.0921        0.141       0.0022\n",
            "Wall time: 39.18553084099949\n",
            "! Best model       21    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10      0.00907      0.00905      2.1e-05       0.0691       0.0858        0.264      0.00413\n",
            "     22    20      0.00997      0.00997     1.04e-06       0.0742         0.09       0.0588     0.000919\n",
            "     22    30       0.0106       0.0105      0.00015       0.0743       0.0922        0.708       0.0111\n",
            "     22    40      0.00988      0.00974     0.000138       0.0674        0.089        0.677       0.0106\n",
            "     22    50       0.0101       0.0101     4.58e-06       0.0739       0.0905        0.124      0.00193\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22     2      0.00907      0.00906     8.96e-06       0.0677       0.0858        0.136      0.00213\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22   40.636    0.002       0.0102     9.39e-05       0.0103       0.0713       0.0909        0.458      0.00715\n",
            "! Validation         22   40.636    0.002       0.0102     8.15e-06       0.0102       0.0712       0.0911        0.141       0.0022\n",
            "Wall time: 40.637013051000395\n",
            "! Best model       22    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10      0.00694      0.00661     0.000334       0.0603       0.0733         1.06       0.0165\n",
            "     23    20       0.0105       0.0105     4.86e-05       0.0717       0.0923        0.402      0.00628\n",
            "     23    30       0.0117       0.0117     2.31e-05       0.0743       0.0976        0.277      0.00433\n",
            "     23    40       0.0113       0.0113     3.81e-05       0.0742       0.0957        0.356      0.00557\n",
            "     23    50       0.0123       0.0122     7.29e-06       0.0748       0.0998        0.156      0.00243\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23     2      0.00893      0.00893     7.91e-06       0.0672       0.0852         0.13      0.00203\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23   41.983    0.002      0.00994     0.000112       0.0101       0.0705       0.0899        0.477      0.00745\n",
            "! Validation         23   41.983    0.002         0.01     7.99e-06         0.01       0.0707       0.0903        0.141       0.0022\n",
            "Wall time: 41.98359050599902\n",
            "! Best model       23    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10       0.0119       0.0119     5.74e-06       0.0776       0.0985        0.138      0.00216\n",
            "     24    20      0.00682      0.00681     3.17e-06       0.0588       0.0744        0.103      0.00161\n",
            "     24    30      0.00989      0.00988     1.71e-05       0.0744       0.0896        0.239      0.00373\n",
            "     24    40      0.00927      0.00922     5.41e-05        0.071       0.0866        0.424      0.00663\n",
            "     24    50      0.00917      0.00915     2.11e-05       0.0658       0.0862        0.265      0.00414\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24     2      0.00881       0.0088     8.32e-06       0.0668       0.0846        0.132      0.00206\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24   43.332    0.002      0.00985     6.02e-05      0.00991       0.0702       0.0895        0.351      0.00548\n",
            "! Validation         24   43.332    0.002      0.00983     8.06e-06      0.00984       0.0701       0.0894        0.141      0.00221\n",
            "Wall time: 43.332859877999\n",
            "! Best model       24    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10       0.0106       0.0102     0.000402       0.0735       0.0909         1.16       0.0181\n",
            "     25    20       0.0111       0.0108     0.000277        0.074       0.0939         0.96        0.015\n",
            "     25    30      0.00997      0.00988     8.94e-05       0.0721       0.0896        0.546      0.00853\n",
            "     25    40       0.0119       0.0117     0.000168       0.0762       0.0975        0.749       0.0117\n",
            "     25    50      0.00918      0.00918     7.32e-08        0.067       0.0864       0.0156     0.000244\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25     2      0.00868      0.00867     7.51e-06       0.0663       0.0839        0.131      0.00205\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25   44.706    0.002       0.0097     0.000206      0.00991       0.0698       0.0888        0.665       0.0104\n",
            "! Validation         25   44.706    0.002      0.00963     8.78e-06      0.00964       0.0695       0.0885        0.147       0.0023\n",
            "Wall time: 44.706445282999994\n",
            "! Best model       25    0.010\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10      0.00646      0.00646     9.47e-08       0.0591       0.0725       0.0178     0.000277\n",
            "     26    20       0.0104      0.00975     0.000654        0.072        0.089         1.48       0.0231\n",
            "     26    30       0.0111       0.0102     0.000849       0.0737       0.0912         1.68       0.0263\n",
            "     26    40       0.0105       0.0104     6.76e-05        0.072        0.092        0.475      0.00741\n",
            "     26    50      0.00832      0.00819     0.000123       0.0658       0.0816        0.641         0.01\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26     2      0.00856      0.00855     7.36e-06       0.0659       0.0834        0.131      0.00204\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26   46.038    0.002      0.00959     0.000222      0.00981       0.0693       0.0883        0.658       0.0103\n",
            "! Validation         26   46.038    0.002      0.00947     8.89e-06      0.00948        0.069       0.0877        0.147       0.0023\n",
            "Wall time: 46.03907341000013\n",
            "! Best model       26    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10       0.0108       0.0108     8.01e-07       0.0761       0.0935       0.0516     0.000807\n",
            "     27    20       0.0108       0.0106     0.000163        0.073        0.093        0.737       0.0115\n",
            "     27    30      0.00888      0.00883     4.95e-05       0.0695       0.0847        0.406      0.00635\n",
            "     27    40      0.00918       0.0088     0.000383       0.0672       0.0846         1.13       0.0176\n",
            "     27    50      0.00803      0.00802     7.89e-06        0.065       0.0807        0.162      0.00253\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27     2      0.00846      0.00845     8.19e-06       0.0655       0.0829         0.14      0.00219\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27   47.378    0.002      0.00948     0.000129      0.00961        0.069       0.0878         0.56      0.00875\n",
            "! Validation         27   47.378    0.002      0.00932     1.11e-05      0.00933       0.0686        0.087        0.158      0.00247\n",
            "Wall time: 47.37862340399988\n",
            "! Best model       27    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10       0.0148       0.0148     4.66e-05       0.0801         0.11        0.394      0.00616\n",
            "     28    20      0.00821       0.0082        4e-07       0.0654       0.0817       0.0365     0.000571\n",
            "     28    30      0.00719      0.00717     2.07e-05       0.0598       0.0764        0.263      0.00411\n",
            "     28    40      0.00955       0.0091     0.000452       0.0668        0.086         1.23       0.0192\n",
            "     28    50     0.000638     0.000109      0.00053      0.00521      0.00939         1.33       0.0208\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28     2      0.00836      0.00835     8.59e-06       0.0652       0.0824        0.147       0.0023\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28   48.764    0.002      0.00934     0.000145      0.00949       0.0685       0.0872        0.557      0.00871\n",
            "! Validation         28   48.764    0.002      0.00918     1.21e-05      0.00919       0.0682       0.0864        0.163      0.00255\n",
            "Wall time: 48.764787427000556\n",
            "! Best model       28    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10      0.00993      0.00905     0.000884       0.0705       0.0858         1.72       0.0268\n",
            "     29    20      0.00882      0.00853     0.000291       0.0656       0.0832        0.985       0.0154\n",
            "     29    30       0.0106       0.0104     0.000135       0.0722       0.0921        0.671       0.0105\n",
            "     29    40       0.0115       0.0114     0.000187       0.0745       0.0961        0.789       0.0123\n",
            "     29    50      0.00629      0.00625     3.95e-05       0.0563       0.0713        0.363      0.00567\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29     2      0.00827      0.00827     8.23e-06       0.0648        0.082        0.141       0.0022\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29   50.127    0.002      0.00923     0.000273       0.0095        0.068       0.0866        0.823       0.0129\n",
            "! Validation         29   50.127    0.002      0.00905     1.14e-05      0.00906       0.0678       0.0858         0.16      0.00249\n",
            "Wall time: 50.12710428399987\n",
            "! Best model       29    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10         0.01         0.01      4.7e-05       0.0726       0.0902        0.396      0.00618\n",
            "     30    20       0.0095       0.0095     1.21e-06       0.0712       0.0879       0.0636     0.000993\n",
            "     30    30       0.0062      0.00619     1.85e-05       0.0584       0.0709        0.248      0.00388\n",
            "     30    40      0.00888      0.00888     6.25e-06       0.0695       0.0849        0.144      0.00225\n",
            "     30    50       0.0114       0.0114     5.26e-06       0.0767       0.0963        0.132      0.00207\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30     2      0.00818      0.00817     9.45e-06       0.0645       0.0815        0.159      0.00249\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30   51.509    0.002      0.00915     7.08e-05      0.00922       0.0677       0.0862        0.384        0.006\n",
            "! Validation         30   51.509    0.002      0.00892     1.38e-05      0.00894       0.0673       0.0852        0.173      0.00271\n",
            "Wall time: 51.50988258600046\n",
            "! Best model       30    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10      0.00784      0.00781     2.63e-05       0.0643       0.0797        0.296      0.00462\n",
            "     31    20      0.00986      0.00979      7.1e-05        0.071       0.0892        0.486       0.0076\n",
            "     31    30      0.00618      0.00618     1.36e-06       0.0557       0.0709       0.0674      0.00105\n",
            "     31    40      0.00989      0.00987     2.25e-05        0.069       0.0896        0.273      0.00427\n",
            "     31    50      0.00915      0.00874     0.000407       0.0692       0.0843         1.16       0.0182\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31     2      0.00808      0.00807     1.16e-05       0.0641        0.081        0.181      0.00283\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31   52.857    0.002      0.00913     7.38e-05       0.0092       0.0676       0.0861        0.408      0.00638\n",
            "! Validation         31   52.857    0.002       0.0088     1.72e-05      0.00882       0.0669       0.0846        0.201      0.00314\n",
            "Wall time: 52.857886088999294\n",
            "! Best model       31    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10      0.00804      0.00782      0.00022       0.0658       0.0797        0.855       0.0134\n",
            "     32    20      0.00814      0.00814     2.63e-06       0.0657       0.0813       0.0936      0.00146\n",
            "     32    30      0.00922      0.00918     4.23e-05       0.0698       0.0864        0.375      0.00586\n",
            "     32    40       0.0111       0.0108     0.000277       0.0736       0.0937         0.96        0.015\n",
            "     32    50      0.00958      0.00946     0.000121       0.0688       0.0877        0.636      0.00994\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32     2      0.00801        0.008     8.95e-06       0.0638       0.0806        0.154      0.00241\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32   54.216    0.002      0.00897     0.000273      0.00924       0.0669       0.0854        0.782       0.0122\n",
            "! Validation         32   54.216    0.002      0.00869     1.33e-05      0.00871       0.0665       0.0841         0.17      0.00266\n",
            "Wall time: 54.216947029999574\n",
            "! Best model       32    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10       0.0101      0.00965     0.000474       0.0727       0.0886         1.26       0.0196\n",
            "     33    20       0.0117       0.0116     9.59e-05       0.0764       0.0971        0.565      0.00883\n",
            "     33    30      0.00896      0.00884     0.000118       0.0695       0.0848        0.626      0.00978\n",
            "     33    40       0.0105       0.0105     1.38e-05       0.0713       0.0922        0.215      0.00335\n",
            "     33    50      0.00845      0.00844      1.1e-05       0.0654       0.0828        0.192        0.003\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33     2      0.00792      0.00791     9.38e-06       0.0634       0.0802         0.16       0.0025\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33   55.563    0.002      0.00886      0.00016      0.00902       0.0666       0.0848        0.629      0.00983\n",
            "! Validation         33   55.563    0.002      0.00859     1.43e-05       0.0086       0.0661       0.0836        0.175      0.00274\n",
            "Wall time: 55.56333378999989\n",
            "! Best model       33    0.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10      0.00774      0.00771     2.62e-05       0.0652       0.0792        0.295      0.00462\n",
            "     34    20       0.0094      0.00905     0.000357       0.0665       0.0858         1.09        0.017\n",
            "     34    30      0.00811      0.00773     0.000384       0.0629       0.0793         1.13       0.0177\n",
            "     34    40      0.00841       0.0082     0.000206       0.0639       0.0816        0.828       0.0129\n",
            "     34    50       0.0137       0.0136     0.000194       0.0773        0.105        0.803       0.0125\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34     2      0.00785      0.00784     1.01e-05       0.0631       0.0798        0.167      0.00262\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34   56.922    0.002      0.00876     0.000221      0.00898       0.0662       0.0844        0.736       0.0115\n",
            "! Validation         34   56.922    0.002      0.00848     1.55e-05       0.0085       0.0657        0.083        0.186       0.0029\n",
            "Wall time: 56.92253848499968\n",
            "! Best model       34    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10      0.00823      0.00819     4.08e-05       0.0666       0.0816        0.369      0.00576\n",
            "     35    20       0.0122       0.0122     4.63e-05       0.0795       0.0994        0.392      0.00613\n",
            "     35    30      0.00615      0.00582     0.000324        0.056       0.0688         1.04       0.0162\n",
            "     35    40       0.0103       0.0103     1.92e-05       0.0705       0.0913        0.253      0.00395\n",
            "     35    50      0.00857      0.00848     9.71e-05       0.0661        0.083        0.569      0.00889\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35     2      0.00776      0.00776     8.07e-06       0.0628       0.0794        0.144      0.00224\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35   58.287    0.002      0.00868     0.000152      0.00884       0.0659        0.084        0.611      0.00954\n",
            "! Validation         35   58.287    0.002      0.00839     1.24e-05      0.00841       0.0654       0.0826        0.165      0.00258\n",
            "Wall time: 58.28764679199958\n",
            "! Best model       35    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10      0.00749      0.00748     6.34e-06       0.0636        0.078        0.145      0.00227\n",
            "     36    20      0.00786      0.00786     1.26e-06       0.0626       0.0799       0.0647      0.00101\n",
            "     36    30       0.0122       0.0121     7.53e-05       0.0794       0.0991        0.501      0.00783\n",
            "     36    40      0.00907      0.00907      1.5e-07       0.0657       0.0858       0.0223     0.000349\n",
            "     36    50      0.00837      0.00836     1.59e-05        0.063       0.0824         0.23      0.00359\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36     2       0.0077      0.00769     8.88e-06       0.0625        0.079        0.156      0.00244\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36   59.680    0.002      0.00862     8.07e-05       0.0087       0.0655       0.0837        0.428      0.00669\n",
            "! Validation         36   59.680    0.002       0.0083     1.39e-05      0.00832        0.065       0.0822        0.173       0.0027\n",
            "Wall time: 59.6808712889997\n",
            "! Best model       36    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10       0.0117       0.0115     0.000128       0.0758       0.0968        0.653       0.0102\n",
            "     37    20      0.00762      0.00752     9.72e-05       0.0643       0.0782        0.569      0.00889\n",
            "     37    30      0.00845       0.0084     5.11e-05       0.0614       0.0826        0.412      0.00644\n",
            "     37    40       0.0104       0.0103     0.000111       0.0722       0.0915        0.609      0.00952\n",
            "     37    50      0.00803      0.00803     1.52e-06       0.0631       0.0808       0.0712      0.00111\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37     2      0.00763      0.00762     7.58e-06       0.0622       0.0787        0.137      0.00214\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37   61.042    0.002      0.00861     0.000115      0.00873       0.0656       0.0837        0.523      0.00818\n",
            "! Validation         37   61.042    0.002      0.00822      1.2e-05      0.00823       0.0647       0.0818        0.162      0.00252\n",
            "Wall time: 61.0430259120003\n",
            "! Best model       37    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10      0.00577      0.00577     1.77e-06       0.0564       0.0685       0.0769       0.0012\n",
            "     38    20       0.0128       0.0127     9.19e-05       0.0753        0.101        0.553      0.00864\n",
            "     38    30      0.00856      0.00855     7.86e-06       0.0671       0.0834        0.162      0.00253\n",
            "     38    40      0.00593      0.00585     7.88e-05       0.0534       0.0689        0.512        0.008\n",
            "     38    50      0.00788      0.00784     3.49e-05       0.0614       0.0798        0.341      0.00532\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38     2      0.00756      0.00756      8.7e-06       0.0619       0.0784        0.154       0.0024\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38   62.402    0.002      0.00844     0.000116      0.00856       0.0649       0.0828        0.501      0.00782\n",
            "! Validation         38   62.402    0.002      0.00815     1.39e-05      0.00816       0.0644       0.0814        0.173       0.0027\n",
            "Wall time: 62.40212146799968\n",
            "! Best model       38    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10      0.00748      0.00712     0.000356       0.0621       0.0761         1.09        0.017\n",
            "     39    20       0.0085      0.00847     3.08e-05       0.0633        0.083         0.32        0.005\n",
            "     39    30      0.00755      0.00755     7.17e-08       0.0639       0.0783       0.0155     0.000241\n",
            "     39    40      0.00954      0.00892     0.000627       0.0682       0.0851         1.44       0.0226\n",
            "     39    50      0.00962      0.00961     1.26e-05       0.0688       0.0884        0.205       0.0032\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39     2      0.00751       0.0075     7.12e-06       0.0617       0.0781        0.129      0.00201\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39   63.743    0.002      0.00843     0.000139      0.00857       0.0648       0.0828        0.571      0.00893\n",
            "! Validation         39   63.743    0.002      0.00808     1.13e-05       0.0081       0.0641       0.0811        0.157      0.00245\n",
            "Wall time: 63.74354550600037\n",
            "! Best model       39    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10      0.00893      0.00889     3.65e-05       0.0672        0.085        0.348      0.00544\n",
            "     40    20      0.00985      0.00981     4.04e-05       0.0686       0.0893        0.367      0.00573\n",
            "     40    30       0.0107       0.0107     5.91e-05        0.074       0.0932        0.444      0.00693\n",
            "     40    40      0.00742       0.0074     1.95e-05       0.0625       0.0776        0.255      0.00399\n",
            "     40    50      0.00732      0.00694     0.000382        0.061       0.0751         1.13       0.0176\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40     2      0.00745      0.00744     7.49e-06       0.0614       0.0778        0.138      0.00215\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40   65.080    0.002       0.0083     0.000114      0.00842       0.0643       0.0822        0.507      0.00792\n",
            "! Validation         40   65.080    0.002      0.00802     1.23e-05      0.00803       0.0638       0.0807        0.163      0.00255\n",
            "Wall time: 65.08090076900044\n",
            "! Best model       40    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10       0.0113       0.0112     0.000113       0.0744       0.0955        0.614       0.0096\n",
            "     41    20      0.00825      0.00821     4.63e-05       0.0632       0.0817        0.393      0.00614\n",
            "     41    30      0.00588      0.00549     0.000392       0.0523       0.0668         1.14       0.0179\n",
            "     41    40      0.00749      0.00749      9.5e-07       0.0627        0.078       0.0562     0.000879\n",
            "     41    50      0.00872      0.00861     0.000111       0.0652       0.0836        0.608       0.0095\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41     2      0.00741       0.0074     6.94e-06       0.0612       0.0776        0.128      0.00199\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41   66.410    0.002       0.0082     0.000146      0.00835       0.0639       0.0817        0.579      0.00904\n",
            "! Validation         41   66.410    0.002      0.00796     1.13e-05      0.00797       0.0636       0.0804        0.157      0.00245\n",
            "Wall time: 66.41084337599932\n",
            "! Best model       41    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10      0.00693      0.00687     5.69e-05       0.0607       0.0747        0.435       0.0068\n",
            "     42    20      0.00752      0.00752     2.32e-08       0.0635       0.0782       0.0088     0.000137\n",
            "     42    30       0.0059      0.00584      6.3e-05       0.0562       0.0689        0.458      0.00716\n",
            "     42    40      0.00814      0.00801     0.000132       0.0661       0.0807        0.663       0.0104\n",
            "     42    50       0.0109       0.0109     3.68e-06       0.0735       0.0939        0.111      0.00173\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42     2      0.00735      0.00734     6.54e-06       0.0609       0.0773        0.123      0.00192\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42   67.760    0.002      0.00817     6.21e-05      0.00823       0.0638       0.0815        0.366      0.00571\n",
            "! Validation         42   67.760    0.002       0.0079     1.07e-05      0.00791       0.0633       0.0801        0.153      0.00239\n",
            "Wall time: 67.76110415800031\n",
            "! Best model       42    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10       0.0121       0.0121     3.58e-05       0.0735        0.099        0.345       0.0054\n",
            "     43    20      0.00592      0.00555     0.000369       0.0517       0.0672         1.11       0.0173\n",
            "     43    30      0.00819      0.00819     2.01e-08       0.0631       0.0816      0.00819     0.000128\n",
            "     43    40      0.00926      0.00919     6.79e-05       0.0685       0.0864        0.475      0.00743\n",
            "     43    50        0.008      0.00799     1.14e-05       0.0662       0.0806        0.194      0.00304\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43     2      0.00728      0.00727     6.22e-06       0.0606       0.0769        0.119      0.00185\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43   69.107    0.002      0.00813     0.000141      0.00827       0.0636       0.0813        0.556      0.00869\n",
            "! Validation         43   69.107    0.002      0.00784     9.92e-06      0.00785        0.063       0.0798         0.15      0.00234\n",
            "Wall time: 69.10722142299892\n",
            "! Best model       43    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10      0.00649      0.00649     6.19e-06       0.0589       0.0726        0.144      0.00224\n",
            "     44    20      0.00857      0.00845     0.000115       0.0677       0.0829        0.619      0.00967\n",
            "     44    30      0.00554      0.00544     0.000103       0.0524       0.0665        0.586      0.00916\n",
            "     44    40      0.00708      0.00706     2.08e-05       0.0583       0.0758        0.263      0.00411\n",
            "     44    50      0.00834      0.00831     2.43e-05       0.0666       0.0822        0.284      0.00444\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44     2      0.00725      0.00725      6.1e-06       0.0604       0.0767        0.112      0.00176\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44   70.475    0.002      0.00811     8.11e-05      0.00819       0.0636       0.0812        0.414      0.00647\n",
            "! Validation         44   70.475    0.002      0.00779     9.16e-06       0.0078       0.0628       0.0796        0.145      0.00226\n",
            "Wall time: 70.47551667100015\n",
            "! Best model       44    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10      0.00877      0.00876     1.52e-05       0.0673       0.0844        0.225      0.00351\n",
            "     45    20      0.00777      0.00766     0.000118       0.0626       0.0789        0.626      0.00978\n",
            "     45    30      0.00588      0.00586     1.37e-05       0.0534        0.069        0.213      0.00334\n",
            "     45    40      0.00832       0.0083     2.22e-05       0.0676       0.0821        0.272      0.00425\n",
            "     45    50      0.00892      0.00865     0.000268       0.0636       0.0839        0.945       0.0148\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45     2       0.0072       0.0072     6.28e-06       0.0602       0.0765         0.12      0.00187\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45   71.883    0.002      0.00809     7.42e-05      0.00816       0.0636       0.0811        0.423      0.00662\n",
            "! Validation         45   71.883    0.002      0.00774     1.06e-05      0.00775       0.0626       0.0793        0.152      0.00238\n",
            "Wall time: 71.88408357099979\n",
            "! Best model       45    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10      0.00772      0.00681     0.000913       0.0603       0.0744         1.74       0.0272\n",
            "     46    20      0.00804      0.00777      0.00027       0.0647       0.0795        0.949       0.0148\n",
            "     46    30      0.00569      0.00567     1.27e-05       0.0516       0.0679        0.206      0.00322\n",
            "     46    40      0.00824      0.00803      0.00021       0.0649       0.0808        0.836       0.0131\n",
            "     46    50      0.00747      0.00636      0.00112       0.0581       0.0719         1.93       0.0301\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46     2      0.00715      0.00715     6.33e-06         0.06       0.0762        0.121      0.00189\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46   73.225    0.002      0.00797     0.000233       0.0082       0.0631       0.0805        0.705        0.011\n",
            "! Validation         46   73.225    0.002      0.00768     1.09e-05       0.0077       0.0623        0.079        0.153      0.00239\n",
            "Wall time: 73.2251778459995\n",
            "! Best model       46    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10       0.0116       0.0116     1.27e-06        0.072       0.0971       0.0652      0.00102\n",
            "     47    20      0.00822      0.00813     8.51e-05       0.0628       0.0813        0.532      0.00832\n",
            "     47    30       0.0065      0.00648     1.34e-05        0.058       0.0726        0.211       0.0033\n",
            "     47    40      0.00755      0.00746     8.88e-05       0.0612       0.0779        0.544       0.0085\n",
            "     47    50      0.00633      0.00629     4.51e-05        0.054       0.0715        0.387      0.00605\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47     2      0.00711      0.00711     5.97e-06       0.0598        0.076        0.117      0.00182\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47   74.562    0.002      0.00816     0.000249      0.00841       0.0636       0.0814        0.687       0.0107\n",
            "! Validation         47   74.562    0.002      0.00764     9.93e-06      0.00765       0.0621       0.0788        0.149      0.00233\n",
            "Wall time: 74.56263175599997\n",
            "! Best model       47    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10      0.00838      0.00825     0.000135       0.0645       0.0819        0.671       0.0105\n",
            "     48    20       0.0115       0.0115     4.06e-06       0.0721       0.0968        0.116      0.00182\n",
            "     48    30      0.00891      0.00888     3.57e-05        0.067        0.085        0.345      0.00539\n",
            "     48    40      0.00783      0.00779      4.6e-05       0.0632       0.0796        0.391      0.00612\n",
            "     48    50      0.00653      0.00643     0.000109       0.0581       0.0723        0.601      0.00939\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48     2      0.00706      0.00706     6.13e-06       0.0596       0.0757        0.119      0.00186\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48   75.893    0.002      0.00798     0.000104      0.00809       0.0631       0.0806        0.491      0.00767\n",
            "! Validation         48   75.893    0.002      0.00758     1.06e-05      0.00759       0.0619       0.0785        0.152      0.00237\n",
            "Wall time: 75.89410021900039\n",
            "! Best model       48    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10      0.00811      0.00794     0.000168       0.0632       0.0804        0.748       0.0117\n",
            "     49    20      0.00618      0.00595     0.000231       0.0555       0.0696        0.876       0.0137\n",
            "     49    30      0.00878      0.00868     9.73e-05        0.067        0.084        0.569      0.00889\n",
            "     49    40      0.00787      0.00779     8.53e-05       0.0633       0.0796        0.533      0.00833\n",
            "     49    50      0.00931      0.00924     7.15e-05       0.0673       0.0867        0.488      0.00763\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49     2      0.00702      0.00701     5.93e-06       0.0593       0.0755        0.113      0.00176\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49   77.231    0.002      0.00788     0.000172      0.00806       0.0627       0.0801        0.638      0.00997\n",
            "! Validation         49   77.231    0.002      0.00753     9.24e-06      0.00754       0.0617       0.0782        0.145      0.00227\n",
            "Wall time: 77.23220797399881\n",
            "! Best model       49    0.008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10      0.00675      0.00673     1.61e-05       0.0579        0.074        0.232      0.00362\n",
            "     50    20      0.00552      0.00528     0.000245       0.0511       0.0655        0.904       0.0141\n",
            "     50    30      0.00924      0.00924     1.01e-10       0.0674       0.0867      0.00058     9.07e-06\n",
            "     50    40      0.00803      0.00754     0.000486       0.0617       0.0783         1.27       0.0199\n",
            "     50    50      0.00727      0.00725     1.81e-05       0.0608       0.0768        0.245      0.00384\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50     2      0.00696      0.00696      6.5e-06       0.0591       0.0752        0.115       0.0018\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50   78.579    0.002      0.00779     0.000243      0.00803       0.0623       0.0796        0.772       0.0121\n",
            "! Validation         50   78.579    0.002      0.00747      8.5e-06      0.00748       0.0614       0.0779        0.142      0.00222\n",
            "Wall time: 78.57980395999948\n",
            "! Best model       50    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10      0.00904      0.00894     0.000102        0.066       0.0853        0.582       0.0091\n",
            "     51    20      0.00533      0.00526     7.17e-05       0.0504       0.0654        0.488      0.00763\n",
            "     51    30      0.00762      0.00742     0.000204       0.0612       0.0776        0.824       0.0129\n",
            "     51    40      0.00679      0.00678      1.5e-05       0.0587       0.0742        0.224      0.00349\n",
            "     51    50      0.00676       0.0067     6.23e-05       0.0589       0.0738        0.455      0.00712\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51     2       0.0069      0.00689     5.79e-06       0.0589       0.0749        0.114      0.00178\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51   79.915    0.002      0.00775     8.74e-05      0.00784       0.0621       0.0794        0.458      0.00715\n",
            "! Validation         51   79.915    0.002      0.00741     9.58e-06      0.00742       0.0612       0.0776        0.147      0.00229\n",
            "Wall time: 79.91554353500032\n",
            "! Best model       51    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10       0.0112       0.0108     0.000334       0.0703       0.0939         1.05       0.0165\n",
            "     52    20      0.00529      0.00527        2e-05       0.0505       0.0655        0.258      0.00403\n",
            "     52    30       0.0062      0.00613     6.78e-05       0.0564       0.0706        0.475      0.00742\n",
            "     52    40      0.00805        0.008     4.98e-05       0.0648       0.0807        0.407      0.00636\n",
            "     52    50      0.00865      0.00864     5.72e-06       0.0674       0.0838        0.138      0.00216\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52     2      0.00685      0.00684     5.93e-06       0.0586       0.0746         0.11      0.00172\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52   81.260    0.002      0.00763     0.000155      0.00778       0.0616       0.0787        0.578      0.00903\n",
            "! Validation         52   81.260    0.002      0.00735      8.5e-06      0.00736        0.061       0.0773         0.14      0.00219\n",
            "Wall time: 81.26029788199958\n",
            "! Best model       52    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10      0.00753      0.00749     4.68e-05       0.0631        0.078        0.395      0.00617\n",
            "     53    20      0.00874       0.0087     3.33e-05       0.0654       0.0841        0.333       0.0052\n",
            "     53    30      0.00734      0.00725     9.16e-05       0.0622       0.0768        0.552      0.00863\n",
            "     53    40      0.00922      0.00916     5.99e-05       0.0683       0.0863        0.447      0.00698\n",
            "     53    50      0.00676      0.00676     2.83e-08       0.0599       0.0741      0.00972     0.000152\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53     2      0.00679      0.00678     5.67e-06       0.0584       0.0742        0.111      0.00173\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53   82.648    0.002      0.00765     9.76e-05      0.00775       0.0618       0.0789        0.468      0.00731\n",
            "! Validation         53   82.648    0.002      0.00729     8.97e-06       0.0073       0.0607        0.077        0.143      0.00223\n",
            "Wall time: 82.6490457839991\n",
            "! Best model       53    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10      0.00753      0.00747     5.62e-05       0.0596       0.0779        0.432      0.00676\n",
            "     54    20       0.0071      0.00708     2.37e-05       0.0587       0.0759        0.281      0.00439\n",
            "     54    30      0.00659      0.00641     0.000183       0.0595       0.0722        0.782       0.0122\n",
            "     54    40       0.0072      0.00718     1.81e-05       0.0621       0.0764        0.246      0.00384\n",
            "     54    50      0.00692       0.0069     1.73e-05       0.0589       0.0749         0.24      0.00375\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54     2      0.00674      0.00673     5.91e-06       0.0582        0.074        0.116      0.00181\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54   84.099    0.002      0.00755     0.000137      0.00769       0.0613       0.0783        0.564      0.00881\n",
            "! Validation         54   84.099    0.002      0.00723     9.69e-06      0.00724       0.0605       0.0767        0.147       0.0023\n",
            "Wall time: 84.10016900299888\n",
            "! Best model       54    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10      0.00725      0.00724     2.94e-06       0.0602       0.0767        0.099      0.00155\n",
            "     55    20      0.00822      0.00784     0.000375       0.0635       0.0799         1.12       0.0174\n",
            "     55    30      0.00502      0.00502     6.58e-07       0.0507       0.0639       0.0468     0.000731\n",
            "     55    40       0.0076      0.00754     6.78e-05       0.0614       0.0783        0.475      0.00742\n",
            "     55    50       0.0104       0.0102     0.000127       0.0691       0.0913        0.651       0.0102\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55     2      0.00668      0.00667     5.67e-06       0.0579       0.0737        0.111      0.00174\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55   85.443    0.002      0.00756     5.84e-05      0.00762       0.0613       0.0784        0.351      0.00548\n",
            "! Validation         55   85.443    0.002      0.00717     8.95e-06      0.00718       0.0603       0.0764        0.142      0.00221\n",
            "Wall time: 85.4432153559992\n",
            "! Best model       55    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10       0.0112       0.0112     1.46e-05       0.0763       0.0954         0.22      0.00344\n",
            "     56    20      0.00795      0.00788      7.5e-05       0.0651         0.08          0.5      0.00781\n",
            "     56    30      0.00742      0.00723     0.000182       0.0621       0.0767        0.779       0.0122\n",
            "     56    40      0.00835      0.00833     2.03e-05       0.0664       0.0823         0.26      0.00406\n",
            "     56    50      0.00657      0.00609     0.000476       0.0573       0.0704         1.26       0.0197\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56     2      0.00664      0.00664     5.97e-06       0.0577       0.0734        0.112      0.00174\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56   86.780    0.002      0.00741     0.000109      0.00752       0.0608       0.0776        0.484      0.00756\n",
            "! Validation         56   86.780    0.002      0.00712     8.33e-06      0.00713         0.06       0.0761        0.139      0.00218\n",
            "Wall time: 86.78052264799953\n",
            "! Best model       56    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10      0.00622      0.00613     8.68e-05       0.0576       0.0706        0.538       0.0084\n",
            "     57    20      0.00767      0.00733      0.00034       0.0623       0.0772         1.06       0.0166\n",
            "     57    30      0.00783      0.00777     6.63e-05       0.0631       0.0795         0.47      0.00734\n",
            "     57    40      0.00842      0.00828     0.000145       0.0633        0.082        0.696       0.0109\n",
            "     57    50      0.00523      0.00519     3.75e-05       0.0528       0.0649        0.354      0.00552\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57     2      0.00659      0.00658     5.66e-06       0.0575       0.0732        0.107      0.00168\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57   88.118    0.002      0.00737     0.000148      0.00752       0.0606       0.0774        0.604      0.00944\n",
            "! Validation         57   88.118    0.002      0.00707     8.14e-06      0.00708       0.0598       0.0758        0.137      0.00213\n",
            "Wall time: 88.1183993020004\n",
            "! Best model       57    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10      0.00569      0.00486     0.000828       0.0504       0.0628         1.66       0.0259\n",
            "     58    20      0.00751      0.00746     4.99e-05       0.0592       0.0779        0.408      0.00637\n",
            "     58    30      0.00675      0.00672     2.69e-05       0.0601       0.0739        0.299      0.00467\n",
            "     58    40      0.00839      0.00829     9.51e-05       0.0656       0.0821        0.563      0.00879\n",
            "     58    50      0.00933      0.00902     0.000304       0.0697       0.0857         1.01       0.0157\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58     2      0.00655      0.00654     5.68e-06       0.0573       0.0729        0.115       0.0018\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58   89.451    0.002      0.00738     0.000123       0.0075       0.0607       0.0774        0.511      0.00799\n",
            "! Validation         58   89.451    0.002      0.00702     9.82e-06      0.00703       0.0596       0.0756        0.146      0.00228\n",
            "Wall time: 89.45127547399898\n",
            "! Best model       58    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10      0.00695      0.00682     0.000136       0.0613       0.0744        0.673       0.0105\n",
            "     59    20      0.00926      0.00926      6.5e-06       0.0682       0.0867        0.147       0.0023\n",
            "     59    30      0.00978      0.00978     7.02e-07       0.0674       0.0892       0.0484     0.000756\n",
            "     59    40      0.00539      0.00538     9.55e-06       0.0503       0.0662        0.178      0.00279\n",
            "     59    50      0.00859      0.00858      9.4e-06       0.0662       0.0835        0.177      0.00276\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59     2       0.0065      0.00649     5.59e-06       0.0571       0.0726        0.113      0.00177\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59   90.826    0.002      0.00725     0.000145      0.00739       0.0601       0.0768        0.523      0.00817\n",
            "! Validation         59   90.826    0.002      0.00697      9.2e-06      0.00698       0.0594       0.0753        0.142      0.00223\n",
            "Wall time: 90.82616936299928\n",
            "! Best model       59    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10      0.00822      0.00821     7.11e-06       0.0658       0.0817        0.154       0.0024\n",
            "     60    20      0.00709      0.00707     1.34e-05       0.0625       0.0758        0.211       0.0033\n",
            "     60    30      0.00484      0.00484     3.69e-07        0.049       0.0627        0.035     0.000547\n",
            "     60    40      0.00691      0.00687     4.25e-05       0.0558       0.0747        0.376      0.00588\n",
            "     60    50      0.00633      0.00626      7.2e-05       0.0579       0.0713         0.49      0.00765\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60     2      0.00646      0.00645     5.36e-06       0.0569       0.0724        0.111      0.00174\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60   92.211    0.002      0.00725     9.83e-05      0.00735       0.0601       0.0768         0.45      0.00703\n",
            "! Validation         60   92.211    0.002      0.00693     9.09e-06      0.00694       0.0592       0.0751        0.141      0.00221\n",
            "Wall time: 92.21188200799952\n",
            "! Best model       60    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10      0.00484       0.0047     0.000137       0.0497       0.0618        0.675       0.0105\n",
            "     61    20      0.00752      0.00729     0.000234       0.0623        0.077        0.884       0.0138\n",
            "     61    30      0.00782      0.00774     8.06e-05       0.0624       0.0793        0.518       0.0081\n",
            "     61    40      0.00595      0.00593     1.92e-05       0.0551       0.0694        0.253      0.00395\n",
            "     61    50      0.00821      0.00811     9.69e-05       0.0639       0.0812        0.568      0.00888\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61     2      0.00642      0.00641      5.4e-06       0.0567       0.0722        0.111      0.00173\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61   93.584    0.002      0.00729     0.000211      0.00751       0.0602        0.077        0.703        0.011\n",
            "! Validation         61   93.584    0.002      0.00689     8.94e-06       0.0069        0.059       0.0748        0.141       0.0022\n",
            "Wall time: 93.58493469099994\n",
            "! Best model       61    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10      0.00714      0.00704     9.93e-05       0.0583       0.0756        0.575      0.00898\n",
            "     62    20       0.0101      0.00986     0.000211       0.0678       0.0895        0.838       0.0131\n",
            "     62    30      0.00781       0.0075     0.000303       0.0631       0.0781         1.01       0.0157\n",
            "     62    40      0.00628      0.00575     0.000537        0.055       0.0683         1.34       0.0209\n",
            "     62    50      0.00735      0.00729     5.72e-05        0.063        0.077        0.436      0.00682\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62     2      0.00638      0.00638     5.26e-06       0.0566        0.072        0.108      0.00169\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62   94.957    0.002      0.00718      0.00016      0.00734       0.0597       0.0764        0.635      0.00992\n",
            "! Validation         62   94.957    0.002      0.00685     8.52e-06      0.00685       0.0589       0.0746        0.137      0.00215\n",
            "Wall time: 94.95727542399982\n",
            "! Best model       62    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10      0.00748      0.00747     1.14e-05       0.0615       0.0779        0.195      0.00304\n",
            "     63    20      0.00795      0.00792     2.91e-05       0.0648       0.0802        0.311      0.00486\n",
            "     63    30       0.0068      0.00678     1.58e-05       0.0607       0.0742        0.229      0.00358\n",
            "     63    40       0.0053      0.00482     0.000479       0.0494       0.0626         1.26       0.0197\n",
            "     63    50      0.00997      0.00996     6.93e-06       0.0696         0.09        0.152      0.00237\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63     2      0.00635      0.00635     5.43e-06       0.0564       0.0718        0.112      0.00176\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63   96.378    0.002      0.00722     0.000115      0.00733       0.0599       0.0766        0.509      0.00795\n",
            "! Validation         63   96.378    0.002      0.00681      9.7e-06      0.00682       0.0587       0.0744        0.142      0.00222\n",
            "Wall time: 96.37858031900032\n",
            "! Best model       63    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10      0.00848      0.00843     4.94e-05       0.0655       0.0828        0.405      0.00634\n",
            "     64    20      0.00011     9.87e-05     1.16e-05       0.0051      0.00896        0.197      0.00307\n",
            "     64    30      0.00708      0.00656     0.000525        0.058        0.073         1.32       0.0207\n",
            "     64    40      0.00833      0.00831      1.6e-05       0.0634       0.0822        0.231      0.00361\n",
            "     64    50      0.00704      0.00704     2.51e-06       0.0578       0.0756       0.0914      0.00143\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64     2      0.00632      0.00631     5.12e-06       0.0563       0.0716        0.108      0.00169\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              64   97.773    0.002      0.00715     5.35e-05       0.0072       0.0596       0.0762        0.347      0.00542\n",
            "! Validation         64   97.773    0.002      0.00677     8.61e-06      0.00678       0.0585       0.0742        0.137      0.00214\n",
            "Wall time: 97.77411355599907\n",
            "! Best model       64    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65    10      0.00483      0.00476        7e-05        0.049       0.0622        0.483      0.00754\n",
            "     65    20      0.00783      0.00775     8.34e-05       0.0635       0.0794        0.527      0.00823\n",
            "     65    30      0.00893      0.00893     5.22e-07       0.0663       0.0852       0.0417     0.000651\n",
            "     65    40      0.00952      0.00951     9.54e-06       0.0677       0.0879        0.178      0.00278\n",
            "     65    50      0.00665      0.00665     3.61e-08       0.0577       0.0735        0.011     0.000171\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65     2      0.00629      0.00628     5.08e-06       0.0561       0.0715        0.103      0.00161\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              65   99.137    0.002      0.00718     2.83e-05      0.00721       0.0597       0.0764        0.244      0.00381\n",
            "! Validation         65   99.137    0.002      0.00673        8e-06      0.00674       0.0584        0.074        0.133      0.00208\n",
            "Wall time: 99.13744365499952\n",
            "! Best model       65    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66    10      0.00912      0.00904      7.3e-05       0.0679       0.0857        0.493       0.0077\n",
            "     66    20      0.00774      0.00727     0.000468       0.0622       0.0769         1.25       0.0195\n",
            "     66    30      0.00483       0.0047     0.000126       0.0498       0.0618        0.648       0.0101\n",
            "     66    40      0.00717      0.00716     1.03e-05       0.0607       0.0763        0.185       0.0029\n",
            "     66    50      0.00684      0.00683     1.12e-05       0.0602       0.0745        0.193      0.00302\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66     2      0.00625      0.00625     5.04e-06        0.056       0.0713        0.101      0.00158\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              66  100.539    0.002      0.00725     0.000138      0.00739       0.0601       0.0768        0.525       0.0082\n",
            "! Validation         66  100.539    0.002      0.00669     7.51e-06       0.0067       0.0582       0.0738        0.129      0.00202\n",
            "Wall time: 100.5399479459993\n",
            "! Best model       66    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67    10      0.00474      0.00473     1.25e-05       0.0502        0.062        0.204      0.00319\n",
            "     67    20      0.00854      0.00849     5.06e-05       0.0656       0.0831        0.411      0.00642\n",
            "     67    30      0.00613      0.00608     4.92e-05       0.0563       0.0703        0.405      0.00632\n",
            "     67    40       0.0053      0.00523     7.33e-05       0.0524       0.0652        0.494      0.00772\n",
            "     67    50      0.00607      0.00602     4.54e-05       0.0566         0.07        0.389      0.00608\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67     2      0.00623      0.00623     4.79e-06       0.0559       0.0711        0.101      0.00158\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              67  102.019    0.002      0.00698     6.91e-05      0.00705       0.0589       0.0753        0.401      0.00627\n",
            "! Validation         67  102.019    0.002      0.00666     7.84e-06      0.00667       0.0581       0.0736        0.131      0.00205\n",
            "Wall time: 102.02016043900039\n",
            "! Best model       67    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68    10      0.00656      0.00636       0.0002       0.0583       0.0719        0.816       0.0128\n",
            "     68    20      0.00663      0.00655     8.65e-05       0.0571        0.073        0.537      0.00839\n",
            "     68    30      0.00994      0.00965      0.00029       0.0691       0.0886        0.982       0.0153\n",
            "     68    40      0.00991      0.00989     2.47e-05       0.0692       0.0896        0.287      0.00448\n",
            "     68    50      0.00596      0.00589     7.24e-05       0.0559       0.0692        0.491      0.00767\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68     2      0.00619      0.00619     4.72e-06       0.0557       0.0709       0.0999      0.00156\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              68  103.408    0.002      0.00697     7.45e-05      0.00704       0.0589       0.0753        0.412      0.00643\n",
            "! Validation         68  103.408    0.002      0.00662     7.81e-06      0.00663       0.0579       0.0734        0.131      0.00205\n",
            "Wall time: 103.40897884699916\n",
            "! Best model       68    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69    10      0.00621       0.0062     9.22e-06       0.0576        0.071        0.175      0.00274\n",
            "     69    20       0.0066      0.00655     5.03e-05       0.0562        0.073        0.409       0.0064\n",
            "     69    30      0.00607      0.00601     5.57e-05       0.0543       0.0699         0.43      0.00673\n",
            "     69    40      0.00841      0.00806     0.000345       0.0625       0.0809         1.07       0.0168\n",
            "     69    50     0.000249     0.000109      0.00014      0.00537      0.00941        0.683       0.0107\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69     2      0.00617      0.00617      4.7e-06       0.0556       0.0708        0.103      0.00161\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              69  104.801    0.002       0.0071     0.000223      0.00733       0.0593        0.076        0.653       0.0102\n",
            "! Validation         69  104.801    0.002      0.00659     8.28e-06       0.0066       0.0578       0.0732        0.133      0.00207\n",
            "Wall time: 104.80155896299948\n",
            "! Best model       69    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70    10      0.00714      0.00701     0.000138       0.0591       0.0755        0.677       0.0106\n",
            "     70    20      0.00923      0.00915     8.16e-05       0.0666       0.0863        0.521      0.00814\n",
            "     70    30       0.0068      0.00668     0.000122       0.0608       0.0737        0.638      0.00997\n",
            "     70    40      0.00637      0.00635     1.83e-05       0.0581       0.0718        0.247      0.00385\n",
            "     70    50       0.0113       0.0113     4.08e-06       0.0779       0.0959        0.117      0.00182\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70     2      0.00616      0.00615     4.74e-06       0.0556       0.0707        0.104      0.00163\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              70  106.185    0.002      0.00705     0.000243       0.0073       0.0591       0.0757        0.658       0.0103\n",
            "! Validation         70  106.185    0.002      0.00657     8.71e-06      0.00658       0.0577       0.0731        0.134       0.0021\n",
            "Wall time: 106.18556995899962\n",
            "! Best model       70    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71    10      0.00665      0.00665     2.33e-06       0.0601       0.0735       0.0881      0.00138\n",
            "     71    20       0.0097       0.0097     4.48e-07       0.0701       0.0888       0.0386     0.000603\n",
            "     71    30      0.00721      0.00681     0.000401       0.0596       0.0744         1.16       0.0181\n",
            "     71    40      0.00651      0.00629     0.000226       0.0562       0.0715        0.868       0.0136\n",
            "     71    50      0.00665      0.00599     0.000661       0.0555       0.0698         1.48       0.0232\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71     2      0.00613      0.00613     4.44e-06       0.0554       0.0706       0.0982      0.00153\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              71  107.591    0.002      0.00712      0.00018       0.0073       0.0595       0.0761         0.65       0.0102\n",
            "! Validation         71  107.591    0.002      0.00654     7.76e-06      0.00655       0.0576       0.0729        0.129      0.00201\n",
            "Wall time: 107.5918590500005\n",
            "! Best model       71    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72    10      0.00575      0.00571     3.97e-05       0.0551       0.0681        0.363      0.00568\n",
            "     72    20       0.0102       0.0102     4.04e-05       0.0698       0.0909        0.367      0.00573\n",
            "     72    30       0.0105      0.00969      0.00077       0.0706       0.0888          1.6        0.025\n",
            "     72    40      0.00676      0.00667     9.12e-05        0.059       0.0736        0.551      0.00861\n",
            "     72    50      0.00495      0.00494     1.37e-05       0.0509       0.0634        0.214      0.00334\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72     2       0.0061       0.0061     4.63e-06       0.0553       0.0704        0.104      0.00163\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              72  108.964    0.002      0.00698     0.000104      0.00708       0.0587       0.0753        0.484      0.00756\n",
            "! Validation         72  108.964    0.002      0.00651     8.52e-06      0.00652       0.0574       0.0728        0.134      0.00209\n",
            "Wall time: 108.96451928900024\n",
            "! Best model       72    0.007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73    10      0.00564      0.00562     1.87e-05        0.053       0.0676         0.25       0.0039\n",
            "     73    20       0.0065      0.00642     8.44e-05       0.0538       0.0722         0.53      0.00828\n",
            "     73    30      0.00692      0.00689     2.93e-05       0.0587       0.0748        0.312      0.00488\n",
            "     73    40       0.0105       0.0101      0.00034       0.0705       0.0907         1.06       0.0166\n",
            "     73    50     0.000173     0.000105     6.77e-05      0.00524      0.00924        0.475      0.00742\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73     2      0.00609      0.00608     4.46e-06       0.0552       0.0703       0.0966      0.00151\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              73  110.311    0.002      0.00704     0.000126      0.00716       0.0591       0.0756        0.544      0.00851\n",
            "! Validation         73  110.311    0.002      0.00649     7.48e-06       0.0065       0.0573       0.0726        0.127      0.00199\n",
            "Wall time: 110.31209791400033\n",
            "! Best model       73    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74    10      0.00535      0.00527     8.08e-05       0.0538       0.0654        0.519      0.00811\n",
            "     74    20      0.00657      0.00647     0.000104       0.0595       0.0725        0.588      0.00918\n",
            "     74    30      0.00551       0.0055     1.09e-05       0.0528       0.0669        0.191      0.00298\n",
            "     74    40       0.0113       0.0112     3.69e-05       0.0769       0.0955         0.35      0.00547\n",
            "     74    50     0.000261     0.000108     0.000153      0.00531      0.00937        0.713       0.0111\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74     2      0.00606      0.00606     4.51e-06       0.0551       0.0702       0.0988      0.00154\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              74  111.686    0.002      0.00682     6.73e-05      0.00689       0.0581       0.0744        0.395      0.00617\n",
            "! Validation         74  111.686    0.002      0.00646     7.61e-06      0.00647       0.0572       0.0725        0.128      0.00201\n",
            "Wall time: 111.68709399699947\n",
            "! Best model       74    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75    10       0.0056      0.00551     8.69e-05       0.0524        0.067        0.538      0.00841\n",
            "     75    20      0.00798      0.00765     0.000328       0.0633       0.0788         1.05       0.0163\n",
            "     75    30      0.00701      0.00667     0.000334       0.0571       0.0737         1.06       0.0165\n",
            "     75    40       0.0109       0.0103     0.000552       0.0701       0.0917         1.36       0.0212\n",
            "     75    50      0.00747      0.00747     1.96e-07       0.0609       0.0779       0.0255     0.000399\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75     2      0.00604      0.00604     4.42e-06        0.055       0.0701        0.096       0.0015\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              75  113.044    0.002      0.00683     0.000319      0.00715       0.0582       0.0745        0.892       0.0139\n",
            "! Validation         75  113.044    0.002      0.00644     7.34e-06      0.00644       0.0571       0.0723        0.125      0.00196\n",
            "Wall time: 113.04439598299905\n",
            "! Best model       75    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76    10      0.00632      0.00631      1.1e-05       0.0587       0.0716        0.192      0.00299\n",
            "     76    20      0.00797      0.00795     2.06e-05       0.0648       0.0804        0.262      0.00409\n",
            "     76    30      0.00673      0.00663     9.82e-05       0.0579       0.0734        0.572      0.00893\n",
            "     76    40        0.007      0.00675     0.000254       0.0577       0.0741         0.92       0.0144\n",
            "     76    50      0.00578      0.00576     1.81e-05       0.0528       0.0684        0.246      0.00384\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76     2      0.00602      0.00602     4.52e-06       0.0549         0.07        0.103       0.0016\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              76  114.386    0.002      0.00687     0.000128        0.007       0.0582       0.0747        0.536      0.00837\n",
            "! Validation         76  114.386    0.002      0.00641     8.16e-06      0.00642        0.057       0.0722        0.131      0.00204\n",
            "Wall time: 114.38666484999885\n",
            "! Best model       76    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77    10      0.00774      0.00769     4.79e-05       0.0616       0.0791        0.399      0.00624\n",
            "     77    20      0.00684      0.00684     5.77e-07       0.0608       0.0745       0.0438     0.000685\n",
            "     77    30      0.00848      0.00848     7.22e-07       0.0666        0.083        0.049     0.000766\n",
            "     77    40      0.00468      0.00435     0.000324       0.0471       0.0595         1.04       0.0162\n",
            "     77    50      0.00629      0.00604     0.000255       0.0556         0.07        0.921       0.0144\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77     2        0.006        0.006     4.32e-06       0.0549       0.0698       0.0954      0.00149\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              77  115.743    0.002      0.00683     0.000197      0.00703       0.0582       0.0745        0.666       0.0104\n",
            "! Validation         77  115.743    0.002      0.00639     7.28e-06       0.0064       0.0569       0.0721        0.125      0.00196\n",
            "Wall time: 115.74390226700052\n",
            "! Best model       77    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78    10      0.00712      0.00712     4.35e-07       0.0566       0.0761       0.0381     0.000595\n",
            "     78    20      0.00643      0.00594     0.000483       0.0556       0.0695         1.27       0.0198\n",
            "     78    30      0.00504      0.00467     0.000369       0.0493       0.0616         1.11       0.0173\n",
            "     78    40      0.00953      0.00917     0.000365       0.0691       0.0863          1.1       0.0172\n",
            "     78    50      0.00703      0.00572      0.00131       0.0543       0.0682         2.09       0.0327\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78     2      0.00599      0.00598     4.33e-06       0.0548       0.0697       0.0942      0.00147\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              78  117.101    0.002      0.00683     0.000487      0.00731       0.0581       0.0745         1.13       0.0177\n",
            "! Validation         78  117.101    0.002      0.00637      7.1e-06      0.00637       0.0568       0.0719        0.123      0.00193\n",
            "Wall time: 117.10126459899948\n",
            "! Best model       78    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79    10      0.00598      0.00597     7.85e-06       0.0556       0.0697        0.162      0.00253\n",
            "     79    20      0.00651      0.00636     0.000145       0.0566       0.0719        0.695       0.0109\n",
            "     79    30      0.00865      0.00855     9.72e-05       0.0651       0.0834        0.569      0.00889\n",
            "     79    40      0.00728      0.00726     2.35e-05       0.0588       0.0768         0.28      0.00437\n",
            "     79    50      0.00687      0.00672     0.000143       0.0588       0.0739         0.69       0.0108\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79     2      0.00596      0.00596     4.17e-06       0.0547       0.0696        0.093      0.00145\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              79  118.493    0.002      0.00676       0.0002      0.00696       0.0579       0.0741        0.666       0.0104\n",
            "! Validation         79  118.493    0.002      0.00635     7.18e-06      0.00636       0.0567       0.0719        0.123      0.00193\n",
            "Wall time: 118.49348482999994\n",
            "! Best model       79    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80    10      0.00886      0.00886     7.31e-08        0.065       0.0849       0.0156     0.000244\n",
            "     80    20      0.00559       0.0055     8.67e-05       0.0538       0.0669        0.537      0.00839\n",
            "     80    30      0.00713      0.00658     0.000546       0.0594       0.0731         1.35       0.0211\n",
            "     80    40       0.0107       0.0107     3.84e-05       0.0707       0.0932        0.357      0.00558\n",
            "     80    50      0.00675      0.00637     0.000376       0.0585        0.072         1.12       0.0175\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80     2      0.00596      0.00595     4.15e-06       0.0546       0.0696       0.0949      0.00148\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              80  119.954    0.002      0.00679     0.000108       0.0069       0.0579       0.0743         0.48       0.0075\n",
            "! Validation         80  119.954    0.002      0.00634      7.4e-06      0.00635       0.0567       0.0718        0.125      0.00195\n",
            "Wall time: 119.95447426899955\n",
            "! Best model       80    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81    10      0.00922      0.00919     2.45e-05       0.0678       0.0864        0.285      0.00446\n",
            "     81    20      0.00691      0.00691     5.55e-06       0.0601       0.0749        0.136      0.00212\n",
            "     81    30      0.00593      0.00592     1.41e-05       0.0554       0.0694        0.217      0.00339\n",
            "     81    40      0.00923      0.00921     2.14e-05       0.0676       0.0865        0.267      0.00417\n",
            "     81    50      0.00577      0.00576     8.81e-06       0.0554       0.0684        0.171      0.00268\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81     2      0.00595      0.00594      4.3e-06       0.0546       0.0695       0.0987      0.00154\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              81  121.386    0.002      0.00699     7.99e-05      0.00707       0.0587       0.0754         0.43      0.00672\n",
            "! Validation         81  121.386    0.002      0.00632     7.97e-06      0.00633       0.0566       0.0717        0.128        0.002\n",
            "Wall time: 121.38638056699892\n",
            "! Best model       81    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82    10      0.00755      0.00754     9.65e-06       0.0642       0.0783        0.179       0.0028\n",
            "     82    20       0.0087      0.00861     8.26e-05       0.0659       0.0837        0.524      0.00819\n",
            "     82    30      0.00531      0.00528     2.43e-05       0.0519       0.0655        0.285      0.00445\n",
            "     82    40      0.00759      0.00755     3.42e-05       0.0623       0.0783        0.338      0.00527\n",
            "     82    50      0.00624       0.0062     3.85e-05       0.0576        0.071        0.358      0.00559\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82     2      0.00593      0.00593     4.37e-06       0.0545       0.0694          0.1      0.00157\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              82  122.814    0.002      0.00682     3.29e-05      0.00686       0.0581       0.0745        0.266      0.00416\n",
            "! Validation         82  122.814    0.002       0.0063        8e-06      0.00631       0.0565       0.0716        0.129      0.00202\n",
            "Wall time: 122.81417671300005\n",
            "! Best model       82    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83    10      0.00678      0.00677     1.07e-05       0.0599       0.0742        0.189      0.00295\n",
            "     83    20      0.00492      0.00491     5.49e-06       0.0513       0.0632        0.135      0.00211\n",
            "     83    30      0.00781      0.00766      0.00015       0.0639       0.0789        0.707        0.011\n",
            "     83    40      0.00928      0.00927     1.27e-05       0.0682       0.0868        0.206      0.00321\n",
            "     83    50      0.00676      0.00664     0.000118       0.0592       0.0735        0.627      0.00979\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83     2      0.00592      0.00591     4.83e-06       0.0545       0.0693        0.113      0.00176\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              83  124.255    0.002      0.00679     6.96e-05      0.00686       0.0579       0.0743        0.395      0.00618\n",
            "! Validation         83  124.255    0.002      0.00628     9.42e-06      0.00629       0.0564       0.0714        0.138      0.00215\n",
            "Wall time: 124.25534277800034\n",
            "! Best model       83    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84    10      0.00579      0.00578     4.79e-06       0.0556       0.0686        0.126      0.00197\n",
            "     84    20      0.00722      0.00717      5.1e-05       0.0598       0.0764        0.412      0.00644\n",
            "     84    30      0.00801       0.0075     0.000515       0.0623       0.0781         1.31       0.0205\n",
            "     84    40      0.00671      0.00621     0.000504       0.0568        0.071          1.3       0.0203\n",
            "     84    50      0.00817      0.00809     8.68e-05       0.0626       0.0811        0.538       0.0084\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84     2      0.00591       0.0059     4.37e-06       0.0544       0.0693       0.0999      0.00156\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              84  125.671    0.002      0.00698     0.000273      0.00725       0.0587       0.0753        0.816       0.0128\n",
            "! Validation         84  125.671    0.002      0.00627     8.03e-06      0.00628       0.0564       0.0714        0.129      0.00201\n",
            "Wall time: 125.67211636499997\n",
            "! Best model       84    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85    10      0.00745      0.00743     1.73e-05       0.0615       0.0777         0.24      0.00376\n",
            "     85    20      0.00923      0.00923      1.1e-08       0.0681       0.0866      0.00604     9.44e-05\n",
            "     85    30      0.00557      0.00555     1.85e-05       0.0538       0.0672        0.248      0.00388\n",
            "     85    40      0.00872      0.00822     0.000499        0.064       0.0818         1.29       0.0201\n",
            "     85    50      0.00618      0.00595     0.000227       0.0552       0.0695        0.869       0.0136\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85     2       0.0059       0.0059     4.36e-06       0.0544       0.0692          0.1      0.00157\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              85  127.102    0.002      0.00669     0.000147      0.00684       0.0575       0.0738        0.579      0.00905\n",
            "! Validation         85  127.102    0.002      0.00625     8.28e-06      0.00626       0.0563       0.0713        0.129      0.00201\n",
            "Wall time: 127.1023863219998\n",
            "! Best model       85    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86    10       0.0055      0.00521     0.000294       0.0518       0.0651        0.989       0.0155\n",
            "     86    20      0.00676      0.00665      0.00011       0.0589       0.0735        0.606      0.00946\n",
            "     86    30      0.00697      0.00683     0.000137       0.0579       0.0745        0.676       0.0106\n",
            "     86    40      0.00853      0.00851     2.19e-05        0.065       0.0832         0.27      0.00422\n",
            "     86    50      0.00657      0.00653        4e-05       0.0566       0.0729        0.365       0.0057\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86     2      0.00588      0.00587     3.97e-06       0.0543       0.0691       0.0955      0.00149\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              86  128.507    0.002      0.00682     0.000205      0.00702       0.0581       0.0744        0.689       0.0108\n",
            "! Validation         86  128.507    0.002      0.00624     7.54e-06      0.00625       0.0563       0.0712        0.125      0.00195\n",
            "Wall time: 128.50775075899946\n",
            "! Best model       86    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87    10      0.00445      0.00443      1.9e-05       0.0484         0.06        0.251      0.00393\n",
            "     87    20      0.00713      0.00704     9.11e-05       0.0612       0.0757        0.551      0.00861\n",
            "     87    30       0.0093       0.0093      3.8e-06       0.0692       0.0869        0.112      0.00176\n",
            "     87    40      0.00716      0.00716     7.56e-07       0.0593       0.0763       0.0502     0.000784\n",
            "     87    50      0.00578      0.00555     0.000227       0.0543       0.0672        0.869       0.0136\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87     2      0.00588      0.00587     4.83e-06       0.0543       0.0691        0.115      0.00179\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              87  129.920    0.002      0.00673     7.59e-05       0.0068       0.0576       0.0739        0.427      0.00667\n",
            "! Validation         87  129.920    0.002      0.00623     9.54e-06      0.00624       0.0562       0.0712        0.138      0.00215\n",
            "Wall time: 129.92051456300032\n",
            "! Best model       87    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88    10      0.00661       0.0066     1.25e-05       0.0586       0.0732        0.204      0.00318\n",
            "     88    20        0.008      0.00787     0.000132       0.0638         0.08        0.662       0.0104\n",
            "     88    30      0.00476      0.00464     0.000121       0.0509       0.0614        0.635      0.00992\n",
            "     88    40       0.0063      0.00628     2.23e-05       0.0532       0.0715        0.272      0.00426\n",
            "     88    50       0.0103       0.0102     5.92e-05       0.0693       0.0911        0.444      0.00694\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88     2      0.00587      0.00587     4.21e-06       0.0542        0.069       0.0978      0.00153\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              88  131.363    0.002      0.00675     7.01e-05      0.00682       0.0577       0.0741        0.395      0.00616\n",
            "! Validation         88  131.363    0.002      0.00621     8.03e-06      0.00622       0.0561       0.0711        0.127      0.00198\n",
            "Wall time: 131.3642144259993\n",
            "! Best model       88    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89    10       0.0083      0.00807     0.000232       0.0631        0.081        0.878       0.0137\n",
            "     89    20      0.00634      0.00627     7.81e-05       0.0557       0.0714         0.51      0.00797\n",
            "     89    30      0.00701       0.0069     0.000106       0.0581       0.0749        0.593      0.00927\n",
            "     89    40      0.00507      0.00507     1.53e-06       0.0506       0.0642       0.0715      0.00112\n",
            "     89    50      0.00711      0.00708     3.75e-05       0.0609       0.0758        0.353      0.00552\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89     2      0.00585      0.00585     3.95e-06       0.0541       0.0689        0.093      0.00145\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              89  132.847    0.002      0.00673     9.09e-05      0.00682       0.0577        0.074        0.446      0.00697\n",
            "! Validation         89  132.847    0.002       0.0062     7.24e-06      0.00621       0.0561        0.071        0.122      0.00191\n",
            "Wall time: 132.84795568699883\n",
            "! Best model       89    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90    10      0.00576      0.00576     1.99e-06       0.0558       0.0684       0.0813      0.00127\n",
            "     90    20      0.00864      0.00864     2.49e-06       0.0644       0.0838       0.0911      0.00142\n",
            "     90    30      0.00795      0.00783     0.000117       0.0617       0.0798        0.625      0.00976\n",
            "     90    40      0.00738      0.00732      6.1e-05       0.0603       0.0771        0.451      0.00704\n",
            "     90    50      0.00634      0.00621     0.000125       0.0556       0.0711        0.644       0.0101\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90     2      0.00584      0.00583     3.98e-06       0.0541       0.0689        0.089      0.00139\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              90  134.278    0.002      0.00682     7.71e-05       0.0069       0.0579       0.0745        0.378      0.00591\n",
            "! Validation         90  134.278    0.002      0.00618     6.73e-06      0.00619        0.056       0.0709        0.118      0.00185\n",
            "Wall time: 134.27834212299967\n",
            "! Best model       90    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91    10      0.00496      0.00494     1.78e-05       0.0525       0.0634        0.244      0.00381\n",
            "     91    20      0.00864       0.0086     4.12e-05        0.065       0.0836        0.371      0.00579\n",
            "     91    30      0.00846      0.00838     7.58e-05       0.0666       0.0825        0.502      0.00785\n",
            "     91    40      0.00663      0.00627     0.000363        0.056       0.0714          1.1       0.0172\n",
            "     91    50      0.00683      0.00681     2.53e-05       0.0601       0.0744         0.29      0.00453\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91     2      0.00584      0.00584     3.85e-06       0.0541       0.0689       0.0926      0.00145\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              91  135.724    0.002      0.00661     0.000122      0.00673       0.0571       0.0733        0.494      0.00772\n",
            "! Validation         91  135.724    0.002      0.00618     7.18e-06      0.00619        0.056       0.0709        0.122       0.0019\n",
            "Wall time: 135.7244257000002\n",
            "! Best model       91    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92    10      0.00874      0.00872      1.8e-05       0.0653       0.0842        0.245      0.00383\n",
            "     92    20        0.009      0.00893     7.75e-05       0.0657       0.0852        0.508      0.00794\n",
            "     92    30      0.00549      0.00538     0.000109       0.0539       0.0661        0.603      0.00942\n",
            "     92    40      0.00838      0.00827     0.000113       0.0638        0.082        0.614      0.00959\n",
            "     92    50      0.00581      0.00576      4.3e-05       0.0546       0.0685        0.378      0.00591\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92     2      0.00583      0.00583     3.86e-06        0.054       0.0688       0.0932      0.00146\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              92  137.161    0.002      0.00671     7.26e-05      0.00678       0.0574       0.0739        0.403      0.00629\n",
            "! Validation         92  137.161    0.002      0.00616     7.26e-06      0.00617       0.0559       0.0708        0.122      0.00191\n",
            "Wall time: 137.16120967700044\n",
            "! Best model       92    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93    10      0.00618      0.00617      9.1e-06       0.0552       0.0708        0.174      0.00272\n",
            "     93    20       0.0048      0.00461      0.00018       0.0488       0.0613        0.775       0.0121\n",
            "     93    30      0.00569      0.00538     0.000312       0.0535       0.0661         1.02       0.0159\n",
            "     93    40      0.00626      0.00626     8.92e-07       0.0566       0.0713       0.0545     0.000851\n",
            "     93    50      0.00618      0.00615      3.4e-05       0.0558       0.0707        0.336      0.00525\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93     2      0.00583      0.00583     3.78e-06        0.054       0.0688       0.0917      0.00143\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              93  138.569    0.002      0.00669     0.000114       0.0068       0.0574       0.0737        0.521      0.00814\n",
            "! Validation         93  138.569    0.002      0.00615     7.22e-06      0.00616       0.0559       0.0707        0.122       0.0019\n",
            "Wall time: 138.56984121499954\n",
            "! Best model       93    0.006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94    10      0.00661      0.00653     8.38e-05       0.0563       0.0728        0.528      0.00825\n",
            "     94    20      0.00716      0.00716     1.37e-06       0.0622       0.0763       0.0675      0.00105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nequip-deploy build --train-dir results/silicon-tutorial/si si-deployed.pth\n",
        "!ls *pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnHEECs1CY4k",
        "outputId": "347d1dd5-4eca-4d26-811a-1fb3d773df64"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "INFO:root:Loading best_model.pth from training session...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/deploy.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\n",
            "INFO:root:Compiled & optimized model.\n",
            "si-deployed.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nequip-evaluate --train-dir results/silicon-tutorial/si --batch-size 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nElT2PbgCgF3",
        "outputId": "bfc8cb6b-d20e-48e1-fdc5-40b883ab2664"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/evaluate.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  trainer = torch.load(\n",
            "Using device: cuda\n",
            "Please note that _all_ machine learning models running on CUDA hardware are generally somewhat nondeterministic and that this can manifest in small, generally unimportant variation in the final test errors.\n",
            "Loading model... \n",
            "/usr/local/lib/python3.10/dist-packages/nequip/train/trainer.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_state_dict = torch.load(traindir + \"/\" + model_name, map_location=device)\n",
            "    loaded model\n",
            "Loading original dataset...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (110 frames) minus training (50 frames) and validation frames (10 frames), yielding a test set size of 50 frames.\n",
            "Starting...\n",
            "  0% 0/50 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            " 10% 5/50 [00:01<00:12,  3.65it/s]\n",
            " 20% 10/50 [00:01<00:06,  5.93it/s]\n",
            " 30% 15/50 [00:04<00:10,  3.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 45/50 [00:04<00:00, 16.25it/s]\n",
            "100% 50/50 [00:04<00:00, 12.12it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  0.058570           \n",
            "              f_rmse =  0.075631           \n",
            "               e_mae =  0.094097           \n",
            "             e/N_mae =  0.001470           \n",
            "               f_mae =  0.058570           \n",
            "              f_rmse =  0.075631           \n",
            "               e_mae =  0.094097           \n",
            "             e/N_mae =  0.001470           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time for Ground Truth using LAMMPS**"
      ],
      "metadata": {
        "id": "eOEE44cTCvSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ase.io import read, write\n",
        "\n",
        "example_atoms = read('./Si_data/sitraj.xyz', index=0)\n",
        "write('./si.data', example_atoms, format='lammps-data')"
      ],
      "metadata": {
        "id": "-YAmau_8D6bU"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lammps_input = \"\"\"\n",
        "units\tmetal\n",
        "atom_style atomic\n",
        "dimension 3\n",
        "\n",
        "# set newton on for pair_allegro (off for pair_nequip)\n",
        "newton on\n",
        "boundary p p p\n",
        "read_data ../si.data\n",
        "\n",
        "# if you want to run a larger system, simply replicate the system in space\n",
        "# replicate 3 3 3\n",
        "\n",
        "# allegro pair style\n",
        "pair_style\tallegro\n",
        "pair_coeff\t* * ../si-deployed.pth Si\n",
        "\n",
        "mass 1 28.0855\n",
        "\n",
        "velocity all create 300.0 1234567 loop geom\n",
        "\n",
        "neighbor 1.0 bin\n",
        "neigh_modify delay 5 every 1\n",
        "\n",
        "timestep 0.001\n",
        "thermo 10\n",
        "\n",
        "# nose-hoover thermostat, 300K\n",
        "fix  1 all nvt temp 300 300 $(100*dt)\n",
        "\n",
        "# compute rdf and average after some equilibration\n",
        "comm_modify cutoff 7.0\n",
        "compute rdfall all rdf 1000 cutoff 5.0\n",
        "fix 2 all ave/time 1 2500 5000 c_rdfall[*] file si.rdf mode vector\n",
        "\n",
        "# run 5ps\n",
        "run 5000\n",
        "\"\"\"\n",
        "!rm -rf ./lammps_run\n",
        "!mkdir lammps_run\n",
        "with open(\"lammps_run/si_rdf.in\", \"w\") as f:\n",
        "    f.write(lammps_input)"
      ],
      "metadata": {
        "id": "5RQGnRCsC0gG"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "!cd lammps_run/ && ../lammps/build/lmp -in si_rdf.in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St_X2LxSDAKk",
        "outputId": "622a59e5-4f87-4d03-d3f1-7c12175f8ba7"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAMMPS (19 Nov 2024 - Development - cd16308-modified)\n",
            "OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:99)\n",
            "  using 1 OpenMP thread(s) per MPI task\n",
            "Reading data file ...\n",
            "  orthogonal box = (0 0 0) to (10.862 10.862 10.862)\n",
            "  1 by 1 by 1 MPI processor grid\n",
            "  reading atoms ...\n",
            "  64 atoms\n",
            "  read_data CPU = 0.001 seconds\n",
            "Allegro is using input precision f and output precision d\n",
            "Allegro: Loading model from ../si-deployed.pth\n",
            "Allegro: Freezing TorchScript model...\n",
            "Type mapping:\n",
            "Allegro type | Allegro name | LAMMPS type | LAMMPS name\n",
            "0 | Si | 1 | Si\n",
            "Neighbor list info ...\n",
            "  update: every = 1 steps, delay = 5 steps, check = yes\n",
            "  max neighbors/atom: 2000, page size: 100000\n",
            "  master list distance cutoff = 6\n",
            "  ghost atom cutoff = 7\n",
            "  binsize = 3, bins = 4 4 4\n",
            "  2 neighbor lists, perpetual/occasional/extra = 1 1 0\n",
            "  (1) pair allegro, perpetual\n",
            "      attributes: full, newton on, ghost\n",
            "      pair build: full/bin/ghost\n",
            "      stencil: full/ghost/bin/3d\n",
            "      bin: standard\n",
            "  (2) compute rdf, occasional\n",
            "      attributes: half, newton on\n",
            "      pair build: half/bin/atomonly/newton\n",
            "      stencil: half/bin/3d\n",
            "      bin: standard\n",
            "Setting up Verlet run ...\n",
            "  Unit style    : metal\n",
            "  Current step  : 0\n",
            "  Time step     : 0.001\n",
            "Per MPI rank memory allocation (min/avg/max) = 3.719 | 3.719 | 3.719 Mbytes\n",
            "   Step          Temp          E_pair         E_mol          TotEng         Press     \n",
            "         0   300           -8324.1808      0             -8321.7378      123705.27    \n",
            "        10   189.24619     -8323.2834      0             -8321.7423      124299.1     \n",
            "        20   78.721095     -8322.3784      0             -8321.7373      123984.09    \n",
            "        30   147.13452     -8322.9174      0             -8321.7192      121781.46    \n",
            "        40   190.97333     -8323.2253      0             -8321.6701      120732.3     \n",
            "        50   117.01483     -8322.5741      0             -8321.6212      121562.96    \n",
            "        60   99.236841     -8322.3923      0             -8321.5842      121651.09    \n",
            "        70   167.27405     -8322.8896      0             -8321.5274      120892.99    \n",
            "        80   169.6051      -8322.8498      0             -8321.4686      121528.94    \n",
            "        90   144.35018     -8322.594       0             -8321.4185      122694.49    \n",
            "       100   194.51589     -8322.9542      0             -8321.3702      122375.06    \n",
            "       110   194.36889     -8322.9034      0             -8321.3205      121937.23    \n",
            "       120   121.81587     -8322.2656      0             -8321.2736      122553.87    \n",
            "       130   152.51312     -8322.4775      0             -8321.2356      122764.02    \n",
            "       140   247.08618     -8323.1869      0             -8321.1748      122646.77    \n",
            "       150   211.75883     -8322.8238      0             -8321.0994      123825.57    \n",
            "       160   156.65444     -8322.3154      0             -8321.0397      124984.67    \n",
            "       170   238.56478     -8322.9204      0             -8320.9776      124427.22    \n",
            "       180   307.32697     -8323.3884      0             -8320.8857      123733.51    \n",
            "       190   220.14783     -8322.5961      0             -8320.8033      123887.07    \n",
            "       200   150.21003     -8321.9699      0             -8320.7467      122854.2     \n",
            "       210   198.92908     -8322.3103      0             -8320.6903      120361.54    \n",
            "       220   190.51958     -8322.1693      0             -8320.6178      120043.7     \n",
            "       230   130.66283     -8321.6272      0             -8320.5631      121806.51    \n",
            "       240   196.32165     -8322.0984      0             -8320.4997      122124.54    \n",
            "       250   286.08738     -8322.7321      0             -8320.4024      121774.05    \n",
            "       260   238.42908     -8322.2486      0             -8320.307       123083.09    \n",
            "       270   187.2773      -8321.7643      0             -8320.2392      124699.09    \n",
            "       280   283.95506     -8322.481       0             -8320.1687      124141.37    \n",
            "       290   337.66001     -8322.8165      0             -8320.0668      123410.97    \n",
            "       300   238.38838     -8321.9303      0             -8319.989       124420.53    \n",
            "       310   218.49589     -8321.7077      0             -8319.9284      124688.64    \n",
            "       320   299.85144     -8322.2995      0             -8319.8577      123834.67    \n",
            "       330   292.57619     -8322.1542      0             -8319.7716      124499.22    \n",
            "       340   268.63258     -8321.8909      0             -8319.7033      125607.25    \n",
            "       350   330.27379     -8322.3186      0             -8319.629       124801.25    \n",
            "       360   349.89459     -8322.4022      0             -8319.5529      123177.51    \n",
            "       370   244.48348     -8321.4862      0             -8319.4953      122454.28    \n",
            "       380   195.63555     -8321.0451      0             -8319.4519      121397.68    \n",
            "       390   273.73677     -8321.6223      0             -8319.3932      119882.12    \n",
            "       400   240.44335     -8321.2836      0             -8319.3256      120722.14    \n",
            "       410   174.42528     -8320.6882      0             -8319.2678      122709.85    \n",
            "       420   324.97494     -8321.8409      0             -8319.1945      123029.31    \n",
            "       430   426.61752     -8322.5627      0             -8319.0886      124535.78    \n",
            "       440   359.13559     -8321.9321      0             -8319.0075      127225.92    \n",
            "       450   300.35522     -8321.4091      0             -8318.9632      128086.28    \n",
            "       460   378.04826     -8321.9935      0             -8318.9149      125735.73    \n",
            "       470   391.4254      -8322.0619      0             -8318.8743      124029.52    \n",
            "       480   287.14169     -8321.1943      0             -8318.856       124468.26    \n",
            "       490   263.08918     -8320.9821      0             -8318.8396      124480.51    \n",
            "       500   358.98952     -8321.7455      0             -8318.8221      122871.21    \n",
            "       510   334.26804     -8321.5237      0             -8318.8016      123094.49    \n",
            "       520   286.72617     -8321.1308      0             -8318.7958      124572.08    \n",
            "       530   337.35826     -8321.5329      0             -8318.7856      124799.85    \n",
            "       540   380.21075     -8321.8773      0             -8318.7811      123288.95    \n",
            "       550   324.14436     -8321.4282      0             -8318.7886      121677.2     \n",
            "       560   235.64728     -8320.7219      0             -8318.803       121631.44    \n",
            "       570   256.49124     -8320.8916      0             -8318.8029      121914.96    \n",
            "       580   297.60098     -8321.225       0             -8318.8015      123100.68    \n",
            "       590   340.35997     -8321.5691      0             -8318.7975      124365.44    \n",
            "       600   411.90199     -8322.156       0             -8318.8017      124750.93    \n",
            "       610   372.05373     -8321.8559      0             -8318.8261      126217.84    \n",
            "       620   306.12364     -8321.3561      0             -8318.8632      127582.15    \n",
            "       630   374.46447     -8321.9515      0             -8318.902       126661.31    \n",
            "       640   420.61451     -8322.3934      0             -8318.9681      124651.19    \n",
            "       650   309.48887     -8321.5673      0             -8319.047       123580.25    \n",
            "       660   192.41008     -8320.6648      0             -8319.0979      123409.16    \n",
            "       670   261.89897     -8321.2682      0             -8319.1354      122291.08    \n",
            "       680   339.96033     -8321.9519      0             -8319.1835      121854.41    \n",
            "       690   282.0784      -8321.5207      0             -8319.2236      123052.58    \n",
            "       700   242.27187     -8321.2321      0             -8319.2592      123982.5     \n",
            "       710   311.54408     -8321.8243      0             -8319.2873      123392.66    \n",
            "       720   339.48237     -8322.0922      0             -8319.3276      122976.2     \n",
            "       730   297.18245     -8321.7871      0             -8319.367       123392.82    \n",
            "       740   288.70723     -8321.7522      0             -8319.4012      123262.34    \n",
            "       750   266.92153     -8321.6103      0             -8319.4366      123145.26    \n",
            "       760   227.8017      -8321.3152      0             -8319.4602      123869.06    \n",
            "       770   284.68483     -8321.7925      0             -8319.4742      124773.75    \n",
            "       780   388.89812     -8322.6621      0             -8319.4952      125359.21    \n",
            "       790   367.87191     -8322.5334      0             -8319.5377      125935.64    \n",
            "       800   257.26674     -8321.6712      0             -8319.5762      125914.84    \n",
            "       810   271.56964     -8321.8185      0             -8319.607       124047.2     \n",
            "       820   314.44378     -8322.1989      0             -8319.6382      122480.08    \n",
            "       830   244.84661     -8321.6624      0             -8319.6685      122274.94    \n",
            "       840   190.5295      -8321.2381      0             -8319.6865      121906.76    \n",
            "       850   242.96309     -8321.6721      0             -8319.6936      121125.08    \n",
            "       860   264.21343     -8321.842       0             -8319.6904      122127.41    \n",
            "       870   233.98257     -8321.5837      0             -8319.6783      124578.51    \n",
            "       880   311.7343      -8322.206       0             -8319.6674      124898.42    \n",
            "       890   370.88535     -8322.6707      0             -8319.6505      124021.88    \n",
            "       900   282.41057     -8321.9364      0             -8319.6366      124525.14    \n",
            "       910   225.73503     -8321.4663      0             -8319.6281      124955.46    \n",
            "       920   289.12473     -8321.9641      0             -8319.6096      123788.59    \n",
            "       930   324.10939     -8322.2232      0             -8319.5839      122866.2     \n",
            "       940   243.95953     -8321.5413      0             -8319.5546      123717.59    \n",
            "       950   243.93972     -8321.517       0             -8319.5305      124419.5     \n",
            "       960   354.37737     -8322.3757      0             -8319.4898      124186.61    \n",
            "       970   351.82247     -8322.3071      0             -8319.4421      124841.66    \n",
            "       980   288.46473     -8321.7569      0             -8319.4078      124944.07    \n",
            "       990   278.23025     -8321.6478      0             -8319.382       123414.19    \n",
            "      1000   266.28334     -8321.5089      0             -8319.3404      122052.2     \n",
            "      1010   226.42445     -8321.147       0             -8319.3031      121921.65    \n",
            "      1020   265.92374     -8321.4228      0             -8319.2573      121914.75    \n",
            "      1030   337.90531     -8321.9452      0             -8319.1935      121947.78    \n",
            "      1040   285.92853     -8321.4589      0             -8319.1304      123580.47    \n",
            "      1050   240.78952     -8321.0389      0             -8319.078       125243.61    \n",
            "      1060   364.31584     -8321.985       0             -8319.0183      125008.29    \n",
            "      1070   452.77728     -8322.6327      0             -8318.9456      124953.79    \n",
            "      1080   352.95709     -8321.7854      0             -8318.9112      125878.65    \n",
            "      1090   254.14701     -8320.9655      0             -8318.8959      125474.05    \n",
            "      1100   315.91193     -8321.4537      0             -8318.8811      122843.8     \n",
            "      1110   359.46144     -8321.7915      0             -8318.8643      121573.22    \n",
            "      1120   268.64864     -8321.0459      0             -8318.8582      123412.06    \n",
            "      1130   271.65081     -8321.0612      0             -8318.849       124407.71    \n",
            "      1140   382.0946      -8321.9528      0             -8318.8412      123494.73    \n",
            "      1150   371.05618     -8321.8599      0             -8318.8383      123556.27    \n",
            "      1160   289.40691     -8321.2052      0             -8318.8485      124505.98    \n",
            "      1170   300.37271     -8321.3034      0             -8318.8573      123990.56    \n",
            "      1180   351.96994     -8321.7396      0             -8318.8734      122314.31    \n",
            "      1190   297.24504     -8321.3107      0             -8318.8901      122073.05    \n",
            "      1200   232.9515      -8320.7985      0             -8318.9015      123289.93    \n",
            "      1210   313.19935     -8321.4585      0             -8318.908       123837.72    \n",
            "      1220   370.47921     -8321.9333      0             -8318.9163      124787.79    \n",
            "      1230   353.2304      -8321.8148      0             -8318.9383      125632.68    \n",
            "      1240   355.40912     -8321.8659      0             -8318.9716      125350.61    \n",
            "      1250   343.62811     -8321.8124      0             -8319.0141      125291.71    \n",
            "      1260   331.506       -8321.7681      0             -8319.0685      124823.96    \n",
            "      1270   300.59463     -8321.5633      0             -8319.1154      123762.44    \n",
            "      1280   302.67845     -8321.6282      0             -8319.1634      121832.24    \n",
            "      1290   259.96114     -8321.328       0             -8319.211       121432.63    \n",
            "      1300   197.03024     -8320.8436      0             -8319.2391      122865.98    \n",
            "      1310   301.53079     -8321.7122      0             -8319.2567      123119.11    \n",
            "      1320   399.19236     -8322.5383      0             -8319.2875      123484.4     \n",
            "      1330   337.91976     -8322.0831      0             -8319.3312      124506.53    \n",
            "      1340   254.88053     -8321.4402      0             -8319.3647      124591.69    \n",
            "      1350   264.63612     -8321.5456      0             -8319.3906      123399.42    \n",
            "      1360   307.03673     -8321.9127      0             -8319.4124      122246.41    \n",
            "      1370   274.1593      -8321.6609      0             -8319.4283      122817.91    \n",
            "      1380   256.50647     -8321.528       0             -8319.4392      123598.53    \n",
            "      1390   292.1839      -8321.8247      0             -8319.4453      123522.44    \n",
            "      1400   295.41419     -8321.8515      0             -8319.4458      124052.03    \n",
            "      1410   308.17007     -8321.9577      0             -8319.4482      124944.39    \n",
            "      1420   340.68534     -8322.2238      0             -8319.4495      125489.11    \n",
            "      1430   323.74748     -8322.0936      0             -8319.4572      125047.58    \n",
            "      1440   276.0736      -8321.7148      0             -8319.4667      123820.89    \n",
            "      1450   267.57462     -8321.6485      0             -8319.4696      122596.29    \n",
            "      1460   282.11546     -8321.7692      0             -8319.4718      121866.73    \n",
            "      1470   244.84469     -8321.4552      0             -8319.4614      122229.52    \n",
            "      1480   222.64586     -8321.2642      0             -8319.4511      122827.27    \n",
            "      1490   299.10174     -8321.8679      0             -8319.4322      122886.85    \n",
            "      1500   347.38908     -8322.2398      0             -8319.4109      123713.37    \n",
            "      1510   298.95121     -8321.8322      0             -8319.3977      125250.16    \n",
            "      1520   311.31784     -8321.9218      0             -8319.3867      124756.47    \n",
            "      1530   346.18094     -8322.1915      0             -8319.3725      123029.72    \n",
            "      1540   271.41858     -8321.5843      0             -8319.374       122912.89    \n",
            "      1550   204.32932     -8321.0307      0             -8319.3667      123674.04    \n",
            "      1560   271.03209     -8321.5584      0             -8319.3512      123368.44    \n",
            "      1570   359.96295     -8322.2546      0             -8319.3233      122808.52    \n",
            "      1580   319.43678     -8321.9008      0             -8319.2995      123680.42    \n",
            "      1590   254.73063     -8321.3582      0             -8319.2838      125025.02    \n",
            "      1600   301.3149      -8321.7204      0             -8319.2667      125150.41    \n",
            "      1610   357.86743     -8322.1569      0             -8319.2427      124534.71    \n",
            "      1620   362.26354     -8322.1759      0             -8319.2258      123202.73    \n",
            "      1630   279.65961     -8321.4934      0             -8319.216       122491.74    \n",
            "      1640   212.06763     -8320.9378      0             -8319.2108      122360       \n",
            "      1650   247.47473     -8321.2054      0             -8319.1902      122307.23    \n",
            "      1660   325.85673     -8321.8117      0             -8319.1582      122760.9     \n",
            "      1670   360.39017     -8322.0637      0             -8319.1289      123649.84    \n",
            "      1680   320.26116     -8321.7073      0             -8319.0993      124801.11    \n",
            "      1690   318.75197     -8321.6703      0             -8319.0746      124486.62    \n",
            "      1700   357.96303     -8321.969       0             -8319.054       123185.65    \n",
            "      1710   325.4137      -8321.6908      0             -8319.0408      123074.37    \n",
            "      1720   253.53183     -8321.0974      0             -8319.0328      123622.53    \n",
            "      1730   237.46716     -8320.9504      0             -8319.0166      123407.1     \n",
            "      1740   314.93698     -8321.5532      0             -8318.9886      122388.77    \n",
            "      1750   363.84746     -8321.9168      0             -8318.9538      122937.53    \n",
            "      1760   346.95247     -8321.7455      0             -8318.9202      124918.69    \n",
            "      1770   322.75406     -8321.5279      0             -8318.8996      125747.9     \n",
            "      1780   331.57433     -8321.5831      0             -8318.8829      124575.4     \n",
            "      1790   350.01655     -8321.7097      0             -8318.8594      123324.42    \n",
            "      1800   333.19948     -8321.5677      0             -8318.8544      123321.73    \n",
            "      1810   320.97167     -8321.4569      0             -8318.8431      122921.17    \n",
            "      1820   311.7087      -8321.3778      0             -8318.8394      121754.73    \n",
            "      1830   268.46923     -8321.0274      0             -8318.8411      121523.49    \n",
            "      1840   276.10468     -8321.0849      0             -8318.8365      122404.81    \n",
            "      1850   331.93777     -8321.529       0             -8318.8259      123978.85    \n",
            "      1860   390.6887      -8321.9968      0             -8318.8153      124730.55    \n",
            "      1870   387.66445     -8321.9844      0             -8318.8275      124156.23    \n",
            "      1880   335.99473     -8321.589       0             -8318.8529      123131       \n",
            "      1890   290.60564     -8321.2508      0             -8318.8843      122842.35    \n",
            "      1900   253.11889     -8320.9787      0             -8318.9175      123455.04    \n",
            "      1910   287.58881     -8321.2823      0             -8318.9404      122938.23    \n",
            "      1920   353.61552     -8321.8576      0             -8318.978       122079.41    \n",
            "      1930   329.56873     -8321.712       0             -8319.0282      123150.3     \n",
            "      1940   272.41597     -8321.2927      0             -8319.0743      125002.64    \n",
            "      1950   314.0383      -8321.676       0             -8319.1187      125271.85    \n",
            "      1960   390.3681      -8322.3643      0             -8319.1853      124193.38    \n",
            "      1970   353.60673     -8322.151       0             -8319.2714      123611.5     \n",
            "      1980   254.11424     -8321.415       0             -8319.3456      123371.95    \n",
            "      1990   246.70688     -8321.4094      0             -8319.4004      122207.26    \n",
            "      2000   289.83978     -8321.8144      0             -8319.4541      121270.7     \n",
            "      2010   260.53488     -8321.6168      0             -8319.4952      122135.09    \n",
            "      2020   226.76077     -8321.3714      0             -8319.5248      123097.51    \n",
            "      2030   268.09947     -8321.7315      0             -8319.5483      123122.7     \n",
            "      2040   321.7321      -8322.185       0             -8319.565       123400.06    \n",
            "      2050   351.87712     -8322.4573      0             -8319.5918      123698.59    \n",
            "      2060   314.86642     -8322.1844      0             -8319.6203      123894.75    \n",
            "      2070   249.60125     -8321.683       0             -8319.6504      123827.11    \n",
            "      2080   210.89815     -8321.3825      0             -8319.6651      123490.71    \n",
            "      2090   251.36928     -8321.7154      0             -8319.6684      122992.47    \n",
            "      2100   319.33288     -8322.2709      0             -8319.6705      123002.63    \n",
            "      2110   301.85203     -8322.1277      0             -8319.6696      124031.14    \n",
            "      2120   279.04642     -8321.9402      0             -8319.6678      124177.79    \n",
            "      2130   284.79372     -8321.987       0             -8319.6678      123383.05    \n",
            "      2140   275.20069     -8321.9042      0             -8319.6631      123093.74    \n",
            "      2150   269.77323     -8321.8519      0             -8319.6551      123038.78    \n",
            "      2160   275.27189     -8321.8823      0             -8319.6407      122564.97    \n",
            "      2170   290.86598     -8321.9919      0             -8319.6233      121428.88    \n",
            "      2180   244.88619     -8321.5938      0             -8319.5996      121773.63    \n",
            "      2190   207.32662     -8321.2665      0             -8319.5781      123221.27    \n",
            "      2200   279.58235     -8321.8145      0             -8319.5377      123559.6     \n",
            "      2210   343.82431     -8322.275       0             -8319.4751      123564.69    \n",
            "      2220   336.1997      -8322.1479      0             -8319.4101      124080.91    \n",
            "      2230   306.61237     -8321.8443      0             -8319.3474      124376.91    \n",
            "      2240   295.65286     -8321.6992      0             -8319.2916      123978.92    \n",
            "      2250   312.61201     -8321.7781      0             -8319.2323      123126.71    \n",
            "      2260   314.20826     -8321.7306      0             -8319.1718      122488.63    \n",
            "      2270   271.90404     -8321.3292      0             -8319.115       122703.66    \n",
            "      2280   261.51206     -8321.1918      0             -8319.0622      123297.51    \n",
            "      2290   328.27346     -8321.6725      0             -8318.9992      123699.01    \n",
            "      2300   382.84792     -8322.0505      0             -8318.9328      123991.79    \n",
            "      2310   345.49048     -8321.693       0             -8318.8795      123922.02    \n",
            "      2320   274.70604     -8321.0877      0             -8318.8507      123394.74    \n",
            "      2330   304.22805     -8321.2968      0             -8318.8193      122120.89    \n",
            "      2340   355.00991     -8321.6851      0             -8318.7941      121556.04    \n",
            "      2350   327.55097     -8321.4463      0             -8318.779       122250.67    \n",
            "      2360   287.62785     -8321.112       0             -8318.7698      122932.54    \n",
            "      2370   269.40041     -8320.9551      0             -8318.7612      123291.88    \n",
            "      2380   314.1747      -8321.3107      0             -8318.7523      123498.79    \n",
            "      2390   381.01289     -8321.8474      0             -8318.7447      124609.63    \n",
            "      2400   427.7701      -8322.2379      0             -8318.7544      125570.93    \n",
            "      2410   408.43458     -8322.114       0             -8318.7879      125094.29    \n",
            "      2420   308.62115     -8321.341       0             -8318.8278      124008.08    \n",
            "      2430   285.80157     -8321.1847      0             -8318.8573      122503.85    \n",
            "      2440   295.50523     -8321.2882      0             -8318.8818      122148.05    \n",
            "      2450   274.71443     -8321.1405      0             -8318.9034      122547.6     \n",
            "      2460   289.47121     -8321.2717      0             -8318.9144      122050.36    \n",
            "      2470   330.89321     -8321.6221      0             -8318.9275      121710.67    \n",
            "      2480   334.8955      -8321.6694      0             -8318.9422      122663.7     \n",
            "      2490   297.27843     -8321.3811      0             -8318.9602      123989.94    \n",
            "      2500   318.76388     -8321.5722      0             -8318.9764      123575.47    \n",
            "      2510   357.5091      -8321.9109      0             -8318.9996      122455.29    \n",
            "      2520   318.59643     -8321.6259      0             -8319.0315      122585.17    \n",
            "      2530   272.14719     -8321.2735      0             -8319.0573      123505.07    \n",
            "      2540   298.92063     -8321.5143      0             -8319.0801      123728.6     \n",
            "      2550   347.48225     -8321.938       0             -8319.1083      123332.91    \n",
            "      2560   324.40132     -8321.7806      0             -8319.1389      123498.06    \n",
            "      2570   298.92131     -8321.6061      0             -8319.1718      124100.18    \n",
            "      2580   317.05387     -8321.785       0             -8319.2031      124352.43    \n",
            "      2590   345.14991     -8322.0544      0             -8319.2437      123533.26    \n",
            "      2600   339.29382     -8322.0569      0             -8319.2939      122137.99    \n",
            "      2610   252.4453      -8321.3919      0             -8319.3361      121827.3     \n",
            "      2620   202.38576     -8321.0117      0             -8319.3636      121783.11    \n",
            "      2630   250.58573     -8321.4216      0             -8319.381       121481.11    \n",
            "      2640   308.53783     -8321.9058      0             -8319.3933      121991.22    \n",
            "      2650   328.13488     -8322.08        0             -8319.4078      123040.44    \n",
            "      2660   285.20615     -8321.7488      0             -8319.4263      124000.53    \n",
            "      2670   255.02645     -8321.5157      0             -8319.4389      123826.13    \n",
            "      2680   301.14366     -8321.8954      0             -8319.4431      122972.74    \n",
            "      2690   342.22757     -8322.246       0             -8319.4592      123152.25    \n",
            "      2700   315.24654     -8322.0419      0             -8319.4748      123843.85    \n",
            "      2710   265.04991     -8321.6504      0             -8319.492       123734.04    \n",
            "      2720   268.90186     -8321.6942      0             -8319.5044      122835.44    \n",
            "      2730   302.53668     -8321.9826      0             -8319.5189      122540.68    \n",
            "      2740   293.60955     -8321.9188      0             -8319.5278      123515.18    \n",
            "      2750   270.72692     -8321.739       0             -8319.5344      124308.7     \n",
            "      2760   301.44108     -8322.003       0             -8319.5483      123366.45    \n",
            "      2770   327.50446     -8322.2284      0             -8319.5614      121705.3     \n",
            "      2780   263.69202     -8321.7234      0             -8319.576       121579.13    \n",
            "      2790   205.31829     -8321.2577      0             -8319.5858      122024.54    \n",
            "      2800   236.84353     -8321.5161      0             -8319.5874      121478.22    \n",
            "      2810   260.16464     -8321.6979      0             -8319.5793      121298.23    \n",
            "      2820   264.93326     -8321.7223      0             -8319.5649      122783.25    \n",
            "      2830   304.56407     -8322.0164      0             -8319.5362      124644.22    \n",
            "      2840   348.58928     -8322.3363      0             -8319.4976      125132.77    \n",
            "      2850   341.17956     -8322.236       0             -8319.4577      124463.37    \n",
            "      2860   303.37562     -8321.8952      0             -8319.4247      123895.91    \n",
            "      2870   289.86303     -8321.7475      0             -8319.387       123662.09    \n",
            "      2880   301.82831     -8321.8082      0             -8319.3503      123100.34    \n",
            "      2890   301.2148      -8321.7558      0             -8319.3029      122138.11    \n",
            "      2900   295.82349     -8321.6682      0             -8319.2592      121176.55    \n",
            "      2910   274.35864     -8321.4501      0             -8319.2159      121213.33    \n",
            "      2920   235.47586     -8321.0841      0             -8319.1666      122642.91    \n",
            "      2930   287.64683     -8321.4555      0             -8319.113       123291.28    \n",
            "      2940   390.24004     -8322.2182      0             -8319.0403      122504.7     \n",
            "      2950   349.32666     -8321.8209      0             -8318.9762      122321.89    \n",
            "      2960   245.15796     -8320.9305      0             -8318.9341      122853.75    \n",
            "      2970   248.80995     -8320.9202      0             -8318.894       123045.22    \n",
            "      2980   340.08144     -8321.6174      0             -8318.848       122895.57    \n",
            "      2990   385.2715      -8321.9289      0             -8318.7915      123202.36    \n",
            "      3000   322.76936     -8321.3846      0             -8318.7562      124572.25    \n",
            "      3010   321.35614     -8321.3436      0             -8318.7267      125077.74    \n",
            "      3020   375.56128     -8321.7648      0             -8318.7065      124651.28    \n",
            "      3030   394.2781      -8321.9073      0             -8318.6965      124474.73    \n",
            "      3040   355.83973     -8321.6062      0             -8318.7085      124716.31    \n",
            "      3050   312.47725     -8321.2654      0             -8318.7208      124183.46    \n",
            "      3060   312.17812     -8321.2852      0             -8318.743       122353.31    \n",
            "      3070   322.09413     -8321.3882      0             -8318.7652      121119.22    \n",
            "      3080   310.95561     -8321.3218      0             -8318.7896      121506.44    \n",
            "      3090   289.36288     -8321.1739      0             -8318.8175      122289.51    \n",
            "      3100   267.97756     -8321.0269      0             -8318.8447      122799.14    \n",
            "      3110   300.61446     -8321.3093      0             -8318.8613      122870.56    \n",
            "      3120   352.76004     -8321.7648      0             -8318.8921      123315.31    \n",
            "      3130   361.30456     -8321.8746      0             -8318.9324      124026.31    \n",
            "      3140   335.20932     -8321.7089      0             -8318.9792      124114.44    \n",
            "      3150   310.15194     -8321.5639      0             -8319.0382      123918.37    \n",
            "      3160   296.25002     -8321.5068      0             -8319.0943      124149.84    \n",
            "      3170   290.11757     -8321.5101      0             -8319.1476      124643.06    \n",
            "      3180   320.75021     -8321.8159      0             -8319.2039      124717.12    \n",
            "      3190   358.33439     -8322.1899      0             -8319.2719      124297.39    \n",
            "      3200   344.34447     -8322.1498      0             -8319.3456      123690.11    \n",
            "      3210   274.61292     -8321.6547      0             -8319.4184      123499.92    \n",
            "      3220   249.39231     -8321.4964      0             -8319.4655      123167.93    \n",
            "      3230   279.38729     -8321.7813      0             -8319.5062      122133.88    \n",
            "      3240   281.68972     -8321.8329      0             -8319.539       121220.56    \n",
            "      3250   240.30217     -8321.5231      0             -8319.5662      121457.58    \n",
            "      3260   212.47435     -8321.3066      0             -8319.5763      122500.61    \n",
            "      3270   264.86925     -8321.7351      0             -8319.5782      123055.97    \n",
            "      3280   320.10105     -8322.1787      0             -8319.572       123638.42    \n",
            "      3290   319.31234     -8322.1727      0             -8319.5724      124654.31    \n",
            "      3300   292.48854     -8321.9516      0             -8319.5697      125336.52    \n",
            "      3310   286.75391     -8321.9077      0             -8319.5725      125150.22    \n",
            "      3320   318.88697     -8322.1712      0             -8319.5744      124352.52    \n",
            "      3330   324.68809     -8322.2213      0             -8319.5773      123951.12    \n",
            "      3340   266.83403     -8321.7546      0             -8319.5817      123957.13    \n",
            "      3350   234.02135     -8321.485       0             -8319.5793      123326.9     \n",
            "      3360   277.37196     -8321.8278      0             -8319.5691      122342.95    \n",
            "      3370   317.09996     -8322.138       0             -8319.5557      122544.08    \n",
            "      3380   289.23403     -8321.9011      0             -8319.5458      123424.72    \n",
            "      3390   249.53676     -8321.5676      0             -8319.5355      123294.08    \n",
            "      3400   258.32379     -8321.6221      0             -8319.5185      122084.64    \n",
            "      3410   279.21861     -8321.7706      0             -8319.4968      121445.97    \n",
            "      3420   272.07653     -8321.6855      0             -8319.4699      122195.71    \n",
            "      3430   272.09633     -8321.6632      0             -8319.4474      122991.92    \n",
            "      3440   288.24439     -8321.7623      0             -8319.415       123070.41    \n",
            "      3450   289.49118     -8321.7378      0             -8319.3804      123349.42    \n",
            "      3460   292.15361     -8321.7281      0             -8319.349       124463.03    \n",
            "      3470   330.85325     -8322.0085      0             -8319.3143      125389.84    \n",
            "      3480   366.48308     -8322.2648      0             -8319.2803      125265.8     \n",
            "      3490   335.87441     -8321.9928      0             -8319.2576      124527.12    \n",
            "      3500   310.2061      -8321.7735      0             -8319.2474      123618.83    \n",
            "      3510   294.61797     -8321.6412      0             -8319.242       123629.33    \n",
            "      3520   261.88418     -8321.3607      0             -8319.228       123994.78    \n",
            "      3530   283.25521     -8321.5233      0             -8319.2167      122709.74    \n",
            "      3540   307.72493     -8321.7086      0             -8319.2026      121216.18    \n",
            "      3550   280.56795     -8321.4723      0             -8319.1876      121587.77    \n",
            "      3560   250.50378     -8321.2084      0             -8319.1685      122697.5     \n",
            "      3570   284.42682     -8321.4633      0             -8319.1471      122577.72    \n",
            "      3580   355.42789     -8322.011       0             -8319.1166      121810.27    \n",
            "      3590   320.8246      -8321.7072      0             -8319.0946      122572.68    \n",
            "      3600   246.86093     -8321.0859      0             -8319.0756      123989.58    \n",
            "      3610   310.8568      -8321.5877      0             -8319.0563      123853.41    \n",
            "      3620   375.38578     -8322.0835      0             -8319.0266      124014.57    \n",
            "      3630   349.99546     -8321.8641      0             -8319.0139      125302.5     \n",
            "      3640   318.83803     -8321.6045      0             -8319.008       126088.65    \n",
            "      3650   335.80372     -8321.7391      0             -8319.0045      125330.25    \n",
            "      3660   362.63335     -8321.9589      0             -8319.0059      123982.21    \n",
            "      3670   333.86423     -8321.7434      0             -8319.0246      123584.21    \n",
            "      3680   292.67548     -8321.4284      0             -8319.045       123256.69    \n",
            "      3690   275.80823     -8321.3237      0             -8319.0777      122321.62    \n",
            "      3700   249.05323     -8321.1409      0             -8319.1127      121649.56    \n",
            "      3710   247.90692     -8321.1609      0             -8319.1421      121498.96    \n",
            "      3720   298.52512     -8321.6162      0             -8319.1852      121938.66    \n",
            "      3730   322.66348     -8321.8784      0             -8319.2508      123003.93    \n",
            "      3740   282.65349     -8321.6249      0             -8319.3232      123881.46    \n",
            "      3750   281.05108     -8321.6903      0             -8319.4016      123670.85    \n",
            "      3760   321.06435     -8322.1023      0             -8319.4877      123437.31    \n",
            "      3770   308.23896     -8322.0881      0             -8319.578       124183.2     \n",
            "      3780   279.49039     -8321.9342      0             -8319.6582      124536.36    \n",
            "      3790   293.98764     -8322.1251      0             -8319.731       123791.9     \n",
            "      3800   292.2051      -8322.1779      0             -8319.7984      123434.9     \n",
            "      3810   246.75789     -8321.8608      0             -8319.8514      124274.08    \n",
            "      3820   243.15627     -8321.8684      0             -8319.8883      124502.43    \n",
            "      3830   306.87926     -8322.4208      0             -8319.9218      123017.95    \n",
            "      3840   296.63718     -8322.3708      0             -8319.9552      122119.95    \n",
            "      3850   203.96418     -8321.6371      0             -8319.9762      122803.95    \n",
            "      3860   195.16035     -8321.5709      0             -8319.9817      122961.21    \n",
            "      3870   277.84779     -8322.2442      0             -8319.9816      121785.77    \n",
            "      3880   270.44847     -8322.1794      0             -8319.9771      121385.32    \n",
            "      3890   182.05816     -8321.4509      0             -8319.9684      122513.18    \n",
            "      3900   201.74918     -8321.5886      0             -8319.9457      123209.85    \n",
            "      3910   313.06341     -8322.454       0             -8319.9046      123286.73    \n",
            "      3920   335.22355     -8322.5861      0             -8319.8563      124062.03    \n",
            "      3930   284.78037     -8322.1341      0             -8319.8151      124547.2     \n",
            "      3940   269.7857      -8321.9738      0             -8319.7768      124158.69    \n",
            "      3950   286.27612     -8322.063       0             -8319.7317      123814.93    \n",
            "      3960   294.20064     -8322.0869      0             -8319.6911      123992.23    \n",
            "      3970   301.68599     -8322.0995      0             -8319.6427      123811.79    \n",
            "      3980   285.20502     -8321.9201      0             -8319.5975      122996.05    \n",
            "      3990   224.94094     -8321.388       0             -8319.5562      122625.48    \n",
            "      4000   244.03428     -8321.5031      0             -8319.5158      122077.95    \n",
            "      4010   331.55072     -8322.1588      0             -8319.4589      121388.23    \n",
            "      4020   314.53178     -8321.9588      0             -8319.3975      121792.73    \n",
            "      4030   228.16584     -8321.2122      0             -8319.3542      122381.65    \n",
            "      4040   228.05483     -8321.1717      0             -8319.3146      121879.11    \n",
            "      4050   311.35        -8321.7992      0             -8319.2638      121311.2     \n",
            "      4060   329.11982     -8321.8889      0             -8319.2088      122650.59    \n",
            "      4070   280.51754     -8321.448       0             -8319.1637      124713.02    \n",
            "      4080   307.38004     -8321.6313      0             -8319.1282      124903.67    \n",
            "      4090   376.60731     -8322.158       0             -8319.0912      124215.24    \n",
            "      4100   361.69774     -8322.0116      0             -8319.0662      124729.35    \n",
            "      4110   317.29323     -8321.6377      0             -8319.0539      125215.99    \n",
            "      4120   334.54691     -8321.7788      0             -8319.0545      124009.69    \n",
            "      4130   329.65326     -8321.7412      0             -8319.0567      122628.77    \n",
            "      4140   283.15781     -8321.3684      0             -8319.0625      122442.63    \n",
            "      4150   270.94376     -8321.2711      0             -8319.0647      122563.63    \n",
            "      4160   299.05559     -8321.4991      0             -8319.0638      121852.09    \n",
            "      4170   299.35343     -8321.501       0             -8319.0632      121368.58    \n",
            "      4180   288.69785     -8321.4124      0             -8319.0614      121765.04    \n",
            "      4190   295.92521     -8321.4681      0             -8319.0583      122482.19    \n",
            "      4200   306.07566     -8321.5479      0             -8319.0554      122973.52    \n",
            "      4210   316.50223     -8321.6287      0             -8319.0513      122921.83    \n",
            "      4220   333.20197     -8321.7626      0             -8319.0492      122555.41    \n",
            "      4230   322.28927     -8321.6857      0             -8319.0612      122938.81    \n",
            "      4240   279.63482     -8321.3421      0             -8319.0649      124109.09    \n",
            "      4250   287.6881      -8321.4153      0             -8319.0725      124783.2     \n",
            "      4260   366.45108     -8322.0634      0             -8319.0792      124498.04    \n",
            "      4270   385.27938     -8322.2411      0             -8319.1037      124414.83    \n",
            "      4280   306.84626     -8321.6349      0             -8319.1361      124835.51    \n",
            "      4290   291.29342     -8321.5431      0             -8319.171       124134.48    \n",
            "      4300   341.7524      -8321.9989      0             -8319.2159      122463.9     \n",
            "      4310   320.06145     -8321.8856      0             -8319.2792      121752.35    \n",
            "      4320   237.65598     -8321.2664      0             -8319.3311      121879.35    \n",
            "      4330   214.86294     -8321.1218      0             -8319.3721      121108.24    \n",
            "      4340   254.19499     -8321.4863      0             -8319.4163      120297.67    \n",
            "      4350   283.676       -8321.7687      0             -8319.4586      121057.11    \n",
            "      4360   283.42559     -8321.8104      0             -8319.5024      122711.99    \n",
            "      4370   289.10607     -8321.9043      0             -8319.55        123539.2     \n",
            "      4380   299.78924     -8322.043       0             -8319.6017      123323.14    \n",
            "      4390   297.47415     -8322.0797      0             -8319.6572      123152.45    \n",
            "      4400   292.67418     -8322.1002      0             -8319.7168      123714.68    \n",
            "      4410   283.20449     -8322.0806      0             -8319.7744      124299.5     \n",
            "      4420   260.16195     -8321.9454      0             -8319.8268      124036.48    \n",
            "      4430   270.70481     -8322.0797      0             -8319.8753      122962.04    \n",
            "      4440   283.44976     -8322.2343      0             -8319.926       122510.17    \n",
            "      4450   261.04009     -8322.0977      0             -8319.972       123050.98    \n",
            "      4460   235.58554     -8321.9258      0             -8320.0073      123228.45    \n",
            "      4470   254.83101     -8322.1099      0             -8320.0347      122176.24    \n",
            "      4480   268.44093     -8322.2395      0             -8320.0535      121187.42    \n",
            "      4490   217.76881     -8321.8384      0             -8320.0651      121786.6     \n",
            "      4500   195.68823     -8321.665       0             -8320.0714      122366.28    \n",
            "      4510   257.67022     -8322.1585      0             -8320.0602      121646.51    \n",
            "      4520   280.57593     -8322.3295      0             -8320.0447      121461.47    \n",
            "      4530   225.79255     -8321.8636      0             -8320.0248      122798.62    \n",
            "      4540   226.86003     -8321.8439      0             -8319.9965      123913.08    \n",
            "      4550   307.57941     -8322.4582      0             -8319.9534      123676.36    \n",
            "      4560   326.36295     -8322.5579      0             -8319.9002      123416.1     \n",
            "      4570   271.21112     -8322.0573      0             -8319.8488      123674.9     \n",
            "      4580   249.22495     -8321.8298      0             -8319.8003      123507.17    \n",
            "      4590   270.72264     -8321.9501      0             -8319.7455      122869.34    \n",
            "      4600   283.34659     -8321.9825      0             -8319.6751      122471.99    \n",
            "      4610   285.67048     -8321.9311      0             -8319.6048      122381.99    \n",
            "      4620   279.6719      -8321.8111      0             -8319.5336      122478.73    \n",
            "      4630   252.30963     -8321.5264      0             -8319.4717      122811.74    \n",
            "      4640   259.26073     -8321.524       0             -8319.4127      122682.27    \n",
            "      4650   330.12799     -8322.0456      0             -8319.3573      121788.09    \n",
            "      4660   331.88696     -8322.0172      0             -8319.3145      121718.54    \n",
            "      4670   252.52192     -8321.3473      0             -8319.2909      122406.14    \n",
            "      4680   236.97591     -8321.2052      0             -8319.2754      122294.15    \n",
            "      4690   299.61928     -8321.6931      0             -8319.2532      121985.74    \n",
            "      4700   322.20151     -8321.8578      0             -8319.234       122864.76    \n",
            "      4710   291.92165     -8321.5943      0             -8319.2171      124363.91    \n",
            "      4720   309.63463     -8321.728       0             -8319.2066      124784.65    \n",
            "      4730   368.15523     -8322.1948      0             -8319.1968      124095.21    \n",
            "      4740   350.70127     -8322.0543      0             -8319.1984      124237.82    \n",
            "      4750   303.39441     -8321.6839      0             -8319.2133      124445.27    \n",
            "      4760   299.56012     -8321.6584      0             -8319.219       123249.26    \n",
            "      4770   293.1652      -8321.6131      0             -8319.2257      121841.03    \n",
            "      4780   256.51751     -8321.3226      0             -8319.2337      122062.76    \n",
            "      4790   255.81731     -8321.3192      0             -8319.236       122932.82    \n",
            "      4800   318.19003     -8321.826       0             -8319.2349      122878.95    \n",
            "      4810   329.01827     -8321.913       0             -8319.2337      122814.56    \n",
            "      4820   281.51549     -8321.5248      0             -8319.2323      123313.29    \n",
            "      4830   281.14148     -8321.5232      0             -8319.2338      123738.36    \n",
            "      4840   323.2607      -8321.8599      0             -8319.2275      123776.89    \n",
            "      4850   328.7701      -8321.9052      0             -8319.2279      123528.97    \n",
            "      4860   302.51515     -8321.6956      0             -8319.2321      123107.29    \n",
            "      4870   276.87826     -8321.494       0             -8319.2393      122947.85    \n",
            "      4880   272.94445     -8321.4608      0             -8319.2381      123215.28    \n",
            "      4890   301.68719     -8321.6909      0             -8319.2341      123716.45    \n",
            "      4900   355.23831     -8322.1288      0             -8319.2359      123733.63    \n",
            "      4910   355.74493     -8322.137       0             -8319.24        123551.18    \n",
            "      4920   277.37321     -8321.5146      0             -8319.2558      123921.14    \n",
            "      4930   247.66513     -8321.2841      0             -8319.2672      123794.06    \n",
            "      4940   330.42665     -8321.9647      0             -8319.2739      122447.18    \n",
            "      4950   340.78137     -8322.061       0             -8319.2859      122109.78    \n",
            "      4960   250.61616     -8321.3429      0             -8319.3021      123037.69    \n",
            "      4970   225.46265     -8321.1446      0             -8319.3086      123119.49    \n",
            "      4980   307.52207     -8321.815       0             -8319.3107      122110.58    \n",
            "      4990   337.46376     -8322.0588      0             -8319.3107      122582.44    \n",
            "      5000   280.57199     -8321.6021      0             -8319.3173      124245.63    \n",
            "Loop time of 36.5011 on 1 procs for 5000 steps with 64 atoms\n",
            "\n",
            "Performance: 11.835 ns/day, 2.028 hours/ns, 136.982 timesteps/s, 8.767 katom-step/s\n",
            "99.0% CPU use with 1 MPI tasks x 1 OpenMP threads\n",
            "\n",
            "MPI task timing breakdown:\n",
            "Section |  min time  |  avg time  |  max time  |%varavg| %total\n",
            "---------------------------------------------------------------\n",
            "Pair    | 36.292     | 36.292     | 36.292     |   0.0 | 99.43\n",
            "Neigh   | 0          | 0          | 0          |   0.0 |  0.00\n",
            "Comm    | 0.026883   | 0.026883   | 0.026883   |   0.0 |  0.07\n",
            "Output  | 0.020027   | 0.020027   | 0.020027   |   0.0 |  0.05\n",
            "Modify  | 0.15254    | 0.15254    | 0.15254    |   0.0 |  0.42\n",
            "Other   |            | 0.01003    |            |       |  0.03\n",
            "\n",
            "Nlocal:             64 ave          64 max          64 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Nghost:            801 ave         801 max         801 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Neighs:           1389 ave        1389 max        1389 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "FullNghs:         2944 ave        2944 max        2944 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "Total # of neighbors = 2944\n",
            "Ave neighs/atom = 46\n",
            "Neighbor list builds = 0\n",
            "Dangerous builds = 0\n",
            "Total wall time: 0:00:38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['font.size'] = 30\n",
        "\n",
        "def parse_lammps_rdf(rdffile):\n",
        "    \"\"\"\n",
        "    Parse the RDF file written by LAMMPS\n",
        "\n",
        "    copied from Boris' class code: https://github.com/bkoz37/labutil\n",
        "    \"\"\"\n",
        "    with open(rdffile, 'r') as rdfout:\n",
        "        rdfs = []\n",
        "        buffer = []\n",
        "        for line in rdfout:\n",
        "            values = line.split()\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            elif len(values) == 2:\n",
        "                nbins = values[1]\n",
        "            else:\n",
        "                buffer.append([float(values[1]), float(values[2])])\n",
        "                if len(buffer) == int(nbins):\n",
        "                    frame = np.transpose(np.array(buffer))\n",
        "                    rdfs.append(frame)\n",
        "                    buffer = []\n",
        "    return rdfs\n",
        "\n",
        "rdf = parse_lammps_rdf('./lammps_run/si.rdf')\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "plt.plot(rdf[0][0], rdf[0][1], 'b', linewidth=5, label=\"Allegro, T=300K\")\n",
        "plt.xlabel('r [$\\AA$]')\n",
        "plt.ylabel('g(r)')\n",
        "plt.title(\"Si-Si bond length: {:.3f}$\\AA$\".format(rdf[0][0][np.argmax(rdf[0][1])]))\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "7aKj3yzhomge",
        "outputId": "b1fd873d-4f94-4986-96a8-f86af887c870"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Line Done"
      ],
      "metadata": {
        "id": "6bs71WeFor0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have SI-SI Bond matching."
      ],
      "metadata": {
        "id": "wkn7Nn-Yo3pj"
      }
    }
  ]
}