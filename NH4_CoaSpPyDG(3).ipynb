{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coarsening Molecular environments"
      ],
      "metadata": {
        "id": "z8wYdH-gO7oJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First setup Nequip (E3NN), wandb, Allegro.\n",
        "\n",
        "# Confirm device is GPU\n",
        "\n",
        "Before you get started, make sure that in your menu bar Runtime --> Change runtime type is set to GPU"
      ],
      "metadata": {
        "id": "GCFxp5yhPC3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save all Run Data"
      ],
      "metadata": {
        "id": "EOXkQ9MyGY7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3zSOzTkSGYXT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XynMFbIsYBUz",
        "outputId": "3a8383d4-9917-4f65-d852-c3d6b638456c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Cloning into 'nequip'...\n",
            "remote: Enumerating objects: 218, done.\u001b[K\n",
            "remote: Counting objects: 100% (218/218), done.\u001b[K\n",
            "remote: Compressing objects: 100% (205/205), done.\u001b[K\n",
            "remote: Total 218 (delta 4), reused 84 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (218/218), 360.63 KiB | 3.47 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Processing ./nequip\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (1.26.4)\n",
            "Collecting ase (from nequip==0.6.1)\n",
            "  Downloading ase-3.23.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (4.66.6)\n",
            "Collecting e3nn<0.6.0,>=0.4.4 (from nequip==0.6.1)\n",
            "  Downloading e3nn-0.5.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (6.0.2)\n",
            "Collecting torch-runstats>=0.2.0 (from nequip==0.6.1)\n",
            "  Downloading torch_runstats-0.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting torch-ema>=0.3.0 (from nequip==0.6.1)\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl.metadata (415 bytes)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (2.5.1+cu121)\n",
            "Collecting opt-einsum-fx>=0.1.4 (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1)\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase->nequip==0.6.1) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (4.55.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip==0.6.1) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.0.2)\n",
            "Downloading e3nn-0.5.4-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.2/447.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading ase-3.23.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Building wheels for collected packages: nequip\n",
            "  Building wheel for nequip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nequip: filename=nequip-0.6.1-py3-none-any.whl size=175386 sha256=204949a0b1be011d24eede4c8b2e3af2fca7bbfd1d3fb31fe17a1b3e7abd3081\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-diowjs8u/wheels/11/64/44/9d30bacb0803dffa7821bb8685dbc60a0830cca339476e4e86\n",
            "Successfully built nequip\n",
            "Installing collected packages: torch-runstats, torch-ema, opt-einsum-fx, ase, e3nn, nequip\n",
            "Successfully installed ase-3.23.0 e3nn-0.5.4 nequip-0.6.1 opt-einsum-fx-0.1.4 torch-ema-0.3 torch-runstats-0.2.0\n",
            "Cloning into 'allegro'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 44 (delta 0), reused 24 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (44/44), 71.53 KiB | 1.40 MiB/s, done.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nequip>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from mir-allegro==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.66.6)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (0.1.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.55.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.0.2)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27432 sha256=b7c9e55d49105e4f9cf03fa801066a85b74233eeef061bb24375093303283ea5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jsm6qi0_/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: mir-allegro\n",
            "Successfully installed mir-allegro-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "\n",
        "# install nequip\n",
        "!rm -rf nequip\n",
        "!git clone --depth 1 \"https://github.com/mir-group/nequip.git\"\n",
        "!pip install nequip/\n",
        "\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "\n",
        "!rm -rf allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup PyTorch, check Nvidia GPUs/CUDA functioning."
      ],
      "metadata": {
        "id": "-sgXfyD6PbuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "print(\"-----------------------------\")\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "!which nvcc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZvdhYOaOp5I",
        "outputId": "c0f32c57-c1bc-4401-f1d3-d50821bb98ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "True\n",
            "1\n",
            "NVIDIA A100-SXM4-40GB\n",
            "-----------------------------\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Thu Dec 12 23:46:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              42W / 350W |      5MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Molecular Dynamics LAMMPS code.\n",
        "We will patch it with pair_allegro and build it with libtorch.\n",
        "\n",
        "We get Radial Function distribution of bond strength N-H using LAMMPS trajectory."
      ],
      "metadata": {
        "id": "xxDAm73fPpBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf lammps\n",
        "!git clone --depth 1 https://github.com/lammps/lammps.git\n",
        "# Stable Release (Simon )\n",
        "#!git clone -b stable_29Sep2021_update2 --depth 1 https://github.com/lammps/lammps.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJrTDEJJ8klU",
        "outputId": "5be41d69-7f62-482f-df7d-662ebe6205a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lammps'...\n",
            "remote: Enumerating objects: 14161, done.\u001b[K\n",
            "remote: Counting objects: 100% (14161/14161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10275/10275), done.\u001b[K\n",
            "remote: Total 14161 (delta 4827), reused 7861 (delta 3650), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14161/14161), 129.17 MiB | 27.93 MiB/s, done.\n",
            "Resolving deltas: 100% (4827/4827), done.\n",
            "Updating files: 100% (13507/13507), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compile it with MKL Library."
      ],
      "metadata": {
        "id": "-RRkCs9iQACg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mkl-include"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBHpSXUc8wsa",
        "outputId": "af3db00d-6ed2-42ca-c4ee-6fe3c924b8fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mkl-include\n",
            "  Downloading mkl_include-2025.0.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading mkl_include-2025.0.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.1/1.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mkl-include\n",
            "Successfully installed mkl-include-2025.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install CMake for builing pair_allegro with libtorch and LAMMPS."
      ],
      "metadata": {
        "id": "PLNlAaznQFRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
        "!sh ./cmake-3.23.1-linux-x86_64.sh --prefix=/usr/local --exclude-subdir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyifJb3q81U1",
        "outputId": "94801e7d-b0a4-4630-d44a-c67e2a6f1344"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-12 23:47:01--  https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241212%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241212T234701Z&X-Amz-Expires=300&X-Amz-Signature=fae2970ca928a52cb30b748e7ac4c463b5a8b0d8ce2e044ddd8a63c50d3f125e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-12 23:47:01--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241212%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241212T234701Z&X-Amz-Expires=300&X-Amz-Signature=fae2970ca928a52cb30b748e7ac4c463b5a8b0d8ce2e044ddd8a63c50d3f125e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46004365 (44M) [application/octet-stream]\n",
            "Saving to: ‘cmake-3.23.1-linux-x86_64.sh’\n",
            "\n",
            "cmake-3.23.1-linux- 100%[===================>]  43.87M   155MB/s    in 0.3s    \n",
            "\n",
            "2024-12-12 23:47:02 (155 MB/s) - ‘cmake-3.23.1-linux-x86_64.sh’ saved [46004365/46004365]\n",
            "\n",
            "CMake Installer Version: 3.23.1, Copyright (c) Kitware\n",
            "This is a self-extracting archive.\n",
            "The archive will be extracted to: /usr/local\n",
            "\n",
            "Using target directory: /usr/local\n",
            "Extracting, please wait...\n",
            "\n",
            "Unpacking finished successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install libtorch. This library should match with NVCC, Nvidia toolchain present in your system."
      ],
      "metadata": {
        "id": "R6AVXNbKQTZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf libtorch\n",
        "!wget wget https://download.pytorch.org/libtorch/cu124/libtorch-cxx11-abi-shared-with-deps-2.5.1%2Bcu124.zip\n",
        "!unzip libtorch-cxx11-abi-shared-with-deps-2.5.1+cu124.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L0y1M-c9HNG",
        "outputId": "5f3501cf-165b-4b5e-a31f-154ef3d44847"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/item.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/le.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/less.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/median.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/min.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/or.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/put.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/random.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/range.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/real.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/round.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/set.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/size.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/square.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/std.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/t.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/take.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/values.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/var.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/where.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_ops.h  \n",
            "   creating: libtorch/include/ATen/hip/\n",
            "   creating: libtorch/include/ATen/hip/impl/\n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPAllocatorMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPStreamMasqueradingAsCUDA.h  \n",
            "   creating: libtorch/include/ATen/mps/\n",
            "  inflating: libtorch/include/ATen/mps/EmptyTensor.h  \n",
            "  inflating: libtorch/include/ATen/mps/IndexKernels.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSAllocator.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSAllocatorInterface.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSDevice.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSEvent.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSGeneratorImpl.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSGuardImpl.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSHooks.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSProfiler.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSStream.h  \n",
            "   creating: libtorch/include/ATen/miopen/\n",
            "  inflating: libtorch/include/ATen/miopen/Descriptors.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Exceptions.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Handle.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Types.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Utils.h  \n",
            "  inflating: libtorch/include/ATen/miopen/miopen-wrapper.h  \n",
            "   creating: libtorch/include/ATen/detail/\n",
            "  inflating: libtorch/include/ATen/detail/AcceleratorHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/CUDAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/FunctionTraits.h  \n",
            "  inflating: libtorch/include/ATen/detail/HIPHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/IPUHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MAIAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MPSHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MTIAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/PrivateUse1HooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/XPUHooksInterface.h  \n",
            "   creating: libtorch/include/ATen/native/\n",
            "  inflating: libtorch/include/ATen/native/Activation.h  \n",
            "  inflating: libtorch/include/ATen/native/AdaptivePooling.h  \n",
            "  inflating: libtorch/include/ATen/native/AmpKernels.h  \n",
            "  inflating: libtorch/include/ATen/native/BatchLinearAlgebra.h  \n",
            "  inflating: libtorch/include/ATen/native/BinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/BucketizationUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/CPUBlas.h  \n",
            "  inflating: libtorch/include/ATen/native/CPUFallback.h  \n",
            "  inflating: libtorch/include/ATen/native/CanUse32BitIndexMath.h  \n",
            "  inflating: libtorch/include/ATen/native/ComplexHelper.h  \n",
            "  inflating: libtorch/include/ATen/native/CompositeRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/CompositeRandomAccessorCommon.h  \n",
            "  inflating: libtorch/include/ATen/native/ConvUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/ConvolutionMM3d.h  \n",
            "  inflating: libtorch/include/ATen/native/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/Cross.h  \n",
            "  inflating: libtorch/include/ATen/native/DilatedConvolutionUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/DispatchStub.h  \n",
            "  inflating: libtorch/include/ATen/native/Distance.h  \n",
            "  inflating: libtorch/include/ATen/native/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/Distributions.h  \n",
            "  inflating: libtorch/include/ATen/native/EmbeddingBag.h  \n",
            "  inflating: libtorch/include/ATen/native/Fill.h  \n",
            "  inflating: libtorch/include/ATen/native/ForeachUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/FractionalMaxPooling.h  \n",
            "  inflating: libtorch/include/ATen/native/FunctionOfAMatrixUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedAdagrad.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedAdam.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedSGD.h  \n",
            "  inflating: libtorch/include/ATen/native/GridSampler.h  \n",
            "  inflating: libtorch/include/ATen/native/GridSamplerUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Histogram.h  \n",
            "  inflating: libtorch/include/ATen/native/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/IndexingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Lerp.h  \n",
            "  inflating: libtorch/include/ATen/native/LinearAlgebra.h  \n",
            "  inflating: libtorch/include/ATen/native/LinearAlgebraUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/LossMulti.h  \n",
            "  inflating: libtorch/include/ATen/native/Math.h  \n",
            "  inflating: libtorch/include/ATen/native/MathBitFallThroughLists.h  \n",
            "  inflating: libtorch/include/ATen/native/MathBitsFallback.h  \n",
            "  inflating: libtorch/include/ATen/native/MaxPooling.h  \n",
            "  inflating: libtorch/include/ATen/native/NonEmptyUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/NonSymbolicBC.h  \n",
            "  inflating: libtorch/include/ATen/native/Normalization.h  \n",
            "  inflating: libtorch/include/ATen/native/Padding.h  \n",
            "  inflating: libtorch/include/ATen/native/PixelShuffle.h  \n",
            "  inflating: libtorch/include/ATen/native/PointwiseOps.h  \n",
            "  inflating: libtorch/include/ATen/native/Pool.h  \n",
            "  inflating: libtorch/include/ATen/native/Pow.h  \n",
            "  inflating: libtorch/include/ATen/native/RNN.h  \n",
            "  inflating: libtorch/include/ATen/native/RangeFactories.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceAllOps.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/ReductionType.h  \n",
            "  inflating: libtorch/include/ATen/native/Repeat.h  \n",
            "  inflating: libtorch/include/ATen/native/Resize.h  \n",
            "  inflating: libtorch/include/ATen/native/ResizeCommon.h  \n",
            "  inflating: libtorch/include/ATen/native/ScatterGatherChecks.h  \n",
            "  inflating: libtorch/include/ATen/native/SegmentReduce.h  \n",
            "  inflating: libtorch/include/ATen/native/SharedReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/SobolEngineOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Sorting.h  \n",
            "  inflating: libtorch/include/ATen/native/SortingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/SparseTensorUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/SpectralOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/StridedRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorAdvancedIndexing.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorAdvancedIndexingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorCompare.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorConversions.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorDimApply.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorFactories.h  \n",
            " extracting: libtorch/include/ATen/native/TensorIterator.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorIteratorDynamicCasting.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorProperties.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorShape.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorTransformations.h  \n",
            "  inflating: libtorch/include/ATen/native/TopKImpl.h  \n",
            "  inflating: libtorch/include/ATen/native/TransposeType.h  \n",
            "  inflating: libtorch/include/ATen/native/TriangularOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/TypeProperties.h  \n",
            "  inflating: libtorch/include/ATen/native/UnaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/Unfold2d.h  \n",
            "  inflating: libtorch/include/ATen/native/Unfold3d.h  \n",
            "  inflating: libtorch/include/ATen/native/UnfoldBackward.h  \n",
            "  inflating: libtorch/include/ATen/native/UpSample.h  \n",
            "  inflating: libtorch/include/ATen/native/batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/group_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/im2col.h  \n",
            "  inflating: libtorch/include/ATen/native/im2col_shape_check.h  \n",
            "  inflating: libtorch/include/ATen/native/layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/verbose_wrapper.h  \n",
            "  inflating: libtorch/include/ATen/native/vol2col.h  \n",
            "   creating: libtorch/include/ATen/native/cpu/\n",
            "  inflating: libtorch/include/ATen/native/cpu/AtomicAddFloat.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/CatKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/ChannelShuffleKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/CopyKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/DepthwiseConvKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/GridSamplerKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/IndexKernelUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Intrinsics.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/IsContiguous.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/LogAddExp.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Loops.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/MaxUnpoolKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/PixelShuffleKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Reduce.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/ReduceUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SampledAddmmKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SerialStackImpl.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SoftmaxKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SpmmReduceKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/StackKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/UpSampleKernelAVXAntialias.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/WeightNormKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/avx_mathfun.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/int_mm_kernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/mixed_data_type.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/moments_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/zmath.h  \n",
            "   creating: libtorch/include/ATen/native/cuda/\n",
            "  inflating: libtorch/include/ATen/native/cuda/Activation.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/BinaryInternal.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CompositeRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CuFFTPlanCache.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CuFFTUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Distributions.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/GridSampler.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/LaunchUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MiscUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Resize.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/RowwiseScaledMM.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ScanKernels.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Sort.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortStable.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Sorting.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorModeKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorTopK.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/jit_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/thread_constants.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CUDAJitLoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CUDALoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/DeviceSqrt.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/EmbeddingBackwardKernel.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ForeachFunctors.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ForeachMinMaxFunctors.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/GridSampler.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/JitLoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/KernelUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Loops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Math.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MemoryAccess.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MultiTensorApply.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Normalization.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/PersistentSoftmax.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Pow.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Randperm.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Reduce.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ScanUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortingCommon.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortingRadixSelect.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorModeKernel.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/UniqueCub.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/UpSample.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/block_reduce.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_amsgrad_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_utils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adamw_amsgrad_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adamw_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/im2col.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/reduction_template.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/vol2col.cuh  \n",
            "   creating: libtorch/include/ATen/native/mps/\n",
            "  inflating: libtorch/include/ATen/native/mps/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphSequoiaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphSonomaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphVenturaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/OperationUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/TensorFactory.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/UnaryConstants.h  \n",
            "   creating: libtorch/include/ATen/native/nested/\n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorBinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorMath.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorTransformerFunctions.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorTransformerUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorUtils.h  \n",
            "   creating: libtorch/include/ATen/native/quantized/\n",
            "  inflating: libtorch/include/ATen/native/quantized/AffineQuantizer.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/AffineQuantizerBase.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/ConvUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/FakeQuantAffine.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/PackedParams.h  \n",
            "   creating: libtorch/include/ATen/native/quantized/cpu/\n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/BinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/EmbeddingPackedParams.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/OnednnUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QnnpackUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QuantUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QuantizedOps.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/RuyUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/XnnpackUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/conv_serialization.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/fbgemm_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/init_qnnpack.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/qembeddingbag.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/qembeddingbag_prepack.h  \n",
            "   creating: libtorch/include/ATen/native/transformers/\n",
            "  inflating: libtorch/include/ATen/native/transformers/attention.h  \n",
            "  inflating: libtorch/include/ATen/native/transformers/sdp_utils_cpp.h  \n",
            "   creating: libtorch/include/ATen/native/utils/\n",
            "  inflating: libtorch/include/ATen/native/utils/Factory.h  \n",
            "  inflating: libtorch/include/ATen/native/utils/ParamUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/utils/ParamsHash.h  \n",
            "   creating: libtorch/include/ATen/quantized/\n",
            "  inflating: libtorch/include/ATen/quantized/QTensorImpl.h  \n",
            "  inflating: libtorch/include/ATen/quantized/Quantizer.h  \n",
            "   creating: libtorch/include/ATen/xpu/\n",
            "  inflating: libtorch/include/ATen/xpu/CachingHostAllocator.h  \n",
            "  inflating: libtorch/include/ATen/xpu/PinnedMemoryAllocator.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUContext.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUDevice.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUEvent.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUGeneratorImpl.h  \n",
            "   creating: libtorch/include/ATen/xpu/detail/\n",
            "  inflating: libtorch/include/ATen/xpu/detail/XPUHooks.h  \n",
            "   creating: libtorch/include/c10/\n",
            "   creating: libtorch/include/c10/xpu/\n",
            "  inflating: libtorch/include/c10/xpu/XPUCachingAllocator.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUDeviceProp.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUException.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUFunctions.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUMacros.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUStream.h  \n",
            "   creating: libtorch/include/c10/xpu/impl/\n",
            "  inflating: libtorch/include/c10/xpu/impl/XPUGuardImpl.h  \n",
            "   creating: libtorch/include/c10/macros/\n",
            "  inflating: libtorch/include/c10/macros/Export.h  \n",
            "  inflating: libtorch/include/c10/macros/Macros.h  \n",
            "  inflating: libtorch/include/c10/macros/cmake_macros.h  \n",
            "   creating: libtorch/include/c10/core/\n",
            "  inflating: libtorch/include/c10/core/Allocator.h  \n",
            "  inflating: libtorch/include/c10/core/AutogradState.h  \n",
            "  inflating: libtorch/include/c10/core/Backend.h  \n",
            "  inflating: libtorch/include/c10/core/CPUAllocator.h  \n",
            "  inflating: libtorch/include/c10/core/CachingDeviceAllocator.h  \n",
            "  inflating: libtorch/include/c10/core/CompileTimeFunctionPointer.h  \n",
            "  inflating: libtorch/include/c10/core/ConstantSymNodeImpl.h  \n",
            "  inflating: libtorch/include/c10/core/Contiguity.h  \n",
            "  inflating: libtorch/include/c10/core/CopyBytes.h  \n",
            "  inflating: libtorch/include/c10/core/DefaultDtype.h  \n",
            "  inflating: libtorch/include/c10/core/DefaultTensorOptions.h  \n",
            "  inflating: libtorch/include/c10/core/Device.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceArray.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceGuard.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceType.h  \n",
            "  inflating: libtorch/include/c10/core/DispatchKey.h  \n",
            "  inflating: libtorch/include/c10/core/DispatchKeySet.h  \n",
            "  inflating: libtorch/include/c10/core/DynamicCast.h  \n",
            "  inflating: libtorch/include/c10/core/Event.h  \n",
            "  inflating: libtorch/include/c10/core/GeneratorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/GradMode.h  \n",
            "  inflating: libtorch/include/c10/core/InferenceMode.h  \n",
            "  inflating: libtorch/include/c10/core/Layout.h  \n",
            "  inflating: libtorch/include/c10/core/MemoryFormat.h  \n",
            "  inflating: libtorch/include/c10/core/OptionalRef.h  \n",
            "  inflating: libtorch/include/c10/core/PyHandleCache.h  \n",
            "  inflating: libtorch/include/c10/core/QEngine.h  \n",
            "  inflating: libtorch/include/c10/core/QScheme.h  \n",
            "  inflating: libtorch/include/c10/core/RefcountedDeleter.h  \n",
            "  inflating: libtorch/include/c10/core/SafePyObject.h  \n",
            "  inflating: libtorch/include/c10/core/Scalar.h  \n",
            "  inflating: libtorch/include/c10/core/ScalarType.h  \n",
            "  inflating: libtorch/include/c10/core/ScalarTypeToTypeMeta.h  \n",
            "  inflating: libtorch/include/c10/core/Storage.h  \n",
            "  inflating: libtorch/include/c10/core/StorageImpl.h  \n",
            "  inflating: libtorch/include/c10/core/Stream.h  \n",
            "  inflating: libtorch/include/c10/core/StreamGuard.h  \n",
            "  inflating: libtorch/include/c10/core/SymBool.h  \n",
            "  inflating: libtorch/include/c10/core/SymFloat.h  \n",
            "  inflating: libtorch/include/c10/core/SymInt.h  \n",
            "  inflating: libtorch/include/c10/core/SymIntArrayRef.h  \n",
            "  inflating: libtorch/include/c10/core/SymNodeImpl.h  \n",
            "  inflating: libtorch/include/c10/core/SymbolicShapeMeta.h  \n",
            "  inflating: libtorch/include/c10/core/TensorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/TensorOptions.h  \n",
            "  inflating: libtorch/include/c10/core/UndefinedTensorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/WrapDimMinimal.h  \n",
            "  inflating: libtorch/include/c10/core/alignment.h  \n",
            "  inflating: libtorch/include/c10/core/thread_pool.h  \n",
            "   creating: libtorch/include/c10/core/impl/\n",
            "  inflating: libtorch/include/c10/core/impl/COW.h  \n",
            "  inflating: libtorch/include/c10/core/impl/COWDeleter.h  \n",
            "  inflating: libtorch/include/c10/core/impl/DeviceGuardImplInterface.h  \n",
            "  inflating: libtorch/include/c10/core/impl/FakeGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/core/impl/GPUTrace.h  \n",
            "  inflating: libtorch/include/c10/core/impl/HermeticPyObjectTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineDeviceGuard.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineEvent.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineStreamGuard.h  \n",
            "  inflating: libtorch/include/c10/core/impl/LocalDispatchKeySet.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PyInterpreter.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PyObjectSlot.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PythonDispatcherTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/SizesAndStrides.h  \n",
            "  inflating: libtorch/include/c10/core/impl/TorchDispatchModeTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/VirtualGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/core/impl/alloc_cpu.h  \n",
            "   creating: libtorch/include/c10/util/\n",
            "  inflating: libtorch/include/c10/util/AbortHandler.h  \n",
            "  inflating: libtorch/include/c10/util/AlignOf.h  \n",
            "  inflating: libtorch/include/c10/util/ApproximateClock.h  \n",
            "  inflating: libtorch/include/c10/util/Array.h  \n",
            "  inflating: libtorch/include/c10/util/ArrayRef.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16-inl.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16-math.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16.h  \n",
            "  inflating: libtorch/include/c10/util/Backtrace.h  \n",
            "  inflating: libtorch/include/c10/util/Bitset.h  \n",
            "  inflating: libtorch/include/c10/util/C++17.h  \n",
            "  inflating: libtorch/include/c10/util/CallOnce.h  \n",
            "  inflating: libtorch/include/c10/util/ConstexprCrc.h  \n",
            "  inflating: libtorch/include/c10/util/DeadlockDetection.h  \n",
            "  inflating: libtorch/include/c10/util/Deprecated.h  \n",
            "  inflating: libtorch/include/c10/util/DimVector.h  \n",
            "  inflating: libtorch/include/c10/util/DynamicCounter.h  \n",
            "  inflating: libtorch/include/c10/util/Exception.h  \n",
            "  inflating: libtorch/include/c10/util/ExclusivelyOwned.h  \n",
            "  inflating: libtorch/include/c10/util/ExclusivelyOwnedTensorTraits.h  \n",
            "  inflating: libtorch/include/c10/util/FbcodeMaps.h  \n",
            "  inflating: libtorch/include/c10/util/Flags.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fn-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fn.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fnuz-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fnuz.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2fnuz-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2fnuz.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_fnuz_cvt.h  \n",
            "  inflating: libtorch/include/c10/util/FunctionRef.h  \n",
            "  inflating: libtorch/include/c10/util/Gauge.h  \n",
            "  inflating: libtorch/include/c10/util/Half-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Half.h  \n",
            "  inflating: libtorch/include/c10/util/IdWrapper.h  \n",
            "  inflating: libtorch/include/c10/util/Lazy.h  \n",
            "  inflating: libtorch/include/c10/util/LeftRight.h  \n",
            "  inflating: libtorch/include/c10/util/Load.h  \n",
            "  inflating: libtorch/include/c10/util/Logging.h  \n",
            "  inflating: libtorch/include/c10/util/MathConstants.h  \n",
            "  inflating: libtorch/include/c10/util/MaybeOwned.h  \n",
            "  inflating: libtorch/include/c10/util/Metaprogramming.h  \n",
            "  inflating: libtorch/include/c10/util/NetworkFlow.h  \n",
            "  inflating: libtorch/include/c10/util/Optional.h  \n",
            "  inflating: libtorch/include/c10/util/OptionalArrayRef.h  \n",
            "  inflating: libtorch/include/c10/util/ParallelGuard.h  \n",
            "  inflating: libtorch/include/c10/util/Registry.h  \n",
            "  inflating: libtorch/include/c10/util/ScopeExit.h  \n",
            "  inflating: libtorch/include/c10/util/SmallBuffer.h  \n",
            "  inflating: libtorch/include/c10/util/SmallVector.h  \n",
            "  inflating: libtorch/include/c10/util/StringUtil.h  \n",
            "  inflating: libtorch/include/c10/util/Synchronized.h  \n",
            "  inflating: libtorch/include/c10/util/ThreadLocal.h  \n",
            "  inflating: libtorch/include/c10/util/ThreadLocalDebugInfo.h  \n",
            "  inflating: libtorch/include/c10/util/Type.h  \n",
            "  inflating: libtorch/include/c10/util/TypeCast.h  \n",
            "  inflating: libtorch/include/c10/util/TypeIndex.h  \n",
            "  inflating: libtorch/include/c10/util/TypeList.h  \n",
            "  inflating: libtorch/include/c10/util/TypeSafeSignMath.h  \n",
            "  inflating: libtorch/include/c10/util/TypeTraits.h  \n",
            "  inflating: libtorch/include/c10/util/Unicode.h  \n",
            "  inflating: libtorch/include/c10/util/UniqueVoidPtr.h  \n",
            "  inflating: libtorch/include/c10/util/Unroll.h  \n",
            "  inflating: libtorch/include/c10/util/WaitCounter.h  \n",
            "  inflating: libtorch/include/c10/util/accumulate.h  \n",
            "  inflating: libtorch/include/c10/util/bit_cast.h  \n",
            "  inflating: libtorch/include/c10/util/bits.h  \n",
            "  inflating: libtorch/include/c10/util/complex.h  \n",
            "  inflating: libtorch/include/c10/util/complex_math.h  \n",
            "  inflating: libtorch/include/c10/util/complex_utils.h  \n",
            "  inflating: libtorch/include/c10/util/copysign.h  \n",
            "  inflating: libtorch/include/c10/util/env.h  \n",
            "  inflating: libtorch/include/c10/util/flat_hash_map.h  \n",
            "  inflating: libtorch/include/c10/util/floating_point_utils.h  \n",
            "  inflating: libtorch/include/c10/util/generic_math.h  \n",
            "  inflating: libtorch/include/c10/util/hash.h  \n",
            "  inflating: libtorch/include/c10/util/int128.h  \n",
            "  inflating: libtorch/include/c10/util/intrusive_ptr.h  \n",
            "  inflating: libtorch/include/c10/util/irange.h  \n",
            "  inflating: libtorch/include/c10/util/llvmMathExtras.h  \n",
            "  inflating: libtorch/include/c10/util/logging_is_google_glog.h  \n",
            "  inflating: libtorch/include/c10/util/logging_is_not_google_glog.h  \n",
            "  inflating: libtorch/include/c10/util/numa.h  \n",
            "  inflating: libtorch/include/c10/util/order_preserving_flat_hash_map.h  \n",
            "  inflating: libtorch/include/c10/util/overloaded.h  \n",
            "  inflating: libtorch/include/c10/util/python_stub.h  \n",
            "  inflating: libtorch/include/c10/util/qint32.h  \n",
            "  inflating: libtorch/include/c10/util/qint8.h  \n",
            "  inflating: libtorch/include/c10/util/quint2x4.h  \n",
            "  inflating: libtorch/include/c10/util/quint4x2.h  \n",
            "  inflating: libtorch/include/c10/util/quint8.h  \n",
            "  inflating: libtorch/include/c10/util/safe_numerics.h  \n",
            "  inflating: libtorch/include/c10/util/signal_handler.h  \n",
            "  inflating: libtorch/include/c10/util/sparse_bitset.h  \n",
            "  inflating: libtorch/include/c10/util/ssize.h  \n",
            "  inflating: libtorch/include/c10/util/static_tracepoint.h  \n",
            "  inflating: libtorch/include/c10/util/static_tracepoint_elfx86.h  \n",
            "  inflating: libtorch/include/c10/util/strides.h  \n",
            "  inflating: libtorch/include/c10/util/string_utils.h  \n",
            "  inflating: libtorch/include/c10/util/string_view.h  \n",
            "  inflating: libtorch/include/c10/util/strong_type.h  \n",
            "  inflating: libtorch/include/c10/util/tempfile.h  \n",
            "  inflating: libtorch/include/c10/util/thread_name.h  \n",
            "  inflating: libtorch/include/c10/util/typeid.h  \n",
            "  inflating: libtorch/include/c10/util/win32-headers.h  \n",
            "   creating: libtorch/include/c10/cuda/\n",
            "  inflating: libtorch/include/c10/cuda/CUDAAllocatorConfig.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDACachingAllocator.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDADeviceAssertionHost.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAException.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAFunctions.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAGuard.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMacros.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMathCompat.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMiscFunctions.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAStream.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAAlgorithm.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDADeviceAssertion.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAGraphsC10Utils.h  \n",
            "  inflating: libtorch/include/c10/cuda/driver_api.h  \n",
            "   creating: libtorch/include/c10/cuda/impl/\n",
            "  inflating: libtorch/include/c10/cuda/impl/CUDAGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/cuda/impl/CUDATest.h  \n",
            "  inflating: libtorch/include/c10/cuda/impl/cuda_cmake_macros.h  \n",
            "   creating: libtorch/include/caffe2/\n",
            "   creating: libtorch/include/caffe2/serialize/\n",
            "  inflating: libtorch/include/caffe2/serialize/crc_alt.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/file_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/in_memory_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/inline_container.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/istream_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/read_adapter_interface.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/versions.h  \n",
            "  inflating: libtorch/include/clog.h  \n",
            "  inflating: libtorch/include/cpuinfo.h  \n",
            "  inflating: libtorch/include/dnnl_config.h  \n",
            "  inflating: libtorch/include/dnnl_debug.h  \n",
            "  inflating: libtorch/include/dnnl.h  \n",
            "  inflating: libtorch/include/dnnl_ocl.h  \n",
            "  inflating: libtorch/include/dnnl_sycl.h  \n",
            "  inflating: libtorch/include/dnnl_sycl_types.h  \n",
            "  inflating: libtorch/include/dnnl_threadpool.h  \n",
            "  inflating: libtorch/include/dnnl_types.h  \n",
            "  inflating: libtorch/include/dnnl_version.h  \n",
            "  inflating: libtorch/include/experiments-config.h  \n",
            "  inflating: libtorch/include/fp16.h  \n",
            "  inflating: libtorch/include/fxdiv.h  \n",
            "   creating: libtorch/include/kineto/\n",
            "  inflating: libtorch/include/kineto/AbstractConfig.h  \n",
            "  inflating: libtorch/include/kineto/ActivityProfilerInterface.h  \n",
            "  inflating: libtorch/include/kineto/ActivityTraceInterface.h  \n",
            "  inflating: libtorch/include/kineto/ActivityType.h  \n",
            "  inflating: libtorch/include/kineto/Config.h  \n",
            "  inflating: libtorch/include/kineto/ClientInterface.h  \n",
            "  inflating: libtorch/include/kineto/GenericTraceActivity.h  \n",
            "  inflating: libtorch/include/kineto/IActivityProfiler.h  \n",
            "  inflating: libtorch/include/kineto/ILoggerObserver.h  \n",
            "  inflating: libtorch/include/kineto/ITraceActivity.h  \n",
            "  inflating: libtorch/include/kineto/LoggingAPI.h  \n",
            "  inflating: libtorch/include/kineto/TraceSpan.h  \n",
            "  inflating: libtorch/include/kineto/ThreadUtil.h  \n",
            "  inflating: libtorch/include/kineto/libkineto.h  \n",
            "  inflating: libtorch/include/kineto/time_since_epoch.h  \n",
            "  inflating: libtorch/include/kineto/output_base.h  \n",
            "  inflating: libtorch/include/libshm.h  \n",
            "  inflating: libtorch/include/nnpack.h  \n",
            "  inflating: libtorch/include/psimd.h  \n",
            "  inflating: libtorch/include/pthreadpool.h  \n",
            "   creating: libtorch/include/pybind11/\n",
            "  inflating: libtorch/include/pybind11/attr.h  \n",
            "  inflating: libtorch/include/pybind11/buffer_info.h  \n",
            "  inflating: libtorch/include/pybind11/cast.h  \n",
            "  inflating: libtorch/include/pybind11/chrono.h  \n",
            "  inflating: libtorch/include/pybind11/common.h  \n",
            "  inflating: libtorch/include/pybind11/complex.h  \n",
            "  inflating: libtorch/include/pybind11/eigen.h  \n",
            "  inflating: libtorch/include/pybind11/embed.h  \n",
            "  inflating: libtorch/include/pybind11/eval.h  \n",
            "  inflating: libtorch/include/pybind11/functional.h  \n",
            "  inflating: libtorch/include/pybind11/gil.h  \n",
            "  inflating: libtorch/include/pybind11/gil_safe_call_once.h  \n",
            "  inflating: libtorch/include/pybind11/iostream.h  \n",
            "  inflating: libtorch/include/pybind11/numpy.h  \n",
            "  inflating: libtorch/include/pybind11/operators.h  \n",
            "  inflating: libtorch/include/pybind11/options.h  \n",
            "  inflating: libtorch/include/pybind11/pybind11.h  \n",
            "  inflating: libtorch/include/pybind11/pytypes.h  \n",
            "  inflating: libtorch/include/pybind11/stl.h  \n",
            "  inflating: libtorch/include/pybind11/stl_bind.h  \n",
            "  inflating: libtorch/include/pybind11/type_caster_pyobject_ptr.h  \n",
            "  inflating: libtorch/include/pybind11/typing.h  \n",
            "   creating: libtorch/include/pybind11/detail/\n",
            "  inflating: libtorch/include/pybind11/detail/class.h  \n",
            "  inflating: libtorch/include/pybind11/detail/common.h  \n",
            "  inflating: libtorch/include/pybind11/detail/descr.h  \n",
            "  inflating: libtorch/include/pybind11/detail/init.h  \n",
            "  inflating: libtorch/include/pybind11/detail/internals.h  \n",
            "  inflating: libtorch/include/pybind11/detail/type_caster_base.h  \n",
            "  inflating: libtorch/include/pybind11/detail/typeid.h  \n",
            "  inflating: libtorch/include/pybind11/detail/value_and_holder.h  \n",
            "   creating: libtorch/include/pybind11/eigen/\n",
            "  inflating: libtorch/include/pybind11/eigen/common.h  \n",
            "  inflating: libtorch/include/pybind11/eigen/matrix.h  \n",
            "  inflating: libtorch/include/pybind11/eigen/tensor.h  \n",
            "  inflating: libtorch/include/qnnpack_func.h  \n",
            "   creating: libtorch/include/tensorpipe/\n",
            "  inflating: libtorch/include/tensorpipe/tensorpipe.h  \n",
            "  inflating: libtorch/include/tensorpipe/config.h  \n",
            "  inflating: libtorch/include/tensorpipe/tensorpipe_cuda.h  \n",
            "  inflating: libtorch/include/tensorpipe/config_cuda.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/\n",
            "  inflating: libtorch/include/tensorpipe/channel/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/channel/error.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/basic/\n",
            "  inflating: libtorch/include/tensorpipe/channel/basic/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/cma/\n",
            "  inflating: libtorch/include/tensorpipe/channel/cma/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/mpt/\n",
            "  inflating: libtorch/include/tensorpipe/channel/mpt/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/xth/\n",
            "  inflating: libtorch/include/tensorpipe/channel/xth/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/common/\n",
            "  inflating: libtorch/include/tensorpipe/common/buffer.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/cpu_buffer.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/device.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/optional.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/cuda_buffer.h  \n",
            "   creating: libtorch/include/tensorpipe/core/\n",
            "  inflating: libtorch/include/tensorpipe/core/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/listener.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/message.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/pipe.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/\n",
            "   creating: libtorch/include/tensorpipe/transport/shm/\n",
            "  inflating: libtorch/include/tensorpipe/transport/shm/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/uv/\n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/factory.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/utility.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/error.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/ibv/\n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/factory.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/utility.h  \n",
            "   creating: libtorch/include/THC/\n",
            "  inflating: libtorch/include/THC/THCAtomics.cuh  \n",
            " extracting: libtorch/include/THC/THCDeviceUtils.cuh  \n",
            "   creating: libtorch/include/torch/\n",
            "  inflating: libtorch/include/torch/custom_class.h  \n",
            "  inflating: libtorch/include/torch/custom_class_detail.h  \n",
            "  inflating: libtorch/include/torch/library.h  \n",
            "  inflating: libtorch/include/torch/script.h  \n",
            "  inflating: libtorch/include/torch/extension.h  \n",
            "   creating: libtorch/include/torch/csrc/\n",
            "  inflating: libtorch/include/torch/csrc/Export.h  \n",
            "  inflating: libtorch/include/torch/csrc/CudaIPCTypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/DataLoader.h  \n",
            "  inflating: libtorch/include/torch/csrc/Device.h  \n",
            "  inflating: libtorch/include/torch/csrc/Dtype.h  \n",
            "  inflating: libtorch/include/torch/csrc/DynamicTypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/Exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/Generator.h  \n",
            "  inflating: libtorch/include/torch/csrc/Layout.h  \n",
            "  inflating: libtorch/include/torch/csrc/MemoryFormat.h  \n",
            "  inflating: libtorch/include/torch/csrc/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/PyInterpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/QScheme.h  \n",
            "  inflating: libtorch/include/torch/csrc/Size.h  \n",
            "  inflating: libtorch/include/torch/csrc/Storage.h  \n",
            "  inflating: libtorch/include/torch/csrc/StorageMethods.h  \n",
            "  inflating: libtorch/include/torch/csrc/StorageSharing.h  \n",
            "  inflating: libtorch/include/torch/csrc/Stream.h  \n",
            "  inflating: libtorch/include/torch/csrc/THConcat.h  \n",
            "  inflating: libtorch/include/torch/csrc/THP.h  \n",
            "  inflating: libtorch/include/torch/csrc/TypeInfo.h  \n",
            "  inflating: libtorch/include/torch/csrc/Types.h  \n",
            "  inflating: libtorch/include/torch/csrc/copy_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/itt_wrapper.h  \n",
            "  inflating: libtorch/include/torch/csrc/python_dimname.h  \n",
            "  inflating: libtorch/include/torch/csrc/python_headers.h  \n",
            "  inflating: libtorch/include/torch/csrc/serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/\n",
            "   creating: libtorch/include/torch/csrc/api/include/\n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/all.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/arg.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/cuda.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/enum.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/expanding_array.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/fft.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/imethod.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/linalg.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/mps.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nested.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/ordered_dict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/python.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/sparse.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/special.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/torch.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/xpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/version.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader_options.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/example.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/iterator.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/worker_exception.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateless.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/datasets/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/chunk.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/map.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/mnist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/shared.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/stateful.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/detail/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/data_shuttle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/queue.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/sequencers.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/samplers/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/custom_batch_request.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/distributed.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/random.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/sequential.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/stream.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/transforms/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/collate.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/lambda.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/stack.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/detail/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/detail/TensorDataContainer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/detail/static.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/cloneable.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/pimpl-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/pimpl.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/functional/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/vision.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/options/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/adaptive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/rnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformercoder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformerlayer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/vision.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/modules/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/adaptive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/common.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/rnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformercoder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformerlayer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any_module_holder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/functional.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/moduledict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/modulelist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/named_any.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/parameterdict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/parameterlist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/sequential.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/parallel/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/parallel/data_parallel.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/utils/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/clip_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/convert_parameters.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/rnn.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/optim/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adagrad.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adam.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adamw.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/lbfgs.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/optimizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/rmsprop.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/sgd.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/reduce_on_plateau_scheduler.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/step_lr.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/serialize/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/input-archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/output-archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/onnx/\n",
            "  inflating: libtorch/include/torch/csrc/onnx/back_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/onnx/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/onnx/onnx.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/api.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/collection.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/combined_traceback.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/containers.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/data_flow.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/events.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/kineto_shim.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/perf-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/perf.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/util.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/orchestration/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/python_tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/vulkan.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/standalone/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/execution_trace_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/itt_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/nvtx_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/privateuse1_observer.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/stubs/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/stubs/base.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/unwind/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/action.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/communicate.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/dwarf_enums.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/dwarf_symbolize_enums.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/eh_frame_hdr.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/fast_symbolizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/fde.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/lexer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/line_number_program.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/mem_file.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/range_table.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/sections.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwind.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwind_error.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwinder.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/python/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/combined_traceback.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/pybind.h  \n",
            "   creating: libtorch/include/torch/csrc/utils/\n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_numpy.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_new.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/byte_order.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/cpp_stacktraces.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/cuda_enabled.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/device_lazy_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/disable_torch_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/invalid_arguments.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/nested.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/numpy_stub.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/object_ptr.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/out_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pycfunction_helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pyobject_preservation.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_arg_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_dispatch.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_numbers.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_raii.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_scalars.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_strings.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_stub.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_symnode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_torch_function_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_tuples.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pythoncapi_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/schema_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/six.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/structseq.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_apply.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_dtypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_flatten.h  \n",
            " extracting: libtorch/include/torch/csrc/utils/tensor_layouts.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_memoryformats.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_qschemes.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/throughput_benchmark-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/throughput_benchmark.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/torch_dispatch_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/variadic.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/verbose.h  \n",
            "   creating: libtorch/include/torch/csrc/tensor/\n",
            "  inflating: libtorch/include/torch/csrc/tensor/python_tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/\n",
            "   creating: libtorch/include/torch/csrc/lazy/backend/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_data.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_device.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/lowering_context.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/cache.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/config.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/debug_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/dynamic_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/hash.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_builder.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_dump_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/lazy_graph_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/metrics.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/multi_wait.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/permutation_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/shape.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/shape_inference.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/thread_pool.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/trie.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/unique.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/util.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/internal_ops/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/internal_ops/ltc_ops.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/ops/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ops/arithmetic_ir_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ops/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/python/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/python/python_util.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/ts_backend/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/config.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/dynamic_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ir_builder.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/tensor_aten_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_autograd_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_backend_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_eager_fallback.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_lowering_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_node.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_node_lowering.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/FunctionsManual.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/InferenceMode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/VariableTypeUtils.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/anomaly_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/autograd_not_implemented_fallback.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/cpp_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/custom_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/edge.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/engine.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/forward_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/function_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/grad_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/graph_task.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/input_buffer.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/input_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/jit_decomp_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_kineto.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_legacy.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_anomaly_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_cpp_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_engine.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_enum_tag.h  \n",
            " extracting: libtorch/include/torch/csrc/autograd/python_fft_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_legacy_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_linalg_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_nested_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_nn_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_saved_variable_hooks.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_sparse_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_special_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_torch_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_variable_indexing.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/record_function_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/saved_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/saved_variable_hooks.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/symbolic.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/variable_info.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/functions/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/accumulate_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/basic_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/generated/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/python_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/python_return_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/VariableType.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/Functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/variable_factories.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/ViewFuncs.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/utils/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/error_messages.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/grad_layout_contract.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/lambda_post_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/python_arg_parsing.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/warnings.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/wrap_outputs.h  \n",
            "   creating: libtorch/include/torch/csrc/xpu/\n",
            "  inflating: libtorch/include/torch/csrc/xpu/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/xpu/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/xpu/Stream.h  \n",
            "   creating: libtorch/include/torch/csrc/cuda/\n",
            "  inflating: libtorch/include/torch/csrc/cuda/CUDAPluggableAllocator.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/GdsFile.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Stream.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/THCP.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/device_set.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/memory_snapshot.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/nccl.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/python_comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/python_nccl.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/\n",
            "   creating: libtorch/include/torch/csrc/distributed/c10d/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/CUDASymmetricMemory-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TraceUtils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/c10d.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/debug.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/error.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/logging.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/python_comm_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/socket.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Backend.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Backoff.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/DMAConnectivity.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/FakeProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/FileStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Functional.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/GlooDeviceFactory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/GroupRegistry.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/HashStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/NCCLUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/NanCheck.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ParamCommsUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/PrefixStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupGloo.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupMPI.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupUCC.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupWrapper.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/PyProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/RankLocal.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Store.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/SymmetricMemory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TCPStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TCPStoreBackend.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Types.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UCCTracing.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UCCUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UnixSockUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Utils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/WinSockUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Work.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/comm.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/default_comm_hooks.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/intra_node_comm.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/logger.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/reducer.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/reducer_timer.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/sequence_num.hpp  \n",
            "   creating: libtorch/include/torch/csrc/distributed/rpc/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/agent_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/message.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/py_rref.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_rpc_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback_no_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc_agent.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc_command_base.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_proto.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/tensorpipe_agent.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/tensorpipe_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/torchscript_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/unpickled_python_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/unpickled_python_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/\n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/context/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/context/container.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/context/context.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/functions/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/functions/recvrpc_backward.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/functions/sendrpc_backward.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/autograd_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rref_backward_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rref_backward_resp.h  \n",
            "   creating: libtorch/include/torch/csrc/dynamo/\n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cache_entry.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/compiled_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpp_shim.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpython_defs.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpython_includes.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/debug_macros.h  \n",
            " extracting: libtorch/include/torch/csrc/dynamo/eval_frame.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/extra_state.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/framelocals_mapping.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/guards.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/python_compiled_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/inductor_ops.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_runner/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner_cpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner_cuda.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/pybind.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_runtime/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/arrayref_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/device_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/model.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/model_container.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/scalar_to_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/thread_local.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/utils_cuda.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/mkldnn_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/oss_proxy_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/proxy_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/tensor_converter.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/c/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/c/shim.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/\n",
            "   creating: libtorch/include/torch/csrc/jit/api/\n",
            "  inflating: libtorch/include/torch/csrc/jit/api/compilation_unit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/function_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/method.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/object.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/serialization/\n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/callstack_debug_info_serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/source_range_serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/export.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/export_bytecode.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/flatbuffer_serializer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/flatbuffer_serializer_jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_read.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_source.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/mobile_bytecode_generated.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/onnx.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/pickle.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/pickler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/python_print.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/source_range_serialization_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/storage_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/type_name_uniquer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/unpickler.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/python/\n",
            "  inflating: libtorch/include/torch/csrc/jit/python/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/module_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/pybind_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_arg_flatten.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_custom_class.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_dict.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_ivalue.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_sugared_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_tree_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/script_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/update_graph_executor_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/utf8_decoding_ignore.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/mobile/\n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/code.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/file_format.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/flatbuffer_loader.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/frame.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/function.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import_data.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import_export_common.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/interpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/method.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/parse_bytecode.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/parse_operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/prim_ops_registery.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/profiler_edge.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/promoted_prim_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/quantization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/register_ops_common_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/upgrader_mobile.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/testing/\n",
            "  inflating: libtorch/include/torch/csrc/jit/testing/file_check.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/testing/hooks_for_testing.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/tensorexpr/\n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/block_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/bounds_inference.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/bounds_overlap.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cpp_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cpp_intrinsics.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cuda_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cuda_random.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/eval.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/expr.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions_core.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/fwd_decls.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/graph_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/half_support.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/hash_provider.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/intrinsic_symbols.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_cloner.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_mutator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_printer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_simplifier.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_verifier.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_visitor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/kernel.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/llvm_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/llvm_jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/loopnest.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/loopnest_randomization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/lowerings.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/mem_dependency_checker.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/reduction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/registerizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/stmt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/tensorexpr_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/unique_name_manager.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/var_substitutor.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/tensorexpr/operators/\n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/conv2d.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/matmul.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/misc.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/norm.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/pointwise.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/quantization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/reduction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/softmax.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/codegen/\n",
            "   creating: libtorch/include/torch/csrc/jit/codegen/cuda/\n",
            "  inflating: libtorch/include/torch/csrc/jit/codegen/cuda/interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/jit_log.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/jit_opt_limit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/resource_guard.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/backends/\n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_debug_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_detail.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_preprocess.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_resolver.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/add_if_then_else.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/annotate_warns.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/autocast.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/bailout_graph.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/batch_mm.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/canonicalize.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/canonicalize_graph_fuser_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/check_strict_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/clear_profiling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/clear_undefinedness.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/common_subexpression_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/concat_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/constant_pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/constant_propagation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/create_autodiff_subgraphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/create_functional_graphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/dead_code_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/decompose_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/device_type_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/dtype_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/eliminate_no_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/erase_number_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fixup_trace_scope_blocks.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fold_conv_bn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fold_linear_bn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/freeze_module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_concat_linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_conv_add_relu_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_conv_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_graph_optimizations.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_linear_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_linear_transpose.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_ops_to_mkldnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fuse_linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fuse_relu.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/graph_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/graph_rewrite_helper.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/guard_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/hoist_conv_packed_params.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_autodiff_subgraphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_fork_wait.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_forked_closures.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inliner.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inplace_check.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/insert_guards.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/integer_value_refinement.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lift_closures.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/liveness.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/loop_unrolling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_grad_of.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_graph.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_tuples.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/metal_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/mkldnn_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/mobile_optimizer_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/normalize_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/onednn_graph_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/onnx.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/pass_manager.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_alias_sensitive.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_dict_idioms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_list_idioms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_non_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/prepack_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/refine_tuple_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_expands.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_inplace_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_mutation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_redundant_profiles.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/replacement_of_old_operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/requires_grad_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/restore_mutation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/shape_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/specialize_autogradzero.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/subgraph_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_cache.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_runtime_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/tensorexpr_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/update_differentiable_graph_requires_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/value_refinement_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/variadic_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/vulkan_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/xnnpack_rewrite.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/quantization/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/dedup_module_uses.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/finalize.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/fusion_passes.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/helper.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/insert_observers.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/insert_quant_dequant.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/quantization_patterns.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/quantization_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/register_packed_params.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/utils/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/check_alias_annotation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/memory_dag.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/op_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/optimization_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/subgraph_utils.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/runtime/\n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/argument_spec.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/autodiff.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/calculate_necessary_args.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/custom_operator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/decomposition_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/decomposition_registry_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/exception_message.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_iterator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/instruction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/interpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/jit_exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/jit_trace.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/logging.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/operator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/operator_options.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/print_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/profiling_graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/profiling_record.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/register_ops_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/script_profile.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/serialized_shape_function_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/shape_function_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/simple_graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/slice_indices_adjust.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_script.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_shape_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_shape_registry_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/vararg_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/variable_tensor_list.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/ir/\n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/alias_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/attributes.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/graph_node_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/graph_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/ir_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/irparser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/named_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/node_hashing.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/scope.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/subgraph_matcher.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/type_hashing.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/frontend/\n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/source_range.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/lexer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/strtod.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parser_constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/function_schema_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parse_string_literal.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/schema_type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/error_report.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tree.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/builtin_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/canonicalize_modified_loop.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/concrete_module_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/convert_to_ssa.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/edit_distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/exit_transforms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/inline_loop_condition.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/ir_emitter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/mini_environment.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/name_mangler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/resolver.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/schema_matching.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/script_type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/source_ref.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/sugared_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tree_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/versioned_symbols.h  \n",
            "  inflating: libtorch/include/xnnpack.h  \n",
            "   creating: libtorch/share/\n",
            "   creating: libtorch/share/cmake/\n",
            "   creating: libtorch/share/cmake/ATen/\n",
            "  inflating: libtorch/share/cmake/ATen/ATenConfig.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Config.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUDAToolkit.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUSPARSELT.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUDSS.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindSYCLToolkit.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Targets.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Targets-release.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/public/\n",
            "  inflating: libtorch/share/cmake/Caffe2/public/cuda.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/xpu.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/glog.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/gflags.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/mkl.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/mkldnn.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/protobuf.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/utils.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/LoadHIP.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/FindCUDA.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/FindCUDNN.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/CMakeInitializeConfigs.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindPackageHandleStandardArgs.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindPackageMessage.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/make2cmake.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/parse_cubin.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/run_nvcc.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake  \n",
            "   creating: libtorch/share/cmake/Tensorpipe/\n",
            "  inflating: libtorch/share/cmake/Tensorpipe/TensorpipeTargets.cmake  \n",
            "  inflating: libtorch/share/cmake/Tensorpipe/TensorpipeTargets-release.cmake  \n",
            "   creating: libtorch/share/cmake/Torch/\n",
            "  inflating: libtorch/share/cmake/Torch/TorchConfigVersion.cmake  \n",
            "  inflating: libtorch/share/cmake/Torch/TorchConfig.cmake  \n",
            " extracting: libtorch/build-version  \n",
            "  inflating: libtorch/build-hash     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch is not compatible with latest python 3.12+, make sure your python, libtorch and nvidia toolchain are in sync as per documentation.\n",
        "Install older Python if needed and make it your default python."
      ],
      "metadata": {
        "id": "EDKlllzLQntg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch;\n",
        "print(torch.utils.cmake_prefix_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaqxNAP8XG7p",
        "outputId": "687ff94d-a133-444e-eb86-ce95be3c59b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/share/cmake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pair_allegro\n",
        "While traversing particles present in receptive field of 4 A, we query LAMMPS for nearest neighbors. This way we get all neighbors, not just neighbors belonging to a particular graph."
      ],
      "metadata": {
        "id": "P2WHCNtYRH7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf pair_allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/pair_allegro.git\n",
        "# Latest is at -- now same as above. Mon Dec 9, 2024\n",
        "#!git clone --depth 1 https://github.com/mir-group/pair_allegro/tree/multicut"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoEfje93DXt3",
        "outputId": "19966645-0cd6-47fd-d9c0-fed1e4db4072"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pair_allegro'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 28 (delta 0), reused 20 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (28/28), 195.32 KiB | 2.33 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAMMPS Patching to enable pair_allegro format."
      ],
      "metadata": {
        "id": "3dGN5mOgRqrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "!cd pair_allegro && bash patch_lammps.sh ../lammps/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaJUONWEDi--",
        "outputId": "7a565fbd-59e3-4c6b-f9dc-85b31d0d9430"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying files...\n",
            "Updating CMakeLists.txt...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build LAMMPS with Libtorch, enabling Allegro integration."
      ],
      "metadata": {
        "id": "BY7VFzQ_SL7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "# Use Python 3.11 Libtorch\n",
        "#!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j$(nproc)\n",
        "# Use downloaded 12.2 CUDA libtorch\n",
        "!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=/content/libtorch -DMKL_INCLUDE_DIR=`python -c \"import sysconfig;from pathlib import Path;print(Path(sysconfig.get_paths()[\\\"include\\\"]).parent)\"` && make -j$(nproc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYwb5zHP9BYo",
        "outputId": "e5e8eb88-50f9-444b-ab30-b1a9e30700b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "-- Running check for auto-generated files from make-based build system\n",
            "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n",
            "-- Found MPI: TRUE (found version \"3.1\")  \n",
            "-- Looking for C++ include omp.h\n",
            "-- Looking for C++ include omp.h - found\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\") found components: CXX \n",
            "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n",
            "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.37\") \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found GZIP: /usr/bin/gzip  \n",
            "-- Found FFMPEG: /usr/bin/ffmpeg  \n",
            "-- Looking for C++ include cmath\n",
            "-- Looking for C++ include cmath - found\n",
            "-- Generating style headers...\n",
            "-- Generating package headers...\n",
            "-- Generating lmpinstalledpkgs.h...\n",
            "-- Found Python3: /usr/local/bin/python (found version \"3.10.12\") found components: Interpreter \n",
            "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"11.0\")\n",
            "-- The following tools and libraries have been found and configured:\n",
            " * Git\n",
            " * MPI\n",
            " * OpenMP\n",
            " * JPEG\n",
            " * PNG\n",
            " * ZLIB\n",
            " * Python3\n",
            "\n",
            "-- <<< Build configuration >>>\n",
            "   LAMMPS Version:   20241119 a78aee5-modified\n",
            "   Operating System: Linux Ubuntu\" 22.04\n",
            "   CMake Version:    3.23.1\n",
            "   Build type:       RelWithDebInfo\n",
            "   Install path:     /root/.local\n",
            "   Generator:        Unix Makefiles using /usr/bin/gmake\n",
            "-- Enabled packages: <None>\n",
            "-- <<< Compilers and Flags: >>>\n",
            "-- C++ Compiler:     /usr/bin/c++\n",
            "      Type:          GNU\n",
            "      Version:       11.4.0\n",
            "      C++ Standard:  17\n",
            "      C++ Flags:     -O2 -g -DNDEBUG\n",
            "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=4;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n",
            "-- <<< Linker flags: >>>\n",
            "-- Executable name:  lmp\n",
            "-- Static library flags:    \n",
            "-- <<< MPI flags >>>\n",
            "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
            "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi\n",
            "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.2\") \n",
            "-- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.2.140\") \n",
            "-- Looking for C++ include pthread.h\n",
            "-- Looking for C++ include pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Caffe2: CUDA detected: 12.2\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 12.2\n",
            "-- Found Python: /usr/local/bin/python (found version \"3.10.12\") found components: Interpreter \n",
            "\u001b[33mCMake Warning at /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:140 (message):\n",
            "  Failed to compute shorthash for libnvrtc.so\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/share/cmake-3.23/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
            "  The package name passed to `find_package_handle_standard_args` (nvtx3) does\n",
            "  not match the name of the calling package (Caffe2).  This can lead to\n",
            "  problems in calling code that expects `find_package` result variables\n",
            "  (e.g., `_FOUND`) to follow a certain pattern.\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:174 (find_package_handle_standard_args)\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could NOT find nvtx3 (missing: nvtx3_dir) \n",
            "\u001b[33mCMake Warning at /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:180 (message):\n",
            "  Cannot find NVTX3, find old NVTX instead\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- USE_CUDNN is set to 0. Compiling without cuDNN support\n",
            "-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\n",
            "-- USE_CUDSS is set to 0. Compiling without cuDSS support\n",
            "-- USE_CUFILE is set to 0. Compiling without cuFile support\n",
            "-- Autodetected CUDA architecture(s):  8.0\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_80,code=sm_80\n",
            "-- Found Torch: /content/libtorch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/lammps/build\n",
            "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
            "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
            "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/command.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/exceptions.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
            "[  3%] Built target utils.h\n",
            "[  3%] Built target citeme.h\n",
            "[  3%] Built target comm.h\n",
            "[  3%] Built target compute.h\n",
            "[  3%] Built target command.h\n",
            "[  3%] Built target domain.h\n",
            "[  3%] Built target error.h\n",
            "[  3%] Built target dihedral.h\n",
            "[  3%] Built target exceptions.h\n",
            "[  3%] Built target fix.h\n",
            "[  3%] Built target force.h\n",
            "[  3%] Built target group.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
            "[  6%] Built target improper.h\n",
            "[  6%] Built target input.h\n",
            "[  6%] Built target info.h\n",
            "[  6%] Built target kspace.h\n",
            "[  6%] Built target lammps.h\n",
            "[  6%] Built target library.h\n",
            "[  6%] Built target lmppython.h\n",
            "[  6%] Built target lattice.h\n",
            "[  6%] Built target memory.h\n",
            "[  6%] Built target lmptype.h\n",
            "[  6%] Built target modify.h\n",
            "[  6%] Built target neighbor.h\n",
            "-- Git Directory: /content/lammps/.git\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/core.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/format.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/platform.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
            "[  8%] Built target angle.h\n",
            "[  8%] Built target atom.h\n",
            "[  8%] Built target variable.h\n",
            "[  8%] Built target fmt_core.h\n",
            "[  8%] Built target bond.h\n",
            "[  8%] Built target neigh_list.h\n",
            "[  8%] Built target fmt_format.h\n",
            "[  8%] Built target output.h\n",
            "[  8%] Built target pair.h\n",
            "[  8%] Built target platform.h\n",
            "[  8%] Built target pointers.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
            "-- Generating lmpgitversion.h...\n",
            "[  9%] Built target region.h\n",
            "[  9%] Built target timer.h\n",
            "[  9%] Built target universe.h\n",
            "[  9%] Built target update.h\n",
            "[  9%] Built target gitversion\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_write.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_allegro.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:535\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/fmt/base.h:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/fmt/format.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/pointers.h:33\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/atom_vec.h:17\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/atom_vec.cpp:14\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvirtual void LAMMPS_NS::AtomVec::write_data_restricted_to_general()\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/lammps/src/atom_vec.cpp:2272:21\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin_memcpy(void*, const void*, long unsigned int)\u001b[m\u001b[K’ specified bound between 18446744056529682432 and 18446744073709551592 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "   30 | \u001b[01;35m\u001b[K                                 __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
            "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_count_type.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_grid.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_write.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid_vtk.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_grid.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_bond_history.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_pair.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_langevin.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_atom.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_global.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_local.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_table.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid2d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid3d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/label_map.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_deprecated.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin_ghost.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi_old.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq_ghost.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_bin.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_nsq.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_trim.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_ghost_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi_old.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_allegro.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_molecular.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/platform.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_ellipsoid.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_id.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_image.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_mol.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:820:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  820 |         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n",
            "      |         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
            "[ 98%] Built target lammps\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
            "[100%] Built target lmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download extxyz format datasets and configuration files.\n",
        "Allegro runs with npz and extxyz formats. SPICE is in HDF5 format. ASE toolchain can read and write files in multiple formats. h5py, ase, ase.io, scipy.spatial are used for this.\n",
        "\n",
        "If running TorchMd-Net, please follow procedure outlined in five-et, by creating processed version of SPICE dataset.\n",
        "\n",
        "All scripts and dataset in extxyz format for DES from SPICE is at https://github.com/v365747/GraphCoarseningGNN"
      ],
      "metadata": {
        "id": "8pNLEQu_SY_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -rf Si_data\n",
        "!rm -rf DES_Config\n",
        "# download DES data of SPICE modal from Vinay's google drive.\n",
        "# Simon Batzner's data for Si-Si lattice\n",
        "!gdown --folder --id --no-cookies https://drive.google.com/drive/folders/1FEwF4i8IDHGmAIQ3RilA0jG9_lEX4Yk0?usp=sharing\n",
        "# DES data from SPICE <downloading it to Si_data.\n",
        "\n",
        "!gdown  --folder --id --no-cookies https://drive.google.com/drive/folders/179oeQ9zSlMp_7FKmO2i8rtcZqqPMJN9n?usp=sharing\n",
        "# Allegro modal SPICE DES Dataset config yaml files.\n",
        "!gdown --folder --id --no-cookies https://drive.google.com/drive/folders/1U57qI1v5x26TnH66XZ1hq9cvdaQEqYdR?usp=sharing"
      ],
      "metadata": {
        "id": "eYnhz_xH-P36"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Si_data\n",
        "!ls DES_Config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35kPXyy3-2SY",
        "outputId": "dcbf9d76-75b3-43f0-d266-6ac15e470434"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DES.xyz  sitraj.xyz\n",
            "DES_tutorial_nequip.yaml  DES_tutorial.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If not downloading files and using your own custom datasets, you can check:\n",
        "\n",
        "Edit /content/allegro/configs/tutorial.yaml removing optimizer_params (unused)*Comment out lines 94-100 of /content/allegro/configs/tutorial.yaml and default_dtype: float64 (line7) and change Line 14 ForceOutput to StressForceOutput*\n",
        "\n",
        "Don't activate conda as it will mess everything up!."
      ],
      "metadata": {
        "id": "7A4lfjdmA1hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!conda info --envs\n",
        "# conda, nglview is bad for allegro as it downgrades environment\n",
        "#!conda init\n",
        "#!conda deactivate"
      ],
      "metadata": {
        "id": "03iY1OAewWMD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copy downloaded configuration files for Allegro run"
      ],
      "metadata": {
        "id": "TUyxLfGaVpM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./results\n",
        "#!nequip-train allegro/configs/tutorial.yaml\n",
        "# Copy SPICE DES extxyz config file, to configs\n",
        "!cp DES_Config/DES_tutorial.yaml allegro/configs/"
      ],
      "metadata": {
        "id": "JL4h2O5Bqwzv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training\n",
        "\n",
        "We train for 50-10 epochs, and keep an eye at validation_f_mae (force components validation error normalized) and normalized validation and training errors for potential energies.\n",
        "\n",
        "First box data using Lattice (using extxyz)."
      ],
      "metadata": {
        "id": "yBl_DBucVxm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install extxyz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCF_4OJvqhaa",
        "outputId": "6fe2b1d8-2c0c-49c3-f3c4-0993836afdaf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting extxyz\n",
            "  Downloading extxyz-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (173 bytes)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from extxyz) (1.26.4)\n",
            "Collecting pyleri>=1.3.3 (from extxyz)\n",
            "  Downloading pyleri-1.4.3.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ase>=3.17 in /usr/local/lib/python3.10/dist-packages (from extxyz) (3.23.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ase>=3.17->extxyz) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase>=3.17->extxyz) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (4.55.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase>=3.17->extxyz) (1.17.0)\n",
            "Downloading extxyz-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyleri\n",
            "  Building wheel for pyleri (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyleri: filename=pyleri-1.4.3-py3-none-any.whl size=31753 sha256=2bc76332ce653d2c17ccb97cb00e8f789b0e042f85bee2e05f2baf64b20df6d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/39/35/b5e25f9c198c5651b57088f36a803dc07c629a04798a82b48a\n",
            "Successfully built pyleri\n",
            "Installing collected packages: pyleri, extxyz\n",
            "Successfully installed extxyz-0.1.3 pyleri-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ase.io import read, write, xyz\n",
        "from ase.io.extxyz import read_extxyz, write_extxyz\n",
        "#from extxyz import read #,iread, write, ExtXYZTrajectoryWriter\n",
        "import numpy as np\n",
        "from numpy import load\n",
        "from ase.visualize import view\n",
        "\n",
        "# define a Lattics (BOX for data)\n",
        "des_prod = np.array([10.0, 10.0, 10.0])\n",
        "example_atoms = read('./Si_data/DES.xyz', index=\":\")\n",
        "\n",
        "for frame in example_atoms:\n",
        "  frame.set_cell(des_prod)\n",
        "  #example_atoms[i].set_pbc([True, True, True])\n",
        "  frame.wrap()\n",
        "write_extxyz('./Si_data/DES_L.xyz', example_atoms)"
      ],
      "metadata": {
        "id": "p9MdfsGJoG2X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple molecules Molecular Dynamics\n",
        "This step takes about 10 mins. We only run training for 100 epochs. To get most acurate run, Allegro/Nequip teams have tun training for 1 week on Single A100 GPU.\n",
        "\n",
        "Trained models are not publicly available."
      ],
      "metadata": {
        "id": "nYbFomRC2OKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Allegro\n",
        "!rm -rf results\n",
        "!nequip-train allegro/configs/DES_tutorial.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdlT2wo1-6C-",
        "outputId": "24221e9b-7c2e-47ab-8247-ecdffe98dd87"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241212_235036-X21zT9vqtLkVz-SsuUv06vqAFApxL5Q4UxC7Uc57up8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDES\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-moose-739946801467655344/allegro-tutorial?apiKey=c9642a9e6f095d80e85d42553309fed789420d69\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-moose-739946801467655344/allegro-tutorial/runs/X21zT9vqtLkVz-SsuUv06vqAFApxL5Q4UxC7Uc57up8?apiKey=c9642a9e6f095d80e85d42553309fed789420d69\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_ase_dataset.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  datas = [torch.load(d) for d in datas]\n",
            "Loaded data: Batch(atomic_numbers=[239550, 1], batch=[239550], cell=[18700, 3, 3], edge_cell_shift=[766582, 3], edge_index=[2, 766582], forces=[239550, 3], pbc=[18700, 3], pos=[239550, 3], ptr=[18701], total_energy=[18700, 1])\n",
            "    processed data size: ~45.49 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(18700)...\n",
            "Replace string dataset_per_atom_total_energy_mean to -35.92002647258334\n",
            "Atomic outputs are scaled by: [H, C, N, O, F, P, S, Cl, Br, I: None], shifted by [H, C, N, O, F, P, S, Cl, Br, I: -35.920026].\n",
            "Replace string dataset_forces_rms to 0.02045545683041229\n",
            "Initially outputs are globally scaled by: 0.02045545683041229, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 37928\n",
            "Number of trainable weights: 37928\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0     2     6.94e+05        0.973     6.94e+05        0.015       0.0202          244           16\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    4.468    0.002        0.915     5.92e+05     5.92e+05       0.0141       0.0196          197         14.3\n",
            "Wall time: 4.468678039999759\n",
            "! Best model        0 591741.723\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10     6.59e+05         1.51     6.59e+05        0.019       0.0251          166         16.6\n",
            "      1    20     1.45e+06        0.767     1.45e+06       0.0134       0.0179          344         24.6\n",
            "      1    30     1.26e+05        0.629     1.26e+05       0.0114       0.0162         87.2         7.27\n",
            "      1    40     1.26e+05         1.21     1.26e+05       0.0179       0.0225         29.1         7.27\n",
            "      1    50     6.59e+05         1.01     6.59e+05       0.0165       0.0205          266         16.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1     2     6.94e+05        0.982     6.94e+05        0.015       0.0203          243           16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   13.716    0.002        0.976     2.79e+06     2.79e+06       0.0145       0.0206          268         23.5\n",
            "! Validation          1   13.716    0.002        0.923     5.91e+05     5.91e+05       0.0142       0.0197          197         14.2\n",
            "Wall time: 13.717160186000001\n",
            "! Best model        1 591308.610\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10     6.58e+05         1.11     6.58e+05       0.0173       0.0216          266         16.6\n",
            "      2    20     1.24e+06         1.66     1.24e+06       0.0204       0.0264          342         22.8\n",
            "      2    30     9.68e+03         1.05     9.68e+03       0.0159        0.021         22.1         2.01\n",
            "      2    40     5.39e+06         3.82     5.39e+06       0.0297         0.04          713         47.5\n",
            "      2    50     2.85e+06         3.22     2.85e+06       0.0297       0.0367          449         34.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2     2     6.92e+05         2.35     6.92e+05       0.0251       0.0314          243           16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2   15.057    0.002         1.53     2.79e+06     2.79e+06       0.0194       0.0255          268         23.4\n",
            "! Validation          2   15.057    0.002         2.19      5.9e+05      5.9e+05       0.0243       0.0303          197         14.2\n",
            "Wall time: 15.057359700999768\n",
            "! Best model        2 590337.054\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10      9.8e+03          2.1     9.79e+03       0.0234       0.0296         22.3         2.02\n",
            "      3    20     2.85e+06         3.07     2.85e+06        0.026       0.0358          449         34.5\n",
            "      3    30     3.02e+05         5.34     3.02e+05       0.0362       0.0473           90         11.2\n",
            "      3    40     6.54e+05         9.38     6.54e+05       0.0558       0.0626          165         16.5\n",
            "      3    50     1.19e+06         14.5     1.19e+06       0.0635       0.0779          223         22.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3     2     6.91e+05         7.85     6.91e+05        0.046       0.0573          243         15.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3   16.385    0.002         5.72     2.79e+06     2.79e+06       0.0395       0.0497          268         23.4\n",
            "! Validation          3   16.385    0.002         7.39     5.89e+05     5.89e+05       0.0445       0.0557          197         14.2\n",
            "Wall time: 16.38537892899967\n",
            "! Best model        3 588880.673\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10     1.35e+06         31.1     1.35e+06       0.0939        0.114          190         23.8\n",
            "      4    20     8.03e+05         58.2     8.03e+05        0.129        0.156          385         18.3\n",
            "      4    30     1.23e+06         67.1     1.23e+06        0.146        0.168          340         22.7\n",
            "      4    40     2.31e+05          138     2.31e+05        0.185         0.24          128         9.83\n",
            "      4    50     4.51e+05          214      4.5e+05        0.249        0.299          192         13.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4     2     6.83e+05           80     6.83e+05        0.147        0.183          241         15.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4   17.707    0.002         72.4     2.79e+06     2.79e+06        0.135        0.177          267         23.4\n",
            "! Validation          4   17.707    0.002         74.7     5.83e+05     5.83e+05        0.141        0.177          196         14.2\n",
            "Wall time: 17.707866098000068\n",
            "! Best model        4 583239.770\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10      1.2e+06          664      1.2e+06        0.444        0.527          202         22.4\n",
            "      5    20     7.46e+05     2.58e+03     7.43e+05        0.895         1.04          212         17.6\n",
            "      5    30     1.28e+06     2.57e+03     1.27e+06        0.859         1.04          185         23.1\n",
            "      5    40     2.37e+05     1.18e+03     2.36e+05        0.553        0.704          149         9.93\n",
            "      5    50     9.37e+04     1.22e+03     9.25e+04         0.61        0.716          106         6.22\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5     2     6.59e+05          951     6.59e+05        0.506        0.631          237         15.5\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5   19.033    0.002     1.26e+03     2.78e+06     2.78e+06        0.551        0.724          265         23.2\n",
            "! Validation          5   19.033    0.002          868     5.65e+05     5.65e+05        0.478        0.604          193           14\n",
            "Wall time: 19.03328486600003\n",
            "! Best model        5 565449.565\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10     1.14e+06     3.33e+03     1.14e+06         0.96         1.18          196         21.8\n",
            "      6    20     1.18e+06     1.15e+04     1.16e+06         1.79         2.19          177         22.1\n",
            "      6    30     2.98e+05     1.13e+04     2.87e+05         1.71         2.17          142           11\n",
            "      6    40     6.77e+06      9.1e+03     6.76e+06         1.48         1.95          532         53.2\n",
            "      6    50     1.05e+06     6.16e+03     1.05e+06          1.1         1.61          272         20.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6     2     6.13e+05     5.09e+03     6.08e+05         1.17         1.46          227         14.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6   20.348    0.002     6.33e+03     2.76e+06     2.77e+06         1.24         1.64          259         22.9\n",
            "! Validation          6   20.348    0.002     4.55e+03     5.27e+05     5.31e+05         1.09         1.38          187         13.5\n",
            "Wall time: 20.349037813999985\n",
            "! Best model        6 531461.932\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10     6.36e+05     7.34e+03     6.29e+05         1.27         1.75          227         16.2\n",
            "      7    20      4.4e+05     1.21e+04     4.28e+05         1.68         2.25          228         13.4\n",
            "      7    30     4.72e+05     2.21e+04      4.5e+05          2.4         3.04          220         13.7\n",
            "      7    40     7.42e+04     1.29e+04     6.13e+04         2.03         2.33         20.3         5.06\n",
            "      7    50     3.77e+05     1.65e+04      3.6e+05         1.97         2.62          233         12.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7     2     5.57e+05     1.11e+04     5.46e+05         1.72         2.15          214           14\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7   21.680    0.002     1.26e+04     2.73e+06     2.75e+06         1.77         2.32          251         22.3\n",
            "! Validation          7   21.680    0.002     9.83e+03     4.81e+05      4.9e+05          1.6         2.03          179           13\n",
            "Wall time: 21.681018927999958\n",
            "! Best model        7 490389.512\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10     7.38e+04     1.13e+04     6.25e+04         1.68         2.17         40.9         5.11\n",
            "      8    20     6.24e+05     9.55e+03     6.14e+05         1.63            2          289           16\n",
            "      8    30     8.84e+05     1.51e+04     8.69e+05            2         2.51          191         19.1\n",
            "      8    40     9.05e+05     2.45e+04     8.81e+05         2.47          3.2          154         19.2\n",
            "      8    50     1.26e+06     1.49e+04     1.25e+06         1.73          2.5          297         22.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8     2     5.15e+05     1.47e+04        5e+05         1.97         2.48          205         13.4\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8   23.013    0.002     1.45e+04     2.72e+06     2.73e+06         1.87         2.49          247           22\n",
            "! Validation          8   23.013    0.002     1.36e+04     4.47e+05      4.6e+05         1.89         2.39          173         12.5\n",
            "Wall time: 23.013656701999935\n",
            "! Best model        8 460466.063\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10     5.75e+05      1.8e+04     5.57e+05         2.17         2.75          275         15.3\n",
            "      9    20     5.66e+06     4.47e+03     5.65e+06        0.915         1.37          729         48.6\n",
            "      9    30     3.32e+05     7.38e+04     2.58e+05         3.24         5.56          104         10.4\n",
            "      9    40     7.11e+05      2.8e+04     6.83e+05         2.65         3.43          355         16.9\n",
            "      9    50     8.35e+05     3.26e+04     8.02e+05         2.81         3.69          165         18.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9     2     4.83e+05     1.88e+04     4.64e+05         2.19         2.81          197         12.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9   24.379    0.002      2.1e+04      2.7e+06     2.73e+06         2.22         3.03          241         21.6\n",
            "! Validation          9   24.379    0.002     1.82e+04     4.21e+05     4.39e+05         2.14         2.77          169         12.2\n",
            "Wall time: 24.38020610500007\n",
            "! Best model        9 439269.923\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10      5.8e+06     1.06e+04     5.79e+06         1.37         2.11          738         49.2\n",
            "     10    20     3.42e+06     3.15e+04     3.39e+06         2.75         3.63          490         37.7\n",
            "     10    30     4.73e+05     2.75e+04     4.45e+05         2.72         3.39          164         13.6\n",
            "     10    40     8.19e+05     3.67e+04     7.82e+05         2.93         3.92          163         18.1\n",
            "     10    50     2.58e+05     6.12e+04     1.97e+05         3.42         5.06          127         9.08\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10     2     4.44e+05     2.74e+04     4.16e+05         2.63         3.39          186         12.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10   25.770    0.002     3.46e+04     2.68e+06     2.72e+06         2.86         3.88          233         21.1\n",
            "! Validation         10   25.770    0.002     2.76e+04     3.87e+05     4.15e+05         2.61          3.4          162         11.7\n",
            "Wall time: 25.770621541999844\n",
            "! Best model       10 414727.884\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10     9.11e+05     3.57e+04     8.75e+05         3.23         3.87          268         19.1\n",
            "     11    20     1.86e+06      1.9e+04     1.84e+06         2.29         2.82          444         27.7\n",
            "     11    30     4.11e+05     1.98e+04     3.91e+05         2.23         2.88          205         12.8\n",
            "     11    40      6.4e+07     1.03e+03      6.4e+07        0.511        0.657          655          164\n",
            "     11    50     8.78e+05     2.37e+04     8.54e+05         2.38         3.15          284         18.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11     2     4.38e+05     2.74e+04      4.1e+05          2.6         3.38          184           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11   27.113    0.002     2.92e+04     2.69e+06     2.72e+06         2.56         3.55          236         21.2\n",
            "! Validation         11   27.113    0.002     2.85e+04     3.83e+05     4.12e+05         2.61         3.45          161         11.6\n",
            "Wall time: 27.1132869879998\n",
            "! Best model       11 411632.699\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10     1.34e+05     7.35e+04     6.03e+04         4.23         5.54         80.4         5.02\n",
            "     12    20     3.77e+05     3.28e+04     3.45e+05          2.9          3.7          120           12\n",
            "     12    30     2.88e+05     4.69e+04     2.41e+05         3.32         4.43          191           10\n",
            "     12    40     7.37e+05     5.81e+04     6.79e+05         3.83         4.93          135         16.9\n",
            "     12    50        1e+06     2.32e+04      9.8e+05         2.75         3.12          223         20.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12     2     4.28e+05     2.98e+04     3.98e+05         2.72         3.53          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12   28.454    0.002     3.24e+04     2.68e+06     2.71e+06         2.78         3.77          233         21.1\n",
            "! Validation         12   28.454    0.002     3.13e+04     3.74e+05     4.06e+05         2.75         3.61          159         11.5\n",
            "Wall time: 28.45515800899966\n",
            "! Best model       12 405716.153\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10      1.8e+06     1.47e+04     1.78e+06            2         2.48          437         27.3\n",
            "     13    20     8.42e+05      3.2e+04      8.1e+05         2.64         3.66          166         18.4\n",
            "     13    30     5.42e+04     1.35e+04     4.07e+04         1.97         2.37         16.5         4.13\n",
            "     13    40     2.92e+05     3.96e+04     2.53e+05         2.93         4.07          195         10.3\n",
            "     13    50     3.36e+06     2.35e+04     3.34e+06         2.35         3.14          486         37.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13     2     4.27e+05     2.87e+04     3.98e+05         2.64         3.47          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13   29.794    0.002     2.95e+04     2.68e+06     2.71e+06         2.56         3.61          234         21.1\n",
            "! Validation         13   29.794    0.002      3.1e+04     3.75e+05     4.06e+05         2.69         3.59          159         11.5\n",
            "Wall time: 29.79493694499979\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10     4.87e+05      2.5e+04     4.62e+05         2.67         3.24          195         13.9\n",
            "     14    20     4.82e+04     1.68e+04     3.14e+04         2.28         2.65         61.6         3.62\n",
            "     14    30     2.84e+05     4.54e+04     2.39e+05          3.2         4.36          190         9.99\n",
            "     14    40     4.66e+05     5.32e+04     4.13e+05         3.29         4.72          184         13.1\n",
            "     14    50     3.58e+05     3.52e+04     3.23e+05         3.06         3.84          256         11.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14     2     4.25e+05     2.93e+04     3.95e+05         2.65          3.5          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14   31.110    0.002     3.09e+04     2.68e+06     2.71e+06         2.66         3.68          233           21\n",
            "! Validation         14   31.110    0.002     3.18e+04     3.73e+05     4.05e+05         2.72         3.64          158         11.5\n",
            "Wall time: 31.111116606999985\n",
            "! Best model       14 404555.749\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10     2.07e+07     2.86e+03     2.07e+07        0.741         1.09          466         93.1\n",
            "     15    20     7.02e+06     1.83e+04        7e+06         2.05         2.77          541         54.1\n",
            "     15    30     5.24e+04     2.25e+04     2.99e+04         2.39         3.07         28.3         3.53\n",
            "     15    40     4.09e+05     1.86e+04     3.91e+05         2.18         2.79          166         12.8\n",
            "     15    50     7.22e+05     6.46e+04     6.58e+05         3.95          5.2          133         16.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15     2     4.26e+05     2.87e+04     3.97e+05         2.63         3.47          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15   32.434    0.002     2.92e+04     2.68e+06     2.71e+06         2.63         3.58          233         21.1\n",
            "! Validation         15   32.434    0.002     3.13e+04     3.74e+05     4.05e+05         2.69         3.61          158         11.5\n",
            "Wall time: 32.43521306100001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10     4.25e+05     2.14e+04     4.04e+05         2.33         2.99          169           13\n",
            "     16    20     4.75e+05     7.46e+04        4e+05         4.49         5.59          155         12.9\n",
            "     16    30     7.16e+05     5.65e+04     6.59e+05         3.81         4.86          166         16.6\n",
            "     16    40      5.6e+05     8.35e+04     4.76e+05          4.2         5.91          212         14.1\n",
            "     16    50     2.09e+07     3.69e+03     2.09e+07        0.833         1.24          467         93.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16     2     4.22e+05     2.99e+04     3.92e+05         2.67         3.54          180         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16   33.753    0.002     3.24e+04     2.67e+06      2.7e+06         2.75         3.77          231         20.9\n",
            "! Validation         16   33.753    0.002     3.26e+04      3.7e+05     4.03e+05         2.74         3.69          157         11.4\n",
            "Wall time: 33.75335994099987\n",
            "! Best model       16 403070.251\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10     4.44e+05     3.72e+04     4.07e+05          3.2         3.94          183         13.1\n",
            "     17    20     3.06e+05     7.45e+04     2.31e+05         4.16         5.58          167         9.83\n",
            "     17    30     7.66e+05     4.17e+04     7.24e+05         3.32         4.18          261         17.4\n",
            "     17    40      4.7e+05     4.05e+04     4.29e+05         3.21         4.11          241         13.4\n",
            "     17    50     6.22e+05     1.02e+05      5.2e+05         4.91         6.53          118         14.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17     2     4.11e+05     3.37e+04     3.77e+05         2.82         3.75          176         11.5\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17   35.080    0.002     4.08e+04     2.66e+06      2.7e+06         3.09         4.26          227         20.7\n",
            "! Validation         17   35.080    0.002     3.68e+04      3.6e+05     3.97e+05         2.89         3.92          155         11.3\n",
            "Wall time: 35.080289726000046\n",
            "! Best model       17 396820.552\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10     7.94e+05     3.73e+04     7.57e+05         2.97         3.95          160         17.8\n",
            "     18    20     3.29e+05     5.37e+04     2.75e+05         3.68         4.74          236         10.7\n",
            "     18    30     6.91e+05     7.04e+04      6.2e+05         4.01         5.43          161         16.1\n",
            "     18    40     8.57e+05     2.45e+04     8.32e+05         2.41          3.2          280         18.7\n",
            "     18    50     4.23e+05     1.27e+04      4.1e+05         1.76         2.31          170         13.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18     2     4.13e+05     3.26e+04      3.8e+05         2.73         3.69          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18   36.436    0.002     3.45e+04     2.66e+06      2.7e+06         2.74         3.92          229         20.8\n",
            "! Validation         18   36.436    0.002      3.6e+04     3.62e+05     3.98e+05         2.82         3.87          155         11.3\n",
            "Wall time: 36.43682988799992\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10      7.7e+04      1.7e+04        6e+04         2.08         2.67         55.1         5.01\n",
            "     19    20     6.91e+06     1.06e+04      6.9e+06          1.5         2.11          537         53.7\n",
            "     19    30     9.96e+05      1.7e+04     9.79e+05         2.24         2.67          405         20.2\n",
            "     19    40     5.84e+05     1.68e+04     5.67e+05         1.97         2.66          247         15.4\n",
            "     19    50     2.08e+07     3.14e+03     2.08e+07        0.754         1.15          467         93.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19     2     4.23e+05     2.91e+04     3.94e+05         2.57         3.49          180         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19   37.798    0.002     2.59e+04     2.67e+06      2.7e+06         2.38         3.38          234         21.1\n",
            "! Validation         19   37.798    0.002     3.22e+04     3.72e+05     4.04e+05         2.65         3.66          157         11.4\n",
            "Wall time: 37.79877245099988\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10      2.8e+05     4.54e+04     2.35e+05         3.13         4.36          188         9.91\n",
            "     20    20     4.44e+04     1.18e+04     3.26e+04         1.81         2.22         44.3         3.69\n",
            "     20    30     3.91e+05     3.13e+04     3.59e+05          2.7         3.62          123         12.3\n",
            "     20    40     7.92e+05     3.43e+04     7.57e+05         2.89         3.79          160         17.8\n",
            "     20    50     8.05e+05     2.38e+04     7.81e+05         2.24         3.16          271         18.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20     2     4.18e+05     3.11e+04     3.87e+05         2.64         3.61          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20   39.139    0.002     3.78e+04     2.66e+06     2.69e+06         2.84         4.07          227         20.6\n",
            "! Validation         20   39.139    0.002     3.45e+04     3.66e+05     4.01e+05         2.72         3.79          156         11.3\n",
            "Wall time: 39.139992337999956\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10     4.97e+05     2.91e+04     4.68e+05         2.68         3.49          252           14\n",
            "     21    20     6.39e+07          419     6.39e+07        0.302        0.419          654          164\n",
            "     21    30     2.78e+05     9.36e+04     1.85e+05         4.77         6.26          149         8.79\n",
            "     21    40     3.68e+05     2.63e+04     3.42e+05         2.62         3.32          120           12\n",
            "     21    50     4.38e+05     3.62e+04     4.02e+05         3.05         3.89          182           13\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21     2     4.13e+05     3.27e+04      3.8e+05         2.69          3.7          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21   40.482    0.002     3.96e+04     2.65e+06     2.69e+06          2.9         4.17          226         20.5\n",
            "! Validation         21   40.482    0.002     3.61e+04     3.62e+05     3.98e+05         2.78         3.88          155         11.3\n",
            "Wall time: 40.48257702000001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10     4.26e+05     1.51e+04     4.11e+05          1.9         2.51          131         13.1\n",
            "     22    20     2.35e+05     6.57e+04      1.7e+05         3.71         5.24          118         8.42\n",
            "     22    30     6.91e+06     9.95e+03      6.9e+06         1.39         2.04          537         53.7\n",
            "     22    40     7.81e+04     1.69e+04     6.12e+04         2.05         2.66         55.7         5.06\n",
            "     22    50     3.34e+06     1.34e+04     3.33e+06         1.72         2.37          485         37.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22     2     4.25e+05     2.87e+04     3.97e+05         2.49         3.47          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22   41.801    0.002     2.61e+04     2.66e+06     2.69e+06         2.28         3.39          233           21\n",
            "! Validation         22   41.801    0.002      3.2e+04     3.73e+05     4.05e+05         2.57         3.65          157         11.4\n",
            "Wall time: 41.801409196999884\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10     5.78e+05     6.83e+04      5.1e+05         4.18         5.34          307         14.6\n",
            "     23    20     6.89e+05     3.42e+04     6.55e+05         2.77         3.78          132         16.6\n",
            "     23    30     3.89e+04     1.25e+04     2.64e+04         1.86         2.29         26.6         3.32\n",
            "     23    40      4.7e+05      3.8e+04     4.32e+05         3.11         3.99          282         13.4\n",
            "     23    50     5.91e+05     1.03e+05     4.88e+05         4.72         6.56          114         14.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23     2     4.15e+05     3.15e+04     3.83e+05         2.61         3.63          178         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23   43.118    0.002     4.25e+04     2.65e+06     2.69e+06         2.99          4.3          224         20.4\n",
            "! Validation         23   43.118    0.002     3.48e+04     3.64e+05     3.99e+05         2.69          3.8          155         11.3\n",
            "Wall time: 43.1187204859998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10     5.53e+04     7.03e+03     4.82e+04         1.42         1.72         53.9         4.49\n",
            "     24    20     4.04e+04     5.37e+03     3.51e+04         1.14          1.5         19.1         3.83\n",
            "     24    30     2.03e+05     1.05e+05     9.71e+04         4.91         6.64         89.2         6.37\n",
            "     24    40     6.39e+07          191     6.39e+07        0.175        0.283          654          163\n",
            "     24    50     8.09e+04      1.7e+04     6.39e+04         2.03         2.67         56.9         5.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24     2     4.16e+05     3.07e+04     3.85e+05         2.55         3.59          178         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24   44.436    0.002     3.24e+04     2.66e+06     2.69e+06         2.58          3.8          228         20.7\n",
            "! Validation         24   44.436    0.002     3.38e+04     3.65e+05     3.99e+05         2.63         3.75          155         11.3\n",
            "Wall time: 44.43617380299975\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10     4.39e+05     3.23e+04     4.07e+05          2.8         3.67          183           13\n",
            "     25    20     3.26e+06      1.1e+04     3.25e+06         1.52         2.14          479         36.9\n",
            "     25    30     5.06e+05     8.62e+04     4.19e+05         4.35         6.01          199         13.2\n",
            "     25    40     2.78e+05     8.54e+03     2.69e+05         1.54         1.89          149         10.6\n",
            "     25    50     4.11e+05     2.44e+04     3.86e+05         2.34         3.19          127         12.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25     2     4.27e+05     2.76e+04     3.99e+05         2.38          3.4          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25   45.854    0.002     2.59e+04     2.66e+06     2.68e+06         2.22         3.38          232         20.9\n",
            "! Validation         25   45.854    0.002     3.05e+04     3.74e+05     4.05e+05         2.46         3.56          157         11.4\n",
            "Wall time: 45.85419845599972\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10     5.67e+05     6.41e+04     5.03e+05         3.92         5.18          305         14.5\n",
            "     26    20      7.1e+06     1.72e+04     7.09e+06         1.82         2.68          545         54.5\n",
            "     26    30     3.05e+06     9.54e+03     3.04e+06         1.51            2          321         35.7\n",
            "     26    40      3.4e+06     1.95e+04     3.38e+06         2.06         2.86          489         37.6\n",
            "     26    50     7.43e+06     7.34e+03     7.42e+06         1.32         1.75          557         55.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26     2     4.17e+05     3.06e+04     3.86e+05          2.5         3.58          178         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26   47.170    0.002     4.11e+04     2.64e+06     2.68e+06         2.85         4.25          223         20.3\n",
            "! Validation         26   47.170    0.002     3.35e+04     3.65e+05     3.99e+05         2.57         3.74          155         11.3\n",
            "Wall time: 47.17067499999985\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10      6.6e+05      5.8e+04     6.02e+05         3.61         4.93          159         15.9\n",
            "     27    20     4.29e+05     5.22e+04     3.77e+05         3.18         4.67          176         12.6\n",
            "     27    30     4.13e+04     1.51e+04     2.61e+04         1.98         2.52         56.2         3.31\n",
            "     27    40     1.26e+06     8.13e+03     1.25e+06         1.37         1.84          298         22.9\n",
            "     27    50     9.46e+05     1.99e+04     9.26e+05         2.33         2.89          276         19.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27     2     4.14e+05     3.11e+04     3.83e+05         2.52         3.61          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27   48.493    0.002     3.74e+04     2.64e+06     2.68e+06         2.67         4.02          225         20.4\n",
            "! Validation         27   48.493    0.002     3.38e+04     3.63e+05     3.96e+05         2.58         3.75          154         11.2\n",
            "Wall time: 48.49396719099968\n",
            "! Best model       27 396496.473\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10     4.89e+04     1.05e+04     3.85e+04         1.59         2.09         32.1         4.01\n",
            "     28    20     3.28e+05     8.73e+04     2.41e+05         5.05         6.04          120           10\n",
            "     28    30     4.01e+05     1.36e+04     3.87e+05         1.78         2.38          204         12.7\n",
            "     28    40     2.76e+05      3.9e+04     2.37e+05         2.82         4.04          189         9.96\n",
            "     28    50     1.76e+06     7.48e+03     1.76e+06         1.36         1.77          434         27.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28     2     4.19e+05     2.93e+04     3.89e+05         2.41          3.5          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28   49.855    0.002     3.07e+04     2.64e+06     2.68e+06         2.38         3.67          226         20.5\n",
            "! Validation         28   49.855    0.002     3.19e+04     3.67e+05     3.99e+05         2.47         3.65          155         11.3\n",
            "Wall time: 49.85562586199967\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10     3.31e+06        1e+04      3.3e+06         1.42         2.05          483         37.2\n",
            "     29    20     5.76e+05      1.5e+04     5.62e+05         1.78          2.5          245         15.3\n",
            "     29    30      3.4e+06     1.89e+04     3.39e+06         1.99         2.81          489         37.6\n",
            "     29    40     2.27e+05     1.46e+04     2.12e+05         2.04         2.47          132         9.42\n",
            "     29    50     3.85e+04     3.73e+03     3.47e+04        0.933         1.25         19.1         3.81\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29     2     4.14e+05     3.04e+04     3.83e+05         2.45         3.56          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29   51.186    0.002     3.59e+04     2.65e+06     2.68e+06          2.6         3.93          223         20.3\n",
            "! Validation         29   51.186    0.002     3.28e+04     3.63e+05     3.95e+05         2.51          3.7          153         11.2\n",
            "Wall time: 51.18687300300007\n",
            "! Best model       29 395302.358\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10     2.97e+06     5.58e+03     2.97e+06         1.12         1.53          317         35.2\n",
            "     30    20     6.92e+06     7.99e+03     6.91e+06         1.16         1.83          538         53.8\n",
            "     30    30     6.01e+05     4.16e+04      5.6e+05         2.84         4.17          321         15.3\n",
            "     30    40     3.38e+06     1.34e+04     3.37e+06         1.64         2.36          488         37.5\n",
            "     30    50     2.09e+07     1.67e+03     2.09e+07        0.564        0.836          468         93.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30     2     4.19e+05     2.86e+04      3.9e+05         2.34         3.46          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30   52.524    0.002     3.02e+04     2.65e+06     2.68e+06         2.33         3.66          227         20.6\n",
            "! Validation         30   52.524    0.002      3.1e+04     3.67e+05     3.98e+05          2.4         3.59          154         11.3\n",
            "Wall time: 52.524354757999845\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10     7.25e+06     3.05e+03     7.24e+06         0.91         1.13          551         55.1\n",
            "     31    20     6.97e+05     4.11e+04     6.56e+05         2.77         4.15          166         16.6\n",
            "     31    30      2.8e+04     1.86e+03     2.61e+04        0.623        0.883         16.5          3.3\n",
            "     31    40     4.82e+05     1.24e+05     3.57e+05         4.85         7.21         97.8         12.2\n",
            "     31    50     3.33e+06     1.08e+04     3.32e+06         1.46         2.13          484         37.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31     2     4.27e+05     2.63e+04     4.01e+05         2.21         3.31          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31   53.837    0.002     2.74e+04     2.64e+06     2.67e+06          2.2         3.49          227         20.6\n",
            "! Validation         31   53.837    0.002     2.85e+04     3.74e+05     4.03e+05         2.27         3.45          156         11.4\n",
            "Wall time: 53.83779860699997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10     9.77e+05     1.64e+04      9.6e+05            2         2.62          401           20\n",
            "     32    20     3.86e+05     1.67e+04     3.69e+05            2         2.65          124         12.4\n",
            "     32    30     7.06e+06     1.38e+04     7.05e+06         1.51          2.4          543         54.3\n",
            "     32    40     4.99e+04     7.37e+03     4.25e+04         1.41         1.76         50.6         4.22\n",
            "     32    50     6.14e+05      5.1e+04     5.63e+05         3.19         4.62          154         15.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32     2     4.22e+05     2.76e+04     3.95e+05         2.26          3.4          180         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32   55.161    0.002     3.62e+04     2.63e+06     2.67e+06         2.51         3.97          223         20.3\n",
            "! Validation         32   55.161    0.002        3e+04      3.7e+05        4e+05         2.31         3.53          154         11.3\n",
            "Wall time: 55.16179276399998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10     3.59e+05     3.61e+04     3.23e+05         2.92         3.89          163         11.6\n",
            "     33    20     2.08e+07     1.08e+03     2.08e+07        0.471        0.671          466         93.3\n",
            "     33    30     1.71e+05     8.27e+04     8.81e+04         4.18         5.88           85         6.07\n",
            "     33    40     2.71e+05     6.39e+03     2.65e+05         1.32         1.63          147         10.5\n",
            "     33    50     5.85e+05     1.26e+04     5.72e+05         1.61         2.29          248         15.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33     2     4.27e+05      2.6e+04     4.01e+05         2.17          3.3          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33   56.468    0.002     2.81e+04     2.63e+06     2.66e+06         2.21         3.51          226         20.5\n",
            "! Validation         33   56.468    0.002     2.82e+04     3.74e+05     4.02e+05         2.22         3.43          155         11.4\n",
            "Wall time: 56.468734768000104\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10     9.47e+05     1.94e+04     9.28e+05         2.19         2.85          394         19.7\n",
            "     34    20     5.66e+05      1.1e+04     5.55e+05         1.53         2.14          122         15.2\n",
            "     34    30     4.56e+04     4.65e+03      4.1e+04         1.26          1.4         16.6         4.14\n",
            "     34    40     4.02e+05     2.93e+04     3.73e+05         2.39          3.5          125         12.5\n",
            "     34    50     5.01e+04     9.25e+03     4.09e+04          1.5         1.97         33.1         4.14\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34     2     4.24e+05     2.64e+04     3.97e+05         2.18         3.33          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34   57.773    0.002     3.31e+04     2.63e+06     2.66e+06          2.4         3.79          223         20.3\n",
            "! Validation         34   57.773    0.002     2.85e+04     3.71e+05        4e+05         2.23         3.45          154         11.3\n",
            "Wall time: 57.77385351899966\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10     4.34e+04     9.52e+03     3.38e+04          1.6            2           64         3.76\n",
            "     35    20     5.59e+06      2.2e+03     5.59e+06        0.693        0.959          725         48.4\n",
            "     35    30     5.36e+05     4.11e+04     4.94e+05         2.85         4.15          302         14.4\n",
            "     35    40     2.65e+05     6.28e+03     2.58e+05         1.29         1.62          146         10.4\n",
            "     35    50     2.61e+05     1.97e+05     6.33e+04         7.21         9.09         51.4         5.14\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35     2     4.25e+05     2.61e+04     3.98e+05         2.14         3.31          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35   59.093    0.002     3.03e+04     2.63e+06     2.66e+06         2.27         3.65          224         20.4\n",
            "! Validation         35   59.093    0.002     2.82e+04     3.72e+05        4e+05         2.19         3.43          154         11.3\n",
            "Wall time: 59.09322113400003\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10     3.89e+05     1.07e+04     3.78e+05         1.51         2.12          164         12.6\n",
            "     36    20     1.19e+06     3.82e+03     1.18e+06        0.968         1.26          289         22.2\n",
            "     36    30     5.57e+06     2.32e+03     5.57e+06         0.73        0.985          724         48.3\n",
            "     36    40     4.14e+05     1.12e+05     3.02e+05          4.5         6.85           90         11.2\n",
            "     36    50      2.6e+05     4.11e+04     2.19e+05         2.85         4.14          182         9.58\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36     2     4.27e+05      2.5e+04     4.02e+05         2.08         3.23          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36   60.429    0.002     2.85e+04     2.63e+06     2.66e+06         2.21         3.55          224         20.4\n",
            "! Validation         36   60.429    0.002      2.7e+04     3.74e+05     4.01e+05         2.13         3.36          155         11.3\n",
            "Wall time: 60.4295672129997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10     2.08e+07          956     2.08e+07        0.437        0.633          467         93.4\n",
            "     37    20     9.72e+05      1.6e+04     9.56e+05         1.93         2.58          400           20\n",
            "     37    30     7.51e+04     1.35e+04     6.16e+04         1.79         2.38         55.8         5.08\n",
            "     37    40     5.49e+04     4.96e+03     4.99e+04         1.14         1.44         54.8         4.57\n",
            "     37    50     5.59e+05     1.45e+04     5.44e+05          1.7         2.46          241         15.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37     2     4.26e+05     2.49e+04     4.01e+05         2.05         3.23          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37   61.800    0.002     3.03e+04     2.62e+06     2.65e+06         2.24         3.64          223         20.3\n",
            "! Validation         37   61.800    0.002      2.7e+04     3.73e+05        4e+05         2.11         3.35          154         11.3\n",
            "Wall time: 61.80095342300001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10      4.7e+05     4.54e+04     4.25e+05         2.95         4.36          280         13.3\n",
            "     38    20     5.04e+04     8.05e+03     4.24e+04          1.4         1.83         33.7         4.21\n",
            "     38    30     3.24e+05     4.39e+04     2.81e+05         2.67         4.29          238         10.8\n",
            "     38    40     2.62e+04     1.89e+03     2.43e+04        0.633         0.89         15.9         3.19\n",
            "     38    50      4.3e+05     1.87e+04     4.11e+05         1.89          2.8          131         13.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38     2     4.29e+05     2.39e+04     4.05e+05         1.98         3.16          182           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38   63.140    0.002     2.85e+04     2.62e+06     2.65e+06         2.14         3.53          224         20.3\n",
            "! Validation         38   63.140    0.002     2.59e+04     3.76e+05     4.01e+05         2.04         3.29          155         11.3\n",
            "Wall time: 63.14034101399966\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10     6.34e+07     3.07e+03     6.34e+07         0.97         1.13          651          163\n",
            "     39    20     2.21e+05     6.91e+04     1.52e+05         3.85         5.38          152         7.98\n",
            "     39    30     4.13e+05      1.4e+04     3.99e+05         1.78         2.42          168         12.9\n",
            "     39    40      6.9e+06     9.38e+03     6.89e+06         1.37         1.98          537         53.7\n",
            "     39    50     5.12e+05     2.72e+04     4.85e+05         2.27         3.37          299         14.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39     2     4.24e+05     2.42e+04     3.99e+05         1.99         3.18          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39   64.499    0.002     3.32e+04     2.62e+06     2.65e+06         2.35         3.81          220         20.2\n",
            "! Validation         39   64.499    0.002     2.61e+04     3.72e+05     3.98e+05         2.05          3.3          154         11.3\n",
            "Wall time: 64.49995709399991\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10     5.32e+05      1.3e+04     5.19e+05         1.62         2.33          265         14.7\n",
            "     40    20      4.7e+04     3.02e+03      4.4e+04        0.905         1.12         34.3         4.29\n",
            "     40    30     4.23e+05     7.58e+03     4.16e+05         1.34         1.78          171         13.2\n",
            "     40    40     1.89e+05     6.98e+04     1.19e+05         4.51          5.4         84.7         7.06\n",
            "     40    50     5.13e+05     1.01e+05     4.12e+05         3.72          6.5          158         13.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40     2     4.26e+05     2.28e+04     4.03e+05         1.91         3.09          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40   65.827    0.002     2.54e+04     2.61e+06     2.64e+06         2.07         3.35          224         20.4\n",
            "! Validation         40   65.827    0.002     2.47e+04     3.74e+05     3.99e+05         1.99         3.21          154         11.3\n",
            "Wall time: 65.82744341899979\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10     2.08e+07          257     2.08e+07        0.231        0.328          467         93.4\n",
            "     41    20      5.4e+05     7.58e+03     5.32e+05          1.2         1.78          119         14.9\n",
            "     41    30     1.05e+06     8.14e+03     1.05e+06         1.31         1.85          230         20.9\n",
            "     41    40     8.39e+05     1.19e+04     8.27e+05         1.44         2.23          279         18.6\n",
            "     41    50     5.69e+05     3.21e+04     5.37e+05         2.45         3.67          150           15\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41     2     4.28e+05     2.16e+04     4.07e+05         1.84            3          182           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41   67.157    0.002      2.4e+04     2.61e+06     2.63e+06         2.05         3.27          223         20.3\n",
            "! Validation         41   67.157    0.002     2.35e+04     3.77e+05        4e+05         1.93         3.13          155         11.3\n",
            "Wall time: 67.15789786999994\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10     5.05e+05     1.34e+05      3.7e+05          4.3          7.5          149         12.4\n",
            "     42    20     3.59e+05     1.53e+04     3.43e+05          1.9         2.53          120           12\n",
            "     42    30     8.87e+05     1.54e+04     8.72e+05         1.97         2.54          267         19.1\n",
            "     42    40     1.23e+05     9.94e+04     2.34e+04         4.52         6.45         43.8         3.13\n",
            "     42    50     2.09e+07          201     2.09e+07        0.216         0.29          468         93.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42     2     4.16e+05     2.35e+04     3.92e+05         1.93         3.13          178         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42   68.480    0.002     3.32e+04     2.59e+06     2.62e+06         2.43         3.82          217         19.9\n",
            "! Validation         42   68.480    0.002     2.54e+04     3.66e+05     3.92e+05         2.02         3.26          152         11.2\n",
            "Wall time: 68.48115629599988\n",
            "! Best model       42 391611.444\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10     5.55e+04     5.22e+03     5.03e+04         1.16         1.48         36.7         4.59\n",
            "     43    20     1.09e+06     6.66e+03     1.09e+06         1.07         1.67          235         21.3\n",
            "     43    30     4.21e+05     6.29e+04     3.58e+05         3.34         5.13         97.9         12.2\n",
            "     43    40     4.06e+05     2.68e+04     3.79e+05         2.07         3.35          176         12.6\n",
            "     43    50     1.17e+05     7.67e+04     4.06e+04         4.02         5.67         57.7         4.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43     2     4.26e+05     2.08e+04     4.05e+05          1.8         2.95          182           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43   69.811    0.002     2.27e+04     2.59e+06     2.61e+06         1.98         3.12          224         20.4\n",
            "! Validation         43   69.811    0.002     2.27e+04     3.75e+05     3.97e+05          1.9         3.08          154         11.3\n",
            "Wall time: 69.81214566399967\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10     7.36e+06     2.06e+03     7.36e+06        0.666        0.929          555         55.5\n",
            "     44    20     4.66e+04     3.86e+03     4.27e+04        0.976         1.27         50.7         4.23\n",
            "     44    30      2.6e+04     5.68e+03     2.04e+04         1.09         1.54         14.6         2.92\n",
            "     44    40     1.13e+06     3.29e+04      1.1e+06         2.11         3.71          279         21.5\n",
            "     44    50     2.67e+05     8.89e+04     1.78e+05         4.69          6.1          129         8.63\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44     2     4.19e+05     2.16e+04     3.98e+05         1.85         3.01          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44   71.137    0.002      3.5e+04     2.55e+06     2.58e+06         2.47         3.81          217         19.9\n",
            "! Validation         44   71.137    0.002     2.35e+04     3.69e+05     3.93e+05         1.95         3.13          153         11.2\n",
            "Wall time: 71.13763381999979\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10      4.9e+05     1.52e+04     4.75e+05         1.78         2.52          254         14.1\n",
            "     45    20     2.83e+05     4.44e+04     2.38e+05          2.8         4.31          220         9.99\n",
            "     45    30     9.85e+05      1.1e+04     9.74e+05         1.66         2.14          222         20.2\n",
            "     45    40     5.41e+04     8.98e+03     4.51e+04         1.44         1.94         52.2         4.35\n",
            "     45    50      3.1e+05     1.56e+05     1.53e+05         5.26         8.09           64            8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45     2     4.17e+05     2.18e+04     3.95e+05          1.9         3.02          179         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45   72.467    0.002     5.26e+04      2.5e+06     2.55e+06         2.75          4.5          216         19.7\n",
            "! Validation         45   72.467    0.002     2.36e+04     3.67e+05     3.91e+05         1.99         3.14          152         11.2\n",
            "Wall time: 72.46797050499981\n",
            "! Best model       45 390620.836\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10     3.94e+05     6.26e+04     3.32e+05         3.29         5.12          153         11.8\n",
            "     46    20     1.11e+05     1.02e+05     9.03e+03         4.51         6.54         27.2         1.94\n",
            "     46    30     2.37e+04     1.55e+04     8.26e+03         1.88         2.54         9.29         1.86\n",
            "     46    40     6.62e+04     1.23e+04     5.39e+04         1.56         2.27           57         4.75\n",
            "     46    50     7.33e+06     3.63e+03     7.33e+06        0.857         1.23          554         55.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46     2     4.14e+05     2.29e+04     3.91e+05         1.95         3.09          178         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46   73.863    0.002     8.18e+04     2.45e+06     2.53e+06          3.1         5.46          212         19.4\n",
            "! Validation         46   73.863    0.002     2.46e+04     3.64e+05     3.88e+05         2.04          3.2          151         11.1\n",
            "Wall time: 73.86393674800001\n",
            "! Best model       46 388212.821\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10     5.98e+04     1.22e+04     4.76e+04         1.63         2.26         35.7         4.46\n",
            "     47    20     5.32e+05     2.81e+04     5.04e+05         2.27         3.43          145         14.5\n",
            "     47    30     7.64e+04     3.62e+04     4.02e+04         2.78         3.89         16.4          4.1\n",
            "     47    40     2.54e+05     6.49e+04     1.89e+05         3.34         5.21          196          8.9\n",
            "     47    50      4.1e+05     1.93e+04     3.91e+05         1.89         2.84          205         12.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47     2     4.14e+05     2.33e+04      3.9e+05         1.99         3.12          178         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47   75.223    0.002     1.01e+05     2.41e+06     2.51e+06         3.26         5.89          211         19.3\n",
            "! Validation         47   75.223    0.002      2.5e+04     3.63e+05     3.88e+05         2.09         3.23          151         11.1\n",
            "Wall time: 75.22370124700001\n",
            "! Best model       47 387546.147\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10        8e+05     2.35e+04     7.76e+05         2.13         3.14          270           18\n",
            "     48    20      6.7e+04     1.56e+04     5.14e+04         1.89         2.56         37.1         4.64\n",
            "     48    30     2.94e+05     1.06e+04     2.83e+05         1.29         2.11          152         10.9\n",
            "     48    40     2.59e+05     1.13e+05     1.46e+05         5.53         6.89           78          7.8\n",
            "     48    50     6.49e+04     3.61e+04     2.89e+04         2.79         3.89         13.9         3.47\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48     2     4.19e+05     2.35e+04     3.96e+05         2.04         3.13          180           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48   76.601    0.002     1.17e+05     2.37e+06     2.49e+06         3.25         6.25          212         19.3\n",
            "! Validation         48   76.601    0.002     2.49e+04     3.65e+05      3.9e+05         2.11         3.22          152         11.2\n",
            "Wall time: 76.60217209099983\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10     7.64e+05     2.39e+04      7.4e+05         2.12         3.16          264         17.6\n",
            "     49    20     9.09e+05     1.14e+04     8.98e+05         1.55         2.18          271         19.4\n",
            "     49    30     3.98e+05     3.83e+04      3.6e+05         2.61            4          172         12.3\n",
            "     49    40     2.47e+05     4.85e+04     1.98e+05         2.97         4.51          200          9.1\n",
            "     49    50      9.5e+04     2.68e+04     6.82e+04          2.3         3.35         64.1         5.34\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49     2     4.18e+05     2.43e+04     3.93e+05         2.15         3.19          179         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49   77.965    0.002     1.13e+05     2.35e+06     2.46e+06         3.29         6.07          208         19.1\n",
            "! Validation         49   77.965    0.002     2.54e+04     3.63e+05     3.89e+05         2.18         3.26          151         11.2\n",
            "Wall time: 77.96622342799992\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10     1.35e+06     4.21e+05     9.27e+05         6.96         13.3          256         19.7\n",
            "     50    20     9.96e+04     9.03e+04     9.31e+03         4.42         6.15         9.87         1.97\n",
            "     50    30     9.21e+04     3.08e+04     6.12e+04         2.47         3.59         60.7         5.06\n",
            "     50    40     2.36e+05     5.11e+04     1.85e+05         3.15         4.62          194         8.81\n",
            "     50    50     5.83e+04     3.42e+04     2.41e+04         2.81         3.78         12.7         3.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50     2     4.08e+05     2.65e+04     3.81e+05          2.3         3.33          176         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50   79.276    0.002     1.33e+05     2.31e+06     2.44e+06         3.56          6.5          203         18.7\n",
            "! Validation         50   79.276    0.002     2.73e+04     3.55e+05     3.83e+05         2.31         3.38          149           11\n",
            "Wall time: 79.27641069599986\n",
            "! Best model       50 382787.639\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10      4.3e+05     4.03e+04     3.89e+05          3.3         4.11          268         12.8\n",
            "     51    20     9.93e+04     9.01e+04     9.22e+03         4.47         6.14         9.82         1.96\n",
            "     51    30     2.26e+05     6.23e+04     1.63e+05         3.47         5.11          182         8.26\n",
            "     51    40     3.51e+05     2.66e+04     3.25e+05         2.54         3.34          117         11.7\n",
            "     51    50     7.17e+05     1.66e+04     7.01e+05         1.85         2.64          257         17.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51     2     4.03e+05     2.86e+04     3.74e+05         2.41         3.46          174         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51   80.594    0.002     1.68e+05     2.25e+06     2.42e+06         3.71         7.16          201         18.5\n",
            "! Validation         51   80.594    0.002      2.9e+04     3.51e+05      3.8e+05         2.41         3.48          148         10.9\n",
            "Wall time: 80.59475763199998\n",
            "! Best model       51 379831.892\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10     5.37e+04     2.47e+04      2.9e+04         2.29         3.21         27.9         3.48\n",
            "     52    20     9.19e+04     7.39e+04      1.8e+04         4.04         5.56         13.7         2.74\n",
            "     52    30     4.22e+04     2.65e+04     1.57e+04         2.62         3.33         10.3         2.56\n",
            "     52    40     1.86e+05     7.45e+04     1.11e+05         4.32         5.58          102         6.82\n",
            "     52    50     1.35e+05     3.67e+04     9.82e+04         2.61         3.92          109         6.41\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52     2     3.94e+05     2.95e+04     3.64e+05         2.47         3.51          171         11.4\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52   81.891    0.002     1.43e+05     2.25e+06      2.4e+06         3.54         6.52          200         18.4\n",
            "! Validation         52   81.891    0.002     3.01e+04     3.45e+05     3.75e+05         2.47         3.55          146         10.8\n",
            "Wall time: 81.89159849099997\n",
            "! Best model       52 374883.443\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10     5.81e+04     4.01e+04      1.8e+04         3.37          4.1         46.6         2.74\n",
            "     53    20     4.31e+05     4.24e+04     3.89e+05         3.44         4.21          268         12.8\n",
            "     53    30     3.42e+05     3.53e+04     3.07e+05         2.98         3.84          113         11.3\n",
            "     53    40     2.42e+06     1.91e+05     2.23e+06         6.19         8.93          275         30.5\n",
            "     53    50     6.95e+05     2.46e+04     6.71e+05         2.31         3.21          302         16.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53     2     3.89e+05     3.06e+04     3.59e+05         2.54         3.58          170         11.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53   83.196    0.002     1.59e+05     2.22e+06     2.38e+06         3.64         6.87          199         18.3\n",
            "! Validation         53   83.196    0.002     3.12e+04     3.42e+05     3.73e+05         2.54         3.61          145         10.7\n",
            "Wall time: 83.19659319799985\n",
            "! Best model       53 372781.750\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10     1.73e+05     3.73e+04     1.35e+05         2.88         3.95          143         7.53\n",
            "     54    20     5.12e+05     6.06e+04     4.52e+05         3.47         5.03          220         13.7\n",
            "     54    30     5.46e+07     4.73e+06     4.99e+07         36.5         44.5          578          144\n",
            "     54    40     3.09e+05      4.1e+04     2.68e+05         3.27         4.14          106         10.6\n",
            "     54    50     2.19e+05     5.77e+04     1.62e+05         3.49         4.92          181         8.23\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54     2     3.81e+05     3.44e+04     3.46e+05         2.73          3.8          166         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54   84.520    0.002     2.05e+05     2.15e+06     2.35e+06         3.97         7.65          194         17.9\n",
            "! Validation         54   84.520    0.002     3.43e+04     3.34e+05     3.68e+05          2.7         3.79          144         10.6\n",
            "Wall time: 84.5205868789999\n",
            "! Best model       54 368247.342\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10     6.32e+05     3.22e+04        6e+05         2.08         3.67          127         15.8\n",
            "     55    20     6.21e+04     2.96e+04     3.25e+04         2.45         3.52         44.3         3.69\n",
            "     55    30     8.26e+04     6.03e+04     2.23e+04         3.66         5.02         15.3         3.05\n",
            "     55    40     7.23e+05     2.19e+04     7.01e+05         2.33         3.03          154         17.1\n",
            "     55    50     5.21e+04     3.18e+04     2.03e+04         2.65         3.65         23.3         2.91\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55     2     3.88e+05     3.43e+04     3.54e+05         2.74         3.79          168         11.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55   85.845    0.002     2.22e+05     2.12e+06     2.34e+06         3.64         7.52          199         18.2\n",
            "! Validation         55   85.845    0.002     3.41e+04     3.39e+05     3.73e+05         2.69         3.78          144         10.6\n",
            "Wall time: 85.84622904299977\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10     4.54e+06     5.58e+05     3.98e+06           11         15.3          612         40.8\n",
            "     56    20     2.96e+05     3.79e+04     2.58e+05         2.89         3.98          146         10.4\n",
            "     56    30     3.64e+05     3.89e+04     3.25e+05         3.05         4.03          117         11.7\n",
            "     56    40     2.89e+05     3.35e+04     2.55e+05         2.69         3.75          145         10.3\n",
            "     56    50     7.62e+06     3.57e+04     7.59e+06         2.85         3.87          563         56.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56     2     3.88e+05     3.53e+04     3.53e+05         2.78         3.84          168         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56   87.204    0.002     2.16e+05     2.11e+06     2.33e+06         3.75         7.68          197         18.1\n",
            "! Validation         56   87.204    0.002     3.52e+04     3.39e+05     3.74e+05         2.73         3.84          144         10.7\n",
            "Wall time: 87.204526343\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10     5.31e+07      5.9e+06     4.72e+07         39.8         49.7          562          141\n",
            "     57    20     4.71e+06     4.14e+05      4.3e+06         7.54         13.2          424         42.4\n",
            "     57    30     7.72e+05     3.92e+04     7.33e+05         2.65         4.05          263         17.5\n",
            "     57    40     4.64e+05     5.81e+04     4.06e+05         3.72         4.93          274           13\n",
            "     57    50     3.84e+05     2.07e+04     3.63e+05         2.07         2.94          259         12.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57     2     3.95e+05     3.77e+04     3.57e+05         2.84         3.97          169         11.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57   88.538    0.002     2.35e+05     2.07e+06     2.31e+06         3.83         8.05          195         17.9\n",
            "! Validation         57   88.538    0.002     3.68e+04     3.42e+05     3.79e+05         2.78         3.93          145         10.7\n",
            "Wall time: 88.53882529299972\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10     3.67e+05     1.82e+04     3.49e+05         2.03         2.76          254         12.1\n",
            "     58    20     1.71e+05     3.83e+04     1.33e+05         2.81            4          142         7.46\n",
            "     58    30     3.77e+04     2.81e+04      9.6e+03         2.69         3.43         34.1            2\n",
            "     58    40     7.58e+04     2.52e+04     5.06e+04         2.45         3.25         78.2          4.6\n",
            "     58    50      2.1e+06     1.54e+05     1.95e+06         6.06         8.02          257         28.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58     2     3.84e+05     3.67e+04     3.47e+05         2.82         3.92          166           11\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58   89.868    0.002     1.97e+05     2.08e+06     2.28e+06         3.57         7.17          192         17.7\n",
            "! Validation         58   89.868    0.002     3.66e+04     3.36e+05     3.72e+05         2.77         3.91          144         10.6\n",
            "Wall time: 89.86905368299995\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10     7.99e+05     2.52e+04     7.74e+05         2.48         3.25          360           18\n",
            "     59    20     5.34e+05     5.08e+04     4.83e+05         3.34         4.61          185         14.2\n",
            "     59    30     4.97e+04      3.6e+04     1.37e+04         2.83         3.88         19.2         2.39\n",
            "     59    40     4.23e+05     8.02e+04     3.43e+05         4.29         5.79          192           12\n",
            "     59    50     3.81e+05      1.9e+04     3.62e+05         2.17         2.82          222         12.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59     2     3.83e+05     3.91e+04     3.44e+05          2.9         4.04          165         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59   91.191    0.002     2.25e+05     2.03e+06     2.25e+06         3.84          7.8          190         17.5\n",
            "! Validation         59   91.191    0.002     3.85e+04     3.34e+05     3.72e+05         2.84         4.01          143         10.6\n",
            "Wall time: 91.19137666699999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10     2.48e+05      3.9e+04     2.09e+05         3.02         4.04          131         9.34\n",
            "     60    20     8.19e+05     2.42e+04     7.94e+05         2.53         3.18          365         18.2\n",
            "     60    30     3.38e+04      1.8e+04     1.59e+04         1.93         2.74         12.9         2.58\n",
            "     60    40     1.39e+05     2.35e+04     1.16e+05         2.51         3.14         76.5         6.96\n",
            "     60    50     3.27e+04     2.67e+04        6e+03         2.54         3.34         26.9         1.58\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60     2     3.81e+05     4.01e+04     3.41e+05         2.95          4.1          164         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60   92.511    0.002     3.83e+05     1.87e+06     2.25e+06          3.9         8.83          188         17.2\n",
            "! Validation         60   92.511    0.002     3.91e+04     3.32e+05     3.71e+05         2.87         4.05          143         10.6\n",
            "Wall time: 92.51113401299972\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10     3.32e+05     1.42e+04     3.18e+05          1.8         2.44          242         11.5\n",
            "     61    20     3.13e+05     3.34e+04     2.79e+05         2.87         3.74          108         10.8\n",
            "     61    30      1.9e+05     1.15e+05     7.44e+04         5.09         6.95         83.7         5.58\n",
            "     61    40     2.46e+04     9.45e+03     1.52e+04         1.82         1.99         10.1         2.52\n",
            "     61    50     4.31e+04     2.87e+04     1.43e+04         2.57         3.47         19.6         2.45\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61     2     3.78e+05     3.78e+04     3.41e+05         2.89         3.98          163         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61   93.837    0.002     1.64e+05     2.08e+06     2.25e+06         3.51         6.97          192         17.7\n",
            "! Validation         61   93.837    0.002     3.74e+04     3.32e+05      3.7e+05         2.83         3.96          143         10.6\n",
            "Wall time: 93.83813282599976\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10      3.5e+05     1.41e+04     3.36e+05         1.82         2.43          119         11.9\n",
            "     62    20     4.57e+04     3.62e+04     9.51e+03         3.03         3.89           16         1.99\n",
            "     62    30     2.32e+05      3.4e+04     1.98e+05         2.79         3.77          127          9.1\n",
            "     62    40     2.23e+04     1.13e+04     1.11e+04         1.97         2.17          8.6         2.15\n",
            "     62    50     3.75e+06     2.06e+06     1.69e+06         12.2         29.4          346         26.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62     2     3.77e+05     4.05e+04     3.36e+05            3         4.12          162         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62   95.187    0.002     2.39e+05     1.96e+06      2.2e+06         3.88         7.99          186         17.1\n",
            "! Validation         62   95.187    0.002     3.96e+04      3.3e+05     3.69e+05         2.92         4.07          143         10.5\n",
            "Wall time: 95.18765793500006\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10      5.2e+04     2.42e+04     2.78e+04          2.6         3.18         40.9         3.41\n",
            "     63    20     2.52e+05     2.34e+04     2.29e+05         2.29         3.13          137         9.79\n",
            "     63    30      6.5e+05     2.76e+04     6.22e+05         2.56          3.4          290         16.1\n",
            "     63    40     2.43e+04     1.41e+04     1.02e+04         1.68         2.43         10.3         2.07\n",
            "     63    50     1.97e+06     1.48e+05     1.83e+06         5.61         7.87          359         27.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63     2     3.79e+05     4.14e+04     3.38e+05         3.05         4.16          163         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63   96.528    0.002     2.82e+05     1.89e+06     2.17e+06         3.98         8.18          187         17.1\n",
            "! Validation         63   96.528    0.002     4.06e+04     3.31e+05     3.71e+05         2.96         4.12          143         10.6\n",
            "Wall time: 96.52903439600004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10     3.76e+05     2.01e+04     3.56e+05         2.24          2.9          220         12.2\n",
            "     64    20     1.28e+06        1e+05     1.18e+06         4.15         6.47          355         22.2\n",
            "     64    30     2.22e+05     7.31e+04     1.49e+05         4.02         5.53          174         7.89\n",
            "     64    40     8.94e+04     3.52e+04     5.43e+04          2.6         3.84         57.2         4.77\n",
            "     64    50     1.77e+05     6.81e+04     1.09e+05         3.64         5.34          128         6.75\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64     2     3.87e+05     4.46e+04     3.43e+05         3.18         4.32          164         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              64   97.888    0.002     2.45e+05      1.9e+06     2.15e+06         4.24         8.11          186         17.1\n",
            "! Validation         64   97.888    0.002     4.35e+04     3.33e+05     3.77e+05         3.08         4.27          144         10.6\n",
            "Wall time: 97.88850265400015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65    10     1.82e+04     1.66e+04     1.52e+03         1.86         2.64         3.99        0.797\n",
            "     65    20      3.8e+06     2.28e+06     1.53e+06           13         30.9          329         25.3\n",
            "     65    30     7.79e+04      3.6e+04      4.2e+04         2.77         3.88         50.3         4.19\n",
            "     65    40     4.53e+04        4e+04     5.37e+03         3.07         4.09           12          1.5\n",
            "     65    50     7.19e+05     4.99e+04     6.69e+05         2.98         4.57          134         16.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65     2     3.77e+05      4.8e+04     3.29e+05         3.33         4.48          160         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              65   99.243    0.002     3.07e+05     1.81e+06     2.11e+06         4.34         8.89          177         16.3\n",
            "! Validation         65   99.243    0.002     4.68e+04     3.25e+05     3.72e+05         3.21         4.43          143         10.5\n",
            "Wall time: 99.24329143799969\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66    10     2.22e+05     4.61e+04     1.76e+05         3.28         4.39          120         8.59\n",
            "     66    20     4.58e+04     3.55e+04     1.03e+04         2.89         3.86         35.3         2.08\n",
            "     66    30     3.48e+05     3.03e+04     3.17e+05          2.9         3.56          242         11.5\n",
            "     66    40     5.59e+04     4.73e+04     8.63e+03         3.11         4.45         22.8          1.9\n",
            "     66    50     6.37e+05     3.48e+04     6.02e+05         2.87         3.82          286         15.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66     2     3.82e+05     4.86e+04     3.33e+05         3.38         4.51          161         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              66  100.575    0.002     2.73e+05     1.81e+06     2.08e+06         4.29         8.33          183         16.8\n",
            "! Validation         66  100.575    0.002     4.72e+04     3.27e+05     3.74e+05         3.24         4.45          143         10.6\n",
            "Wall time: 100.5760747459999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67    10     4.01e+05     4.22e+04     3.58e+05         3.47          4.2          257         12.2\n",
            "     67    20      4.2e+05     2.61e+04     3.93e+05         2.49         3.31          231         12.8\n",
            "     67    30     7.55e+05     7.25e+04     6.82e+05         3.84         5.51          253         16.9\n",
            "     67    40     8.11e+05     3.86e+04     7.73e+05         2.88         4.02          252           18\n",
            "     67    50     3.61e+05     1.13e+05     2.48e+05         5.14         6.87          122         10.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67     2        4e+05     5.39e+04     3.46e+05         3.55         4.75          165         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              67  101.908    0.002     3.12e+05     1.74e+06     2.05e+06         4.77         9.18          183         16.6\n",
            "! Validation         67  101.908    0.002     5.16e+04     3.34e+05     3.86e+05         3.38         4.65          144         10.7\n",
            "Wall time: 101.90881962299954\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68    10     3.15e+06     9.38e+04     3.06e+06         4.84         6.27          358         35.8\n",
            "     68    20     1.78e+05      1.5e+05     2.82e+04         6.05         7.91         27.5         3.44\n",
            "     68    30     6.68e+05     8.23e+04     5.85e+05         4.27         5.87          203         15.6\n",
            "     68    40      2.1e+07     3.83e+04      2.1e+07         2.71            4          468         93.7\n",
            "     68    50     7.66e+06     1.34e+05     7.53e+06         5.59          7.5          561         56.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68     2     3.82e+05     5.63e+04     3.26e+05         3.66         4.86          159         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              68  103.242    0.002     3.27e+05     1.69e+06     2.01e+06         4.69         9.29          170         15.8\n",
            "! Validation         68  103.242    0.002     5.45e+04     3.22e+05     3.77e+05         3.49         4.78          142         10.5\n",
            "Wall time: 103.24272791600015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69    10     7.57e+06     1.37e+05     7.43e+06         5.64         7.56          558         55.8\n",
            "     69    20     1.92e+05     7.63e+04     1.16e+05         3.74         5.65          132         6.96\n",
            "     69    30     1.45e+06      7.5e+05     6.98e+05         10.5         17.7          222         17.1\n",
            "     69    40     4.02e+05     9.02e+04     3.12e+05         4.49         6.14          240         11.4\n",
            "     69    50     1.24e+06     1.27e+05     1.12e+06         4.88         7.29          346         21.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69     2     3.85e+05     5.75e+04     3.28e+05          3.7          4.9          160         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              69  104.567    0.002     3.01e+05     1.67e+06     1.97e+06         4.68         8.81          175         16.1\n",
            "! Validation         69  104.567    0.002     5.55e+04     3.23e+05     3.79e+05         3.52         4.82          143         10.6\n",
            "Wall time: 104.56780354300008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70    10     3.89e+05     3.76e+04     3.51e+05         3.28         3.97          121         12.1\n",
            "     70    20     5.96e+04     5.27e+04     6.91e+03          3.5         4.69         13.6          1.7\n",
            "     70    30     2.26e+06     1.18e+06     1.08e+06         11.7         22.2          276         21.2\n",
            "     70    40      3.6e+05     1.14e+05     2.46e+05         5.28          6.9          122         10.1\n",
            "     70    50     3.84e+06     1.23e+06     2.61e+06         16.8         22.7          495           33\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70     2     3.88e+05     5.96e+04     3.28e+05         3.77            5          160         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              70  105.884    0.002     4.09e+05     1.53e+06     1.94e+06         5.06          9.8          172         15.7\n",
            "! Validation         70  105.884    0.002     5.78e+04     3.23e+05     3.81e+05          3.6         4.92          143         10.6\n",
            "Wall time: 105.88465204699969\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71    10     6.52e+05     5.84e+04     5.94e+05         3.52         4.94          236         15.8\n",
            "     71    20     8.81e+04     4.61e+04      4.2e+04         3.47         4.39         62.9         4.19\n",
            "     71    30     2.48e+05     3.95e+04     2.09e+05          3.1         4.06          131         9.34\n",
            "     71    40      1.7e+05     6.41e+04     1.06e+05          3.8         5.18          146         6.66\n",
            "     71    50     6.19e+04      5.6e+04     5.91e+03         3.72         4.84         26.7         1.57\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71     2     3.86e+05     5.84e+04     3.28e+05         3.71         4.95          160         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              71  107.283    0.002      3.1e+05     1.61e+06     1.92e+06          4.8         9.13          174         15.9\n",
            "! Validation         71  107.283    0.002     5.72e+04     3.22e+05     3.79e+05         3.56         4.89          143         10.6\n",
            "Wall time: 107.28412305999973\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72    10     3.71e+07     8.44e+06     2.86e+07         41.3         59.4          438          109\n",
            "     72    20        1e+06     1.51e+05     8.53e+05         6.49         7.94          246         18.9\n",
            "     72    30     1.23e+05     6.98e+04     5.35e+04         4.23          5.4           71         4.73\n",
            "     72    40      3.8e+06     1.91e+06     1.88e+06         13.1         28.3          281         28.1\n",
            "     72    50     7.03e+05     5.05e+04     6.52e+05         3.43          4.6          231         16.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72     2     3.93e+05     6.36e+04     3.29e+05         3.89         5.16          161         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              72  108.602    0.002      3.7e+05      1.5e+06     1.87e+06         5.53         10.3          168         15.4\n",
            "! Validation         72  108.602    0.002     6.28e+04     3.22e+05     3.85e+05         3.73         5.13          143         10.6\n",
            "Wall time: 108.60298187900025\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73    10      3.1e+05     9.26e+04     2.17e+05         4.88         6.23         95.4         9.54\n",
            "     73    20     1.98e+05     6.18e+04     1.36e+05         3.75         5.08         83.1         7.56\n",
            "     73    30     6.24e+05     7.53e+04     5.49e+05         4.95         5.61          136         15.2\n",
            "     73    40     8.83e+05     1.11e+05     7.72e+05         5.33         6.81          234           18\n",
            "     73    50     1.14e+06     1.59e+05     9.83e+05         5.57         8.16          324         20.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73     2     3.83e+05     6.59e+04     3.17e+05         3.97         5.25          157         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              73  109.956    0.002     3.68e+05     1.47e+06     1.84e+06         5.42         9.96          163           15\n",
            "! Validation         73  109.956    0.002     6.58e+04     3.15e+05     3.81e+05         3.83         5.25          141         10.5\n",
            "Wall time: 109.956540743\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74    10     7.69e+06      1.9e+05      7.5e+06         6.66         8.92          560           56\n",
            "     74    20     1.55e+06     6.62e+05     8.91e+05         11.7         16.6          251         19.3\n",
            "     74    30     1.83e+06     1.17e+06     6.67e+05         13.1         22.1          217         16.7\n",
            "     74    40     3.78e+06     1.53e+06     2.25e+06         17.8         25.3          460         30.7\n",
            "     74    50     1.16e+06     1.25e+05     1.03e+06         4.97         7.23          333         20.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74     2     3.77e+05     6.47e+04     3.12e+05         3.95          5.2          156         10.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              74  111.367    0.002     4.24e+05     1.42e+06     1.84e+06         5.48         10.6          165         15.1\n",
            "! Validation         74  111.367    0.002     6.49e+04     3.12e+05     3.77e+05         3.81         5.21          140         10.4\n",
            "Wall time: 111.36759320999954\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75    10     2.64e+05     5.64e+04     2.07e+05          3.8         4.86         93.1         9.31\n",
            "     75    20     2.58e+05     4.83e+04     2.09e+05         3.05          4.5          131         9.36\n",
            "     75    30     1.84e+05     1.13e+05     7.05e+04          5.3         6.88         43.5         5.43\n",
            "     75    40     1.11e+06     1.26e+05     9.81e+05         5.42         7.25          263         20.3\n",
            "     75    50     7.66e+04     4.13e+04     3.53e+04         3.18         4.16         65.3         3.84\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75     2     3.57e+05     5.83e+04     2.99e+05         3.75         4.94          152         10.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              75  112.703    0.002     3.43e+05     1.44e+06     1.78e+06         4.87         9.57          161         14.8\n",
            "! Validation         75  112.703    0.002     5.97e+04     3.05e+05     3.65e+05         3.65         4.99          139         10.3\n",
            "Wall time: 112.70347775099981\n",
            "! Best model       75 365171.528\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76    10     5.84e+05     5.61e+04     5.28e+05         3.71         4.84          267         14.9\n",
            "     76    20     1.68e+05     3.98e+04     1.28e+05          3.1         4.08          103         7.32\n",
            "     76    30     3.57e+05     3.62e+04     3.21e+05         3.11         3.89          116         11.6\n",
            "     76    40     5.73e+05     4.57e+04     5.27e+05          3.3         4.37          223         14.9\n",
            "     76    50     2.99e+05     9.19e+04     2.07e+05         4.87          6.2         93.1         9.31\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76     2     3.55e+05     5.73e+04     2.98e+05         3.74          4.9          152         10.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              76  114.046    0.002     4.13e+05     1.34e+06     1.76e+06          5.4         10.3          160         14.6\n",
            "! Validation         76  114.046    0.002     5.95e+04     3.05e+05     3.64e+05         3.65         4.98          139         10.3\n",
            "Wall time: 114.04663996999989\n",
            "! Best model       76 364115.882\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77    10     2.03e+05     1.52e+05     5.12e+04         5.18         7.98         55.5         4.63\n",
            "     77    20      1.7e+06     1.23e+06     4.68e+05         13.9         22.7          182           14\n",
            "     77    30     7.69e+05      2.3e+05     5.39e+05         7.47         9.81          135           15\n",
            "     77    40     5.14e+04     4.81e+04      3.3e+03         3.77         4.48         5.88         1.18\n",
            "     77    50      1.2e+05     5.03e+04     7.01e+04         3.38         4.59          119         5.42\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77     2     3.41e+05     5.85e+04     2.82e+05          3.8         4.95          147         9.99\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              77  115.393    0.002     4.36e+05     1.32e+06     1.75e+06         5.53         11.2          152           14\n",
            "! Validation         77  115.393    0.002      6.1e+04     2.96e+05     3.57e+05          3.7         5.05          137         10.2\n",
            "Wall time: 115.39339512900006\n",
            "! Best model       77 357056.869\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78    10      2.1e+05     5.32e+04     1.57e+05          3.4         4.72         89.2         8.11\n",
            "     78    20     1.16e+05     4.92e+04     6.64e+04         3.22         4.54          116         5.27\n",
            "     78    30     6.88e+05     5.34e+04     6.34e+05         3.54         4.73          228         16.3\n",
            "     78    40     7.25e+04     4.58e+04     2.67e+04         3.48         4.38         50.1         3.34\n",
            "     78    50     1.31e+06     6.47e+05     6.63e+05         10.7         16.5          216         16.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78     2     3.29e+05     5.65e+04     2.72e+05         3.71         4.86          144         9.76\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              78  116.710    0.002     3.94e+05     1.37e+06     1.76e+06          5.3         10.5          153         14.1\n",
            "! Validation         78  116.710    0.002     5.92e+04     2.91e+05      3.5e+05         3.62         4.97          135           10\n",
            "Wall time: 116.71086529100012\n",
            "! Best model       78 349795.211\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79    10     3.17e+05     2.79e+05     3.74e+04         6.43         10.8         51.4         3.96\n",
            "     79    20     7.99e+05     4.57e+04     7.53e+05         2.87         4.37          142         17.7\n",
            "     79    30      6.7e+05     8.78e+04     5.82e+05         4.42         6.06          203         15.6\n",
            "     79    40     1.85e+05     5.89e+04     1.26e+05         3.75         4.96         58.1         7.26\n",
            "     79    50     3.31e+05     2.72e+04     3.04e+05         2.82         3.37          113         11.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79     2     3.23e+05     5.28e+04      2.7e+05         3.56          4.7          144         9.71\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              79  118.028    0.002     4.28e+05     1.26e+06     1.69e+06         4.88         9.97          156         14.2\n",
            "! Validation         79  118.028    0.002     5.55e+04      2.9e+05     3.45e+05         3.49         4.82          135           10\n",
            "Wall time: 118.02908966599989\n",
            "! Best model       79 345418.976\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80    10     5.69e+04     5.47e+04     2.21e+03         3.69         4.79         7.69        0.961\n",
            "     80    20     4.96e+04     4.93e+04          298         3.52         4.54         6.01        0.353\n",
            "     80    30     5.47e+05     3.95e+04     5.08e+05         3.17         4.07          262         14.6\n",
            "     80    40      1.1e+06     4.73e+05     6.29e+05          9.5         14.1          211         16.2\n",
            "     80    50     5.48e+05     6.88e+04     4.79e+05         3.92         5.37          212         14.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80     2     3.19e+05     5.03e+04     2.69e+05         3.47         4.59          143         9.67\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              80  119.343    0.002     3.88e+05     1.27e+06     1.66e+06          4.8         9.78          154         14.1\n",
            "! Validation         80  119.343    0.002     5.31e+04     2.89e+05     3.42e+05          3.4         4.71          135         9.98\n",
            "Wall time: 119.34375588900002\n",
            "! Best model       80 342453.634\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81    10     1.27e+05      1.1e+05     1.74e+04         4.79         6.78         43.2          2.7\n",
            "     81    20     5.58e+05     3.69e+04     5.21e+05         2.97         3.93          295         14.8\n",
            "     81    30     1.14e+06      4.3e+05     7.11e+05         9.01         13.4          224         17.2\n",
            "     81    40     1.67e+06     8.66e+05     8.02e+05         11.9           19          165         18.3\n",
            "     81    50     2.65e+05     8.59e+04     1.79e+05         4.62            6          104         8.66\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81     2     3.03e+05     4.95e+04     2.53e+05         3.43         4.55          138         9.33\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              81  120.667    0.002     3.71e+05     1.28e+06     1.65e+06         4.81         9.82          149         13.8\n",
            "! Validation         81  120.667    0.002     5.23e+04     2.81e+05     3.34e+05         3.36         4.67          133         9.84\n",
            "Wall time: 120.66753136900024\n",
            "! Best model       81 333714.747\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82    10     1.16e+05     3.06e+04     8.58e+04         2.59         3.58         83.9         5.99\n",
            "     82    20     1.84e+06     1.02e+06     8.13e+05         12.8         20.7          166         18.4\n",
            "     82    30     1.17e+06     4.65e+05     7.04e+05         8.81         13.9          223         17.2\n",
            "     82    40     5.85e+04     2.96e+04      2.9e+04         2.63         3.52         59.2         3.48\n",
            "     82    50     5.77e+05     2.91e+04     5.48e+05         2.59         3.49          303         15.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82     2     2.95e+05     4.73e+04     2.48e+05         3.35         4.45          136         9.19\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              82  122.043    0.002     5.57e+05     1.13e+06     1.69e+06         4.76         11.7          147         13.4\n",
            "! Validation         82  122.043    0.002        5e+04     2.79e+05     3.29e+05         3.29         4.57          132         9.77\n",
            "Wall time: 122.04431509599999\n",
            "! Best model       82 328597.502\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83    10     5.11e+04     5.03e+04          777         3.57         4.59         7.98         0.57\n",
            "     83    20     6.46e+05      4.9e+04     5.97e+05         3.51         4.53          221         15.8\n",
            "     83    30     1.13e+05     2.76e+04      8.5e+04         2.48          3.4         83.5         5.96\n",
            "     83    40     1.22e+05     1.06e+05     1.62e+04         4.71         6.65         41.6          2.6\n",
            "     83    50     5.95e+05      3.3e+04     5.62e+05         2.88         3.72          307         15.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83     2     2.93e+05     4.76e+04     2.45e+05         3.34         4.46          135         9.13\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              83  123.393    0.002     2.92e+05     1.41e+06      1.7e+06         4.68         9.15          151         14.1\n",
            "! Validation         83  123.393    0.002     4.97e+04     2.77e+05     3.27e+05         3.26         4.56          132         9.74\n",
            "Wall time: 123.3935070159996\n",
            "! Best model       83 326953.811\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84    10     4.97e+04     4.97e+04         9.28         3.54         4.56         1.06       0.0623\n",
            "     84    20     1.39e+04     9.14e+03     4.74e+03          1.5         1.96         16.9         1.41\n",
            "     84    30     4.55e+05     7.28e+04     3.83e+05         4.41         5.52          127         12.7\n",
            "     84    40     9.06e+04     4.26e+04      4.8e+04         2.95         4.22         98.6         4.48\n",
            "     84    50     7.06e+04     4.68e+04     2.39e+04         3.31         4.42         37.9         3.16\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84     2      2.9e+05     4.89e+04     2.41e+05         3.37         4.52          134         9.06\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              84  124.705    0.002     4.57e+05     1.15e+06     1.61e+06         4.88         10.8          144         13.2\n",
            "! Validation         84  124.705    0.002     5.02e+04     2.75e+05     3.25e+05         3.27         4.58          131         9.71\n",
            "Wall time: 124.7057302309995\n",
            "! Best model       84 325407.267\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85    10     1.87e+05     1.47e+05     4.01e+04         6.18         7.84         61.4         4.09\n",
            "     85    20      3.9e+04     3.84e+04          587         3.12         4.01         3.97        0.496\n",
            "     85    30     1.03e+06     3.43e+05     6.83e+05         8.52           12          220         16.9\n",
            "     85    40     6.96e+05     8.11e+04     6.15e+05         4.16         5.83          209           16\n",
            "     85    50     3.68e+05     3.33e+05     3.55e+04         6.32         11.8         50.1         3.86\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85     2     2.84e+05     4.66e+04     2.37e+05         3.32         4.41          133         8.95\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              85  126.024    0.002     3.75e+05     1.24e+06     1.61e+06          4.8         10.1          150         13.8\n",
            "! Validation         85  126.024    0.002      4.8e+04     2.73e+05     3.21e+05         3.21         4.48          130         9.64\n",
            "Wall time: 126.02506494099998\n",
            "! Best model       85 321397.640\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86    10     6.15e+05     1.44e+04        6e+05         2.04         2.45          174         15.8\n",
            "     86    20     2.12e+05     3.76e+04     1.74e+05         3.05         3.97          119         8.53\n",
            "     86    30     3.74e+04     3.32e+04     4.12e+03          3.3         3.73         5.25         1.31\n",
            "     86    40     7.25e+05     1.03e+05     6.22e+05         4.73         6.56          210         16.1\n",
            "     86    50     1.31e+05     7.45e+04     5.61e+04         4.22         5.58           92         4.84\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86     2     2.84e+05      4.9e+04     2.35e+05         3.41         4.53          132         8.89\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              86  127.358    0.002     3.92e+05     1.17e+06     1.56e+06            5         10.1          144         13.2\n",
            "! Validation         86  127.358    0.002     5.02e+04     2.72e+05     3.22e+05         3.28         4.58          130          9.6\n",
            "Wall time: 127.35910461599997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87    10     2.18e+05     2.27e+04     1.95e+05         2.46         3.08          190         9.04\n",
            "     87    20     2.29e+05     5.87e+04      1.7e+05         3.98         4.96         84.4         8.44\n",
            "     87    30     5.69e+04     3.75e+04     1.94e+04         3.23         3.96         42.7         2.85\n",
            "     87    40     4.17e+05     8.31e+04     3.34e+05         4.62          5.9          118         11.8\n",
            "     87    50     2.39e+05     8.71e+04     1.52e+05          4.5         6.04         95.7         7.97\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87     2     2.79e+05     4.86e+04      2.3e+05         3.38         4.51          131         8.78\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              87  128.679    0.002     4.57e+05      1.1e+06     1.56e+06         4.83         10.6          141         12.9\n",
            "! Validation         87  128.679    0.002     4.88e+04      2.7e+05     3.19e+05         3.22         4.52          129         9.56\n",
            "Wall time: 128.6800618139996\n",
            "! Best model       87 318711.874\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88    10     2.08e+04     2.07e+04         38.9         2.17         2.95         1.53        0.128\n",
            "     88    20     2.61e+05      3.8e+04     2.23e+05         2.71         3.99          135         9.66\n",
            "     88    30     7.98e+06     1.75e+05     7.81e+06         6.99         8.57          572         57.2\n",
            "     88    40     2.52e+05     5.97e+04     1.93e+05         3.82            5         98.8         8.98\n",
            "     88    50     2.12e+07     7.17e+04     2.11e+07         3.38         5.48          470           94\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88     2     2.73e+05     5.02e+04     2.23e+05         3.45         4.58          128         8.62\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              88  130.008    0.002     4.25e+05     1.15e+06     1.58e+06         4.86         11.1          139         12.8\n",
            "! Validation         88  130.008    0.002     4.96e+04     2.66e+05     3.16e+05         3.25         4.56          128         9.49\n",
            "Wall time: 130.00869739300015\n",
            "! Best model       88 315770.777\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89    10     7.11e+05     7.76e+04     6.33e+05         4.21          5.7          212         16.3\n",
            "     89    20     4.75e+05     3.62e+04     4.39e+05         3.31         3.89          122         13.6\n",
            "     89    30     1.22e+05     2.47e+04     9.76e+04         2.37         3.22         51.1         6.39\n",
            "     89    40     2.51e+04     2.28e+04     2.28e+03         2.74         3.09         3.91        0.977\n",
            "     89    50     2.38e+05     6.64e+04     1.72e+05         4.18         5.27         84.8         8.48\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89     2     2.67e+05     4.81e+04     2.18e+05         3.39         4.49          126         8.47\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              89  131.338    0.002     4.23e+05     1.18e+06      1.6e+06         4.94         10.6          143         13.2\n",
            "! Validation         89  131.338    0.002     4.78e+04     2.64e+05     3.12e+05          3.2         4.47          127         9.41\n",
            "Wall time: 131.33852317199944\n",
            "! Best model       89 311592.143\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90    10      2.4e+07     1.33e+07     1.07e+07         53.6         74.5          268         66.9\n",
            "     90    20     2.29e+05     3.21e+04     1.97e+05         2.59         3.66          163         9.07\n",
            "     90    30     2.57e+05     5.04e+04     2.07e+05         3.53         4.59          195          9.3\n",
            "     90    40      4.6e+04     2.17e+04     2.43e+04          2.3         3.01         54.3         3.19\n",
            "     90    50     1.01e+05     4.73e+04     5.41e+04         3.36         4.45         90.4         4.76\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90     2     2.63e+05     4.53e+04     2.17e+05         3.32         4.35          126         8.41\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              90  132.659    0.002     4.63e+05     1.08e+06     1.55e+06         4.72         10.5          144         13.1\n",
            "! Validation         90  132.659    0.002     4.54e+04     2.63e+05     3.09e+05         3.12         4.36          127         9.37\n",
            "Wall time: 132.65971667099984\n",
            "! Best model       90 308850.140\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91    10     7.87e+06     1.37e+05     7.74e+06         6.29         7.57          569         56.9\n",
            "     91    20     2.18e+05     2.75e+04      1.9e+05         2.44          3.4          161         8.92\n",
            "     91    30     6.05e+05        2e+05     4.05e+05         6.83         9.15          117           13\n",
            "     91    40     8.41e+05        5e+04     7.91e+05         3.13         4.57          146         18.2\n",
            "     91    50     2.25e+05     5.46e+04     1.71e+05         3.82         4.78         84.5         8.45\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91     2     2.66e+05     4.56e+04      2.2e+05         3.32         4.37          127         8.48\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              91  134.010    0.002     3.26e+05     1.16e+06     1.49e+06         4.48         9.73          141           13\n",
            "! Validation         91  134.010    0.002     4.51e+04     2.65e+05      3.1e+05          3.1         4.35          127          9.4\n",
            "Wall time: 134.01076731400008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92    10     2.01e+05     3.08e+04      1.7e+05         2.52         3.59          152         8.43\n",
            "     92    20     1.49e+05     1.19e+05        3e+04          4.8         7.06         56.7         3.54\n",
            "     92    30     2.22e+07      1.1e+07     1.12e+07         54.5         67.9          273         68.4\n",
            "     92    40     7.33e+05     7.74e+04     6.55e+05         4.09         5.69          215         16.6\n",
            "     92    50     4.97e+05     4.89e+05     8.26e+03         6.96         14.3         24.2         1.86\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92     2     2.57e+05     4.65e+04     2.11e+05         3.35         4.41          123         8.25\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              92  135.344    0.002     3.93e+05     1.07e+06     1.46e+06         4.68         9.92          137         12.6\n",
            "! Validation         92  135.344    0.002     4.54e+04      2.6e+05     3.06e+05          3.1         4.36          126          9.3\n",
            "Wall time: 135.3444827809999\n",
            "! Best model       92 305681.321\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93    10     8.35e+05     4.81e+04     7.87e+05            3         4.48          145         18.1\n",
            "     93    20     6.21e+05     1.02e+04     6.11e+05          1.7         2.06          176           16\n",
            "     93    30     2.14e+07     1.07e+07     1.08e+07         54.6         66.8          268         67.1\n",
            "     93    40     8.15e+04     3.39e+04     4.76e+04         2.58         3.77         98.2         4.46\n",
            "     93    50     1.09e+05     5.46e+04     5.44e+04         3.58         4.78         90.6         4.77\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93     2      2.6e+05      4.6e+04     2.14e+05         3.36         4.39          125         8.32\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              93  136.657    0.002     3.79e+05     1.05e+06     1.43e+06         4.61         9.66          139         12.7\n",
            "! Validation         93  136.657    0.002     4.53e+04     2.61e+05     3.07e+05         3.11         4.35          126         9.31\n",
            "Wall time: 136.65780410600019\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94    10     4.18e+05     3.96e+04     3.79e+05         3.21         4.07          189         12.6\n",
            "     94    20     7.35e+04     2.14e+04     5.21e+04         2.28         2.99         65.3         4.67\n",
            "     94    30     5.12e+04     4.78e+04     3.39e+03         3.47         4.47         20.3         1.19\n",
            "     94    40     3.68e+05     7.41e+04     2.94e+05          4.1         5.57          111         11.1\n",
            "     94    50      8.7e+05     4.63e+04     8.24e+05         2.92          4.4          149         18.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94     2     2.53e+05     4.69e+04     2.06e+05         3.41         4.43          122         8.12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              94  137.980    0.002        4e+05     1.02e+06     1.42e+06          4.7         10.3          134         12.2\n",
            "! Validation         94  137.980    0.002     4.61e+04     2.57e+05     3.03e+05         3.15          4.4          125         9.22\n",
            "Wall time: 137.98044820700034\n",
            "! Best model       94 303300.110\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95    10     7.39e+05     8.27e+04     6.56e+05         4.22         5.88          215         16.6\n",
            "     95    20     3.19e+05     4.61e+04     2.73e+05         3.52         4.39          171         10.7\n",
            "     95    30      5.8e+06      5.6e+06     1.99e+05         19.4         48.4         91.4         9.14\n",
            "     95    40     1.82e+04     1.82e+04         53.5         2.11         2.76          1.8         0.15\n",
            "     95    50     5.85e+04     2.12e+04     3.73e+04         2.39         2.98         19.8         3.95\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95     2     2.51e+05     4.69e+04     2.04e+05         3.41         4.43          121         8.05\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              95  139.308    0.002     4.31e+05     1.04e+06     1.47e+06         4.88           11          133         12.2\n",
            "! Validation         95  139.308    0.002     4.62e+04     2.56e+05     3.02e+05         3.14          4.4          124         9.18\n",
            "Wall time: 139.30874935300017\n",
            "! Best model       95 301831.356\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96    10     7.93e+06     1.42e+05     7.79e+06         6.43         7.72          571         57.1\n",
            "     96    20     2.38e+05     4.46e+04     1.94e+05         3.13         4.32          189            9\n",
            "     96    30     4.25e+05     1.56e+04      4.1e+05         2.12         2.55          118         13.1\n",
            "     96    40     1.98e+05     4.36e+04     1.54e+05         3.39         4.27         80.4         8.04\n",
            "     96    50     3.97e+05     3.94e+05     3.66e+03         6.32         12.8         16.1         1.24\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96     2     2.46e+05     4.47e+04     2.01e+05          3.3         4.32          120         7.94\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              96  140.627    0.002     3.58e+05     1.03e+06     1.39e+06         4.34         9.41          136         12.4\n",
            "! Validation         96  140.627    0.002     4.34e+04     2.55e+05     2.98e+05         3.02         4.27          124         9.14\n",
            "Wall time: 140.62808992600003\n",
            "! Best model       96 298005.961\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97    10     1.95e+04     1.95e+04       0.0529         2.14         2.86       0.0565      0.00471\n",
            "     97    20     1.23e+05     1.09e+05     1.36e+04         4.65         6.75         38.1         2.38\n",
            "     97    30      2.4e+04     2.13e+04      2.7e+03         2.25         2.99         12.8         1.06\n",
            "     97    40      1.9e+07     9.31e+06      9.7e+06         53.1         62.4          255         63.7\n",
            "     97    50     1.05e+06     4.57e+05     5.89e+05         10.3         13.8          204         15.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97     2     2.42e+05     4.37e+04     1.98e+05         3.29         4.28          118         7.85\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              97  141.939    0.002     3.38e+05     1.02e+06     1.36e+06         4.56         9.21          134         12.3\n",
            "! Validation         97  141.939    0.002     4.33e+04     2.53e+05     2.96e+05         3.03         4.26          123         9.08\n",
            "Wall time: 141.93971317199976\n",
            "! Best model       97 295914.760\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98    10     4.06e+04     1.98e+04     2.08e+04         2.19         2.88         50.2         2.95\n",
            "     98    20     1.98e+05     7.54e+04     1.23e+05         4.11         5.62         85.9         7.16\n",
            "     98    30     3.58e+06     2.47e+06     1.11e+06         17.2         32.2          215         21.5\n",
            "     98    40      4.7e+05     2.37e+04     4.46e+05         2.53         3.15          246         13.7\n",
            "     98    50     1.01e+05     3.87e+04     6.19e+04         2.93         4.02         96.7         5.09\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98     2     2.41e+05     4.16e+04        2e+05         3.21         4.17          119         7.87\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              98  143.250    0.002     4.49e+05     9.68e+05     1.42e+06         4.66         10.6          139         12.4\n",
            "! Validation         98  143.250    0.002     4.14e+04     2.53e+05     2.94e+05         2.96         4.16          123         9.07\n",
            "Wall time: 143.25070222900013\n",
            "! Best model       98 294134.406\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99    10     1.71e+06     8.39e+05     8.71e+05         11.6         18.7          172         19.1\n",
            "     99    20     1.77e+05     1.48e+05     2.84e+04         6.28         7.88         51.7         3.45\n",
            "     99    30     1.56e+05     4.24e+04     1.14e+05         3.08         4.21         55.2          6.9\n",
            "     99    40     1.05e+06     5.11e+05     5.36e+05         10.7         14.6          195           15\n",
            "     99    50     4.41e+05     4.86e+04     3.92e+05         3.58         4.51          192         12.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99     2     2.43e+05     4.15e+04     2.01e+05          3.2         4.17          119          7.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              99  144.567    0.002     2.85e+05     1.12e+06     1.41e+06         4.54         9.54          137         12.8\n",
            "! Validation         99  144.567    0.002     4.13e+04     2.53e+05     2.94e+05         2.95         4.16          123         9.07\n",
            "Wall time: 144.56805383200026\n",
            "! Best model       99 294126.299\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100    10     2.06e+05      3.1e+04     1.75e+05         2.87          3.6          180         8.55\n",
            "    100    20     1.78e+05     4.86e+04     1.29e+05         3.47         4.51         73.6         7.36\n",
            "    100    30     1.89e+04     1.85e+04          468         2.29         2.78         3.54        0.442\n",
            "    100    40     1.47e+05     1.19e+05     2.86e+04         4.68         7.04         55.4         3.46\n",
            "    100    50     4.19e+05     1.06e+05     3.13e+05         4.75         6.67          114         11.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100     2     2.35e+05     4.39e+04     1.91e+05         3.27         4.28          115         7.63\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train             100  145.932    0.002     3.84e+05     9.43e+05     1.33e+06         4.81           10          126         11.6\n",
            "! Validation        100  145.932    0.002     4.36e+04     2.48e+05     2.91e+05         3.02         4.27          122         8.95\n",
            "Wall time: 145.93276422100007\n",
            "! Best model      100 291324.365\n",
            "! Stop training: max epochs\n",
            "Wall time: 145.94636841400006\n",
            "Cumulative wall time: 145.94636841400006\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mDES\u001b[0m at: \u001b[34mhttps://wandb.ai/anony-moose-739946801467655344/allegro-tutorial/runs/X21zT9vqtLkVz-SsuUv06vqAFApxL5Q4UxC7Uc57up8\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241212_235036-X21zT9vqtLkVz-SsuUv06vqAFApxL5Q4UxC7Uc57up8/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zvsbPHkkV3og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Potential file for MD simulations using LAMMPS"
      ],
      "metadata": {
        "id": "PPxZ3549XMFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!nequip-deploy build --train-dir results/silicon-tutorial/si si-deployed.pth\n",
        "!nequip-deploy build --train-dir results/DES-tutorial/DES as-deployed.pth\n",
        "!ls *pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnHEECs1CY4k",
        "outputId": "73f8d14e-cbb3-4750-b10b-8fa9e7214da7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "INFO:root:Loading best_model.pth from training session...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/deploy.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\n",
            "INFO:root:Compiled & optimized model.\n",
            "as-deployed.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check model on test dataset, that model did not see."
      ],
      "metadata": {
        "id": "cyUEWf0lXbPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!nequip-evaluate --train-dir results/silicon-tutorial/si --batch-size 5\n",
        "!nequip-evaluate --train-dir results/DES-tutorial/DES --batch-size 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nElT2PbgCgF3",
        "outputId": "c726dc78-6651-44a5-c946-6515bc7b950f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/evaluate.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  trainer = torch.load(\n",
            "Using device: cuda\n",
            "Please note that _all_ machine learning models running on CUDA hardware are generally somewhat nondeterministic and that this can manifest in small, generally unimportant variation in the final test errors.\n",
            "Loading model... \n",
            "/usr/local/lib/python3.10/dist-packages/nequip/train/trainer.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_state_dict = torch.load(traindir + \"/\" + model_name, map_location=device)\n",
            "    loaded model\n",
            "Loading original dataset...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (18700 frames) minus training (50 frames) and validation frames (10 frames), yielding a test set size of 18640 frames.\n",
            "Starting...\n",
            "  0% 0/18640 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  0% 5/18640 [00:01<1:16:16,  4.07it/s]\n",
            "  0% 10/18640 [00:01<48:31,  6.40it/s] \n",
            "  0% 15/18640 [00:03<1:26:20,  3.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  0% 50/18640 [00:03<16:13, 19.10it/s]  \n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  0% 85/18640 [00:04<07:55, 38.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 120/18640 [00:04<04:51, 63.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 155/18640 [00:04<03:19, 92.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 190/18640 [00:04<02:27, 125.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 225/18640 [00:04<01:55, 159.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 260/18640 [00:04<01:34, 193.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 300/18640 [00:04<01:19, 229.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 335/18640 [00:04<01:11, 256.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 370/18640 [00:04<01:05, 278.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 410/18640 [00:04<01:00, 299.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 450/18640 [00:05<00:57, 314.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 490/18640 [00:05<00:55, 326.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 530/18640 [00:05<00:54, 334.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 570/18640 [00:05<00:53, 339.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 610/18640 [00:05<00:52, 342.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 650/18640 [00:05<00:52, 345.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 690/18640 [00:05<00:52, 343.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 725/18640 [00:05<00:52, 343.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 760/18640 [00:05<00:51, 345.34it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 800/18640 [00:06<00:51, 347.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 835/18640 [00:06<00:52, 342.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 870/18640 [00:06<00:51, 343.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 910/18640 [00:06<00:51, 345.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 945/18640 [00:06<00:51, 345.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 985/18640 [00:06<00:50, 347.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 1020/18640 [00:06<00:50, 345.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1055/18640 [00:06<00:51, 344.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1090/18640 [00:06<00:51, 343.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1130/18640 [00:07<00:50, 345.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1165/18640 [00:07<00:50, 345.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1200/18640 [00:07<00:50, 345.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1240/18640 [00:07<00:50, 347.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1275/18640 [00:07<00:49, 347.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1310/18640 [00:07<00:49, 346.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1345/18640 [00:07<00:49, 347.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1380/18640 [00:07<00:49, 347.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1420/18640 [00:07<00:49, 348.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1460/18640 [00:07<00:49, 349.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1500/18640 [00:08<00:48, 351.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1540/18640 [00:08<00:48, 352.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1580/18640 [00:08<00:48, 350.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1620/18640 [00:08<01:06, 254.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1655/18640 [00:08<01:01, 274.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1695/18640 [00:08<00:57, 295.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1730/18640 [00:08<00:54, 309.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1770/18640 [00:08<00:52, 322.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1805/18640 [00:09<00:51, 328.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1845/18640 [00:09<00:49, 337.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1885/18640 [00:09<00:48, 343.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1925/18640 [00:09<00:47, 349.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 1965/18640 [00:09<00:47, 354.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2005/18640 [00:09<00:46, 355.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2045/18640 [00:09<00:46, 355.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2085/18640 [00:09<00:46, 354.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2125/18640 [00:09<00:47, 344.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2160/18640 [00:10<00:48, 342.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2195/18640 [00:10<00:48, 341.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2230/18640 [00:10<00:47, 343.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2270/18640 [00:10<00:47, 347.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2310/18640 [00:10<00:46, 349.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2350/18640 [00:10<00:46, 350.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2390/18640 [00:10<00:46, 349.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2425/18640 [00:10<00:46, 348.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2465/18640 [00:10<00:45, 352.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2505/18640 [00:11<00:46, 349.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2540/18640 [00:11<00:46, 348.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2580/18640 [00:11<00:45, 351.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2620/18640 [00:11<00:45, 351.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2660/18640 [00:11<00:46, 345.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2700/18640 [00:11<00:45, 349.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2740/18640 [00:11<00:45, 349.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2780/18640 [00:11<00:44, 353.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2820/18640 [00:11<00:44, 353.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2860/18640 [00:12<00:44, 356.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2900/18640 [00:12<00:44, 356.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2940/18640 [00:12<00:43, 357.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2980/18640 [00:12<00:43, 357.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 3020/18640 [00:12<00:43, 359.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 3060/18640 [00:12<00:43, 360.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3100/18640 [00:12<00:43, 358.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3140/18640 [00:12<00:43, 358.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3180/18640 [00:12<00:43, 358.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3220/18640 [00:13<00:43, 357.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3260/18640 [00:13<00:42, 358.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3300/18640 [00:13<00:43, 356.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3340/18640 [00:13<00:42, 357.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3380/18640 [00:13<00:42, 356.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3420/18640 [00:13<00:42, 357.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3460/18640 [00:13<00:42, 355.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3500/18640 [00:13<00:42, 356.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3540/18640 [00:13<00:42, 354.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3580/18640 [00:14<00:42, 357.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3620/18640 [00:14<00:41, 359.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3660/18640 [00:14<00:41, 360.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3700/18640 [00:14<00:41, 360.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3740/18640 [00:14<00:41, 361.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3780/18640 [00:14<00:41, 357.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3820/18640 [00:14<00:41, 355.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3860/18640 [00:14<00:41, 356.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3900/18640 [00:14<00:41, 356.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3940/18640 [00:15<00:41, 357.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3980/18640 [00:15<00:40, 359.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4020/18640 [00:15<00:40, 359.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4060/18640 [00:15<00:40, 358.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4100/18640 [00:15<00:40, 359.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4140/18640 [00:15<00:40, 359.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4180/18640 [00:15<00:40, 358.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4220/18640 [00:15<00:40, 358.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4260/18640 [00:15<00:40, 359.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4300/18640 [00:16<00:39, 360.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4340/18640 [00:16<00:40, 356.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4380/18640 [00:16<00:39, 358.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4420/18640 [00:16<00:39, 359.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4460/18640 [00:16<00:39, 359.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4500/18640 [00:16<00:39, 360.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4540/18640 [00:16<00:39, 358.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4580/18640 [00:16<00:39, 359.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4620/18640 [00:16<00:39, 358.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4660/18640 [00:17<00:39, 357.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4700/18640 [00:17<00:39, 357.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4740/18640 [00:17<00:38, 358.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4780/18640 [00:17<00:38, 361.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4820/18640 [00:17<00:38, 361.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4860/18640 [00:17<00:38, 361.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4900/18640 [00:17<00:38, 357.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 4940/18640 [00:17<00:38, 357.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 4980/18640 [00:17<00:38, 357.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5020/18640 [00:18<00:38, 358.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5060/18640 [00:18<00:37, 358.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5100/18640 [00:18<00:37, 358.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5140/18640 [00:18<00:37, 359.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5180/18640 [00:18<00:37, 360.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5220/18640 [00:18<00:37, 358.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5260/18640 [00:18<00:37, 357.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5300/18640 [00:18<00:37, 355.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5340/18640 [00:19<00:37, 353.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5380/18640 [00:19<00:37, 353.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5420/18640 [00:19<00:37, 355.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5460/18640 [00:19<00:37, 355.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5500/18640 [00:19<00:36, 357.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5540/18640 [00:19<00:36, 359.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5580/18640 [00:19<00:36, 358.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5620/18640 [00:19<00:36, 355.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5660/18640 [00:19<00:36, 354.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5700/18640 [00:20<00:36, 355.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5740/18640 [00:20<00:36, 355.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5780/18640 [00:20<00:36, 355.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5820/18640 [00:20<00:35, 356.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5860/18640 [00:20<00:35, 358.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5900/18640 [00:20<00:35, 361.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5940/18640 [00:20<00:34, 362.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5980/18640 [00:20<00:35, 360.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 6020/18640 [00:20<00:35, 357.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6060/18640 [00:21<00:35, 358.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6100/18640 [00:21<00:34, 358.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6140/18640 [00:21<00:34, 358.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6180/18640 [00:21<00:34, 360.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6220/18640 [00:21<00:34, 361.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6260/18640 [00:21<00:34, 361.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6300/18640 [00:21<00:34, 360.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6340/18640 [00:21<00:34, 358.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6380/18640 [00:21<00:34, 354.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6420/18640 [00:22<00:34, 354.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6460/18640 [00:22<00:33, 358.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6500/18640 [00:22<00:33, 359.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6540/18640 [00:22<00:33, 359.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6580/18640 [00:22<00:33, 361.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6620/18640 [00:22<00:33, 361.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6660/18640 [00:22<00:33, 360.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6700/18640 [00:22<00:33, 355.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6740/18640 [00:22<00:33, 356.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6780/18640 [00:23<00:33, 354.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6820/18640 [00:23<00:33, 355.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6860/18640 [00:23<00:33, 356.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6900/18640 [00:23<00:33, 353.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6940/18640 [00:23<00:32, 354.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6980/18640 [00:23<00:32, 355.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7020/18640 [00:23<00:32, 357.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7060/18640 [00:23<00:32, 357.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7100/18640 [00:23<00:32, 356.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7140/18640 [00:24<00:32, 357.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7180/18640 [00:24<00:31, 358.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7220/18640 [00:24<00:31, 359.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7260/18640 [00:24<00:31, 360.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7300/18640 [00:24<00:31, 362.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7340/18640 [00:24<00:31, 363.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7380/18640 [00:24<00:31, 361.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7420/18640 [00:24<00:31, 361.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7460/18640 [00:24<00:31, 359.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7500/18640 [00:25<00:30, 360.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7540/18640 [00:25<00:30, 361.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7580/18640 [00:25<00:30, 360.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7620/18640 [00:25<00:30, 357.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7660/18640 [00:25<00:30, 360.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7700/18640 [00:25<00:30, 361.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7740/18640 [00:25<00:30, 363.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7780/18640 [00:25<00:29, 362.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7820/18640 [00:25<00:29, 360.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7860/18640 [00:26<00:29, 360.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7900/18640 [00:26<00:29, 361.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 7940/18640 [00:26<00:29, 361.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 7980/18640 [00:26<00:29, 363.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8020/18640 [00:26<00:29, 361.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8060/18640 [00:26<00:29, 362.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8100/18640 [00:26<00:29, 361.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8140/18640 [00:26<00:29, 361.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8180/18640 [00:26<00:29, 359.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8220/18640 [00:27<00:28, 360.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8260/18640 [00:27<00:28, 360.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8300/18640 [00:27<00:28, 359.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8340/18640 [00:27<00:28, 361.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8380/18640 [00:27<00:28, 361.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8420/18640 [00:27<00:28, 363.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8460/18640 [00:27<00:28, 362.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8500/18640 [00:27<00:27, 363.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8540/18640 [00:27<00:27, 362.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8580/18640 [00:28<00:27, 361.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8620/18640 [00:28<00:27, 360.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8660/18640 [00:28<00:27, 361.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8700/18640 [00:28<00:27, 361.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8740/18640 [00:28<00:27, 363.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8780/18640 [00:28<00:26, 365.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8820/18640 [00:28<00:26, 365.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8860/18640 [00:28<00:26, 364.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8900/18640 [00:28<00:26, 364.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8940/18640 [00:29<00:26, 361.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8980/18640 [00:29<00:26, 362.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 9020/18640 [00:29<00:26, 359.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9060/18640 [00:29<00:26, 359.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9100/18640 [00:29<00:26, 360.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9140/18640 [00:29<00:26, 360.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9180/18640 [00:29<00:26, 360.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9220/18640 [00:29<00:26, 359.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9260/18640 [00:29<00:26, 360.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9300/18640 [00:30<00:25, 360.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9340/18640 [00:30<00:25, 360.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9380/18640 [00:30<00:25, 360.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9420/18640 [00:30<00:25, 357.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9460/18640 [00:30<00:25, 356.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9500/18640 [00:30<00:25, 358.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9540/18640 [00:30<00:25, 357.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9580/18640 [00:30<00:25, 357.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9620/18640 [00:30<00:25, 357.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9660/18640 [00:31<00:25, 359.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9700/18640 [00:31<00:24, 359.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9740/18640 [00:31<00:24, 358.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9780/18640 [00:31<00:24, 361.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9820/18640 [00:31<00:24, 362.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9860/18640 [00:31<00:24, 364.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9900/18640 [00:31<00:24, 364.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9940/18640 [00:31<00:24, 361.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 9980/18640 [00:31<00:23, 361.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10020/18640 [00:32<00:23, 361.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10060/18640 [00:32<00:23, 359.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10100/18640 [00:32<00:23, 356.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10140/18640 [00:32<00:23, 355.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10180/18640 [00:32<00:23, 356.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10220/18640 [00:32<00:23, 355.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10260/18640 [00:32<00:23, 354.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10300/18640 [00:32<00:23, 356.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10340/18640 [00:32<00:23, 356.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10380/18640 [00:33<00:23, 356.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10420/18640 [00:33<00:22, 357.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10460/18640 [00:33<00:22, 357.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10500/18640 [00:33<00:22, 359.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10540/18640 [00:33<00:22, 359.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10580/18640 [00:33<00:22, 357.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10620/18640 [00:33<00:22, 350.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10660/18640 [00:33<00:22, 351.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10700/18640 [00:33<00:22, 351.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10740/18640 [00:34<00:22, 353.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10780/18640 [00:34<00:22, 352.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10820/18640 [00:34<00:22, 355.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10860/18640 [00:34<00:21, 356.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10900/18640 [00:34<00:21, 356.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 10940/18640 [00:34<00:21, 352.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 10980/18640 [00:34<00:21, 350.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 11020/18640 [00:34<00:21, 349.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 11055/18640 [00:34<00:22, 343.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11095/18640 [00:35<00:21, 347.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11135/18640 [00:35<00:21, 348.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11170/18640 [00:35<00:21, 347.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11210/18640 [00:35<00:21, 348.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11250/18640 [00:35<00:21, 350.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11290/18640 [00:35<00:20, 353.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11330/18640 [00:35<00:20, 354.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11370/18640 [00:35<00:20, 352.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11410/18640 [00:35<00:20, 354.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11450/18640 [00:36<00:20, 355.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11490/18640 [00:36<00:20, 357.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11530/18640 [00:36<00:19, 359.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11570/18640 [00:36<00:19, 357.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11610/18640 [00:36<00:19, 358.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11650/18640 [00:36<00:19, 359.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11690/18640 [00:36<00:19, 359.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11730/18640 [00:36<00:19, 359.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11770/18640 [00:36<00:19, 357.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11810/18640 [00:37<00:19, 358.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11850/18640 [00:37<00:18, 358.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11890/18640 [00:37<00:18, 360.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11930/18640 [00:37<00:18, 360.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11970/18640 [00:37<00:18, 362.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 12010/18640 [00:37<00:18, 363.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12050/18640 [00:37<00:18, 362.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12090/18640 [00:37<00:17, 364.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12130/18640 [00:37<00:18, 359.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12170/18640 [00:38<00:17, 359.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12210/18640 [00:38<00:17, 358.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12250/18640 [00:38<00:17, 359.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12290/18640 [00:38<00:17, 360.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12330/18640 [00:38<00:17, 360.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12370/18640 [00:38<00:17, 363.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12410/18640 [00:38<00:17, 361.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12450/18640 [00:38<00:17, 362.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12490/18640 [00:38<00:17, 361.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12530/18640 [00:39<00:16, 360.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12570/18640 [00:39<00:16, 359.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12610/18640 [00:39<00:16, 359.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12650/18640 [00:39<00:16, 360.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12690/18640 [00:39<00:16, 361.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12730/18640 [00:39<00:16, 362.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12770/18640 [00:39<00:16, 360.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12810/18640 [00:39<00:16, 360.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12850/18640 [00:39<00:16, 359.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12890/18640 [00:40<00:15, 361.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12930/18640 [00:40<00:15, 360.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 12970/18640 [00:40<00:15, 358.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13010/18640 [00:40<00:15, 357.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13050/18640 [00:40<00:15, 358.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13090/18640 [00:40<00:15, 358.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13130/18640 [00:40<00:15, 358.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13170/18640 [00:40<00:15, 356.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13210/18640 [00:40<00:15, 357.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13250/18640 [00:41<00:15, 356.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13290/18640 [00:41<00:14, 357.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13330/18640 [00:41<00:14, 356.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13370/18640 [00:41<00:14, 357.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13410/18640 [00:41<00:14, 361.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13450/18640 [00:41<00:14, 359.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13490/18640 [00:41<00:14, 358.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13530/18640 [00:41<00:14, 355.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13570/18640 [00:41<00:14, 355.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13610/18640 [00:42<00:14, 357.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13650/18640 [00:42<00:13, 356.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13690/18640 [00:42<00:13, 355.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13730/18640 [00:42<00:13, 357.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13770/18640 [00:42<00:13, 358.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13810/18640 [00:42<00:13, 360.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13850/18640 [00:42<00:13, 359.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13890/18640 [00:42<00:13, 359.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13930/18640 [00:42<00:13, 359.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13970/18640 [00:43<00:13, 358.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 14010/18640 [00:43<00:12, 359.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 14050/18640 [00:43<00:12, 358.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14090/18640 [00:43<00:12, 357.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14130/18640 [00:43<00:12, 360.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14170/18640 [00:43<00:12, 361.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14210/18640 [00:43<00:12, 361.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14250/18640 [00:43<00:12, 362.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14290/18640 [00:43<00:12, 362.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14330/18640 [00:44<00:11, 362.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14370/18640 [00:44<00:11, 361.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14410/18640 [00:44<00:11, 361.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14450/18640 [00:44<00:11, 359.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14490/18640 [00:44<00:11, 359.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14530/18640 [00:44<00:11, 358.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14570/18640 [00:44<00:11, 359.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14610/18640 [00:44<00:11, 359.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14650/18640 [00:44<00:11, 360.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14690/18640 [00:45<00:10, 359.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14730/18640 [00:45<00:10, 360.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14770/18640 [00:45<00:10, 358.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14810/18640 [00:45<00:10, 358.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14850/18640 [00:45<00:10, 354.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14890/18640 [00:45<00:10, 351.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14930/18640 [00:45<00:10, 353.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14970/18640 [00:45<00:10, 355.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15010/18640 [00:45<00:10, 359.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15050/18640 [00:46<00:09, 359.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15090/18640 [00:46<00:09, 360.34it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15130/18640 [00:46<00:09, 359.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15170/18640 [00:46<00:09, 355.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15210/18640 [00:46<00:09, 353.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15250/18640 [00:46<00:09, 354.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15290/18640 [00:46<00:09, 354.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15330/18640 [00:46<00:09, 357.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15370/18640 [00:46<00:09, 358.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15410/18640 [00:47<00:09, 356.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15450/18640 [00:47<00:08, 357.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15490/18640 [00:47<00:08, 358.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15530/18640 [00:47<00:08, 358.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15570/18640 [00:47<00:08, 359.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15610/18640 [00:47<00:08, 361.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15650/18640 [00:47<00:08, 361.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15690/18640 [00:47<00:08, 360.34it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15730/18640 [00:47<00:08, 360.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15770/18640 [00:48<00:07, 360.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15810/18640 [00:48<00:07, 362.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15850/18640 [00:48<00:07, 362.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15890/18640 [00:48<00:07, 362.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15930/18640 [00:48<00:07, 362.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 15970/18640 [00:48<00:07, 361.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16010/18640 [00:48<00:07, 360.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16050/18640 [00:48<00:07, 361.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16090/18640 [00:48<00:07, 359.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16130/18640 [00:49<00:06, 360.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16170/18640 [00:49<00:06, 361.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16210/18640 [00:49<00:06, 361.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16250/18640 [00:49<00:06, 360.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16290/18640 [00:49<00:06, 360.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16330/18640 [00:49<00:06, 361.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16370/18640 [00:49<00:06, 357.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16410/18640 [00:49<00:06, 358.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16450/18640 [00:49<00:06, 359.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16490/18640 [00:50<00:05, 358.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16530/18640 [00:50<00:05, 359.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16570/18640 [00:50<00:05, 361.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16610/18640 [00:50<00:05, 360.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16650/18640 [00:50<00:05, 362.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16690/18640 [00:50<00:05, 363.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16730/18640 [00:50<00:05, 360.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16770/18640 [00:50<00:05, 358.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16810/18640 [00:50<00:05, 358.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16850/18640 [00:51<00:04, 358.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16890/18640 [00:51<00:04, 361.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16930/18640 [00:51<00:04, 361.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16970/18640 [00:51<00:04, 364.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 17010/18640 [00:51<00:04, 362.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 17050/18640 [00:51<00:04, 363.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17090/18640 [00:51<00:04, 361.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17130/18640 [00:51<00:04, 359.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17170/18640 [00:51<00:04, 359.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17210/18640 [00:52<00:03, 362.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17250/18640 [00:52<00:03, 362.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17290/18640 [00:52<00:03, 365.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17330/18640 [00:52<00:03, 366.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17370/18640 [00:52<00:03, 366.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17410/18640 [00:52<00:03, 367.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17450/18640 [00:52<00:03, 362.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17490/18640 [00:52<00:03, 364.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17530/18640 [00:52<00:03, 363.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17570/18640 [00:53<00:02, 363.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17610/18640 [00:53<00:02, 361.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17650/18640 [00:53<00:02, 361.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17690/18640 [00:53<00:02, 362.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17730/18640 [00:53<00:02, 361.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17770/18640 [00:53<00:02, 361.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17810/18640 [00:53<00:02, 360.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17850/18640 [00:53<00:02, 360.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17890/18640 [00:53<00:02, 357.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17930/18640 [00:54<00:01, 358.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17970/18640 [00:54<00:01, 361.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18010/18640 [00:54<00:01, 359.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18050/18640 [00:54<00:01, 361.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18090/18640 [00:54<00:01, 360.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18130/18640 [00:54<00:01, 362.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18170/18640 [00:54<00:01, 360.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18210/18640 [00:54<00:01, 362.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18250/18640 [00:54<00:01, 361.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18290/18640 [00:55<00:00, 361.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18330/18640 [00:55<00:00, 361.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18370/18640 [00:55<00:00, 362.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18410/18640 [00:55<00:00, 360.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18450/18640 [00:55<00:00, 360.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18490/18640 [00:55<00:00, 360.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18530/18640 [00:55<00:00, 359.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18570/18640 [00:55<00:00, 357.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18610/18640 [00:55<00:00, 358.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18640/18640 [00:56<00:00, 332.68it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  4.030784           \n",
            "              f_rmse =  7.503667           \n",
            "               e_mae =  284.517317         \n",
            "             e/N_mae =  31.603284          \n",
            "               f_mae =  4.030784           \n",
            "              f_rmse =  7.503667           \n",
            "               e_mae =  284.517317         \n",
            "             e/N_mae =  31.603284          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time for Ground Truth using LAMMPS**"
      ],
      "metadata": {
        "id": "eOEE44cTCvSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup LAMMPS\n",
        "This is how we get RDF (Radial Function Distribution) for Molecular dynamics run, before and after coarsening."
      ],
      "metadata": {
        "id": "-KWEL5c4X8SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "example_atoms = read('./Si_data/DES_L.xyz', index=0)\n",
        "#example_atoms = load('./benchmark_data/aspirin_ccsd-train.npz')\n",
        "#write('./si.data', example_atoms, format='lammps-data')\n",
        "write('./des.data', example_atoms, format='lammps-data')"
      ],
      "metadata": {
        "id": "-YAmau_8D6bU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual MD validation using LAMMPS that does not use ML\n",
        "Since we verified that Allegro was doing pretty well, we will stick to matching with Allegro and not run LAMMPS for coarsened molecules."
      ],
      "metadata": {
        "id": "kOIRt_rJYLmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lammps_input = \"\"\"\n",
        "units\tmetal\n",
        "atom_style atomic\n",
        "dimension 3\n",
        "\n",
        "# set newton on for pair_allegro (off for pair_nequip)\n",
        "newton on\n",
        "boundary p p p\n",
        "read_data ../des.data\n",
        "\n",
        "# if you want to run a larger system, simply replicate the system in space\n",
        "# replicate 3 3 3\n",
        "\n",
        "# allegro pair style\n",
        "pair_style\tallegro\n",
        "pair_coeff\t* * ../as-deployed.pth N H\n",
        "\n",
        "# N, H mass.\n",
        "mass 1 14.0067\n",
        "mass 2 1.00784\n",
        "\n",
        "velocity all create 300.0 1234567 loop geom\n",
        "\n",
        "neighbor 1.0 bin\n",
        "neigh_modify delay 5 every 1\n",
        "\n",
        "timestep 0.0001\n",
        "thermo 1\n",
        "\n",
        "# nose-hoover thermostat, 300K\n",
        "fix  1 all nvt temp 300 300 $(100*dt)\n",
        "\n",
        "# compute rdf and average after some equilibration\n",
        "comm_modify cutoff 7.0\n",
        "compute rdfall all rdf 1000 cutoff 5.0\n",
        "fix 2 all ave/time 1 2500 5000 c_rdfall[*] file des.rdf mode vector\n",
        "\n",
        "# run 5ps\n",
        "run 5000\n",
        "\"\"\"\n",
        "!rm -rf ./lammps_run\n",
        "!mkdir lammps_run\n",
        "with open(\"lammps_run/des_rdf.in\", \"w\") as f:\n",
        "    f.write(lammps_input)\n",
        "# DONE USING ALLEGRO FOR VALIDATION"
      ],
      "metadata": {
        "id": "5RQGnRCsC0gG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running LAMMPS\n",
        "This step is not needed for our coarsening. We match RDF (Radial Distribution Function) output of binned atoms, which are binned wrt to their energies.\n",
        "\n",
        "MDAnalysis package can also be used for RDF analysis."
      ],
      "metadata": {
        "id": "QzVl5f-cYmTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDF distribution\n",
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "!cd lammps_run/ && ../lammps/build/lmp -in des_rdf.in"
      ],
      "metadata": {
        "id": "St_X2LxSDAKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b282ca6e-7993-4833-d8ba-619b8d9bb693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAMMPS (19 Nov 2024 - Development - a78aee5-modified)\n",
            "OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:99)\n",
            "  using 1 OpenMP thread(s) per MPI task\n",
            "Reading data file ...\n",
            "  orthogonal box = (0 0 0) to (10 10 10)\n",
            "  1 by 1 by 1 MPI processor grid\n",
            "  reading atoms ...\n",
            "  5 atoms\n",
            "  read_data CPU = 0.005 seconds\n",
            "Allegro is using input precision f and output precision d\n",
            "Allegro: Loading model from ../as-deployed.pth\n",
            "Allegro: Freezing TorchScript model...\n",
            "Type mapping:\n",
            "Allegro type | Allegro name | LAMMPS type | LAMMPS name\n",
            "0 | H | 2 | H\n",
            "2 | N | 1 | N\n",
            "Neighbor list info ...\n",
            "  update: every = 1 steps, delay = 5 steps, check = yes\n",
            "  max neighbors/atom: 2000, page size: 100000\n",
            "  master list distance cutoff = 5\n",
            "  ghost atom cutoff = 7\n",
            "  binsize = 2.5, bins = 4 4 4\n",
            "  2 neighbor lists, perpetual/occasional/extra = 1 1 0\n",
            "  (1) pair allegro, perpetual\n",
            "      attributes: full, newton on, ghost\n",
            "      pair build: full/bin/ghost\n",
            "      stencil: full/ghost/bin/3d\n",
            "      bin: standard\n",
            "  (2) compute rdf, occasional\n",
            "      attributes: half, newton on, cut 6\n",
            "      pair build: half/bin/atomonly/newton\n",
            "      stencil: half/bin/3d\n",
            "      bin: standard\n",
            "Setting up Verlet run ...\n",
            "  Unit style    : metal\n",
            "  Current step  : 0\n",
            "  Time step     : 0.0001\n",
            "Per MPI rank memory allocation (min/avg/max) = 3.7 | 3.7 | 3.7 Mbytes\n",
            "   Step          Temp          E_pair         E_mol          TotEng         Press     \n",
            "         0   300           -93.914709      0             -93.759597     -5712.2258    \n",
            "         1   305.14425     -93.935175      0             -93.777403     -5597.0139    \n",
            "         2   386.15632     -93.97267       0             -93.773011     -5158.8697    \n",
            "         3   545.62247     -94.060696      0             -93.778587     -4855.8189    \n",
            "         4   786.76945     -94.181125      0             -93.774333     -4854.0613    \n",
            "         5   1113.7454     -94.354688      0             -93.778836     -4319.3773    \n",
            "         6   1531.3431     -94.575038      0             -93.783272     -4065.177     \n",
            "         7   2045.7423     -94.871263      0             -93.813531     -3961.7071    \n",
            "         8   2664.7828     -95.17065       0             -93.792849     -3202.3015    \n",
            "         9   3398.3314     -95.541488      0             -93.784412     -2362.5125    \n",
            "        10   4259.1566     -95.9947        0             -93.792543     -1255.2019    \n",
            "        11   5263.8462     -96.561066      0             -93.839443      298.64596    \n",
            "        12   6433.2245     -97.17867       0             -93.852431      2616.5453    \n",
            "        13   7792.5628     -97.962178      0             -93.933107      5080.7927    \n",
            "        14   9370.7829     -98.856795      0             -94.01172       8153.1835    \n",
            "        15   11199.483     -99.973434      0             -94.182847      11992.784    \n",
            "        16   13306.843     -101.27152      0             -94.391347      16664.195    \n",
            "        17   15708.023     -102.8304       0             -94.708713      21600.159    \n",
            "        18   18391.732     -104.7265       0             -95.217224      26723.307    \n",
            "        19   21299.005     -106.85691      0             -95.844462      31912.856    \n",
            "        20   24307.693     -109.3156       0             -96.747535      36467.155    \n",
            "        21   27222.01      -111.98402      0             -97.909141      39430.888    \n",
            "        22   29786.45      -114.78463      0             -99.383823      40499.302    \n",
            "        23   31725.814     -117.52415      0             -101.12062      39825.682    \n",
            "        24   32794.634     -119.99592      0             -103.03976      37029.493    \n",
            "        25   32823.954     -122.005        0             -105.03368      33220.19     \n",
            "        26   31754.167     -123.36184      0             -106.94365      29598.952    \n",
            "        27   29649.992     -124.05704      0             -108.72679      26950.287    \n",
            "        28   26709.322     -124.1119       0             -110.30209      26852.546    \n",
            "        29   23243.259     -123.65781      0             -111.6401       29459.838    \n",
            "        30   19614.977     -122.83499      0             -112.69325      35266.264    \n",
            "        31   16170.999     -121.9097       0             -113.54863      43435.889    \n",
            "        32   13185.951     -121.07182      0             -114.25415      52518.619    \n",
            "        33   10844.353     -120.36873      0             -114.76176      61781.733    \n",
            "        34   9247.3283     -120.03243      0             -115.25118      70157.664    \n",
            "        35   8430.5442     -119.96716      0             -115.60823      77251.592    \n",
            "        36   8384.1554     -120.27839      0             -115.94344      82878.562    \n",
            "        37   9066.8488     -121.02043      0             -116.3325       86906.361    \n",
            "        38   10416.336     -122.11021      0             -116.72454      89214.63     \n",
            "        39   12347.68      -123.64699      0             -117.26273      89774.14     \n",
            "        40   14749.565     -125.47043      0             -117.8443       88924.834    \n",
            "        41   17470.366     -127.64101      0             -118.60812      87460.129    \n",
            "        42   20296.983     -130.05883      0             -119.56447      85428.719    \n",
            "        43   22949.454     -132.61591      0             -120.75011      83725.513    \n",
            "        44   25101.17      -135.15422      0             -122.17589      82175.685    \n",
            "        45   26452.361     -137.44466      0             -123.76771      80774.074    \n",
            "        46   26840.251     -139.39062      0             -125.51312      78938.761    \n",
            "        47   26311.236     -140.92792      0             -127.32395      74838.695    \n",
            "        48   25094.726     -142.09827      0             -129.12328      68290.142    \n",
            "        49   23492.412     -142.96902      0             -130.82249      58484.556    \n",
            "        50   21762.269     -143.68511      0             -132.43314      46354.042    \n",
            "        51   20060.609     -144.27374      0             -133.90159      32914.394    \n",
            "        52   18436.243     -144.73242      0             -135.20013      18969.275    \n",
            "        53   16854.626     -145.08823      0             -136.3737       5484.047     \n",
            "        54   15245.236     -145.33978      0             -137.45737     -6688.2855    \n",
            "        55   13555.377     -145.39883      0             -138.39015     -16915.159    \n",
            "        56   11782.835     -145.26073      0             -139.16852     -24546.71     \n",
            "        57   9985.094      -145.00747      0             -139.84477     -29388.345    \n",
            "        58   8254.1225     -144.63929      0             -140.37157     -31863.634    \n",
            "        59   6684.0516     -144.25277      0             -140.79685     -32541.422    \n",
            "        60   5351.7704     -143.89167      0             -141.12459     -31845.729    \n",
            "        61   4305.866      -143.60289      0             -141.37659     -30336.1      \n",
            "        62   3566.4741     -143.44917      0             -141.60516     -28750.471    \n",
            "        63   3127.0693     -143.37292      0             -141.7561      -27282.589    \n",
            "        64   2958.4886     -143.45066      0             -141.921       -26158.294    \n",
            "        65   3014.3414     -143.61579      0             -142.05725     -25621.234    \n",
            "        66   3237.4944     -143.8567       0             -142.18278     -25117.973    \n",
            "        67   3565.4135     -144.21359      0             -142.37012     -24644.864    \n",
            "        68   3936.2142     -144.54099      0             -142.5058      -24012.984    \n",
            "        69   4294.3184     -144.90228      0             -142.68194     -22904.041    \n",
            "        70   4593.1022     -145.25907      0             -142.88425     -21019.353    \n",
            "        71   4797.6931     -145.5565       0             -143.0759      -18143.916    \n",
            "        72   4888.2859     -145.8095       0             -143.28206     -14379.936    \n",
            "        73   4863.0661     -146.03382      0             -143.51942     -9909.3934    \n",
            "        74   4738.921      -146.17805      0             -143.72784     -4729.3433    \n",
            "        75   4544.8634     -146.28954      0             -143.93966      576.61323    \n",
            "        76   4319.2356     -146.32116      0             -144.08794      5605.7745    \n",
            "        77   4102.3047     -146.40049      0             -144.27944      10158.95     \n",
            "        78   3924.9554     -146.4626       0             -144.43324      13990.619    \n",
            "        79   3803.9275     -146.59183      0             -144.62505      16679.675    \n",
            "        80   3738.1057     -146.69865      0             -144.7659       18390.547    \n",
            "        81   3710.2616     -146.88932      0             -144.97097      19182.566    \n",
            "        82   3692.4181     -147.01235      0             -145.10322      19177.699    \n",
            "        83   3652.8984     -147.13531      0             -145.24661      18863.098    \n",
            "        84   3565.2176     -147.24231      0             -145.39895      18048.965    \n",
            "        85   3414.6613     -147.3137       0             -145.54818      16930.523    \n",
            "        86   3202.5863     -147.33903      0             -145.68316      15902.712    \n",
            "        87   2945.2836     -147.33196      0             -145.80913      14717.851    \n",
            "        88   2671.092      -147.30921      0             -145.92815      13321.406    \n",
            "        89   2412.6303     -147.26182      0             -146.01439      11733.676    \n",
            "        90   2197.3685     -147.25325      0             -146.11712      9783.927     \n",
            "        91   2041.4551     -147.26758      0             -146.21207      7396.5646    \n",
            "        92   1946.0643     -147.2759       0             -146.2697       4886.8488    \n",
            "        93   1897.8847     -147.33022      0             -146.34893      2040.5717    \n",
            "        94   1872.2542     -147.3843       0             -146.41627     -807.16243    \n",
            "        95   1840.6813     -147.42963      0             -146.47793     -3678.6118    \n",
            "        96   1777.9079     -147.45738      0             -146.53813     -6388.6244    \n",
            "        97   1668.8488     -147.47055      0             -146.60768     -8749.2558    \n",
            "        98   1512.013      -147.44478      0             -146.66301     -10688.988    \n",
            "        99   1320.6332     -147.41249      0             -146.72967     -12289.24     \n",
            "       100   1119.7751     -147.34212      0             -146.76315     -13539.778    \n",
            "       101   938.89051     -147.26508      0             -146.77964     -14511.636    \n",
            "       102   805.58357     -147.249        0             -146.83248     -15504.255    \n",
            "       103   739.58173     -147.21278      0             -146.83039     -16334.862    \n",
            "       104   748.18072     -147.25861      0             -146.87177     -17235.188    \n",
            "       105   824.93225     -147.31117      0             -146.88465     -17977.938    \n",
            "       106   952.4566      -147.41438      0             -146.92192     -18760.346    \n",
            "       107   1106.083      -147.53855      0             -146.96666     -19306.745    \n",
            "       108   1258.7526     -147.65796      0             -147.00714     -19444.713    \n",
            "       109   1388.6428     -147.76968      0             -147.0517      -19330.033    \n",
            "       110   1482.3242     -147.83768      0             -147.07126     -18755.543    \n",
            "       111   1534.872      -147.91814      0             -147.12455     -17816.982    \n",
            "       112   1550.8753     -147.97592      0             -147.17406     -16445.617    \n",
            "       113   1541.6598     -148.00267      0             -147.20557     -14776.51     \n",
            "       114   1519.0694     -148.05963      0             -147.27421     -13062.996    \n",
            "       115   1491.6508     -148.09796      0             -147.32672     -11070.817    \n",
            "       116   1461.8745     -148.13606      0             -147.38022     -9172.4136    \n",
            "       117   1425.892      -148.14812      0             -147.41088     -7238.1333    \n",
            "       118   1375.4256     -148.17598      0             -147.46483     -5290.8924    \n",
            "       119   1300.7986     -148.14626      0             -147.4737      -3467.9904    \n",
            "       120   1194.5851     -148.13644      0             -147.51879     -1593.5775    \n",
            "       121   1055.0636     -148.10428      0             -147.55877      173.66141    \n",
            "       122   887.74751     -148.05641      0             -147.5974       1872.8662    \n",
            "       123   704.35578     -147.99677      0             -147.63259      3532.936     \n",
            "       124   522.11157     -147.90252      0             -147.63257      5057.04      \n",
            "       125   359.88595     -147.80858      0             -147.6225       6507.3377    \n",
            "       126   234.79456     -147.77598      0             -147.65458      7479.9556    \n",
            "       127   159.55994     -147.76821      0             -147.68571      8106.5746    \n",
            "       128   140.00704     -147.75105      0             -147.67866      8422.5197    \n",
            "       129   174.82084     -147.76705      0             -147.67666      8472.0722    \n",
            "       130   255.84038     -147.79001      0             -147.65773      8214.1309    \n",
            "       131   369.3856      -147.86212      0             -147.67113      7549.5283    \n",
            "       132   499.07671     -147.9305       0             -147.67246      6554.2751    \n",
            "       133   628.73456     -148.03814      0             -147.71306      5237.2341    \n",
            "       134   743.85245     -148.11747      0             -147.73287      4039.4358    \n",
            "       135   834.34653     -148.15958      0             -147.72819      2520.7739    \n",
            "       136   895.29716     -148.21868      0             -147.75577      1022.4581    \n",
            "       137   927.10813     -148.25411      0             -147.77475     -559.9999     \n",
            "       138   934.1864      -148.28588      0             -147.80287     -2333.6544    \n",
            "       139   922.27474     -148.32249      0             -147.84563     -4127.3497    \n",
            "       140   896.90343     -148.33191      0             -147.86818     -5904.3646    \n",
            "       141   861.76322     -148.31596      0             -147.87039     -7852.4312    \n",
            "       142   818.34046     -148.31079      0             -147.88767     -9530.5374    \n",
            "       143   765.55637     -148.30322      0             -147.9074      -11422.947    \n",
            "       144   701.52443     -148.2952       0             -147.93248     -12896.12     \n",
            "       145   625.05578     -148.26528      0             -147.9421      -14287.174    \n",
            "       146   536.61024     -148.24982      0             -147.97237     -15449.034    \n",
            "       147   439.92638     -148.1848       0             -147.95734     -16256.342    \n",
            "       148   341.65343     -148.1885       0             -148.01186     -16955.698    \n",
            "       149   250.09058     -148.0973       0             -147.96799     -17253.873    \n",
            "       150   174.36218     -148.08651      0             -147.99636     -17494.652    \n",
            "       151   122.65502     -148.06638      0             -148.00296     -17439.171    \n",
            "       152   100.23612     -148.05862      0             -148.00679     -17064.033    \n",
            "       153   108.6102      -148.05746      0             -148.0013      -16649.985    \n",
            "       154   145.15713     -148.08994      0             -148.01489     -16117.062    \n",
            "       155   203.45628     -148.10151      0             -147.99631     -15242.618    \n",
            "       156   274.6805      -148.16849      0             -148.02647     -14478.986    \n",
            "       157   348.95799     -148.21861      0             -148.03818     -13340.602    \n",
            "       158   416.91316     -148.2644       0             -148.04884     -12030.051    \n",
            "       159   471.74293     -148.30339      0             -148.05948     -10537.889    \n",
            "       160   509.46764     -148.31444      0             -148.05103     -8840.0534    \n",
            "       161   529.51876     -148.35898      0             -148.0852      -7059.7593    \n",
            "       162   533.8937      -148.36155      0             -148.08551     -5174.7798    \n",
            "       163   526.15497     -148.34705      0             -148.07501     -3369.68      \n",
            "       164   510.29741     -148.37111      0             -148.10727     -1251.6118    \n",
            "       165   489.82916     -148.38546      0             -148.1322       442.49803    \n",
            "       166   466.96777     -148.34645      0             -148.105        2250.0716    \n",
            "       167   442.02185     -148.37099      0             -148.14244      3824.9917    \n",
            "       168   414.00313     -148.34005      0             -148.12599      5248.0554    \n",
            "       169   381.9476      -148.33279      0             -148.13531      6486.6219    \n",
            "       170   345.16731     -148.33067      0             -148.1522       7606.8453    \n",
            "       171   304.16311     -148.301        0             -148.14374      8576.3327    \n",
            "       172   261.23372     -148.28086      0             -148.14579      9221.2538    \n",
            "       173   219.80145     -148.29991      0             -148.18626      9759.4075    \n",
            "       174   184.3644      -148.27106      0             -148.17573      9987.6034    \n",
            "       175   159.49835     -148.2761       0             -148.19363      9952.5181    \n",
            "       176   148.38348     -148.25639      0             -148.17967      9761.9609    \n",
            "       177   152.37743     -148.27087      0             -148.19209      9390.3071    \n",
            "       178   170.53302     -148.26474      0             -148.17656      8764.077     \n",
            "       179   199.45425     -148.29109      0             -148.18796      7750.0927    \n",
            "       180   233.88698     -148.30548      0             -148.18455      6671.8957    \n",
            "       181   267.8133      -148.31273      0             -148.17426      5623.0797    \n",
            "       182   295.43315     -148.33136      0             -148.17861      4240.299     \n",
            "       183   312.48392     -148.36257      0             -148.201        2962.1747    \n",
            "       184   316.95086     -148.37423      0             -148.21036      1657.9065    \n",
            "       185   309.11662     -148.35995      0             -148.20013      348.93353    \n",
            "       186   291.37348     -148.36575      0             -148.2151      -886.25735    \n",
            "       187   267.82497     -148.37707      0             -148.2386      -2082.8366    \n",
            "       188   242.95458     -148.38652      0             -148.2609      -3185.2333    \n",
            "       189   220.54822     -148.3599       0             -148.24587     -4099.8102    \n",
            "       190   203.20102     -148.35032      0             -148.24526     -5062.3163    \n",
            "       191   191.77403     -148.35714      0             -148.25799     -5880.9065    \n",
            "       192   185.80832     -148.36516      0             -148.26908     -6361.8626    \n",
            "       193   183.70953     -148.35366      0             -148.25868     -6957.4871    \n",
            "       194   183.26051     -148.34888      0             -148.25413     -7166.2877    \n",
            "       195   182.60717     -148.34788      0             -148.25347     -7143.9714    \n",
            "       196   180.8313      -148.34371      0             -148.25021     -6994.2953    \n",
            "       197   178.23563     -148.35037      0             -148.25821     -6717.125     \n",
            "       198   175.81595     -148.36089      0             -148.26999     -6230.0822    \n",
            "       199   174.95012     -148.35134      0             -148.26088     -5395.1916    \n",
            "       200   176.92977     -148.37208      0             -148.2806      -4590.7699    \n",
            "       201   182.3632      -148.38303      0             -148.28874     -3664.0029    \n",
            "       202   190.75223     -148.36166      0             -148.26304     -2603.0389    \n",
            "       203   200.3956      -148.36905      0             -148.26544     -1477.2       \n",
            "       204   208.85608     -148.36433      0             -148.25634     -234.54701    \n",
            "       205   213.39741     -148.41218      0             -148.30184      874.20727    \n",
            "       206   211.57336     -148.37615      0             -148.26676      2206.3032    \n",
            "       207   201.93724     -148.40515      0             -148.30074      3356.8217    \n",
            "       208   184.58027     -148.41121      0             -148.31577      4567.442     \n",
            "       209   161.04024     -148.36752      0             -148.28426      5805.2622    \n",
            "       210   133.97061     -148.38555      0             -148.31628      6885.4264    \n",
            "       211   107.00453     -148.38156      0             -148.32624      7867.5905    \n",
            "       212   84.045715     -148.35455      0             -148.3111       8691.0754    \n",
            "       213   68.176899     -148.35324      0             -148.31799      9507.9888    \n",
            "       214   61.266584     -148.32845      0             -148.29678      9874.9016    \n",
            "       215   63.724399     -148.33029      0             -148.29734      10238.583    \n",
            "       216   74.531803     -148.33206      0             -148.29353      10260.871    \n",
            "       217   91.469724     -148.36402      0             -148.31672      10160.13     \n",
            "       218   111.54609     -148.37746      0             -148.31978      9842.8898    \n",
            "       219   131.83805     -148.38279      0             -148.31462      9422.3085    \n",
            "       220   149.89133     -148.37237      0             -148.29487      8865.7753    \n",
            "       221   164.13357     -148.39182      0             -148.30696      8195.4562    \n",
            "       222   173.85727     -148.42221      0             -148.33232      7170.6377    \n",
            "       223   179.1964      -148.42767      0             -148.33502      6220.6288    \n",
            "       224   181.0243      -148.39784      0             -148.30424      5393.9718    \n",
            "       225   180.03833     -148.43071      0             -148.33762      4170.1744    \n",
            "       226   176.69047     -148.43153      0             -148.34017      3028.5402    \n",
            "       227   171.11052     -148.40981      0             -148.32134      1852.8065    \n",
            "       228   163.09028     -148.42006      0             -148.33574      772.15416    \n",
            "       229   152.12449     -148.42383      0             -148.34517     -447.03806    \n",
            "       230   137.60157     -148.42103      0             -148.34989     -1548.2178    \n",
            "       231   119.46109     -148.39785      0             -148.33609     -2440.1502    \n",
            "       232   98.260937     -148.3929       0             -148.3421      -3280.1459    \n",
            "       233   75.555059     -148.38163      0             -148.34257     -3940.4725    \n",
            "       234   53.409651     -148.37036      0             -148.34274     -4467.5472    \n",
            "       235   34.356767     -148.34997      0             -148.3322      -4844.8559    \n",
            "       236   20.870086     -148.33205      0             -148.32126     -5057.8807    \n",
            "       237   14.77834      -148.3457       0             -148.33806     -5163.9166    \n",
            "       238   17.18353      -148.3537       0             -148.34482     -5140.1909    \n",
            "       239   27.951509     -148.35301      0             -148.33856     -5017.4156    \n",
            "       240   45.759122     -148.3668       0             -148.34314     -4787.5582    \n",
            "       241   68.330768     -148.38384      0             -148.34851     -4430.3418    \n",
            "       242   92.796483     -148.36527      0             -148.31729     -3804.6585    \n",
            "       243   116.1959      -148.39208      0             -148.332       -3124.1153    \n",
            "       244   136.05705     -148.42425      0             -148.35391     -2307.3429    \n",
            "       245   150.61965     -148.41089      0             -148.33301     -1433.3831    \n",
            "       246   159.06595     -148.44237      0             -148.36013     -482.77866    \n",
            "       247   161.53804     -148.4443       0             -148.36078      467.63418    \n",
            "       248   158.82693     -148.45283      0             -148.37071      1597.0617    \n",
            "       249   152.17856     -148.44559      0             -148.3669       2705.7842    \n",
            "       250   142.92918     -148.40275      0             -148.32885      3655.847     \n",
            "       251   132.16622     -148.40684      0             -148.3385       4643.8283    \n",
            "       252   120.42091     -148.4255       0             -148.36324      5390.4297    \n",
            "       253   107.92055     -148.4237       0             -148.36791      6207.6812    \n",
            "       254   94.614384     -148.42462      0             -148.3757       6833.9916    \n",
            "       255   80.53604      -148.42011      0             -148.37847      7409.754     \n",
            "       256   66.050963     -148.39947      0             -148.36532      7828.8758    \n",
            "       257   52.024665     -148.41293      0             -148.38603      8175.5568    \n",
            "       258   39.628242     -148.36611      0             -148.34562      8301.582     \n",
            "       259   30.157732     -148.36288      0             -148.34729      8293.6402    \n",
            "       260   25.053017     -148.37147      0             -148.35852      8079.3987    \n",
            "       261   25.404874     -148.38616      0             -148.37303      7835.3334    \n",
            "       262   31.624728     -148.37671      0             -148.36036      7453.9672    \n",
            "       263   43.356423     -148.38977      0             -148.36735      6845.2972    \n",
            "       264   59.313971     -148.37561      0             -148.34494      6249.8498    \n",
            "       265   77.424306     -148.41548      0             -148.37545      5400.696     \n",
            "       266   95.323538     -148.40614      0             -148.35686      4525.5417    \n",
            "       267   110.78114     -148.42526      0             -148.36798      3617.4226    \n",
            "       268   122.08548     -148.44498      0             -148.38186      2643.0544    \n",
            "       269   128.25649     -148.41648      0             -148.35017      1740.4655    \n",
            "       270   128.90238     -148.42226      0             -148.35562      774.02029    \n",
            "       271   124.63857     -148.42846      0             -148.36401     -171.90633    \n",
            "       272   116.74964     -148.4417       0             -148.38133     -1115.2569    \n",
            "       273   106.84792     -148.45555      0             -148.40031     -2055.2828    \n",
            "       274   96.514612     -148.42109      0             -148.37119     -2759.102     \n",
            "       275   86.974332     -148.43745      0             -148.39248     -3432.724     \n",
            "       276   78.889143     -148.41346      0             -148.37267     -4134.4677    \n",
            "       277   72.364909     -148.42308      0             -148.38567     -4678.7501    \n",
            "       278   67.111424     -148.40507      0             -148.37037     -5165.8325    \n",
            "       279   62.774941     -148.4068       0             -148.37434     -5376.4421    \n",
            "       280   58.985821     -148.40412      0             -148.37362     -5567.858     \n",
            "       281   55.531309     -148.41263      0             -148.38391     -5634.3753    \n",
            "       282   52.640269     -148.39442      0             -148.3672      -5583.4308    \n",
            "       283   50.776336     -148.40743      0             -148.38117     -5508.8624    \n",
            "       284   50.449403     -148.42371      0             -148.39763     -5158.0442    \n",
            "       285   52.149392     -148.4164       0             -148.38943     -4755.2898    \n",
            "       286   56.229886     -148.41045      0             -148.38137     -4286.6899    \n",
            "       287   62.525235     -148.43036      0             -148.39803     -3786.0313    \n",
            "       288   70.305106     -148.42542      0             -148.38907     -3145.5768    \n",
            "       289   78.42343      -148.43672      0             -148.39617     -2603.8384    \n",
            "       290   85.526801     -148.42877      0             -148.38455     -1874.843     \n",
            "       291   90.288729     -148.42313      0             -148.37644     -1207.1512    \n",
            "       292   91.882801     -148.46091      0             -148.4134      -558.16437    \n",
            "       293   89.923768     -148.42155      0             -148.37506      231.85701    \n",
            "       294   84.569939     -148.41969      0             -148.37597      814.51424    \n",
            "       295   76.757249     -148.40942      0             -148.36973      1381.7254    \n",
            "       296   67.698016     -148.42736      0             -148.39236      1887.4846    \n",
            "       297   58.905644     -148.43071      0             -148.40025      2304.8054    \n",
            "       298   51.766235     -148.42067      0             -148.3939       2733.1019    \n",
            "       299   47.232571     -148.40138      0             -148.37696      3000.6267    \n",
            "       300   45.716009     -148.42039      0             -148.39675      3208.4379    \n",
            "       301   47.047658     -148.41651      0             -148.39218      3308.2466    \n",
            "       302   50.568531     -148.41555      0             -148.3894       3249.6131    \n",
            "       303   55.302818     -148.43799      0             -148.4094       3020.9432    \n",
            "       304   60.310831     -148.43499      0             -148.4038       2780.9244    \n",
            "       305   64.722204     -148.40509      0             -148.37162      2527.6228    \n",
            "       306   67.982887     -148.42883      0             -148.39368      2037.0799    \n",
            "       307   69.944953     -148.44269      0             -148.40652      1473.4647    \n",
            "       308   70.740878     -148.45711      0             -148.42053      865.99089    \n",
            "       309   70.698366     -148.4475       0             -148.41095      263.69865    \n",
            "       310   70.299905     -148.41407      0             -148.37772     -410.64503    \n",
            "       311   69.830845     -148.43612      0             -148.40002     -991.62647    \n",
            "       312   69.263586     -148.39627      0             -148.36045     -1793.238     \n",
            "       313   68.327782     -148.43502      0             -148.39969     -2529.4637    \n",
            "       314   66.543026     -148.44122      0             -148.40681     -3244.4179    \n",
            "       315   63.370981     -148.42913      0             -148.39636     -3921.7117    \n",
            "       316   58.5652       -148.43044      0             -148.40016     -4528.4875    \n",
            "       317   52.099674     -148.39246      0             -148.36552     -5014.4433    \n",
            "       318   44.28266      -148.4238       0             -148.40091     -5525.4549    \n",
            "       319   35.94434      -148.42099      0             -148.40241     -5922.3398    \n",
            "       320   28.269199     -148.41408      0             -148.39946     -6209.7316    \n",
            "       321   22.408413     -148.40376      0             -148.39217     -6508.8793    \n",
            "       322   19.425827     -148.399        0             -148.38896     -6658.7674    \n",
            "       323   20.046547     -148.39919      0             -148.38883     -6719.5994    \n",
            "       324   24.403312     -148.39136      0             -148.37874     -6586.0213    \n",
            "       325   32.033515     -148.41171      0             -148.39515     -6337.9109    \n",
            "       326   41.97383      -148.40985      0             -148.38815     -6173.6175    \n",
            "       327   52.893863     -148.44074      0             -148.41339     -5822.7365    \n",
            "       328   63.443859     -148.45632      0             -148.42352     -5416.5923    \n",
            "       329   72.413846     -148.41342      0             -148.37598     -4768.1638    \n",
            "       330   78.883942     -148.4426       0             -148.40182     -4322.9208    \n",
            "       331   82.369704     -148.4391       0             -148.39651     -3592.7776    \n",
            "       332   82.907077     -148.44924      0             -148.40637     -2843.9362    \n",
            "       333   80.905263     -148.43544      0             -148.39361     -2113.3258    \n",
            "       334   76.936644     -148.46393      0             -148.42415     -1415.5179    \n",
            "       335   71.739919     -148.4258       0             -148.38871     -581.67547    \n",
            "       336   65.882269     -148.43299      0             -148.39893      31.24233     \n",
            "       337   59.688906     -148.42665      0             -148.39579      765.56521    \n",
            "       338   53.201761     -148.44924      0             -148.42173      1424.86      \n",
            "       339   46.403775     -148.42445      0             -148.40046      1986.4059    \n",
            "       340   39.228699     -148.42605      0             -148.40577      2473.2877    \n",
            "       341   31.779609     -148.42476      0             -148.40833      2982.2706    \n",
            "       342   24.506702     -148.42432      0             -148.41165      3389.96      \n",
            "       343   18.091018     -148.42181      0             -148.41245      3528.2048    \n",
            "       344   13.340776     -148.37833      0             -148.37144      3759.4177    \n",
            "       345   11.057656     -148.40621      0             -148.40049      3802.4614    \n",
            "       346   11.981136     -148.41769      0             -148.41149      3729.1953    \n",
            "       347   16.512452     -148.41857      0             -148.41004      3562.4563    \n",
            "       348   24.526174     -148.41328      0             -148.4006       3307.4315    \n",
            "       349   35.399096     -148.43076      0             -148.41245      3017.2634    \n",
            "       350   47.993996     -148.43315      0             -148.40834      2535.9967    \n",
            "       351   60.904634     -148.4151       0             -148.38361      2002.9006    \n",
            "       352   72.600571     -148.44664      0             -148.4091       1389.0374    \n",
            "       353   81.740811     -148.44161      0             -148.39935      792.11724    \n",
            "       354   87.464027     -148.46238      0             -148.41716      103.83497    \n",
            "       355   89.33531      -148.45214      0             -148.40595     -612.18918    \n",
            "       356   87.534874     -148.43166      0             -148.3864      -1214.8466    \n",
            "       357   82.751207     -148.46593      0             -148.42315     -1909.352     \n",
            "       358   75.860831     -148.43558      0             -148.39636     -2583.6148    \n",
            "       359   67.895313     -148.45515      0             -148.42005     -3159.0927    \n",
            "       360   59.784579     -148.46099      0             -148.43008     -3748.8681    \n",
            "       361   52.207657     -148.41683      0             -148.38983     -4279.6271    \n",
            "       362   45.566565     -148.42212      0             -148.39856     -4658.5325    \n",
            "       363   39.8178       -148.42043      0             -148.39984     -5027.1042    \n",
            "       364   34.844045     -148.40682      0             -148.3888      -5251.6297    \n",
            "       365   30.670303     -148.40239      0             -148.38653     -5280.394     \n",
            "       366   27.258154     -148.41488      0             -148.40078     -5344.9786    \n",
            "       367   24.757202     -148.42821      0             -148.41541     -5372.4413    \n",
            "       368   23.5784       -148.43464      0             -148.42245     -5203.4391    \n",
            "       369   24.14971      -148.43454      0             -148.42205     -4963.9744    \n",
            "       370   26.87561      -148.39829      0             -148.38439     -4605.449     \n",
            "       371   32.002728     -148.41796      0             -148.40141     -4192.8249    \n",
            "       372   39.366243     -148.40071      0             -148.38036     -3606.4409    \n",
            "       373   48.438514     -148.431        0             -148.40596     -3069.0905    \n",
            "       374   58.318621     -148.42926      0             -148.39911     -2331.3612    \n",
            "       375   67.851725     -148.43123      0             -148.39615     -1676.1055    \n",
            "       376   75.796523     -148.4424       0             -148.40321     -957.88448    \n",
            "       377   81.09975      -148.47605      0             -148.43412     -219.60613    \n",
            "       378   83.185582     -148.42105      0             -148.37804      584.37471    \n",
            "       379   81.792238     -148.44177      0             -148.39948      1357.0642    \n",
            "       380   77.281307     -148.43342      0             -148.39346      2015.1448    \n",
            "       381   70.581291     -148.43935      0             -148.40285      2702.2577    \n",
            "       382   62.68923      -148.43748      0             -148.40507      3380.482     \n",
            "       383   54.793933     -148.44881      0             -148.42048      3923.6695    \n",
            "       384   48.018514     -148.41575      0             -148.39092      4448.8242    \n",
            "       385   43.098838     -148.44652      0             -148.42424      4817.0687    \n",
            "       386   40.266726     -148.41423      0             -148.39341      5125.3962    \n",
            "       387   39.351869     -148.4213       0             -148.40096      5255.8034    \n",
            "       388   39.937999     -148.43236      0             -148.41171      5268.5046    \n",
            "       389   41.464693     -148.43671      0             -148.41527      5246.6772    \n",
            "       390   43.418117     -148.43299      0             -148.41054      5183.5116    \n",
            "       391   45.382814     -148.4136       0             -148.39013      4940.7517    \n",
            "       392   47.16102      -148.40396      0             -148.37957      4508.3649    \n",
            "       393   48.825538     -148.41501      0             -148.38976      4172.394     \n",
            "       394   50.582106     -148.43493      0             -148.40878      3616.328     \n",
            "       395   52.609769     -148.45227      0             -148.42507      3151.7922    \n",
            "       396   55.074196     -148.44864      0             -148.42017      2512.3435    \n",
            "       397   57.910141     -148.44725      0             -148.41731      1868.9504    \n",
            "       398   60.724787     -148.44008      0             -148.40869      1222.7231    \n",
            "       399   62.893962     -148.43951      0             -148.40699      521.2238     \n",
            "       400   63.767663     -148.43134      0             -148.39837     -176.01934    \n",
            "       401   62.879049     -148.42432      0             -148.39181     -762.89406    \n",
            "       402   59.963952     -148.44808      0             -148.41708     -1275.7281    \n",
            "       403   55.097268     -148.4317       0             -148.40321     -1747.1575    \n",
            "       404   48.845413     -148.41806      0             -148.3928      -2168.8862    \n",
            "       405   42.171538     -148.41752      0             -148.39572     -2537.3497    \n",
            "       406   36.165083     -148.45549      0             -148.43679     -2815.147     \n",
            "       407   31.870437     -148.4263       0             -148.40982     -2984.3085    \n",
            "       408   30.191038     -148.4177       0             -148.40209     -3020.7605    \n",
            "       409   31.628459     -148.41515      0             -148.3988      -2932.1079    \n",
            "       410   36.055613     -148.4321       0             -148.41346     -2849.2811    \n",
            "       411   42.839551     -148.40524      0             -148.38309     -2517.5768    \n",
            "       412   50.908871     -148.44085      0             -148.41453     -2166.8131    \n",
            "       413   59.017579     -148.4211       0             -148.39059     -1747.7581    \n",
            "       414   65.928426     -148.41152      0             -148.37744     -1153.3424    \n",
            "       415   70.692807     -148.44351      0             -148.40696     -613.12653    \n",
            "       416   72.925144     -148.43534      0             -148.39764      139.26368    \n",
            "       417   72.559521     -148.44763      0             -148.41011      818.71335    \n",
            "       418   69.951439     -148.45967      0             -148.42351      1644.6139    \n",
            "       419   65.693161     -148.42318      0             -148.38921      2470.1289    \n",
            "       420   60.510918     -148.45478      0             -148.42349      3301.227     \n",
            "       421   55.061501     -148.45965      0             -148.43118      3976.66      \n",
            "       422   49.725934     -148.43732      0             -148.41161      4714.6913    \n",
            "       423   44.568594     -148.41456      0             -148.39151      5488.5243    \n",
            "       424   39.519108     -148.41676      0             -148.39633      6063.3305    \n",
            "       425   34.560782     -148.41235      0             -148.39448      6532.5444    \n",
            "       426   29.64965      -148.38204      0             -148.36671      7146.0556    \n",
            "       427   24.988172     -148.40106      0             -148.38814      7523.1979    \n",
            "       428   21.08391      -148.39567      0             -148.38477      7617.0388    \n",
            "       429   18.610547     -148.39513      0             -148.38551      7759.0327    \n",
            "       430   18.390274     -148.42906      0             -148.41955      7745.2743    \n",
            "       431   21.113589     -148.44316      0             -148.43225      7537.7803    \n",
            "       432   27.22848      -148.42819      0             -148.41411      7247.3373    \n",
            "       433   36.656284     -148.43097      0             -148.41202      6920.9532    \n",
            "       434   48.746789     -148.44388      0             -148.41868      6323.8469    \n",
            "       435   62.281168     -148.43492      0             -148.40272      5746.9187    \n",
            "       436   75.634326     -148.43449      0             -148.39538      5085.756     \n",
            "       437   87.191342     -148.42476      0             -148.37968      4289.2955    \n",
            "       438   95.411698     -148.46277      0             -148.41344      3452.4228    \n",
            "       439   99.322718     -148.44849      0             -148.39714      2635.0972    \n",
            "       440   98.502841     -148.44064      0             -148.38971      1771.8349    \n",
            "       441   93.174997     -148.43789      0             -148.38972      864.98955    \n",
            "       442   84.236378     -148.43263      0             -148.38908     -10.562918    \n",
            "       443   72.933699     -148.43667      0             -148.39896     -891.61363    \n",
            "       444   60.805726     -148.41202      0             -148.38058     -1616.3743    \n",
            "       445   49.221122     -148.42029      0             -148.39484     -2279.7073    \n",
            "       446   39.181213     -148.39337      0             -148.37311     -2889.7694    \n",
            "       447   31.239265     -148.38783      0             -148.37167     -3523.3245    \n",
            "       448   25.533898     -148.4061       0             -148.3929      -4010.1067    \n",
            "       449   21.941989     -148.40495      0             -148.39361     -4278.1215    \n",
            "       450   20.27766      -148.39641      0             -148.38592     -4371.8297    \n",
            "       451   20.309783     -148.42078      0             -148.41028     -4674.7529    \n",
            "       452   21.95874      -148.40625      0             -148.3949      -4486.6748    \n",
            "       453   25.392538     -148.41571      0             -148.40258     -4449.7677    \n",
            "       454   30.991177     -148.39855      0             -148.38252     -4132.6243    \n",
            "       455   39.023371     -148.40707      0             -148.38689     -3815.1838    \n",
            "       456   49.51966      -148.42523      0             -148.39963     -3315.7322    \n",
            "       457   62.250508     -148.42919      0             -148.39701     -2666.9586    \n",
            "       458   76.476802     -148.42326      0             -148.38372     -2071.6223    \n",
            "       459   90.932341     -148.40892      0             -148.3619      -1351.7236    \n",
            "       460   104.03764     -148.45045      0             -148.39665     -668.17953    \n",
            "       461   114.24354     -148.45358      0             -148.39451      193.88495    \n",
            "       462   120.13697     -148.46081      0             -148.39869      885.43871    \n",
            "       463   120.84335     -148.42367      0             -148.36119      1756.566     \n",
            "       464   116.1969      -148.45557      0             -148.39549      2455.6964    \n",
            "       465   106.80346     -148.42247      0             -148.36725      3230.0832    \n",
            "       466   93.960912     -148.43667      0             -148.38809      3915.7939    \n",
            "       467   79.43668      -148.41413      0             -148.37306      4483.0956    \n",
            "       468   65.163991     -148.41277      0             -148.37908      4931.4857    \n",
            "       469   52.965307     -148.42809      0             -148.40071      5353.3831    \n",
            "       470   44.16474      -148.39064      0             -148.3678       5721.1829    \n",
            "       471   39.329228     -148.44118      0             -148.42084      5753.5315    \n",
            "       472   38.378482     -148.39021      0             -148.37037      5848.8515    \n",
            "       473   40.696158     -148.38158      0             -148.36054      5717.6932    \n",
            "       474   45.313556     -148.4234       0             -148.39997      5430.8929    \n",
            "       475   51.319253     -148.41122      0             -148.38468      5111.7479    \n",
            "       476   57.947904     -148.40946      0             -148.3795       4614.0635    \n",
            "       477   64.659194     -148.41211      0             -148.37867      3935.9661    \n",
            "       478   71.332767     -148.42853      0             -148.39164      3201.5884    \n",
            "       479   78.106468     -148.44738      0             -148.407        2521.1626    \n",
            "       480   85.301921     -148.42997      0             -148.38586      1709.3236    \n",
            "       481   93.046828     -148.41652      0             -148.36841      981.83317    \n",
            "       482   101.00442     -148.42478      0             -148.37255     -7.0566855    \n",
            "       483   108.56122     -148.42855      0             -148.37241     -933.10494    \n",
            "       484   114.83638     -148.42341      0             -148.36404     -1937.8962    \n",
            "       485   118.62619     -148.41294      0             -148.35161     -2699.7426    \n",
            "       486   118.91393     -148.43563      0             -148.37415     -3649.7248    \n",
            "       487   115.23599     -148.42962      0             -148.37004     -4334.5894    \n",
            "       488   107.64381     -148.42413      0             -148.36847     -5173.4481    \n",
            "       489   97.056652     -148.41637      0             -148.36619     -5643.1509    \n",
            "       490   84.954016     -148.40109      0             -148.35716     -6117.492     \n",
            "       491   73.175063     -148.37233      0             -148.3345      -6505.203     \n",
            "       492   63.720704     -148.41313      0             -148.38018     -6817.3751    \n",
            "       493   58.240708     -148.40271      0             -148.3726      -6875.7157    \n",
            "       494   57.732286     -148.39067      0             -148.36082     -6894.1521    \n",
            "       495   62.193307     -148.3873       0             -148.35514     -6761.9179    \n",
            "       496   70.752281     -148.41486      0             -148.37827     -6667.0636    \n",
            "       497   81.756103     -148.38985      0             -148.34758     -6201.1136    \n",
            "       498   93.193597     -148.42064      0             -148.37245     -5847.8093    \n",
            "       499   103.23419     -148.41741      0             -148.36404     -5225.3267    \n",
            "       500   110.43818     -148.42166      0             -148.36456     -4560.9711    \n",
            "       501   114.073       -148.40723      0             -148.34825     -3766.1427    \n",
            "       502   114.14578     -148.41743      0             -148.35841     -2998.3723    \n",
            "       503   111.39307     -148.40436      0             -148.34677     -2148.6836    \n",
            "       504   106.80025     -148.41523      0             -148.36001     -1227.8927    \n",
            "       505   101.62907     -148.38943      0             -148.33689     -425.28696    \n",
            "       506   96.900574     -148.41397      0             -148.36387      456.38623    \n",
            "       507   92.990272     -148.39261      0             -148.34453      1313.923     \n",
            "       508   89.81192      -148.42867      0             -148.38224      1950.292     \n",
            "       509   86.779168     -148.38402      0             -148.33915      2662.1571    \n",
            "       510   83.263897     -148.39108      0             -148.34803      3180.8361    \n",
            "       511   78.867116     -148.37342      0             -148.33265      3731.1031    \n",
            "       512   73.7794       -148.37632      0             -148.33817      4196.2956    \n",
            "       513   68.622603     -148.38112      0             -148.34564      4259.873     \n",
            "       514   64.424633     -148.38163      0             -148.34832      4378.9356    \n",
            "       515   62.709631     -148.37574      0             -148.34332      4383.4498    \n",
            "       516   65.111391     -148.37668      0             -148.34302      4146.5257    \n",
            "       517   72.69025      -148.39047      0             -148.35289      3820.1644    \n",
            "       518   85.632192     -148.42996      0             -148.38568      3266.9541    \n",
            "       519   103.14976     -148.39278      0             -148.33945      2702.6993    \n",
            "       520   123.38734     -148.41357      0             -148.34978      1921.1003    \n",
            "       521   143.74662     -148.39603      0             -148.32171      1048.8711    \n",
            "       522   161.18678     -148.42073      0             -148.33739      129.4441     \n",
            "       523   173.05895     -148.43877      0             -148.3493      -876.3547     \n",
            "       524   177.56087     -148.41653      0             -148.32472     -1998.1033    \n",
            "       525   173.93517     -148.42439      0             -148.33446     -2992.8341    \n",
            "       526   162.54861     -148.43848      0             -148.35444     -4097.4611    \n",
            "       527   145.00423     -148.39338      0             -148.31841     -5096.667     \n",
            "       528   123.95686     -148.37981      0             -148.31572     -5947.1184    \n",
            "       529   102.15926     -148.36354      0             -148.31072     -6858.0506    \n",
            "       530   82.164187     -148.39875      0             -148.35627     -7796.9865    \n",
            "       531   65.887202     -148.35415      0             -148.32008     -8581.3042    \n",
            "       532   54.275936     -148.33866      0             -148.3106      -9217.9453    \n",
            "       533   47.433132     -148.34931      0             -148.32479     -9851.3539    \n",
            "       534   44.774323     -148.3409       0             -148.31775     -10260.13     \n",
            "       535   45.412899     -148.34726      0             -148.32378     -10462.555    \n",
            "       536   48.796188     -148.37452      0             -148.34929     -10574.586    \n",
            "       537   54.671243     -148.3466       0             -148.31833     -10506.15     \n",
            "       538   63.312519     -148.35201      0             -148.31928     -10286.719    \n",
            "       539   75.397685     -148.35329      0             -148.31431     -9918.6933    \n",
            "       540   91.865102     -148.37283      0             -148.32533     -9296.0413    \n",
            "       541   113.3548      -148.36615      0             -148.30755     -8577.1973    \n",
            "       542   139.6641      -148.40062      0             -148.3284      -7832.7135    \n",
            "       543   169.55673     -148.41442      0             -148.32675     -6872.7077    \n",
            "       544   200.54196     -148.38936      0             -148.28567     -5806.4288    \n",
            "       545   229.45857     -148.42558      0             -148.30694     -4723.4516    \n",
            "       546   252.6008      -148.43352      0             -148.30292     -3579.5787    \n",
            "       547   266.51465     -148.44834      0             -148.31054     -2339.1771    \n",
            "       548   268.71874     -148.45807      0             -148.31913     -963.36251    \n",
            "       549   258.35113     -148.43824      0             -148.30466      214.10249    \n",
            "       550   236.39272     -148.41094      0             -148.28871      1640.5928    \n",
            "       551   205.33365     -148.37464      0             -148.26847      2855.6927    \n",
            "       552   169.38638     -148.38301      0             -148.29543      4068.1988    \n",
            "       553   133.33389     -148.35644      0             -148.2875       5067.4611    \n",
            "       554   101.42947     -148.3454       0             -148.29296      5991.2263    \n",
            "       555   77.05852      -148.31374      0             -148.2739       6693.6466    \n",
            "       556   62.313412     -148.29657      0             -148.26435      7197.3045    \n",
            "       557   57.876579     -148.33055      0             -148.30062      7527.1431    \n",
            "       558   62.857777     -148.31195      0             -148.27945      7803.4055    \n",
            "       559   75.228309     -148.31706      0             -148.27817      7685.0749    \n",
            "       560   92.749827     -148.32883      0             -148.28087      7536.7735    \n",
            "       561   113.59376     -148.3452       0             -148.28647      7188.9237    \n",
            "       562   136.53415     -148.36261      0             -148.29202      6751.1447    \n",
            "       563   160.90419     -148.3306       0             -148.2474       6100.6331    \n",
            "       564   186.84428     -148.36764      0             -148.27103      5236.0389    \n",
            "       565   214.71581     -148.38371      0             -148.27269      4253.8404    \n",
            "       566   244.3576      -148.41345      0             -148.2871       3273.9039    \n",
            "       567   274.72301     -148.41723      0             -148.27519      2077.4412    \n",
            "       568   303.77785     -148.44457      0             -148.2875       794.96509    \n",
            "       569   328.73169     -148.42413      0             -148.25416     -546.66095    \n",
            "       570   346.14999     -148.41959      0             -148.24062     -1769.0898    \n",
            "       571   352.69434     -148.42782      0             -148.24546     -2944.8991    \n",
            "       572   346.24206     -148.4229       0             -148.24388     -4171.9977    \n",
            "       573   326.61522     -148.40338      0             -148.23451     -5148.9277    \n",
            "       574   295.51748     -148.41008      0             -148.25729     -6061.0026    \n",
            "       575   256.60222     -148.37524      0             -148.24256     -6744.2452    \n",
            "       576   215.36541     -148.33612      0             -148.22477     -7311.8406    \n",
            "       577   177.60241     -148.33229      0             -148.24046     -7695.7913    \n",
            "       578   148.50318     -148.29538      0             -148.21859     -7864.9445    \n",
            "       579   131.81216     -148.30411      0             -148.23595     -7989.4218    \n",
            "       580   129.13357     -148.3025       0             -148.23573     -7879.2297    \n",
            "       581   139.6961      -148.28241      0             -148.21018     -7468.3035    \n",
            "       582   160.6302      -148.30742      0             -148.22436     -7055.091     \n",
            "       583   187.71197     -148.29915      0             -148.20209     -6407.6385    \n",
            "       584   216.5525      -148.29008      0             -148.17811     -5493.3149    \n",
            "       585   243.31973     -148.32892      0             -148.20311     -4388.402     \n",
            "       586   265.60547     -148.35227      0             -148.21494     -3218.3251    \n",
            "       587   282.54359     -148.34005      0             -148.19396     -1731.1569    \n",
            "       588   294.75768     -148.3374       0             -148.185       -190.32816    \n",
            "       589   304.03477     -148.357        0             -148.1998       1393.6516    \n",
            "       590   312.22658     -148.38419      0             -148.22276      2987.5455    \n",
            "       591   320.5487      -148.37673      0             -148.21099      4680.0869    \n",
            "       592   329.18826     -148.38641      0             -148.21621      6283.082     \n",
            "       593   336.8196      -148.33591      0             -148.16176      7911.9369    \n",
            "       594   341.1908      -148.37062      0             -148.19421      9294.7345    \n",
            "       595   339.96024     -148.37158      0             -148.1958       10546.817    \n",
            "       596   331.16145     -148.34682      0             -148.17559      11728.445    \n",
            "       597   314.43473     -148.35778      0             -148.19521      12656.567    \n",
            "       598   291.46406     -148.33512      0             -148.18442      13467.341    \n",
            "       599   265.47467     -148.30805      0             -148.17079      14112.638    \n",
            "       600   241.41458     -148.28752      0             -148.1627       14493.75     \n",
            "       601   224.67647     -148.28133      0             -148.16517      14550.049    \n",
            "       602   219.49698     -148.27091      0             -148.15743      14283.452    \n",
            "       603   228.52129     -148.2703       0             -148.15214      13903.745    \n",
            "       604   251.58437     -148.31505      0             -148.18497      12988.697    \n",
            "       605   285.50094     -148.29969      0             -148.15207      12021.943    \n",
            "       606   324.8098      -148.30856      0             -148.14062      10739.315    \n",
            "       607   362.59081     -148.33276      0             -148.14528      9136.1855    \n",
            "       608   391.778       -148.32468      0             -148.12212      7566.8814    \n",
            "       609   407.18232     -148.3297       0             -148.11917      5840.2622    \n",
            "       610   405.88765     -148.32976      0             -148.1199       4129.6571    \n",
            "       611   388.2018      -148.31847      0             -148.11775      2468.3192    \n",
            "       612   357.40611     -148.27794      0             -148.09315      698.01072    \n",
            "       613   318.9017      -148.27437      0             -148.10949     -915.2667     \n",
            "       614   279.08201     -148.24788      0             -148.10359     -2481.1202    \n",
            "       615   243.77602     -148.25085      0             -148.12481     -4050.7311    \n",
            "       616   217.28177     -148.21915      0             -148.10681     -5452.571     \n",
            "       617   201.82224     -148.19213      0             -148.08778     -6552.091     \n",
            "       618   197.25212     -148.21655      0             -148.11456     -7722.6233    \n",
            "       619   201.71883     -148.21841      0             -148.11411     -8562.6302    \n",
            "       620   212.60249     -148.20324      0             -148.09331     -8998.8509    \n",
            "       621   227.50633     -148.19489      0             -148.07726     -9308.7699    \n",
            "       622   244.84466     -148.24549      0             -148.1189      -9267.207     \n",
            "       623   264.59471     -148.23661      0             -148.0998      -8889.5781    \n",
            "       624   288.24967     -148.2303       0             -148.08126     -8177.2857    \n",
            "       625   318.10519     -148.26065      0             -148.09617     -7156.5557    \n",
            "       626   356.06582     -148.22797      0             -148.04387     -5961.1874    \n",
            "       627   402.55962     -148.28463      0             -148.07649     -4651.4876    \n",
            "       628   455.55152     -148.28996      0             -148.05442     -3030.0264    \n",
            "       629   510.30644     -148.33816      0             -148.07431     -1350.9454    \n",
            "       630   559.71952     -148.35325      0             -148.06385      410.23617    \n",
            "       631   595.13818     -148.36581      0             -148.0581       2541.7701    \n",
            "       632   608.9185      -148.36023      0             -148.0454       4533.0312    \n",
            "       633   595.80445     -148.32865      0             -148.0206       6597.3665    \n",
            "       634   554.42419     -148.31178      0             -148.02512      8686.2881    \n",
            "       635   487.70725     -148.29364      0             -148.04147      10766.164    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Radial Function Analysis\n",
        "We will check it for N and H atoms as a test.\n",
        "\n"
      ],
      "metadata": {
        "id": "HqHRJtqZ7qHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['font.size'] = 30\n",
        "\n",
        "def parse_lammps_rdf(rdffile):\n",
        "    \"\"\"\n",
        "    Parse the RDF file written by LAMMPS\n",
        "\n",
        "    copied from Boris' class code: https://github.com/bkoz37/labutil\n",
        "    \"\"\"\n",
        "    with open(rdffile, 'r') as rdfout:\n",
        "        rdfs = []\n",
        "        buffer = []\n",
        "        for line in rdfout:\n",
        "            values = line.split()\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            elif len(values) == 2:\n",
        "                nbins = values[1]\n",
        "            else:\n",
        "                buffer.append([float(values[1]), float(values[2])])\n",
        "                if len(buffer) == int(nbins):\n",
        "                    frame = np.transpose(np.array(buffer))\n",
        "                    rdfs.append(frame)\n",
        "                    buffer = []\n",
        "    return rdfs\n",
        "\n",
        "rdf = parse_lammps_rdf('./lammps_run/des.rdf')\n",
        "plt.figure(figsize=(15, 8))\n",
        "# g(r) is probability of finding a paticle at distance r.\n",
        "plt.plot(rdf[0][0], rdf[0][1], 'b', linewidth=5, label=\"NH4+ , T=300K\")\n",
        "plt.xlabel('r [$\\AA$]')\n",
        "plt.ylabel('g(r)')\n",
        "plt.title(\"NH4 N-H bond length: {:.3f}$\\AA$\".format(rdf[0][0][np.argmax(rdf[0][1])]))\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7aKj3yzhomge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensure cutoff_r (receptive field) is 4 Angstrom for SPICE dataset.\n",
        "Let us do radial function analysis. ngl can be used for viewing molecular trajectories."
      ],
      "metadata": {
        "id": "6bs71WeFor0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Molecular Dynamics analysis using Radial Distribution function."
      ],
      "metadata": {
        "id": "wkn7Nn-Yo3pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Confirmation of Molecular dynamics run.\n",
        "We can see how particles are interacting for SPICE dataset."
      ],
      "metadata": {
        "id": "Gl63qiyw33hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhance modal:\n",
        "from ase.visualize import view"
      ],
      "metadata": {
        "id": "XFJ-li4cb6VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#si_traj = read('./Si_data/sitraj.xyz', index=\"::\")\n",
        "si_traj = read('./Si_data/DES_L.xyz', index=\"::\")"
      ],
      "metadata": {
        "id": "rzbb9J6WmR9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "si_cell_prod = np.array([10.0, 10.0, 10.0])\n",
        "for i in range(len(si_traj)):\n",
        "  si_traj[i].set_cell(si_cell_prod)\n",
        "  si_traj[i].set_pbc([True, True, True])\n",
        "  si_traj[i].wrap()"
      ],
      "metadata": {
        "id": "F7bJ5b7BnQ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nglview\n",
        "!pip install ipywidgets==7.7.2 nglview\n",
        "#!pip install pytraj\n",
        "#!jupyter nbextension enable --py widgetsnbextension\n",
        "\n"
      ],
      "metadata": {
        "id": "E3kdzwhamhIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart Run-time for Nglview to work"
      ],
      "metadata": {
        "id": "Vs8gZotd92Ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use last slider (after ball size) to step through MD simulation (if it dows not show atoms moving)\n",
        "\n",
        "From https://www.quora.com/How-did-nitrogen-bond-with-4-hydrogen-in-NH4-when-its-valency-is-only-3\n",
        "\n",
        "One can see how NH4 bonding happens and nnot all N-H bonds are of equal size.\n",
        "\n",
        "We will now coarsened 3 H atoms into one and see how it impacts bonding outcome."
      ],
      "metadata": {
        "id": "Y03EAp224-mD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2024-12-11 at 5.32.33 AM.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "pHHAciu47TLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "import nglview as nv\n",
        "view(si_traj, viewer='ngl')\n",
        "# Animation sometimes does not autostart, so use lider to step through\n",
        "# SLIDER after ball size is next-step in simulation...\n",
        "# View below will change as one steps through.\n",
        "\n",
        "# See below for how NH4 happens. (One bond is ionic)\n",
        "# https://www.quora.com/How-did-nitrogen-bond-with-4-hydrogen-in-NH4-when-its-valency-is-only-3\n",
        "\n"
      ],
      "metadata": {
        "id": "DoH2ueMa4NzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "Vovoan395GyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COARSENING\n",
        "We coarsen similar atoms as bag o atoms for molecular dynamcis puroses and treat atoms with charge as separate kind of atom.\n",
        "\n",
        "For NH4, we hav 3 H atoms, that look similar.\n",
        "\n",
        "Based on Geometry, we will keep charged H atom separate.\n",
        "\n",
        " Coarsening of this molecule will make NH4 as U-N-H where U represents a bag carrying combined energies of 3 H atoms.\n",
        "\n",
        "N-H atoms bond at 109.5 degree in tetrahedral form:\n",
        "Source : https://learnbiochemistry.wordpress.com/2011/08/16/chapter-2-water-hydrogen-bonding/\n",
        "\n",
        "![Screenshot 2024-12-11 at 6.16.55 AM.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABDAAAALgCAYAAABieocDAAABXGlDQ1BJQ0MgUHJvZmlsZQAAKJF1kL1Lw1AUxU80UjFFFASXCoEORYhSYsHBxdqhCIKxVfzY8lHbYhsfSUQEwd1JF8Fdiv+AVMTBwbFbQUH8AwougpBFS7yvVdsq3sfl/Dicd7lcoG9QZ6wkAijbnpNJL8gbm1tyqIEwJIiYgKqbLktq2hJF8K295T9A4Fqf4rNGb43MTqUmRWLRlWPrtPg331NDVs41ST+o4yZzPEBQiLV9j3E+JB5zaCniE875Nl9wNtp83cqsZlLENeIRs6BbxM/EitHl57u4XNozv3bg24dz9lqWz6GOYBkWDuDSy5HKSGP2n3yilU9hF4ySDorIowCP/iTJYSjRBBmLsGFiGgqxijh1gt/59/063tE8MBcjeOl42Xvg6hUYv+x40Tow3ABuzpnu6D9XFXzR3Z5R2yxVgYGzIHhbB0KTQPMxCN6rQdCsAP1PwJ3/CQohZAhGHe4hAAAAimVYSWZNTQAqAAAACAAEARoABQAAAAEAAAA+ARsABQAAAAEAAABGASgAAwAAAAEAAgAAh2kABAAAAAEAAABOAAAAAAAAAJAAAAABAAAAkAAAAAEAA5KGAAcAAAASAAAAeKACAAQAAAABAAAEMKADAAQAAAABAAAC4AAAAABBU0NJSQAAAFNjcmVlbnNob3TgsKc/AAAACXBIWXMAABYlAAAWJQFJUiTwAAAB12lUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNi4wLjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczpleGlmPSJodHRwOi8vbnMuYWRvYmUuY29tL2V4aWYvMS4wLyI+CiAgICAgICAgIDxleGlmOlBpeGVsWURpbWVuc2lvbj43MzY8L2V4aWY6UGl4ZWxZRGltZW5zaW9uPgogICAgICAgICA8ZXhpZjpQaXhlbFhEaW1lbnNpb24+MTA3MjwvZXhpZjpQaXhlbFhEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlVzZXJDb21tZW50PlNjcmVlbnNob3Q8L2V4aWY6VXNlckNvbW1lbnQ+CiAgICAgIDwvcmRmOkRlc2NyaXB0aW9uPgogICA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgrY7WHNAAAAHGlET1QAAAACAAAAAAAAAXAAAAAoAAABcAAAAXAAAR+CUirSowAAQABJREFUeAHsnQV4XNe1/ddopBnRiJnJlmWSZY5jiClxmJmhSdqmzSu8pv2X0jav9F4xxUDDzAyGxInZsi3bMouZWSOW/vuc0Tga+TqxYsUWrPN9o5l7Lv/OlTR33b3XNn17R08f2EiABEiABEiABEiABEiABEiABEiABEhgBBMwUcAYwaPDQyMBEiABEiABEiABEiABEiABEiABEtAEKGDwQiABEiABEiABEiABEiABEiABEiABEhjxBChgjPgh4gGSAAmQAAmQAAmQAAmQAAmQAAmQAAlQwOA1QAIkQAIkQAIkQAIkQAIkQAIkQAIkMOIJUMAY8UPEAyQBEiABEiABEiABEiABEiABEiABEqCAwWuABEiABEiABEiABEiABEiABEiABEhgxBOggDHih4gHSAIkQAIkQAIkQAIkQAIkQAIkQAIkQAGD1wAJkAAJkAAJkAAJkAAJkAAJkAAJkMCIJ0ABY8QPEQ+QBEiABEiABEiABEiABEiABEiABEiAAgavARIgARIgARIgARIgARIgARIgARIggRFPgALGiB8iHiAJkAAJkAAJkAAJkAAJkAAJkAAJkAAFDF4DJEACJEACJEACJEACJEACJEACJEACI54ABYwRP0Q8QBIgARIgARIgARIgARIgARIgARIggXEpYPT29qBq33bUHt6trwCT2Qy/mBT4RSfDaguA1ddmeGX0dLaj+nAWGosOo68XcLd4ISQ1HQHxEw2XZycJkAAJkAAJkAAJkAAJkAAJkAAJkMDwEBiXAkZPVwd2/Oc3OPT2E5qim7sFMfPPQdzCixAYn4yAmERDuu2Ntdj74oPI/+RNoAew+ARi8mW3Y+K51xkuz04SIAESIAESIAESIAESIAESIAESIIHhITA+BQyJpNj0l/uw6/m/aYpmqzeSl1yE5BVXIzglDUHxEwzp2uursePRB5Cz5iUdgeHpG4j06+7FlCvuNFyenSRAAiRAAiRAAiRAAiRAAiRAAiRAAsNDgAKGcBySgPHwAziy+iWgD/D0o4AxPJcht0ICJEACJEACJEACJEACJEACJEACn0+AAobwGZKA8VC/gCHrUcD4/IuLc0mABEiABEiABEiABEiABEiABEhguAhQwBCSFDCG63LidkiABEiABEiABEiABEiABEiABEjgqyFAAUO4mi1WRE6dh/DpC+AXGQvfsEhD2h0tjcj94GWUZG2CWSqXeAeE0APDkBQ7SYAESIAESIAESIAESIAESIAESGB4CVDAEJ4mN3cEREbDOyQSFpsfPHx8DSn3dnaiPu8g6ksL4eHpC1twOAUMQ1LsJAESIAESIAESIAESIAESIAESIIHhJUABQ/E0meDpHwCrp49EY1jg5mExpNzX0422xnr9svj4wS8kAtNVFZLLWIXEEBg7SYAESIAESIAESIAESIAESIAESGCYCFDA6AepojDczG5azJCQjOPi7evpkhKqvbD6+cGmBIxrRMC4lALGcYFxBgmQAAmQAAmQAAmQAAmQAAmQAAkMAwEKGALR5OYmYkS4VBUJgbunN8zyMmq93Z1orSxBU201LN6esAWFYfq1jMAwYsU+EiABEiABEiABEiABEiABEiABEhhOAhQwhKZZUkbCJqYjZMJMeIdFwUu8MIxaV2sTija+h7L9O+HhYYavMvG8XgSMyxmBYcSLfSRAAiRAAiRAAiRAAiRAAiRAAiQwXAQoYAhJVYUkduYSRM9ZAb+YZNiiEgz5djTV4cBrj6Jw2xq4SdSGj38QTTwNSbGTBEiABEiABEiABEiABEiABEiABIaXAAUM4Wm2eiNp0flIXHYlgpMmITAhxZBye101djz6WxxZ9wrcIMaffoEUMAxJsZMESIAESIAESIAESIAESIAESIAEhpcABQzhqQSM5CUXIXnF1QhOSUNQ/ARDyvZ6ETAeegBHVr+k51PAMMTEThIgARIgARIgARIgARIgARIgARIYdgIUMAQpBYxhv664QRIgARIgARIgARIgARIgARIgARIYVgIUMAQnBYxhvaa4MRIgARIgARIgARIgARIgARIgARIYdgIUMAQpBYxhv664QRIgARIgARIgARIgARIgARIgARIYVgLjU8Do6sCOx36DQ28/rmG6uVsQc8a5iF90EQJikxEQnWAIub2xFruffxD569/Q861i4jnlkjswcdV1hsuzkwRIgARIgARIgARIgARIgARIgARIYHgIjE8Bo7sL+157CHnrHGacJnd3RM1apsuo2sJiYAuNMqTb0dKAg+8/jeJtq/V8i68/Jiy/ColnXmC4PDtJgARIgARIgARIgARIgARIgARIgASGh8C4FDB6e3tQuXcLqg/t0BRNbu7wj0+Ff+xEeNoCYPWxGdLt7mxD1cEdqCs8oOe7W70QljpTqpZMNlyenSRAAiRAAiRAAiRAAiRAAiRAAiRAAsNDYFwKGApdj0Rh9HZ1aYomE2Aym+XlAZNMqJdR6+vrQ29PN/q6ux2z3QA3s0VeZqPF2UcCJEACJEACJEACJEACJEACJEACJDBMBMatgDFM/LgZEiABEiABEiABEiABEiABEiABEiCBU0CAAsYpgMxdkAAJkAAJkAAJkAAJkAAJkAAJkAAJnBwBChgnx49rkwAJkMCYJKC8gnp7OtFcmo/y3ZvQWlHicp4+EbGInHEmbFHxjlQ6N6bSuQAawRP2ukqUZn6MhnyHn5NXUDiiZi9BUOL49HOqzN6Ksh3r0d1u16MWOmUOYmYvg7un1wgeRR4aCZAACZAACYxPAhQwxue486xJgARI4HMJKJ+grvYmlG5bh93P/AWVh/e4LB8xKR3p1/+X3PieBQ9PP5jdPVzmc2LkEqiRsdz+0P0o2LpGH2RwbApm3vojpKy8cuQe9Fd4ZLuf/ROynv4T7M0Nei+Tz7sB877xADz9g77CvXLTJEACJEACJEACX4YABYwvQ43rkAAJkMAYJ9DRXI/GklyUbFuLw+8+g6qcfS5nHDZxKlLPu17KTy+Df3QKrFLBiW10EKg5lIWt//gpcje8qw84LCkNs+74MSacc+3oOIFhPspdT/0vdj71B7TWVustT7vkNpzx7d+KgBE8zHvi5kiABEiABEiABE6WAAWMkyXI9UmABEhgDBJoqSxB2e7NKMtcJyLGGtQV57mcZVBcMmLmrkD07KWITF8A37Bol/mcGLkEKGC4jg0FDFcenCIBEiABEiCBkUyAAsZIHh0eGwmQAAmcJgL1BYeR+9HrKNv5CWpzdsPeUAuLj68cTR86W1vhGxCCwJTpiJ61BElLL0Fg/ITTdKTc7VAJtFaXo3DTu6jLydar+oREIPaMVQiZmD7UTY2J5SlgjIlh5EmQAAmQAAmMEwIUMMbJQPM0SYAESGAoBKoO7EL2K4+iYvcGNNeWoq+nBz7+gXoTrY31MJndYQuOQmTGIky57A6EiScG2+gg0NfXh77eXvT19eoDNplMMLm5wWRyGx0nMMxHSQFjmIFycyRAAiRAAiTwFRKggPEVwuWmSYAESGC0EdDVR7o7dOWRPc/9HeXZ29HR0gRbUBAipp8hN719qNyzGc0N9bD6+CFq2lxMv/YePc/N3QK341Qjqcvbp/007DUVGkmARGxEzlyM7rZWqJQGe20FuuwtIox4wMPbVypipCF82nw93SHmio3FR1CXux+d4s3R09kGs8Wql/OLSUHwhKmw+oXAw8tH9u96E16xZxNKtn+k96N2HDZ5FqJmLUZDca7eb2dTg96vWSpOqP2Gpc2U/Z6BLjmujuZGvc+6vGz0tLehp6td70MtF5g8BcEpU8XA1Ff3GY2zYqWOvb1J/ESKctEkFV06W+rR1dqoF1eigYdvgD52/9gk+EUnai8Rq4/NZXOdrc0o2boaVfszdb+7nGfopFlSASYB9QUH0VKer8+hW45RNTcPCzwDQuETGoXApFT4RUilGA+rVItx1/PtdRUo2SJpQXn79bRXcLikAy1HcPJUPd1aVYpiMfhsKDykp33CYvT8wIRUPT34R2Npnpi9rtXnp+ap84iW7flHJ+lFG4qO6DSklopiPe0Xk4goGfs+qXRTfXAXWqvKHGMv145i6x83QV9P7lZvtAu/ptJciRbZj47GGhn7dn1+ajm1n6CUafAKDO0f+y9XCWeoAkZHS6O+NpRHTKOkVnU216GrpUH/bqgTVGNq8Q2Ef0ySnEuyHlPPQR4x6vesePNqlO/6RDNRYxMyaSaCJkyT6jAHxX8mxzGmch2qZhI2noEh8AqKRGBiKgJku2pMzfL7wkYCJEACJEAC44kABYzxNNo8VxIgARL4AgK6+kibVB/ZvhZZT/4RpXvVTXMfwlOnY/Jld+rP+195SKqS7JXPJkSnz0XGjd9F1JylchMp1UiOc0OVs+Yl7Hj0f1Cd5yjdGTd7MTJu+T7a6qrFJPQ51MtNW2tDHTwsFngFBCNuwSpMu/pbsk1vNJYWoGjTByhY/xaaq0vR0doET19feInJokphSZbqGf6xE+ATHHH0Jt15mllP/wG7lEGjpMCoNmnVVZh1231y87gGh99/Aa3VZWipq4K3VJxQpo2p516H6df9lwgq1WgSYSB37Wso/PQdtNsb5Ua1Eb5BISIOhCBh0flIOftqeMvNv3dguHN3Lu8qwkEde2NJHorkZrVs10a01ZSipbZML2dy84AtNBq+IjBESzWXmLlnwU/K0toG+YkoQWHLP36MA+89p9fzET4p51wj3iMLUbjhbagyoG2NtfKq1/Pd5eY+ICIBwROnIXHJBbLtRXps3D0cZUFrDu3Gtn/9FHnCVLUQESZm3S4mnnI+qqntbfvXz1Eowo9q4XJTPfuOnyDxrEv09OAfxeKRkvnvX6A0e5ueFT11Lmbf9XPEikeKagWfvo3Mh3+FChGqVIuePg8ZN30XPd3dOPTO06g9kq3H3uzhDm+/IMSIMey0a+4RzmFoKisQ8WYN8j9+E40VhVpM8/T21tdIhHivJK+4EkFJk/U4mEVA+zJtqAJGkwgx6riKt6yVcrSfwF5dhCa5LkWR0bu3hcTAKyRars3FiDtzJfwi4+EvYtPA1tPVgc0P/hBZL/xDd5stnpgo13HyiqtkTN/SZV3bZUxb62v0fCUOBopg45+QhqQl5yH+zLN19R93T5+Bm+VnEiABEiABEhjzBChgGA2xPDUzNzXCvbkZ7k1N8Ghu0Uv1mt3Q7eePbn8bun390OvDLw5G+NhHAiQwegmo6iMNxTkiYKzD4feeRW1RHqy+NoSlzsTUy7+mnzLvEwGj6nCW3Ew2IyQ+GRPPu06befrHTDhuNZKc1S8i85EHjlYzCU2ejJj5KyUioQmVezdrEaFTtqdu1Kw+3ghPmys3c1egu6tTIgX2oj53HxrEl6NNlulub4G7RExYJBIhKD4VoVPnIWrGQsSICGAZFL0w+OY0On0+YuevksiKbFTt24p2iW5oF2HC4m2D1csLsfNWyH6vFuEhB7USNVIvUR91hYclIqMdPR2t8mTdXy8XMjEDYVPnI3qmY7+uIy4+IfZW8QpplJvRd4XlR3LDW4gWESLaJcpERZGoplI2rJKW4+njD1tErIgwKYg7Y6UWM1RkhzpH1ZSAsflvP8L+d57R0xaJfAlJnAQfETqaJPqhpaYcnfKkvkv2qZqKTvEOCIJPSBTCRUyImD4fYVPmIVAiG1T7IhPPir1bsO2fP5Myq2v18hGp6Zj9tZ+K18mlenrwj+Itq6Us6y9QIqavqsWkn4E5d4qAIeOrWsEnb2H7I79C+b4dejo4YaI2gO2VUr1q7JtryuQ6kOgbN3c99qETZyBp+RU6TUlFv6ixasg/BLv8X+5ut+vzU9dIgETfqPOKVGM/Zwk8Rfz4Mm3wNXK8KiRdsm8VmVO85UOJUFmNpuJ8NIvZbbu9GR1NdSJg9OndW/0CYZXryRYeI4JUAmLmLUf8wnNdonW0gPGX+7DzuQf1Om5Shlj9TgTI9awiTpSJbkdbm1xDTXq+YuMdGCyvUDnnOXLOCyRaaDZCJkz/MqfMdUiABEiABEhg1BKggGE0dJLr7VlWCp+yEthKSuFXWamX6nZ3R1N0DJpjomCPikZXqPFTN6NNso8ESIAERgOBlspSqT6yUd90qyiM1sY6iTqQG2F52j31stv1TVr2q4860kgkksBXbqpi5ixHlFQjiZpxplQjiTI8zcEChgr79wmORq+kBKgb2C6740bNuXKIlPYMn74Q7Y3VqD68Ey2S6tHbKmKyeDcMbN7BkbKdKHkqfRGmXXUXvINCB86W6AvXEpm+EvHgLfvtbKpBU1Ux1E30wBYpN4cRGUvEuDQL5UeydOpIr91+9ObUuaxfRJxsJwaTLrgB02W/A5tKHbHXV+ib0D3P/tUROaHvbR03uAOX/eyzSaIKAjH5wpswScp4egdG6AgTNX+wgOFYx6QCYI45Lsc8x08ltgTIDbRKTUhddZ2IM8v0jNMtYHhKBIlvkIx9T5f4q5Tr9IuBxx0s6UXh088UUaZFxn4Hmutr0ddmR59EbAxsKvrFR7YTf6ZE61xxJ2yRsQNnn/DnwdfI8QSMtoZqGddKZL/4T+x780kRU9pkH58/pmqMpl58s6RZ3esSrTNYwDh6sOJH4hRCjvYN+KDEqcCYZEkNmoqJ51yN5GXGUTEDVuFHEiABEiABEhhTBChgGAynqasbPgf2IfTQQUTlHEBEZaFeqktCo0viJ6JiQhrqUtPQnpRksDa7SIAESGD0EqiXaIPcdVJ9ZMd6iT7Yg165GQ9KmqY9C5KWXSwCQp9UJ3lD5+7X5e7VnheqGkmUSuVYdqk85U8xPHlDAUOEEZ/QSIk+CJfohg601lTqm/UmSeuwSOqISq+QnBSJwuiQiIJgnV7RLU+l9XISst9YWSY+AB4SEeGLlGWXYdatP9C+DwMPYPDNqa/4QijhwVcEaFt4BNoktcReUyWpJCVoKC+BX1iEbCNG31x3i7hiC48Wr4UEnVJi7z++hvIi8a0Q7wp5yj718rsw+7YfDtylaCw9ki6yQQxQN0mqyvso3rkRZk9vSePwkoiVCQiQCATVeuV/jfL1qJM0k56OdpjlXGJmLpKKIOcgcrp6wp6hlztGwJDIDbP4Q3jbbFC+FMprwV22r8Qd5bPRUlUkaSpVElnSpFNjAhMmibjzDRmfy/T2RoaAIeMgY2ELDxMRqUfGoEKn8zRJhRR3SSOySfSISbw8lO+Ip0Q0qNSa3q4ul2tEQjRkDHyQuPB8zLzlB5qDPsEh/hh8jRxPwKiQ1JpyiTIp3vIBird/LGYj7nC3WhEk/iVBEj2hPE1Uq8s9KJFLR0TgUFE7dsTPXYoYqfKiyg1HyUu1YwUM06BrJFWPqVkiM1SUTbNcc/baShF8anQkhorumHr53ZgsYhcbCZAACZAACYwnAhQwDEbb1NmJoC1bEL9zO1KPZCKlLUcv1Q4LDgRPR07aLJTMnouW6QzdNMDHLhIggVFMoOpAllQfeQTlWZ9KWkcZvCVdLloiLKLF4yIy/Ux9ZuVZGxwRGpnrYJeUDl+JgIiSaiRTL78DoZJuYNQMBQyJhIiSG/ZJ518lN3SdcnO4HRWy7YrsLfKk25H77y83araoJInwWIyUFZegTZ7Gl+/JlOU+RYVKAen3fZgs3hXz7vm1TsUYuP/BN6fOCIzExaskeuJa1OUfRoXe3icok9SJns4Ovbp6yu0nr/iF58h+L0Plvl2S7pApXNajVIQJ51PyOeLjcca3fjdwl9rb4cCbjyNnzStollSUehEVrJLe4G0LRMrKKzBhlcNrQqUjHHzjCRRufA/tEm3Q09EmT9eTHE/XV12DZClPq9pgAUOlG1jEFNJPBJ4J51yFxKUXS+RJuFSK6Ra/jrdQLIaa1ft3oKbgiF4/WAwfZ976I6RddKueHhkCRjQiJNUkVcZeGVjqsZfIn8psSSmpdhi92kTc8hMj0MiMhUhefrGOeDC6RiYuvxTzv/k/Ov1Cn+AQfwy+Ro4nYBz+4AXxTXleDFkPoU5MWS2SWmX1sok3yIXC9hbRMxwmqQfffgq5q1+V1JImnVoSJKKeTdKrJspYpV1wkz66YwQMibwYeI1MPP9aHYXjIR4XBeKJoUSTqn2Z4j3TX/o2OBQzb/ye+M/89xDPlouTAAmQAAmQwOgmQAHDYPwoYBhAYRcJkMCYJuCsPlIhFUZ2P/s3qT6yTXtcBEXHYoKIA8obQhllqqYqgigfAHUzV19WLL4FNkSJz0K6VCMJnzZPbuSk4sWgaiSDBYxgiWYLFxPKKLk5jRGTSfU0vbmiRPwS3kTOh3KTKJ9VCxdBJGb+2RKRoHwc5miBoVnSXPLWvoKc1S/oSAO13IkKGBGSThGRsVj2e6Z4TSwWAaROUj1KcfidJ3Fo9cvaY0FtL1ZFQoiRaNjk2eIjMVs/+W+RCI2Dbz2GQ3Ij6yxBaixgdCHrmT/i4DtP6QgPJbJESHWTUPFriJbtRs10CEEq6qR42zodrVG9fztqCw5JOkyopELEY8qlX5Ob3ZvVoRwjYKjUkPDUGXJc83VFFXV8qgJLX0+fvrlWJpyH33sGxbtEaJE20gSMIBFpwiRFRIlX0bMWSRSDlx77IhFycj54FnUl+fq4nT4pSjgLnzJba0ZG18ipEjCyX/k39r3+CFokSkRFAYVPnI7QyXPlPJQHx1K57h1VUEq2r5cIpk+lasxWVIiQ5BMSriulTL74NsyQVBLVBgsYSpQaeI0oLkq8UBVlVKWZupw9Ynj6FPI2fqjXV9cJBQyNgj9IgARIgATGGQEKGAYDTgHDAAq7SIAExjQBR/WRRh1ZkfWUVB/Z46goEZk2A+k33CtpDVL1QKqMQCwoutqbdFWQ3c/8BRUHd2suMTPmY4aqRiKpJB5e/lCh7wPbYAEjWgSJ1Atv1eUy/aW6gqo2otqBNx/Dzsd+IyH4uXo6Yf5yXZFCmWZ6S5UR53azX/oHdj7+OzT0Cx0nKmAkS/WQSZfegWCpXOEXlShP/x03ndsf/iV2PPWno6aJaWJMOv26e+ErESCqyohJeRNI2yJmmtuf+D8pOOHw4jAUMCTlZeu/f45drz2CXkkjcBObhEny9D1VIiD8IhOkrKnDq6FX/JaayvLFLHQ/DsqyuWL46SYVNmzi45Fx3XfEW+Mevc/BERg+wWHyNF8iNFZeLaVKE3T1Fb1g/w91w7v1Hz/BYamgotpIEzDUjXrqhbfIjf9iPQZWH199nIOvkVgRBqZe9U1dTtc7KExSSzz1coOvkVMlYOx48vfY9fxf0S7Gs8oXRe037ZI7RdhLlnFIPHqNNJYVSAWbQhx49WEcfP85mGRMVZne2XIuc7/2c30OgwUMs4g4RteIXlh+qIokm//6Q+x9/T+6iwKGkwzfSYAESIAExhsBChgGI04BwwAKu0iABMY0AV19RPL2SzM/whG56XKUSQX8wyIROilDSn3GyhNmR5nK3u5OnZOvUhEaq8o1l/DUaZgokRpRs85CQOzEY6qRDL45jZtzFmZc/x2p5DFPL+sUJgbfnCrBYaZ4TAQlT9VRBs7Iji8rYEwS48OMm38A38g4eEoahqoEotpgAWP6pbdh1td+pv0XPMRjw9lOVMDY8vcfY6c8sYf4NqjSstOlgsv0a78j5+onVV389eZUFEd7cwNaxN9g1xO/x8EPXxTfBw9dqjXj+u9ihogYqg0WMPxCIzBFvDcmXXS7ZmcRH4iBbaQLGNGSOpIupWpVBIZVxsDsYdWHP/gaUd4RM2/9oUTBzJGx9z0qNg2+Rk6VgJH52K+x89k/o0O8RfpkXKdedDNm3PJDEbhC9Xk4RS41pmqZrCd+i90v/1uPqZuYb86+9tuY9/Vf6XMdLGAoDxOja8Q5rhQwnCT4TgIkQAIkMN4JUMAwuAIoYBhAYRcJkMCYJqBKfCrjSSVglIq3hcrxH0oLjk+RlIxl/dVIFh5TjWTwzWnSwlWYe/evxKhypstuTvTm9MsKGMfzNxgsYGRccw8W3Ps7KdnpeOrvPMgTFTA2DyiR6SECw+wbvos5d93v3IzL+xfdnA4WMPyl0kvGLfdh+tWOCA2XjcnESBcw4kW8mnv3/VIKVFKHBrThvkYGbPpzP56oB8aJXiPOnW1+8D5sf/z/9KQy+Jxz8/cx/57f6OnBAsbJXiPOffKdBEiABEiABMY6AQoYBiNMAcMACrtIgATGNIF6ib7IXfMaSnd8jIa8vRJZUTak8/UPi0ZgslQrkQiMlBWXShSGazWS4b45pYBBAcOZZnSqIjAoYAzpTwIXJgESIAESIIGvhAAFDAOsFDAMoLCLBEhgTBOoPrQbe19+WMqjOqqPdDbVO6oqqBSL/jSLYwBICoSYQeiqG6qCgm9QpKQFLMHUK76G0InTXBYf7wLGLBWBcefPhInpqFeCAtQnZWpVBMaWB5W/wWOamcPf4PtSYeL7enokR2Co4y/eshqZD/8CJbu36OONkRSROXf+HLHzV+rpgk/ewvZHfoXyfTv09OiNwPgVdjz9R3SKB4ZqGdd8UyrQ/FZK2nodM6Yystj84I+QKX4pqjkiMP5bIjB+racZgaEx8AcJkAAJkAAJDJkABQwDZBQwDKCwiwRIYEwTqNi7Fbue/D8pEbpRVx/x9fdHSGoGbGJOaNHVEFxNOXvFA6CzvRXNJXmoPpyF1sYm8XbwRbSkBcyUUPlwqRgysI03AWPLP8QDQ/wPnB4Y0y69XTww7pVSmYHae0Ox6RUj0A4RiprKCpH11P9KFZSX+j0wxMRT/EGO54ExUlJIujvbdRnbkq2rsfeFB1G+f6ce8rEqYBz1wJDSwX1Sbn3KhTdixs3igSEGo54yrkc9MGRM2+WVJaafe1552OGBYVUeGPdi3t2/1IwoYGgM/EECJEACJEACQyZAAcMAGQUMAyjsIgESGJME1BN09SrbuR6ZD/0CRTs2yONik5SInCJGkXfr6iOqCocq0zmwdbW1wl5fiaKN72Pfqw+h6sg+FU6AOClNOueun2t/A3VD57ypG28CxtaH7kfW64+iR1Uh6elF6orLMeG8myS1JkkqbyRolL093WgozpUSmdk49PbjUiLzfZilJK0tKESX2zxeFZJTL2BMFyPVHyNp6aU6kkCNqY4ckZv0xpJclEgp2CPvP4PqHLkGpI1VAWOniExZzz8Ie0sjeltbkXLWRWKkehsCpCRwgFQicRrCNoqo11Cch0Nv/QeHxJjVzccH7lKFZKaqQnKHisI5towqPTA0Fv4gARIgARIggS8kQAHDABEFDAMo7CIBEhiTBJQQ0VJVor0vDkgpz6rcg7pKRljqDEyVyhmOsqg+R8uXOiE4yq62SNnVj5EtT5mrj+yRyI1GhKekIe1Sx3q+odFHhY9xJWB0dyFLqlUcevdptNVXo62hDmETp0s1l1mInrMU0VJ9Q7UeiWAo2rJG2H+C2sO7xDg1B74h4bBFxmPyJXcg7YKb9HKnO4UkKC4FKVKyNXbBKgTKzbrV5o8ue7OYhR5C8da1qNizBfU5u4/6poxVASNbyqLue/0R2KvL0FxTgbDkKQiWKKVoKR0cO38FnBVyiretFSPc9ag5uBOVUqnHNzQSXlKpZPLFtyH9mm85xl5K7Q6n0aveKH+QAAmQAAmQwDggQAHDYJDHqoDR29uDXgl7Vc3NYjn6ZcsAAbtOkoB6OqlKTfZ2d8NNSgSa3d1PcotcnQS+GgIqiqJqf6YWMAo/fVduzCphC4lB+LQzMOXSWxExbe7n7rhMfA/2i3dDZfYWNFeXwj80HPGLz5eb9KUInTxbl5hUGxhPAoaKrDjw9lPIXfcamooOoU6iLKy2QHjJK2nZxWJyerlmqsSjQ+8+g6LNa9HR1iJ/nzugxIKglGmYsPJKJMkTftVOtYBRmb0V2/71cxRs/UgiLXpgk7KtYWlzEZmxCDGzz5KSulFoq6tCRfY25Hz4Mmpz9qJNfCF6JKVItbEqYBxZ/TKOSJpPQ/4+EZuOwN3LBqukVyUsOhepF1wvZV4daVaH33sOeR+9KWParFOEghNS4R+XKiLQFZh03vWaEVNINAb+IAESIAESIIEhE6CAYYBsrAoYqrRewadv6xDghEUXIiBugsHZs2s4CHS2NqNg/RuokS/28QvPlZu5s4Zjs9wGCQw7gaayfBRseE8EjI9QLUJGr0QPhE6arSMvEhaeh0C5+fq8VnMkG/nr35ISrJ9IKsRuXXY0TISL6NlL5do/D34STaDauBIwxNuiYs9mucHfgqIN76Jo+3pt9Ojh5YXA6AT4RSdpJn09XWiQqIv68hIRL9o0u7g5SxC34DyET50rRqjperlTLWDUHN6D3c//BWWZH6NV0kTEjxK24DB4h0TCJzQKFi9vdHW0o1sEGPW3rq2+RotXbQ21+njHqoBRdWCnjOlWGdN3UChRFoBZxtWKwKg4x//TfrPbxuIc1JUUoKejQ6Js7EiQ6Iz4RefLmM5DhLxUo4ChMfAHCZAACZAACQyZAAUMA2RjTcBQTwO75YtU6bbVyHrmT7qyQPr135VqAWfB3eopT43MBhTYdTIE7HUV2P6v+yW8eg1mSPWBqVd+42Q2x3VJ4CsjUJu3HwffekIEjPVoKi+Ep68NiUsuRMzclWLimQ6f4MjP3XdTeZFOISjdvkZf710dbbBFxIuAcRbSLrwFQYmT9PrjScBQEVhtDVWSmlOG7BcfxOHVr0mlli4owaJX/DBU5RZnU9UpTG5m+bvsAR8/f0w8/0akXnizRK4oY8ggvdipFjCUqHL4/edQIgJGc0kOmmsr5bi7tceJPiDxwHAzu2tRIyh5qvZMqTuiUkjK9eyxKmC0NdaIWFOF/ZJKckAiZzrtdhnTbmHT4zKmMqD6/6pJIjLMHh5IO/caTL36m3pMvQJCNSMKGBoDf5AACZAACZDAkAlQwDBANtYEDB15IWXsVJ671cdfn7HKVbdFxSNx8UXwj00xoMCukyGgQsPLJK+9QcLHIzMWS/j1zJPZHNclga+MgEofyXr6TzqCQqUBBMrfhSmX34W4M8+VvP1wWLxdzTsHH4j6W9IqqSOFG9/DwTf+g4aKEnj5+otAuhgZN3wPoWkZepXxJGCoE1Z/AzrtTZIeslobpDYW5aFZxJ526etoqtNMlHhh9Q8WXgFiBDkBwSmTESVpGhHTzxDTR2+4W7z0cqdawGhrqEG18m/Ytw3lWZvQWHhIR2J0Njfq47HI+PoGheroHBVpoyIxjnz4PKpzD+j5Y1XA6G5vk8gTu/i+fISS7evk7/sRNJUWor21QaqxyJiKcKWaGlNPbz+JykhGUNIknXqjBD0Pq4ypjKtqFDA0Bv4gARIgARIggSEToIBhgGysCRhFmz5A5iOqdJsJc+78uTwJ7JRqA7+U0FdPzL3rfh3qbYBBPzHsabeLYYZZIjW85LtZL7pluk+8NFRzc7doZ3WncZn6QtYtT1/7JHzaMd/aP99NTw/+oZeXL4TqyaNyaFeh6+oLotqPamYPT93vrGKgDO/0fBXPLM3d4qnDsp3zdafBj24JzVbrHa+pmwS1/8FNraP2aZZ5ZvGx6JFzU2UDBzapsaDXNcuxDGzqCaxaX/lgqG2r9Qc2VT5RbU8xMGrqxkYxH7ye0bLsI4GTIVCxdwt2PPo/KJOUB9VCU6Zi5i0/kjSGc4a02UJJQ9n5+G9QIxEdqkWnL8Cs23+sw+bVdN5Hr2HXE7/X3gFqOk7MLGfd/hOE9KdJqD7VDr/3jC4p2ihCiGqJkoYy+2s/lSoPrilv+8VMcffTf0SLeDGoNmHZpZh95/3wDYvW084fe57/K/Y8+xfxaHDcfE869zqpknL/0egG53Lq2HbLsio1QrUpl9wuJS9/odM6nMuo9+1SqSXrxb+rGqi6O12erM+96xcDFzn6Wf0dUFEtjaX5KNnyEcp3bxLviDIdnaEWUn87vQIj4StpNvELzpaKL8tdIi+cG2qtLpf93o8ja1/RXT4BIUgXcWjKZV9zLuLy3lB0WP7mP4B8SV9RzT8iBjNu/G9MPNfhv1B7ZC8yH/0ViqR6iGqBMcnIuPkHSF7u8OZQ0RZdbU2oF+Eif/27qJQSuy2SatTWLOkk0rxsIrgkpGmhJXHpxWiXMdjxn9+gUkQP1cInzcSs23509H+L+h+kro1qqbaiWpQINOraiJg2X087fwz3NeLc7he9n+g14txOc2UJmiqKJKpxPUp3foq2mhK01kn0Sb+Aoar2eAVHI+6MFUg863yXyAvnNtTffuUzki3Guap5eFikzO635Xfvh85FXN7bRfTa/u/7cfC9Z3W/EgmnX3cvpl/zbZflOEECJEACJEACY50ABQyDEaaA4YBSI+7p+evfhNUvAAlL5Euq5DcXfPKmdmBXS4RIlQLV77xhqJQboYJP3tYhtmq+MvBLPOtieAeFq8ljWqmUbSwUT46gpCl6ObU/5dHRJTnVqinDuMSzLoFFygqqVrxlteTqvy1GcQ4xQrn5J0pZPyVkfF4rkOiTQslZdgorLstKKLTK00+U8xjY1Bd49WVa5YCr+THzVsj06/KFVeU9f9aUuJGw8AJxoF/5Wad8Unnhav1aCatOWHTB0S/yzoXaG2uR9/HrqJIbA6PmKTcoCYsv1DcIRvPZRwLDRaBV0hzKsj5Fc2me3qS3eByoKACnT8OJ7keV0yzftQH2mjK9ii06WW/HRyowqKZuqstkfntdpZ72j5uISInS8JbqDANbXd4+vR1nlEJgkopKWCx/hwIHLoaaw7slOmCD/L1o0v1BE6ZL1McSiRjxdVlORZiU795w9O9GiFQCiZq1+Ji/G0rIqZDtKSFVtbBp82S/S45JsSsXVuVZG4/erEbMOFMfn8tO+yeUgNEpx9chkQvKa6S5vFiOtxHdEoWhm6QaeEhUnNUWJCaPKfCPSdCCpzPywrnNTnurMFGVSrJ0l7u3DVEzFuq/wc5lBr4rdmW7PkV9v5hklVSUyIyF+m+tWs4ulVEqZH6DCBSqeUq6SpTMD4hP1dNKRFbiq/K0UH4OzRXF6JAIA6dJp1miCLyCIiRVKFaXEO1uswuTT9Aiy6nmK/2RMxZLNZU4Pd0k15Y6HlW9QzWbeIBEzVgEn7AoPe38MdzXiHO7X/R+oteIczsdrS3oFEFMiVMtZYXyuUGqsohA1i9guMuYenj76wiMgPgUl8gL5zaUqXbZzk/kf8AW3WUSo+fI9IXH/ZuvxHMV1VdzcIdeXv3vUcsrvxk2EiABEiABEhhPBChgGIw2BQwHlJw1L2GHPMXzDAiWp3M/lBuTcmS/9HfUlxXoBWLkC3T69f+FwMTJejpv3asy/x9o6v+SGj9nuTwl/I6EGU8Wt3ZvedroGomx54W/Ydfjv0NEujyNu+3HKNm6GvteeQh2+TKoWoqICjNu/J7sP0RP73/1IeyT3OOOfqf71BVXYsZN35cnmGHypd8bgyMxVLRIl7z2iO/HvjceRU//E1O9sf4fbiJgTLlIStvJfjxUyLYzvFe+LG76y33Y+/p/MOPqb4iHxTeR9eTvcWj1iwNX1w706imoCrlX5+i88WitLceWB38k4eMfYubN/430676j11ORF+q4GiX0ePczf0T+5g9ctuec8BOzvClXfB0Tzr5ab5eRGE4yfCcBEiABEiABEiABEiABEhivBChgGIw8BQwHFKeAoZ4gqjBvm4Q5q9xed4tVL9ApT6FUJIF6UqeahzwV9JT5yrRMtc6WZj0/ZFKGjpQY/KTVKWB4yBPTkNQMcbeP1Os7TUXVU0u1fWfKikXClr1k+yq9QrWOJpV3XAsViZG07LJj0i1U5IWK6FD52uq4RKswbG2yjU7ZVrxESiQuuUgvo1JHlICx++V/IyJ1uj5/tY3BT4FVVIdav7erUyItLjwaiXE8AUOFAavIjOp92/UxWXz9DI+pp6tLn5tKI0mQYxocam24EjtJgARIgARIgARIgARIgARIYAwToIBhMLhjRcBQrvfd9haUZn6EvRIZobwmZt5yn9xsd0ku+u+08/20q+/RqRruXr4wSwjrwOYUMGoKj0hYtjfixKxNRUqo1BHVDr/7lORR/1pKABZLWT0vJC5YpfOaAxMcVQf2v/Ywdjz2W21gOe+b/yPhtBMHbh5OAaNVxANVXnDi8isw+46faiFDLbjrqf/Fzqf+gI7WVlhlfqrKXf/az0REcDjzZz76AHY89UekrboW87/9u2NCx7f+86fIfPIPSL/ybiy49/faNd/lAGRCRURs/usPsOu5v2HOTd+DOk7VnALGrhf+Ltu1wdvmh2nX3IMMySMf2JSAsvmvP0SelEydKdEgGTf9QM8+noChQrF3PPIrXWIx/frvIO3i2wZu7ujnuvz92Pr3n6AuNxszb/0R0i669eg8fiABEiABEiABEiABEiABEiCB8UiAAobBqI8VAUNVH8mXp/0tVaWSZhGqc5UjJGdW5TerXG/loN4uudC2qAQkiZfE4GokTgFD+UEoD4hoKbsakbFActYdnhZOAcMs5mMx4gGh8tQjJa3E098hMJyogKEiL2Lnn62FFLW+h5ej6oFTwAiQfOk4PX+h3r7TMPNUCBh7JKUlbs5ZYsZ2juQmLxBDwrkuV8xQBQzljaHy51sqCvX2gidMc9mec4IChpME30mABEiABEiABEiABEiABEjAQYAChsGVMFYEDGV6qRzzxVlMIht+dkxVAZVisV2iATw8fTDnbqlGIgZ4A5tTwPAKjsC8r//yaDUB5zJOASMgbgLmfeMBBIuJ3sB2ogJG9KyzMO+eX2tDuIHrOwWMxIXnY/63fnuM2d+pEDCy33wMs2/4rq5aMPDYnJ+HKmA413O+KyM3ZVqqKqUMbA2Fh3XFBuWVwQiMgWT4mQRIgARIgARIgARIgARIYLwSoIBhMPIUMBxQKGDch69awFDVAPLXvQJVAWFg62xpkgome7TfBwWMgWT4mQRIgARIgARIgARIgARIYLwSoIBhMPKjXcBQ9eVVqoKq6rFbKnD0yTmmX3svVKTDwFa8bQ32PPsnqbzhI9VEvielPpdIST8/8cJwmHBSwPjqBAxH5EUT6gsOIfvFv6N4UHlWNb/Dbod/WCQjMAZetPxMAiRAAiRAAiRAAiRAAiQwbglQwDAY+tEuYCjvizx5ql8qNePr8vbpMwxKmgxvSQUZ2Fql3Kma72Z2R1DyVMTMPkuqeVx+1AuDAsZXJ2C0NdQgb+3LqNqfKeMi1VcCHL4hzvFpq62SEqwfoKuthQKGEwrfSYAESIAESIAESIAESIAExjUBChgGwz/aBYyS7Wux/V/3o2z/Dnj6SHURMdn8vNYjJUDbpSRqTPoZmCteGJEzFunFKWAMv4ChIis6WxqhPC6ypTJMXe4+pF93L1LPv8lliGji6YKDEyRAAiRAAiRAAiRAAiRAAiQAChgGF8FYETBaa8qlesbZCEhINTjLz7rqcvdDGX76RSdSwBhURnW4PTCU6WfumpdQuW8bfEKi4ReThPBp8xCUOPmzAZFPFDBccHCCBEiABEiABEiABEiABEiABChgGF0Do1XAUN4XHc0NKN+1AftffwRubmbMuEm8LWYtNTrNo31Fmz9E1tN/kEgNKyZfeoe+obbaApC//k3seOQBsArJ8FUhaSovxNa//T8ppboBM2+5D1Ov/MbRcVAfVMlaNYa1h7Ow66k/oKkklykkLoQ4QQIkQAIkQAIkQAIkQAIkMF4JMALDYORHq4ChDCHV0/2WymL4RsQiMCFNlz71DY8xOMvPuprKi1CVvVUMJQ+iVdZVkRjJK65E9aFdFDCGuYzqFwkYrbXlyF39ko6IqcvbL/4kZgoYn12q/EQCJEACJEACJEACJEACJDCOCVDAMBj80SpglGZ+hG3/vh89He2YfefPkLDwfIOzO35X3kevIfPhX8HqH4R5X/8lWqpKR7WAseup/xOfiX8iaeklmHL53TCJGHBM6+tD9sv/FEPNVzD1iq8j4+Yf6EV6Otux6S/D74HRUlmCnY//FhW7N2Li+Tci8axLXA6ppaII+197BKWZH6Pd3gpbcCgFDBdCnCABEiABEiABEiABEiABEhivBChgGIw8BYyxIWBUHchEtRiZNpXlo1miTCBixTHNBNgi4+SViLDJs/VLLfNVCRid9hZUif9FjaSINEs6iao2MrCZrVbYIuKlqw+FG96VcrhNFDAGAuJnEiABEiABEiABEiABEiCBcUuAAobB0I9WAaNi72ZkiW9CT2cH0m/4rpRF/Xzvi8Gnrsp27n7mT7D6BWLGjf8Nu6Qz7JFp5YGRceP3EZI6w2UVFbGx59k/wy82GRk3/UBSVia5zD/ywXPY89xfETIxXUc2+EUnucw/+PYT2Pv8g4iYcSZm3nwffEKjXObve/UhZL/4d8SKEenMW34IT/9gl/l7Xvibnp+8/HJ9k+/h5eMy3zmx68n/xb6X/wVVAWRwM5ncJPLiLjm++1xmKT+RHf/5NY68/7z4VNwtlUK+4zLfOdHRXI8dj/0WRRvfxbSr7pFIj7v0LHt9NXY9/huUbv8IU6++B5Mvvt25in5vrijGrid+i4JP33HpVwymX/MtEVLmyPzfiZnnAUy/9l6kSEoPGwmQAAmQAAmQAAmQAAmQAAmMZwIUMAxGf7QKGK21Fajet12MIHt0JMEXeV8MPnUVpVC1fzvMVk+ETZmL7rZWmc6EEgbCpsyBV0CIyyqNYjCp5lttgQiX+Ur4GNhUqVC1PW8RQFR0g8XXf+BsXWlDRUgovw51wz5YgKjN2YvqAzsclTrkeMwWT5f1aw7vhoqyCEpM08frZnZ3me+cqDqwU7aTKUENvc6uAe8mhKbN0sc3oFOLHVXCsi4nW88PnZQxcPbRz0roUBEVDUU5CEubjeAJ0/S87vY2VO7fhuayAr3toKQpR9dRH3QkhsxvFEYDm4ePn96OV1CYbDcT7Y3Ven3/2AkDF+NnEiABEiABEiABEiABEiABEhh3BChgGAz5aBUwDE6FXSRAAiRAAiRAAiRAAiRAAiRAAiQwJghQwDAYRgoYBlDYRQIkQAIkQAIkQAIkQAIkQAIkQAKnkQAFDAP4FDAMoLCLBEiABEiABEiABEiABEiABEiABE4jAQoYBvApYBhAYRcJkAAJkAAJkAAJkAAJkAAJkAAJnEYCFDAM4FPAMIDCLhIgARIgARIgARIgARIgARIgARI4jQQoYBjAp4BhAIVdJEACJEACJEACJEACJEACJEACJHAaCVDAMIBPAcMACrtIgARIgARIgARIgARIgARIgARI4DQSoIBhAJ8ChgEUdpEACZAACZAACZAACZAACZAACZDAaSRAAcMAPgUMAyjsIgESIAESIAESIAESIAESIAESIIHTSIAChgF8ChgGUNhFAiRAAiRAAiRAAiRAAiRAAiRAAqeRAAUMA/gUMAygsIsESIAESIAESIAESIAESIAESIAETiMBChgG8ClgGEBhFwmQAAmQAAmQAAmQAAmQAAmQAAmcRgIUMAzgU8AwgMIuEiABEiABEiABEiABEiABEiABEjiNBChgGMCngGEAhV0kQAIkQAIkQAIkQAIkQAIkQAIkcBoJUMAwgE8BwwAKu0iABEiABEiABEiABEiABEiABEjgNBKggGEAnwKGARR2kQAJkAAJkAAJkAAJkAAJkAAJkMBpJEABwwA+BQwDKOwiARIgARIgARIgARIgARIgARIggdNIgAKGAXwKGAZQ2EUCJEACJEACJEACJEACJEACJEACp5EABQwD+BQwDKCwiwRIgARIgARIgARIgARIgARIgAROIwEKGAbwKWAYQGEXCZAACZAACZAACZAACZAACZAACZxGAhQwDOBTwDCAwi4SIAESIAESIAESIAESIAESIAESOI0EKGAYwKeAYQCFXSRAAiRAAiRAAiRAAiRAAiRAAiRwGglQwDCATwHDAAq7SIAESIAESIAESIAESIAESIAESOA0EqCAYQCfAoYBFHaRAAmQAAmQAAmQAAmQAAmQAAmQwGkkQAHDAD4FDAMo7CIBEiABEiABEiABEiABEiABEiCB00iAAoYBfAoYBlDYRQIkQAIkQAIkQAIkQAIkQAIkQAKnkQAFDCP4nV0I2LUTUXuzkHBkD+Jrc/RSnW5W5ERPQVFaOiqnz4B90iSjtdlHAiRAAiRAAiRAAiRAAiRAAiRAAiQwzAQoYBgB7e6Gd14uAvLzEJyfi+CKEr1Ut7sHquOTUJuQhObEZHTExBitzT4SIAESIAESIAESIAESIAESIAESIIFhJkABwwhoby88aqrgWVMNz6pqeNXV66V63cywh4eiPTQUHSGh6AkINFqbfSRAAiRAAiRAAiRAAiRAAiRAAiRAAsNMgAKGEdC+Ppg6OuDW2QlTexvcOrr0Un0mE/o8rej19ESf1Yo+Dw+jtdlHAiRAAiRAAiRAAiRAAiRAAiRAAiQwzAQoYAwzUG6OBEiABEiABEiABEiABEiABEiABEhg+AlQwBh+ptwiCZAACZAACZAACZAACZAACZAACZDAMBOggDHMQLk5EiABEiABEiABEiABEjgpAn3ix1acA8/yfJfN9IkfW3t0MrqjEl36OUECJEAC44UABYzxMtI8TxIgARIgARIgARIggdFBQCriRb/2L0zb+IrL8XZ7WJC17AbUnHujSz8nSIAESGC8EKCAMV5GmudJAiRAAiRAAiRAAiQwOgh0dSLt3z/G8sznXY630+SOD5feiYKbfuTSzwkSIAESGC8EKGCMl5HmeZIACZAACZAACZAACYwOAhQwRsc48ShJgAROOYFxLWD0SbnUtoZatMuro7EG7Y21egDczB6wRcXDNzweZosF5kHlUnu6u9BcVoDW6hL0dHRCbcc7OByeQWGw+gbA4u1zygeSOyQBEiABEiABEiABEhjlBOQ7pUfRYfgUHsDktc9hZuEmlxPqghnbJ63A4eVXixdGCr0wXOhwggRIYDwQGNcCRm9PD8r3bkXVvkzUHMxEQ8EBPeZmiycSF1+ChCWXwiswCJ7+AS7XQkdzA458+AKKt36IjqZ69Ml2wtMXIEJewUlT4B+d5LI8J0iABEiABEiABEiABEjgCwnId8roV/+BjE9fhL+9BoHdLS6r9MKEaksAqgJjkb3sOtSec73LfE6QAAmQwFgnMK4FjB4xSCratBrF29ehcvcnKBchQzUPb18kLjhXRIyLEZKajuCUyS7XQVtDDXY/+2fkrXsVdonaUEJI3NxliD3zXIRPnouQ5Ckuy3OCBEiABEiABEiABEiABI5PQCIvCg7Bp+igYeTFwPV6RMRoEy+MrLQVyF10GdriJqKbD88GIuJnEiCBMUyAAoaBgGEWh+fgxFQEpUxHyorLkbTkYpdLgAKGCw5OkAAJkAAJkAAJkAAJnAwBKZsa/bJEXnzyHALsdQjocY28GLjpPplQkRj17jbU+kdh14qbWJVkICB+JgESGNMEKGAMEjBM7u4wiQeGl58/fAJDJY3kYiSddQl8wqLgFRQu/y4gnhnjKwKj096Ctvpq9HR26F8GN3cP8QbxhIeXDyw+vnAzuxv+knS2NmtWyjOkr7cP7larLG+Tdy+YrZ4wmdwM12MnCZAACZAACZAACYwXApaCg/AWz4u0j17ArIKNQzrtTpMZmakrkLP4MtjjUtEdkzyk9bkwCZAACYw2AhQwBgoY+3fApG6srRa49QIWETNCJqQjbOp8xJyxElEzF0PdcneMMwGjoSQPlXu3i+Fpjb6+PUSE8AoIhW9ELAJiEo9rWtpQdATV+3eiXTxDVJqNd3CY+IMkwCckUsSgMCghhI0ESIAESIAESIAExjOB6Jf/hpkfPwv/NhV50TokFCoSo0EiMaolEiNr5c2oXnXDkNbnwiRAAiQw2ggMu4DRZQe6muQlkW9dzRCDS1ck7lnbD3YAAEAASURBVN7iMeEHOB++u0vBDovts2nXpb/aqcEeGBUHsmALj4DFL1Cqkkh1kpYW+AaFwic8FtFzlslrKfwjE+AuKSbjyQOjQoxOj3z4IprLi/SAePoHwzcqQYs7kelz4SXTRq0861PkffymVGupQI+UA/MTsSN88mwEJqTCPy4F7hYvo9XYRwIkQAIkQAIkQAJjnoC7VBvxKjmMtNXPYW7+Jyd1vnY3C9as/AaKrv3eSW2HK5MACZDASCcw7AJGaynQmAe05AP2AsnRc2QdHOXgGQn4JAJu/Q/ffWMB/5TPpo8ueAo+DBYwqo7sddxgT5iG+iNZaCjOEQWmD25u7vCPTUHQxGlIEmPPsEkzx5WAkb/+TWQ9/UcHDxkXb0mlCUxIQ+TMRWJ0eiF8Jb3GqOWuewX7Xv2XlJwtQnd7GwIS0xAzfwUips5DaOqs40ZuGG2LfSRAAiRAAiRAAiQwlgiEvvkwpq5/HiFNlQjvajypU6OAcVL4uDIJkMAoIjBsAkZvF9AjYkXDIaA2C2gtkFSL8j6o/oHNEgJ4RZlg6hcw/FOB0AxHVIabVSIxlMnEKWrHCBg5+xE7ZwnCMxahSVygm0XAaJGogyaJIPCWSAzfsGikLL8CUTL/8PvPomTrmnFRhSRn9YvIfOQBVOXs0yMTIKkjgVJpJUaiUlJWXgWbTBu1w+89jSyp1tJQVoDOlmaESUWXuIXnIjJ9oYgY82GVVBQ2EiABEiABEiABEhgPBNzL8mGVlzwd06ebsvZ5zDiwGlaZtogt58m0DqlKsjN1GXIWXoLW+DR0y4M3NhIgARIYiwSGTcBQ6SIddSJe7ASqt/ahq16iLzrlT/Sgv8cS4Qb10m6Y8hYwzYTwBSJqSGSGNejUppIcI2Dk7kf83KWInrdcUl960dXaiuLN76Ns7xZtWuktxp4xs5YiNG0WKrO3oDZnLwWMzxEwDr3zNHY/82fUKwFDDD3D0yhgjMU/IjwnEiABEiABEiCBLyYQuO4lJG94XXzWHF+OI6tyEWWv0F+J3fpFjS/eivESygujUfKya/yisPPsW+mFYYyJvSRAAmOAwLAJGG1VEnVRIgLGLqB+Rx+6T9CDyC/NhJC5gE3SSnzkQb5ka5yyNljAqBYBI27eMsQuWCWGk5HideGNw+88jsJtq9HV1i7RISaEpUwWL4dk2GvKYK+t/FIChqnVDu+8I/CpqNDn2uXjg+bkZHSHi4ozAtuXjsB4VyIwnpYIjPICdEgEBgWMETi4PCQSIAESIAESIIFTQiD6xT9j3trHjwoYtm47fPokfHlAU7EZFdYQVAdGI7i+HMEdtXCX6Az3AQKHWqZPBAuT/jlgZfnYLiHOu1KXIvfMi9GSMJmRGK54OEUCJDAGCAybgNEs/o6Nkj7SIFkGzQf70NN2YnS8E03wnyKRGJJKEph2ar0wjidgxC08DyEp0+EbGo19r/xTjChfR1tjHdqbGmELDhHTUX85OdHKpbKGXcw+VYWNuLkifJx5rnhozEWIpFd8XrOUlmLKy89g2oEterEq/0hsu+Aq1C1a8nmrnbZ5X1rAkBSS3c9JBEYpU0hO2+BxxyRAAiRAAiRAAiOCQOLj/4Pl6x+FW394slkECPOg1BEVSbExbRUOrrwByZJiMu3AOnhLSLMPPsvJ7pZl1HKO9R3pKM4TZCSGkwTfSYAExiqBYRMwmgqAumwlXoiB5xERMNpPDJlXnAl+Il4ETAaCpgHmU1hZ87gCxqLzESFCRGDcROR/8jqKt36I2sN7UV+SD5PZTUw93eDhrnJhzGiT1Ag1PRQBw5qfh4X/eRALSjdoSLWmAGydshQlU8QM5ARar7sZ3Z5e8vJGt5eUdbGcGDS9TnAQ+ryGVv1jsIBhCwmHLTIeoZMyED1rCbykPKpRUx4huWtfRqN4iHTb7RKBkUEPDCNQ7CMBEiABEiABEhizBCz5B+Cbn42Jn76Kmfkb1SMww3NVwkSneFmsW3Qr8qSaSOCGt5CwfTXiS7IRZy/V66g12yUeo0NClr3EaM6KQeX++rfcZrJg++Szkbv4UrTHpqInKt5wn+wkARIggdFGgALGptUo3r4Olbs/gTOFJK5fwAhJmYqWykLUFx5EzgcvoXT7R+hob0WnXfJjTGadUtLb3QmLmFGejIDRJf+IGt38YFc1Zk+gdZitqPcJRmNAKBpDItBhk7q0J9BaQsNQN30ausPCT2DpzxYZLGCoCBRvmw3esm8lZHh4SS1cg9ZYkofa3ANob27UUSoRk2dSwDDgxC4SIAESIAESIIGxSyDy1X9i7ton4NdeD/8e+3FPtAUeaDF7Y8uyW1Fy7Xfg1lAFd3kINPU1eeh18EO9nkodaXDzQquHD/zEgM6v1/iJYbfIJCoVpTR8Ag4vvwaNSy457n45gwRIgARGE4FhEzB0+VSpOtqwV1JJsk88hcQn2YTA6YDfRInCmDBCUkj6BYzQieno6WpDu6SJFG38QASM9XJDvhe1xfno6+5Ab7cjnM/TP/CkBIyhXjCd8g+u0WRDs8UfTT6B6LAaCwiDt1sek4gjK1ehPUVAD6ENFjDMEv3h4eMNLxFuPP2DJWpGubIe29rra9BYWYYuJfhIi5gyiwLGsZjYQwIkQAIkQAIkMAYJWPL2wZaXjQkb35DIiw3HjbzoEVFCvYr84lAcMxXFC85H08ILHEQ625H20E+wPPMFPa0EjHqzF5rkO2Cblz+6JBo3ojYfYR3inj+gqeXs8oCszhqIzefejapLvjZgLj+SAAmQwOglMGwCRnst0FbmqEJSs01MPFtODIr/FDHGlCokvhLZ5hWlszJObMVhWOqLUkjCUmegV5yie3s60VpTjcaSHBx883EUSWpEV5tdbsyb9FGcagFD/VPqlsxHx0usnUxuJ0TjcOhkbLzqVjTPnHVCyzsXGixgmKxWmCUNxVsiL6w+fpJJ4+Fc1OW9s6URTTVVUl7XYYhCAcMFDydIgARIgARIgATGMAEVeaFMO/3a6mHrbZNvb8atQ6SNNons3TbzUuRfeCe6A0LQFxDsWNhAwGgySSSuNQDZcy5E1exlmPbGvzAnb/0xG1d+GA1mH3y86psou/KeY+azgwRIgARGI4FhEzBU1ZHORvHB2C1lVLfL52pHJZK+Qal5IhrD3fZZuVQtYJwBeIaIlYN4Y57gvfiwsD4RAcO5ox4x6lRGnoffeRKFG95Fc2meVNcQ51JpQxUw3KsqkfDBu0g8mOXc/Oe+u/X2wCr/wKzd8urqgHtP9+cuP3imVQxJ/HqbkGtLwdrrv47GufMHL/K504MFDItfALzl5RsWDX+pyOLh5Wu4fmPREdTk7Baj0waJZOkSXxGmkBiCYicJkAAJkAAJkMCYIaAjLyRid8KmN5CRv+kYo87BJ1roHYniqFTkS+WQxqVXuM4+RsBweGA0uvti09l3ovzC2xD+wdOYsOUdRNQVILzTNRKjTaqS7J54FnLnn4+W5OnoihtaFK7rwXCKBEiABE4/gWETMJShcq/cVzceERFjjxh55gL2oj70ulaHgmck4J1gglt/1oG//B0NThdRQ+wfTmUJVYV+KAJGX58IMu12lO/ZpF/Fm95H2Z5tegSHKmCY5GbeXCdlsZqbT+gKMLe3wae6Er7V1fCTd6/mEwxv6d+6f30VkmsOotw3algEDP+wSPjFTkD4tPmIP3MVfEIkdMagFXz6Bg6+/YSkkZSiq7UVYanpTCEx4MQuEiABEiABEiCBsUPgRCMvnGf8yeTzsO/ir6NHvMUQLK+BbZCAoWapyApl0rl2xV0ouva7MFWXwVp0GOlvP4w5hRsHrq2XbZanh9W2KGSuugM1q653mc8JEiABEhhtBIZNwHCeuL0KaC0R8UK9SuWPbKdzjuPdUwpWeMV8JlZ4y99p39jPpl2X/mqnVPnTqgM7UH0oS6qMZOkUkfCp8xA2bQGCEiYiQKILBjYleDSLqWdjUQ5Ktq1F9f6dMrsP7uIJETZN1psyT9abBL/IuIGrnfRnU6e4XtTXwlJXB2ttHTxaHJ4SJ7phr4Y6hMkxt4hXR+7yk/fACIiIRaCUio2ZswwpK6+CTaaN2mEpo5r17J/RUMYyqkZ82EcCJEACJEACJDB2CFjF78J2ZA8mbH4LMwq+OPKi1CscFeEpOHTGhahZdpVEIbuJR7zZFYiBgKEWaJfIitXL7kLhjfehT77PmuqrMf2xX2Dxvrdd1++fsovgsWPySuQtuAj2hDT0RCcaLsdOEiABEhjpBIZdwJCKTuJ5IC8RLlT0RX+p66Mc3KxSKlVFX/QnArp5yLT0mfqnjy54Cj6oqIq2hkp5VaNVIhvaGuslLSIGvqHR8PQLhNVXcl0GNLV8j6RwKFPKxqJcNJeXirItRa+krKlKp/ANiYZVUiusPsYpFQM2NbSPsl+TiCeQyA237h75RzW0FBK1rrmtDb0e7ugJUGVUPYe0/8EpJCcqYBx652nsfubPqFcChpSbDU9jBMaQwHNhEiABEiABEiCBUUMg4tV/Yf6a/8Bfqo34SnWQL/pq++nUC7D/8m/Ld7MQ/VInahr8hfhEBAz5nugmhvPTHvkZFme/ZchLVSWptAShLCwFB1dez6okhpTYSQIkMBoIDLuAMRpO+rNjlLQQ+cfQ3dkmppxt6O5oh4enLyzi6WC2WKS6hqgrBk0Ze3aIr0Nna4vEX4iYIP9slA+Eh6ePYz13d4O1Rm/XyQgYWU/3R2DYKWCM3iuAR04CJEACJEACJHA8Ah4FB+CbtxcpUm1kVu4Gqf0hedUGrUzCkMtDk9FnFvN1MX07ItEQtSuvMVhyQNcJCBh6aXsLwtc8jwnb30NkVS7CO8Rdf0DTaSdyZDWewdhy/tdRLd4ZbCRAAiQwGgmMcwFDIkQkRES/evskWkRC8MSIQ4fwiShxjAreP8IqEqNPRAwVCdGn+kRiN4l79BetNxovEHXMFDBG68jxuEmABEiABEiABL5qAsHvPI4Zax5HaHMlgsTV3s3x7fCY3W5IPQfZl3xDXOutEr0rUbFSir5Xoi8+t52ogCHfS90kothalofpr/79uFVJ6sVJ/6Pz7kHFZV//3N1yJgmQAAmMVALjXsAYqQMzko6LAsZIGg0eCwmQAAmQAAmQwEgg4F6SA++iQ0jY+CamHVgLH8mjthhEX5R6SeRFRCqOzD0PNUsvBzwsx3pdHO+ETlTAkPXVwzVTaxPC172ElB0fILIyFxEdNS5bVl4Ye1IWIX/uKjRNzEBXfKrLfE6QAAmQwEgnQAFjpI/QCDg+ChgjYBB4CCRAAiRAAiRAAiOKQMBHryD14xcdKRtt1Tp1xMj3YmPaudh35b3oCgpHr1+QTj0+XpTvMSc4BAGjV/uldcLc2gjPikKJxPjbMZEYKpWkVQzpqm2R2HbeXag5h1VJjmHODhIggRFNgALGcAyP/MPwkBKnVnl51lTBU/wxjFqXjw/qUyejMz7eaPaI7Svevhb7X30YjaV5+hi9AsMQIHXEw6fMRczcZfCWf8hGrXDjBzj47tNorSwW89M2BCSkIiJjEUInpCM4aar4hkjtXDYSIAESIAESIAESGIUEIl4T084PHxXTzjr49nWJNKATi4+eiUgJ6JDU5A3zr0XOTf/PkToilUaG1ETASH3051iy9XnZWp9+qfUHViFxbk9VI9FRGFLJxNxYjcmP/RIL9rzVv55zKce7EjGyJp6FvPmr0Jqcju7YCa4LcIoESIAERigBChjDMDCmVjsCsrIQlZ2FxJw9iK4vMNxqjU8YPr3gOlStXGU4f6R21hUcREnmR7DXVehDtPgG6IorflKCK1DKxlp9XKu1OM+jJjcbZXs2ob2hBr3dXfAJjUKgCB+2iDhd6cXsIeVn2EiABEiABEiABEhgFBKIfe6PWLr6n1JxpMPQ96LRzRNNFj/sWHAlii77JuDpBbehGr13dSJJyqOeufk5ePb1wNqfomIoYIhHm2oqusNNyqpOfPIBzNn1JrxkncGpLVJDD3XuvigPisfec29Dw9IrR+EI8JBJgATGI4HTJmD0tDvKrXa3Ad3y2Slaq5Kq7l6O0qpmeZe/wSO+mdo74ZWXi+CD+zF523qkVu2VfxYd8s9CasoOaFXmYKxeeh2Kzr0APRJ90Oc5tHKmAzZ1Sj/a5Z9gY2muLoWqduzu6Q2Ltx88/YPgHRgi1VqMhYjWuko0lhfK+LbKYPfA4uMHL1neKgKIxdcfbm6Dap2f0rPizkiABEiABEiABEhg6AQsOXsRcGgnkqXix/TCLfAQYWFgE0t4qFdeQBIKJ8xBefpiNM9ePjTvC+cGRcBIePLXmL/xWfHYkO+bsmXVjAQM5yrq3STl6wM3vIH4HWsQX7wXMW1VA2frr92dIrtUWwOx8cJvofqC213mc4IESIAERiqB0yZgdEqWharw1CavdvEXEt8h3TwDAK9gwKpeYswsVaZGfhPF29TeAY/KSqS+9SrSszcgqKsO/mhxOfZat0BsmH0+8paejbbwCKn5Hegyf6RO9Eq1lZ7OThkjxz9NNwl/NEkJMFWxxc0sVVuOozI513NUeoEIFm7y5EGqtcg66rOM7kg9ZR4XCZAACZAACZAACRgSiHj93zjjg4cR2FEPLzHuHJw6ouIx2qQ6XWb6+Th88TfRGxSGPpt8wZXvS8f7zmS4I9UpAkbcU7/H3I3PwbenDb79D8e+SMBQX6xNLQ2wlOUj/cU/Ym7eJ8fsQsVr1Hr44+MLvo2Ki+88Zj47SIAESGAkEjhlAkZnkwgW9Y5Xl4gXXTLd3Qx0yj1+l7z6o97gIbYIFslIMPuIUC3vvmIXYUuWm99R8LDe1NKKoMytiM3KxOTDmUhuz3cZ8xZ440hIGvJTp6NsWgaaJ6SOqkgMl5PhBAmQAAmQAAmQAAmMIwI68uLgDiRnvo9pBVsk0tY18kIJAsoks9A3GkUxU1A8eyXqz7wQJlU2VXwpvlSTh0i2Te8hZsdaJOZmIqm5UG/mCwWM/p2ZWpoQuu5FiRb5ENFVh6UqiTw5HNCUF8bepDNRIMfaOGkOusSvjI0ESIAERjKBUyZgNMnf28ZDQHMO0JrbB0kXdIgWEnnhEnknD+ZV1IWOvJD3sMUmxF0kgoZlJGPsPzaldtvb4FWYjwXPPYozyja4HLT6p9Yu/+6qrWHInHs2ihYsGVWRGC4nwwkSIAESIAESIAESGEcEIt54CGe8/xCC2uvFj+LYyAuVNqK8JTZPORf7L5XIi5BI9EnVEdNQjTsHMO2TJ3xurS0wN1Rh6nP/i4X73tFzT1TAUMaeJvEisxYfQcZrf8Pswo0Dtu4QXNrdLKj0jcSWC76O2rOvc5nPCRIgARIYaQROmYBRswuo3CQpI8V96JA0PBfR4nOoBM0xIXK5pJVIoQtJ0xsVnhjmhnpErVuDCVlbEFFTgpD2KhdPDGckRmHKFFQlTkRTQhI6wqS0lr/f55DgLBIgARIgARIgARIggVNNwCoG7QEHJPphx4eYWrgV1r5uw0Mo9o5EUfRkiWY4G3WLLpYvrp5fPvJi8B7a7Zj80E+wbOdLek6X1BbZKZETOWdciJaUdHQlpg1e4+i0EjHcqsow5elfY96+9+EhkSPuTvO5/qVaxXB0b9ICFMxagca0uYzEOEqPH0iABEYagVMmYJStBUre6UNXY794oeLsTqD5JJsQMBXwl4i2gEmjI5UEPb1wa2mBZ2kxUj94B1MObkVwVy38IGaW0lQkhrL4bHAPwKHYdBROmYnKGRnoSEo6ASJchARIgARIgARIgARI4FQR+KLIC+dxbEw9G3uuuBc9odGAnzx1O4nIC+c2j74PEjDUd8kadxuq/KKxd+VNqD33hqOLDv6gojjMYqye8uwfkLHrbama0i4P1lxFGB0lbPJApS0KWy78BmpXXjt4M5wmARIggRFB4JQJGKUfioDxpggY4nsxlOadaIJtIhA4BQgSIWM0eGE4z8+tWRygd2xH1L49iCzJQ1R9CQI662DrFzI6pBhWmWcUisOTkTt9DmqnTENHOCMxnPz4TgIkQAIkQAIkQAKni4BH/j74SbWRpG3vIz1/s04bMTqWUu8IFEvkRf6ss1GjIi+kXOqX9rww2oHqGyRgqOeAHRJH0SClUDedfRfKrrrneGvqflWVxH/LB4je9RGS8zIRZy8zXL7aIwAfX/RfqLzwdsP57CQBEiCB002AAsZXOQLKE6OtHR41NQjfuR0J2buQUpKNmK5SvVepXSL6tzvq3fxxMDYD+dNmoWrGTLQnJX6VR8VtkwAJkAAJkAAJkAAJfAGB4PeeRMYHjyCsuQIBPe1SW8Q4fHjTpFXYe+330R0snhfevsMbeeE8xkEChupW3yPtJgvWrrwbRdd937mk8bv+TtoKS2kuZjz/f5iTt95wOQoYhljYSQIkMIIInDIBo1z+Tpa9L1VH6vrQIwaex/kfcAwaW6oJQTMkEi9FqpHIazij8Y7Z2VfUYWoTY8//z955gMd1len/HUmj3nuvVre65F5jO8VJICG00MKyCyx1Ydlddv8sbCEQWmBhgUBIFlJI73Gandhxr5IlWbItq/feexnN//uufJSZ0chWYrWRv/M8N3fOveeec+7vKpbuO+/5vuoaeFdWILSiBBEUSCmkrwF+k5SWhcowOTFayIlRE5qIso3b0JWRiUl3DxgdbSFy6QJBk26FgBAQAkJACAgBIbAEBBxqL8GjsghRlLkjuewQPGjJhV5bAGw+mUYK0FYXmoyaNTejc/tHKI2ei3mD+axZETC4+2ESMPbtJAHj0/88p9F0fd3wf/cFxBa8jfDmCwixyErST7EwzseuR03WDvSmrsVYNK3flrIsCUySKNVSdARtpafN5mfnoEdQ2noEpeaZHZeKEFgpBBZNwGg7BbS8S//QNhsxoeJgzIGiT6YOQZsAV1pO6BJ8OTvJHK5bVk1o7aFufILiYvTDtaEOgRdLkXX0TST3X9SmOeXEsEerPggn1908lZ0kOAQGL8oZLkUICAEhIASEgBAQAkJg0Qj47H8OafseQ1BnNfzGeilcJkeImFmOxu9E8ce+DUNQ+FTMC521VjOv+0BH5knA0LKS9PfAqbYMWc//Grl1FGHfpPCdjukc0OIeiuO3f42ykkgsDBM8y+qjYXwcZx78T5S+/H9m83KgtL3pn/g6Mj97FVeO2VVSEQK2Q2DRBIyeMqC7mASMFlrGRymoDUNTqVRJ1DZzZFA6alA2J5AADHtnHXzSKZXqOgrkTO/yFKvIJrKQzPr4Jyag7+2Be2U5Ml5/AWl1+XDVEquOa5f0ww0V/omoi01Ba2wi+qIoO0lwsGQnmRWonBACQkAICAEhIASEwPwSCHr1YWx444/wHe6guBecGNV86QjHvKgLX42a7J3o2HQ7fcPmNv8xLyxvab4EDPpSDfTNvX1zLVb/9afIu7DXalaSKSfGOtRm7kDP6nXixLB8HsugbhgfxfFffxcFT/6v2Wwc3TyR89lvI++LPzA7LhUhsFIILJqAMdJB4gXFCxqiFKrD7RR4iEQMimeJsW5zR4YDZRLV++jgSMGbnfynMo/4ppGYQaKGzs62sXMUaB1tjg2NSHr5WaSXHJ2RnYRjYnTa++JSZAaqKTtJW1Y2RmNibPvGZfZCQAgIASEgBISAELARAqHP/Q7b3/wtvCcGZ4gXfAvHkyjbyN3fxURAKIzOrouzvnmeBAz1COzaGhH/5P3ILHpj1qwk4zp7NLMT40PfoKwkn1SXyn6ZEBABY5k8CJnGohNYNAGDfgdgfGBqG6NMJPx54vLeQOeMk1P3bk+/BxzcSMQgtwU7LlyDaAtdnN8Ni0XfrrcP3oX5iKAt7eIJxIzWmg09DGeKiRHyXnaSlNXixDAjJBUhIASEgBAQAkJACMwvAafyIviWHEdM4QGk1J2ekXVkjBaTjNjpcTL7DpR9+rswevnSl2uL9O3aPAsYuoE+eJ3ah9DCg1hVcXLWrCS99If5uYStqMndhf6EbBgi4uYXuvT2gQmIgPGB0cmFNk5g0QQMdqzREkLwnsUKTbC4XGdnnnaemrDLQltCyHve7Em8oG1FFY4EPToG18pL2PjXP2Ft6wmz22NUBvol2WXno2UnqUrPJSdGDkajo8zaSUUICAEhIASEgBAQAkJgfggEvfIQNr7+APzJJqynP1Qtl4700frmXkdPFOZ9CDV3/D0JGH6wc3CYn8Gv1ss8CxjaH+IjI3BqKEfWEz9FbvVhqzMYp9wrXfSNYkNAPEpv/Tv0bbrVajs5uPgERMBYfOYy4vIgsGgCxvK43eU1C/vODoQcPID4opOIa7qIEEOr2QSHKDtJK2UnqYxMRclNH0J/VpbZeakIASEgBISAEBACQkAIXBsBp/Ji+J47hpiiA0gm54WLcSo2merVQFIGR8Ko8Y5BZdIGtKVtRH/GFop94brwsS/UJEaGEf/nH2LTyafgSF9zcVYULu83C4nqTu3tervgd+RVRJ3dj6iGcwgZoXXeJoVlnFH6Uq3RPQwn7/gGund+wuSsfGy7UIDmwsMwkMBkrQQk5yI0ZzPs9RTcb56LrQoYQ50taCo4jL6GCjMirv4hCMnaBK9wSjspRQhcgYAIGFeAs+Cn2IkxMgq3Sxex6cmHkNdOqVpMinJiVLtE4+2P/h06tu80OSsfhYAQEAJCQAgIASEgBK6VAAft3Pja7+FPUean0qXyX2DvlVFyIQyTJTg/9SaU3fUPMASGQUeBO7FYy0d4KmPDiH7858g7+jTcDMNwx5TIcq0CBgf01I0Ow6nuErKe/Clyao6+d+OXPzGNFmd/HLrjO2i/+TMzzl/PB8498zsU/fVXGKbMLtZK8u7PIO/L/wknj/nPLGirAkbbhTPIf/hHaCARw7T4xSQh+/P/iujNt5kels9CYAYBETBmIFn8A/qWZkTvfQ0pxScQ1lMHv8lus0nUO4Zh/02fQtPWHTC4ulHAqPlXcc0GlIoQEAJCQAgIASEgBFY4AacKcl4UHyXnxbvkvDhDzosxq3dc6xaGipgsNGVtQ++am2B09Vg854Wa0dgIIp/8FbJJwPCkQHKexlHtzDULGJf71/V0wP/wK4ii+B9RDSUIGaXo+yalz84FF2PWoDZjK7rSNmE8Jtnk7PXzsf3iWTSdPYSJYQrmR6WF/navP3sUYwO9ViGkf+TvsP4bP4GTJ2Un+AClseAgWgqPWL1y0mBAw8l9NL55Klx7SqMamb2ZnB/kErJS7J1cEJK5CUGpa6ycXfhDrSUncfKBH6DmxNtmgwUnZiD3i99H7PY7zY5LRQhYEhABw5LIEtTthobgUluNoJJC5B5+A4kDlHPWpDQ7BOHY+ttQt2ErhoNDYPCafxXXZDj5KASEgBAQAkJACAiBFU8gcM//YdOe3yGAnBcOtCTDMuaFAnAsfgeKPvlPmAiKgo6+SFpU54WaBAkYEc/9FhlHnoH3SA+8J4e1M/MlYGDSAOPgIJyqS5H77P3Irj+pRtb2lEePltHYoYmykhy981vo2vFxs/PXS6XkuT+g6PH7MUhLb7hMjI1hghws08H8LEBcq4Bx6g8/QOHTv7PodarK8QMnxkZh4PFNCwUTdHB0gYMTpXC0UpzdPJDx6W8h4+5vWTm78IdEwFh4xit9BBEwlsET1k1MwL63B+7VlUg8sA+JVYXwJeXbA5SehUqfzh1VvvGoj05CS2wS+mLiMBISgkkvyjkrRQgIASEgBISAEBACQmDOBNh54Vd0BNHFB5FEzgvXWZwXja4hqInORG3mdnStuxlGN8/Fd16ouzJMwLXwEAKLjyH+3LtI7Jr6soszoxTGbETlut3oi8/EeGyquuJ97Y38NkzLSRzqypH9+A+RW3mY5AqWLcwLZyUpjVuP2qwb0Ju6DhORCeYNVnit+Klfo+CRn6OvrXlOd3qtAsbx//0uTv/lF3Maa66N3Hz8kP25f0LW5/5lrpfMa7v+5jrUHH4VXZWlZv16BEcgatNt8ItPMzsuFSFgSUAEDEsiS1XnXxodnQg6cRyxRaeRUF+E0Impfxx57aGWlcTeFxfDM1CdlouW7FyMSVaSpXpaMq4QEAJCQAgIASFgowQCX3sEm179DQJHOuj1n50X1suJxF0o/Mz/I+dFOKB3upwmz3rbRTk6QTlB+ruQSsE8txa/rA3Js2/Xe6HVJxIlOz+DrhvvvqapONRXIP2xHyLn0rvkSjESH/4r9L3CwUwH7BzR4BODgju+hr7Nd7x38jr4JALGtT/kSXrnmSTnyCSJcqZFR2knefmLnf1KSz9pepfyeT4IiIAxHxTnqQ+7wSE419XBl4J6xhSfRnRTGfxMnBiclaTNKQRNfpFoiElAe3wy+hIT6Rdr0DzNQLoRAkJACAgBISAEhMDKJKCvKoV36TFEn30Xq2tOwm1yKo6E5d02ugSjOjIDdTk70L3xNnJeeFg2Wbr60ABS//Tv2H72eW0OLGD0UHyKTrdA5N/0t+i47fPXNDe73k74nHwTEYUHEVOdj7DhNrP+OPcJLyVpoKUkx+/8Nrqvs6UklgJGYFwKAlPz0EvCT0tZEcbp+ZiWa3Vg1Bx6FXXH3zLtcvqzkZb9tJacQsvFwulj/MFe74iQlGz4J2aZHVcVB2dXRG68BeG529Uh2QsBmyIgAsZyelyX7Xv69nYEnTyliRgJ9YXkxGjRZskauJF+aQzCBe2OAaiIWo2S3R/GYEbmcroLmYsQEAJCQAgIASEgBJYdAZ+3n0Tuq79HcF8T3ClVqqW7QE34eNw2nL37uzCExlC2ERdyXtipU0u/tyJg9Osc0UFZQs6QgNF+xxevbY5aYIVxioVxHtl/vQ85teYBIlXnjS6BOPSRf0LnrmtzfKj+bGVvKWCk3PIp5P39f6Ji79MofPLXGOwyD356rQKGgVw3k+PWg8tyFpJTv/8+Cp/9gxk+R1rqlPXJryP7b/7V7Ph7FZ0mctg5OLx3SD4JARsiIALGMnxYyonhRTExQirOI5xSW0X01MB3skebLUXMQL/OjfKRx+HMrjvQkbcGBg9PGB2tB+tZhrcoUxICQkAICAEhIASEwKIQ4LgOnpcKEF6wH0mXDsN7YoDSpc6M78AxL6pjslHHmTbWUrYRd6+li3kxGxkLAYO/3Bqlvwt7HTxw5KYvo/ljX5/tyvd13K67Hb7HXkckZSWJqSuamZXE3gVlkXmoS9+Mjsyt101WkubCw6g7+ibGhvo1nsFp6xC1+XaUvvAHFDxGwT072804X6uAYdaZRcVW06ha3MaCVXvo/anx9AEMdjRdcQyfqESE5d0AV7/gK7aTk8uHgAgYy+dZTM+EAynpaLMfGIRzUx2CiwuRd3gP4gcr3mtDnxr1YTi+8VY0rN2E4ZAwGDwlqOc0IPkgBISAEBACQkAICAEi4Hl0D1Lf/DNCW8vhP9ZH4oXBKpcT8Teg8HPfx0RIFIz2DtBRNodlVywEDDW/QZ0T3rnxK6i7+zvq0LXtKU6BcWQYTlUlyH3qZ8huOGXWHwsnk+QKbqSlJIc/8o/ovuFjZudXaoVTlxoN4zBOTsUG0ZGLwY5+Vgof/4UIGMvsoVcffAX5D/8IbZXnrziz6LU3YM2X/4uW3Iij/YqgltFJETCW0cOwnIpunOyNfb1waWqE//kSeLc0mjUZ8PFFe3IaBqOjMeHhBaOzs9l5qQgBISAEhIAQEAJC4Hon4Pv2U1hLS0eC+hop4wgFwrQITMkxL6qislCfvR3dG24h54X38kU2i4AxRIE139711XkTMFRWEn3tRWT89cfIrjxiNahnD2UlKYvOQy25VroztmCcvs2+HsvZx34uAsYsD36oswUNp95Bd83FWVpMHXYLCEM4OSG8oz5YZpve+nI0kONioLVe67CXnFfNRUfR23plB4Z/dAJCszbDxW8qpqBvbArC1uyEq0/AFeerTrZdOIPGM+9ibKBXHdL2gck55OzYiW6KvdOYfwDjw4Nm5y0rQeTmiVi7k5b3UMBgKVckIALGFfEsg5O8FpE3UsLZlWFajPzNgB2ty+T9cvyWwHSy8lkICAEhIASEgBAQAktAIHDPX7DtlV8iYKzbasaRk7FbkP+pf8NEWBx09GXQsnReKG6LJGCo4RzIhp/6xE+QfvEARWAzwNFC/OEgomMkCdV7ReLkXd9Gz5Y71aXX1V4EjNkfd/vFszj9x/9A9Ym3Z29EZ4LiVyPnC99DzLYPX7HdbCdrjryG/Ifu1YKpam2MkzBMUKYTi/cny+t19C7FLhr1LhW78WbNkTHXdK6lz/+RUuv+DH3tU9kjVf/JN38Ca7/yI1S8/QwKH/sFBnq61Cmr+7Q7voB1X/sRHGnpmpQrExAB48p85KwQEAJCQAgIASEgBISADRJwKi+CP2UciS45jISGwhlZR9iLwfEj8lNuROmnKWhncOTyi3lhyX2RBQzOSuKZ/w7Cig4hvvwkwoemAsurafFXayxiNLiF4chHv4Pu7R9Vp66rvQgYsz/utgsFOPWH76PqyJuzN6IzwYkZyP3i9xG7/f2JYJwBhh0e7IJoKT6GnpYGs3F8w6IRuHotXP3NY1ywQ6P1/BkMdJpn2gmISUJI5kaEUZaWMHJEXM2JUfLs71Hwl5/OGDc4MR0h2VvQQ86TxnOnyKHRZzYvy0poag5Cc7bStg0R626EvYPesonULxMQAUN+FISAEBACQkAICAEhIARWHIGANx7Flpd/haCRzhnLRvhmh2lRRB8FoyzKuAWVd/8jDGRhX/ZlkQUMzn8HivfgyLEwHr0X2XXHrSLiAKjvfvSfr5tYGJYQRMCwJPJefaEFjNqjb+DMQz9EU0k+mS0ovo2F4yJqzXbkfekHCE7f8N6k6BNnjsn/vx+jvcp8aQs7sHT29li15Vas+coP4RubanadZWU2AUNH2Yu4H053ayQn/dUKO0F0dvbIuOuLWPv1++Do6n61S67b8yJgXLePXm5cCAgBISAEhIAQEAIrj4BTeTECzh5A9LnDiG+c6bwwkGNggrZar2hUJm1EW/omDGRuhtHNY/nDWGQBQ8XCsOtuhd/p/Yg4dwixVfkIGTH/1rqXYmFURGajdvUmtGffcN1kJVE/MEshYHBA0bqjr6Ox4KCahra3t9cjfN0uLZ6E2Yklqgy2N6Hu+JvoqpoKpjnc2YqWcyfQVV9lNqMP6sDgpSNnHvxvNJWeMeuPnRdB5LwIzduOyPU3wYMcVqalk/6dqDvxFpryD5MT4/QMJ0b8tttJwLgXfqtWm1424/NsAoZlw6DENASnrYeDi5t2qv1CPokuZzAxMmTWlJ0Y7NxgB0jEupvIiSHpbs0AUUUEDEsiUhcCQkAICAEhIASEgBCwWQIBbz6OLS/dj+CRDpIpZhaO2TCks8dZWjpykZeOhERPr3+f2XqZHVkCAcNIcQSM9LJsRy9SzjXnyYnx38iqOzkDDC8naXAPx6GP/hO6t9014/xKPrAUAgbzZIFJE5ks4GougmUUH890nm2lp2hJyQ9Qc+Ids1nPt4ARmbeVYln8Jy0H2Ww1rs0UNyPK33iCnBj3ob3a3Ikx3wJG6oc+h3Vf/REFC51aylL4yE+R//gvMWwZG4MdILRlfvwrWP+Nn8DB2dWMk1REwJCfASEgBISAEBACQkAICIEVQMCpgpwX+fsp5sURcl4UUcyLEat3VesaiksJ69CcuQX9OTtg9FjGWUcs7kBHmQzi/3Iv1p96Bk7GCQqqOWVNn+8sJGpY7SVP2d/J4u5I6/nTH6egnpWH4UQ+FgeLoJ6aEyOCnRgb0U5sx2OSVVcrer9UAoYtQm0tOYmTD7CAYR7U8/0KGBz7ov7kPjRx7AtydKjYFz7kvAimjB5hudvIiXITPEPMnReWzDovFVM/eylTyCFyYpwiJ0a71iQgJhHBGRumnBDk4HDx9re8VKtfzYHBzouQdIqpQRlWIqgftTSk6ewhNBCDpsLDaDp3GhOjw2b9Z33iq1j3jZ9C7yIChhkYqogDw5KI1IWAEBACQkAICAEhIARsjoD/W3/F1hfvp+UNUy8gs93AqRjKOvKZ/4fxyAToyFWwrLOOWN7E8BCin/gFco49DQ/DMNwxrrVYKAHDcniH2ktIfPpXSL34LjxJIHImEcNaqaegnoc+9i/kxPiItdMr7pgIGHN/pPMlYNQdexOnH/wvLUCm6eiROVuQ9/f/ibDsraaHr/r50uuP4czDP0ZHzSWztvHbP4y1X7sXvjEpZsdV5WoCRurtnyUh4j64+YWoS8z2HIeDnRgjvd1mx0XAMMNhVhEBwwyHVISAEBACQkAICAEhIARsiYBj5Tn4F7yLKMo2El9/VnuxtjZ/DjRZsWoNGtI3ozvnBhi9/ChoHqWjt6UyMQ7XoiMIKD6OhHP7kdhVps1+sQQMzkriTuMHFx9B8vmDiBgyTx2pUHY6eOJ84mbUZ21DX8paTITGqFMrci8Cxtwf6/UiYAQlkPNCy2Zi7rywJCUChiWRq9dFwLg6I5trYbkWbi7fLMy8hm/b2spRm8MhExYCQkAICAEhIARWIAH+20VHSxh833kGG1/8H4QONmlLGqz99cItT8dtwdnP/wfGI+JtnoZusA+r//Tv2Fr4onYvmoCx8yuoo2wqfK9z+dvvWiDwch2OhZFZd0p7BpZ9jVF62i69J2rD03D+9i9iMHubZZMVVRcBY+6P83oRMFJu/TTWUzYRt8ArZzcSAWPuPzuqpQgYisQK2E+MjWC0vwcjFAxmmBRyDrikd/OCs5cP3HwDZ11DNdLbhb6GKrq2F5O0zlHv6gZX3wA4efrA0d1L8hCvgJ8NuQUhIASEgBAQAiuNgL66FH5nyQVwjmJe1BXAyzCkpUu1FDAaXYJRHpuHxsyt6F2zE5PkvLD1YilgsGBQGpaFqvTt6KL19qPJOQt6i3adLfA5/TbCiw5hVdVphIx2mI1HIT8xotOjJiAeZ+78B/Svv9ns/EqriIAx9ycqAoY5KxEwzHnMpSYCxlwo2Uib0YFe9DbWoLe+Ej0U2MYwNg63gFB4RcTCPy5FEyWs3UpPXQUaKQBOf3M9DIYJuPj4wy82GV7hMaQahpPwMZXux9q1ckwICAEhIASEgBAQAotKgJwXME7C++CLyH35NwjvqYMLBbO0twgoyVkx2I1wJnoT8j/9bxiPTrK9mBezgLUUMCbpPvt1juhw9seZW76I9g/97SxXzs9hdr9wdhKni/lY++RPkNmUb7XjOspKcvguykqy5Q6Al+sso8wYVif8AQ+KgDF3cCJgmLMSAcOcx1xqImDMhZKNtBkkNby1NB/tFwrQUXYWhvFReIbHISApG+G5W+EVZn39If9DUvbG4+iuuoCJsVG4B0YgOHODdp0vCRnOHj42QkCmKQSEgBAQAkJACKx0Ag70JY1X6Qn69v8gEiqOw2e8D3pNqjC/cy3mBWUbaUjbjJ6sLZj0DtBeoBd6eYX5LBamZilgsFAzoHNAl6M3Tt7yZbTd8eWFGfhyr9rSY3LtOtJSkoynfoHV1ce1gJ6WWUl67F1RFZaB2rRNaMvdtWKzkoiAMfcfNxEwzFmJgGHOYy41ETDmQslG2vQ11aCWcipzOqHWc8fIgTEMH4qYG5q1GXE77oLfqtVW76Tu+FsofPznaLtUhNGBAfiExyJi480IzdmGkNXr4OpDv/ClCAEhIASEgBAQAkJgGRBwP7UPqa/9CZFNFzTxQqUSNZ0auy/yYzYj/54faM4L03Mr4fNMAQMY4bgTjh44fvPfo+UjX1uU29RXX0DCs79BctlhWsIzSE4Y61lJaj0icISdGJs/RE4M+xXnxBABY+4/biJgmLMSAcOcx1xqImDMhZKNtPnAAsaxt8D/8LKAMTZIAgYtOREBw0YeukxTCAgBISAEhMB1RsDryKvIe5GWjnRVwcU4PmPpCMeD4KCWxUnbUfrxb2OCg3bS8oWV4LxQj9qagDFBLoxecjwcuumraPr4N1XTBd3rejrgUXICwUWHkVJ6AOGzZiXxwMWYtahP34ruzM0wRMQt6LwWu3MRMOZOXAQMc1YiYJjzmEtNBIy5ULKRNh9UwKg9Sg6MR36OVhIwxodJwIgUAcNGHrlMUwgIASEgBITA9UOAliyAYnX5738Wm1/+H4QNtVi99z47Z3Q7++LC6u2o3P0FGGgJrc7BYUULGArEoJ0T3r7xa6j/5LfVoUXZO9PS5Zy//hhplJWEY5FwdhjTMk5BPft1zqgKTsG5u76Owdwdpqdt/rMIGHN/hCJgmLMSAcOcx1xqImDMhZKNtBEBw0YelExTCAgBISAEhIAQeN8EHCvOIfjUW4gsPYbY5hJ4Tg6b9WGg1+Zx2mq9o3Ep7Qa0p67DUFIujJRVjYNHrmQHhgKxVAKGlpWEMsKEUlaShPLjCBlpV1PS9hxklJ0xVX7xOP2x76B/3U1m5229IgLG3J+gCBjmrETAMOcxl5oIGHOhZCNtLAWMidEh+MYkIzhjI2K33wlfykRirdQf34eix3+FtvJz4sCwBkiOCQEhIASEgBAQAktHYNIAjI3B79CL2PISOy+arc6FY0AM2OlRkrAV5z/6D5iIWuKsI5QpRccZU6hw8o2pj6bOBBZVrB03v73LXVzO4kGZPLi/wT6s/tO/Y2vhi2aNl0rAUJPgrCR5f70PqfVn6GlMamlt1Tne13pE4shHvo2ejbfB6KCfiodh2sBGP4uAMfcHN18CRv2JfTjz8L1opgQGnLjAyA4tKhFZG5Hzt99DWM5Wcl450uqxqf9nZpvhpMGAyfExXHrzCRQ++jN01FZoTe3ItWVH16/acivyvvxf8IlOtNpFybO/R8Fffoqelgaz8ym3fhrrv34fZXQMMztuWREBw5LI1esiYFydkc20sBQwRgZ64O4fBM+wWAQkZsCVUqpaK91V59FwYj96mhvImTkCv+gEiYFhDZQcEwJCQAgIASEgBBadgOa8OPkmos4fR4wV5wVLAuy+qHMLQ3nyRrSkbUR/2gYYlzjriHNfE9z7G+Bg7wC9oyMmxscxTi9Kquj1ejjo+fgYHR9Xh6f3k/RCZpgwYILSlY7TS9ZEQBwm/aO188tVwLDraIZXPsXCKCYnRsVJhIx2TN8Pf+ixd0NN6GrUrt6I1rW3YIxS266EIgLG3J/ifAkYfY1VaMw/iMYzB9BUcIjeY+q1SXiHRCAwOQehudsRvelWeg+ynoVRzbj94lnUHnkNzYWH0XaxEIPdUz+zgfTFbyhlcQyjpAZh1Jezl5+6xGwvAoYZjkWpiICxKJgXZxBLAaO/sxlO7u5wdveAM2US0Tu7WZ3ISG8Xeul/+pGebu28f2ySCBhWSclBISAEhIAQEAJCYNEI0Eu7jtK7+x16CZso5sVsASI5vsIIpRAtjtuIcx/7R4zFpi5uzAtyWthZxHxg14Rv2zmEthbDxVEPZ2dnjIyMYHR0dBqfk5OT2XEtNen0WWiixhg5T/i6ERIy+uM20L2t01rYsQPj4f/AlqKXTa4AltqBwfdgJMHFpfQk1j15H9Lo/q2VGs9oHP3It9C9bjdA4o6WmcRaQxs5JgLG3B/UfAkYasQaEh/OPPjfaCo9ow5p+4jsTcj5wve0bIx2jk4znBiTFE/HQP++lO99ipwXv0BHzSWz6+O33Y41X7l31iyOqrEIGIrE4u1FwFg81gs+kqWA0dfRBAdXN7jQ5uTuDQcnZ6tzGBvsR197C8b6e7XzImBYxSQHhYAQEAJCQAgIgUUk4FhJMS9OsPPiGDkvSmfEvFBTaXANxsXETWhK34S+9I0w+gQuaswLt5EO+Ay3wtHeDnp2VEyw02ICXmPd8B3thqPeAfb29jCQIGPqtNBe9umFn2Nz8Mbn1Hk+x+IFb3zMQG6MIc8QjHgEabdtPzyE5Jf+jKzyowqDtl8OAgZors4XTiPvifuwuuksLSXhoJ7mpcvBHeURWahP24SOvF1TmWLMm9hUTQSMuT+uxRIwfEIiEZCcRS6KGxC9mZwYoeZOjCnnxR40nz1CmRjJedFl7hYSAWPuz3SxW4qAsdjEF3A8SwGjlwQMexYwXGjz8Ia93rqAMT7Uj4HOVoywgEG/MEXAWMCHJF0LASEgBISAEBACVyZA34yy88L38CvkvPg1IgYbrbbnZSMTOnsUR61HwSe+g9FV6YvivOAMG3bkulAluL8SMT2X4O6kB7sqWHQYHh7W3BUuLi6aOKHamu65jWk7VVdtuM6ODeXMMBU47EdHEHVgL+LrL1Dz9wSCpRYw1NydKCtJ+tO/QHL1CUp1OwE9xcMwLZzydZh8K1VBKSgg18xg7k7T0zb3WQSMuT+y+RYw6o7vRcGff4wWWgoyQf9fTJIDyLRE5mxB9t/8G4JWrzE9jKr9L6DwsV+gvbrM7Lg9iZD2Ti6I3XATcr/0fYonaD2GoLpIHBiKxOLtRcBYPNYLPpKlgDE82AsPv2B4Ra5CYGI2BZGxHgOjq6oU9Sf3asFnDGRT9KX2ERtvRiit+QpZvQ6utPxEihAQAkJACAgBISAEFoOAvuYC/E/tRVTpEaxqKIa3YcjqsI3kvChL2qw5L3op5sXkIjkvvCZ6ETjRBfvLIoavcRCBk/2aqMFig8p24kBBAHmbrXBsC9648HUsUKg6H1OODSVccEwM3jRHxuAAQg4eQFhlMZxJIHC8LBAsFwHDrqsFHueOIazoMJIuHELocBvf0nThrCR855V+CTj5iX/BwJobp8/Z4gcRMOb+1OZbwOhrrEbT2UNoPL1fi4mhYmGoGfmERsI/MQsu/O+DSelrqkL7pWJyXphnzOHYF2F5N2hBQEMpEOhssS9UVyJgKBKLtxcBY/FYL/hIlgKGYWwYPqQahmZtRtyOu2Zdw1V3/C0UPv5zsk8VYXRgAD7hsSJgLPjTkgGEgBAQAkJACAgBMwKXnReeFLAz55XfIbKrCi70Ys5LEEyLgb65H9fZ4Vz0BhR++rua88L0/Lx/5hgXtGnLIOg/EWNNSBltgBO9grNYoWJZsFuCN0eK6cAxL5SQcbX5WDotLNsrZwaLHLxxTIyx/n54HT0Gv4tF8JwYgMfkVGwNTcDY8SXUf/wbWlv2Zujs9bTNLqRYjjefdecLZ5Dz5E+RQllJHIyGGc+ymmJhHL/zH9C95iYYnV2AJZrntd6zCBhzJzjfAoYaue7YW5QN5D60V5Roh1gAnBgZmuHIUO3Vnh0XDvSzp7KVRK3didwvkvOCYunMpYiAMRdK89tGBIz55bmkvX1gAYP+h+d/eFnAGCNF3ydCBIwlfZAyuBAQAkJACAiB65CAvr4CvgXvIrzkCGJr8uE73qu98FomQWx0CUSZFvNiM3ozN8PoOxUXYqGQcYwLb4px4WynoxgXevgYhxAw0Q8nPce8oEwil50WyjGh6nOdj3JiWDow1PXqOC9NUW0NJJToqqvhUFONkPISBPVNpZYd1ulxJH0Xyrfs1sSUcRIwHKIy4BiWoLpb1L0dLWf2ouwOoUWHkHTpmPWsJMEpqKOsJM3rb8W4jWYlEQFj7j9WCyVgcFaSprOHMdTepE2mi5xcTWcOzkhvajnTAEpeEL5mB9z8Q7RT3pR+OSR7M1y8/S2bWq2LgGEVy4IeFAFjQfEubucfVMCoPUoOjEd+jlYSMMaHScCIFAFjcZ+cjCYEhIAQEAJCQAi4FRxE+ksPILqpmFwFQxQ3wWAVSlFwFk5+6t8wkpy7KDEvAvoqEddzEZ4kWHBMC/VNLTsvrhTjwurkr3CQnRbsrrBWeOkIn+M2XNiJwZ/He3oQuXcPomqnvnUeIcnndFQqLqVmYIBctUMGOzisuQ0uGUsTY8JI8zbSN+GuxUew/qmfIrW91NrtocJnFY7e/W/ot9GlJMVP/xbFT/wPhik7jGlJvuXTyPvSf8CJYtFJmSLQdqEABf93L+opeKZp8SfxKuuef6GAm7eZHv62wgAKAABAAElEQVTAn2sO78HZR35G2UUuXrGPyNxtWrYS/4SMK7ab7eT5lx9G0eO/xEC3+VKUhBs+Qk6O/4BbwJQwMtv1hX/9JYqf/A1GyS1iWlI/9Hms+dJ/ae4Q0+PymWI0fzPfYO7LEyo2S0AEDJt9dDJxISAEhIAQEALXPQHX/APIePH3JGCco4wjI7MLGCHZOPWZ72kCBq3TmPNSjQ8K2KYEDApqWhCdhvK0LPT29mJwghwYSylgkNBCagsJGEex7okfI7VtSmixfBa2LmDwS3lL0REYRqcEJnV//sk52lJue72TOnTd74coPgpn/uhrqDRj4UJx+0IyN8ErPM7s+Aet9FL/zYVHMNzZcsUuvCLiEUzL7T9ozL9OiqPRTD/f4xbilV98BoLJIeZICRWuVFpLT9HPzjFMjk8tA1NtA1Lz6GdnC2UYXprlX2oey3EvAsZyfCofcE4iYHxAcHKZEBACQkAICAEhsOQE3E6/g/QXfouYllJ4GEetZK7g2BcOOBezDkV3/wtG6QVhIYtOi31hQOBADWLIheGp15k5MFSsC+XIuJa5KEcFx9CwVnh5Cp9TDgzlyGAHRsRbr047MEa1rCypqEzJQPfgIHonyYGReSMcUzZD5+ikxcOw1v9CH3OhgJ7rHv8RUlqLp2KJWAzIsTCO3fFN9OTthNHFHXDQW7SQqhAQAkJgioAIGCvoJ0EEjBX0MOVWhIAQEAJCQAhcZwQ8TryJNc/cj5iuci2rhp1F8E6OfXExcQua6FvNvvSNixL7wmeoFcG6IYQ6jMHZgcQAinfBwgVvLDqwsMBxMHj7oEIGx7bgTcXQsHzsKgaG2pvGwjBSMM/g116kbCSF2mUTFOC03i0AjYHhaPbzR4d/IEbcAzDuEwaHaI6FkWjZ/aLUnUtOIPuJnyCl6SwJU8YZwTx77N1QF5SE6tSNaNp4uxYLg4OmaoFTF2WGMogQEAK2QkAEDFt5UnOYZ19THepPvaOlEGorOQ7D2Ah8opMRkrUJsTfcCb9Zouly/uTCx36J9nKKgUH5k73DohCx/kYKYLMFwalrP7Clag5TliZCQAgIASEgBITA9U5gfAx2QwPwO/461u+h7CMDDWZEeK0zZ9M4F5KFU5/8LoZT8hY09oWOsmXYTZLzYrBWc174uznBx8dnOiUqZxjh2BcsJLAjgsWMa4mFwX0oZwWLIqrwZ95UdhM+ziIHx8KYjpVBwdcDXnoOweX52mWcK2WAAnn26D1RuSoZjYmJ2lKSAcpZap+5C47J5MSg5Qy6RXY4OJYVIPmF/0VC5Ul4UXwT51nim5T5JuDwJ/8Vg2t2aeKFZQBXxUb2QkAIXL8ERMBYQc9+oK2J1mAdQ1vpSbRfzCcBYxRekYkISlmDiHU74R2xyurdNhUdxfmX/4SuilJN9HAPiqDcx1sQuHoN/FdlwtnT1+p1clAICAEhIASEgBAQAtdKwLGyBGFHXkHUhWOIbL8ET4N5HIEx+r6e04NeiFuHko9+a2rpyALGvuCsI75DLQjSDSOMnBeuTnotLSoLFcptwU4MFhM4K4i9vf20uPFBWHAf3JdyYqg+2HHBG6dj5U05MNRec2309ZoJGCx/jJMLo9/OGRfi0ym7R7omgIwYKAgoOTPGvMOgj8lc9Kwkdt2tcKOUqiGUjSS1+G2EDbep2zTbXyIB4ygJGP2XBQxxYJjhkYoQEAJEQASMFfRjMNzTjs6KYnTSHwLd1RcoGMwYPEJitTzGQalr4B4UZvVu2yuKUHX4JfTWVWCSRA83/2AEJWXDJyYVXhGJcHTztHqdHBQCQkAICAEhIASEwAcmwM4LCnznS0tH1u/5PaIG6md0xS/kfXYuaHcLREXqFtTccg8mohZmGYQdxbywJ2dAwMCU88LP1dGq84KXivDGcSjYIcHiwvupz7jJywfYhcFuC9Uv11mk4GwnvLHrQsXIUE6Msa5O+L347LQDQ/U9QrFCCuMyUZOVqx1ikaSH4mUMUFBPO8pI4pi8AXaOrrDTO6pLFmXvQl+yZT3zSyTW5cPJOA4HTJqNW+UVg5O3fw09tEyIi5HcIpOu9HcoLdGRIgSEgBBgAiJgrKCfg4kRsiD2dGCEhAzeG8n+6Ehpm1y8A+BKkX0dXSkokpXCbbvrL2G0r0tLdcXtXP2C4Ez5j53cfSCRk61Ak0NCQAgIASEgBITANRFwrCLnxeEp50VEezm8DOZpBA20iID8B6jzisLFzJvQnrYeI/GZMHotjDPUc6IPweMdCDAOwN84CFe9/QznBbsteAkJbywusKjAzgxXV9fppR0qvSqfYxFCLTnhz9NLP6yQs3RisEjBG4/JG5/nTTkweD9JDgzPZ/4K/wunzHocJQGjJDEH9XnrtfY8V06rOjg6jmE3P0xQTAzXhDVwoZSri1nsOprgce44wsiJkXzhEEJGO8yG51gY9YEJ6PcK1I53hiegafOHMb5AopXZ4FIRAkLAJgiIgGETj0kmKQSEgBAQAkJACAiBFUKAnRcDvfA9tXfKedFfZ/XGhun7+R69K8pWrcfFD30ZY7SsVUdLN9jxMJ/FjmJe6CbHET7ejpSJRniZZBthZ4VljAs3Nze4u7trYsTQEMVzIDGD6/x5kDJ/cDwMa3UWEPop6OZsRcW8YJGDBQflxFDt1XnlzODzpErA/clH4FN6QjXT9ixglFEcs4a1G7V5KvGE58hOjCFi65q3G+5p22Dn4kpfVjmbXb9QFSO7VkiUcTt7EOuf+TmSOi6QRMURTqyXSu84nPjQ19CbvR2T7AheZMeI9VnJUSEgBJaSgAgYS0lfxhYCQkAICAEhIASEwHVGwLGqlJwXLyP6/FGEd1TMcF4oHA2uwTiXtgPN6ZsxlJQDow99K385HoRqMx971xFahjHUTJlGRhFJ7/EuejvN8WAa84IdEEo44TgYfE45I9glwWICix18bLY6ixK8zVbYUcHnuR/eLNur89w/f+bNQGKE9/NPIuDiabNuZxMwWMhgIWVojJwcnoEw+kfBK20jPGIXNiWtmhyLMLTuBi4Usy3r2V8ioakQLsYJklPeC16q2vK+x96dHBnxqEndhKYtd2A8MsH0tHwWAkLgOiQgAsZ1+NDlloWAEBACQkAICAEhsOgEKDua5rw4/TbW7XkA0f21VqcwTpEohu0ccTEqD4Uf+QZGSLxYKOeFHTkvAgYbENNfBX9X/XTMC1PnhRIUlCNCxbxgJ4aHh4fmvGBRgJ0Xqs5OC+XUYNcD11l40F7gL98196MEDz7O4gLHuOB+eBmKqvO4vClnBrflvrTsJCoGxqUzZiw1AWP1OjRedmDwtbyZOjF6e3sxQpx9NtwB75ydsHdxg72ji1k/C1VxLCvEqlcfQlz5CQSMdcPVOLuww3Mo94nHyTu+gd6srTC6egCLnEVloThIv0JACLx/AiJgvH9mcoUQEAJCQAgIASEgBITA+ySgOS8OvUTOi2MI75zdedHoEoQL6Ts150V/YjYMvkEL6rwI0g2R+2ICbk4O2nIQS+fFbDEvVEYSFZeCM5PwtcodYVln8YA3Vbhfjp3BAgeLCyxKcF98nRI2uK6cGKbOCzXmZH8fPJ9+fEYMjDE7PcopZkjzhi3a9Sx2KAGDl7nwmJqYYTBi3CMAdoEx8MnYAk9aprMYxa67HS4UeD6YlpKkn32dspK0XnHYNkcfVMTkoi59C7pzb4AhKPKK7eWkEBACK5eACBgr99nKnQkBISAEhIAQEAJCYOkJjJPzor8XPmfe0ZwXMX01VufEzoshcgSUReeh+GP/iJHkHKvt5uug30AdVvVeRICzvea8YEGChQMWIdgFoZaMsJOCNxYB+MVfOS3UeXYydHd3ay4JnhuLEryp7CHsmGAHhRIO1Py5DffL7gx2cFgWdV1HRwe6urqml6aweMH9acIKfXZ74i/wLjludjkLGBXpG9Cycat2HYsgyoXB4/F9sNDC99TZ2YkRe2cE33wPAtbfbtbPQldcKKBn5nP/g/j6s3CdHJ2RlUSNz1lVehzcUUGpdC/c+RWMkrAlRQgIgeuTgAgY1+dzl7sWAkJACAgBISAEhMCiEHCoLUPQsVcRVXIU0S0X4WuY+bLOE2l0DsT51O1oytyKwZQ8TFIGtYUsgcONSB6qhJedQRMDlKCg4lsogYKFDXZFKIFDOS/U+ZdeegkvvviiJhDwfFevXo3c3FzExcVh1apVmgOC3Q4sjnA/qrAQwa4KFhdYTLAsfJyvO3ToEE6ePDkVu4KEBz8/PwQGBiIpKQnxISFWBQxeQnIxZQ0a123SumUxRAkYyn2hXBksnow5usH3hrvhk3uz5TQWtK5lJaHUqqFnDyHlwgHKStJpdTwD7DCqs0dZeDYKPvFPGKblMVKEgBC4PgmIgHF9Pne5ayEgBISAEBACQkAILCwBzjYy2AePwoPIfP1BxLSVwRmTMwI2as4LeyeUReai6I6vYphevHUcNJMCWS5E0VGaefvJMYRS1pHk8Sa46ShZK8Wj4PgVvr6+00s4VKwJPsfiAwscnF2EY2KYlh/+8Ie4//77wU4MLlu3bsVtt92GdevWYf16Sv1Ky0PY9aAEEnWtcmSwiMFjWRYWL/i6xx57DHv27NGyh3AGkZiYGG3bsmULNmZkwOXJR+F17hi94tNKm8vBMFnAKEnKReOaDdrc+R6UYMEChhIzlKtkggQM17xb4L56E+xdPRctFobKSuKevx8bnvkFErrKrpiVpMJnFU7d9hX0ZG7GpKcPZSVZnOwpls9G6kJACCwdAREwlo69jCwEhIAQEAJCQAgIgRVLwKGpGr4UsDO85AhiagvhT8Ea7elu7SwyTjS5BKI0fReaKQbDQELWgsW8UKA564j/UCMCjEMIth+Dq+OUu4LFCRYxeIkIiw3sgOCXfXZkcN3SeaH6u5qAwcs92GGhHBfqOuXIUIKCOq72LGywiPHwww+DXR4sXlgKGBsyM2H/3DNwKTisZfNwgkG7nAWMSxwDYz296NP4vKkYGmrJiqr39fVheHyCsryEwD44Dl4UZ8IjNl1NY0H3mnBD4o1r4WHKSvIrxLeco/sYnyFyqUn02Luh0T8O1amb0bj9LoxHxKtTshcCQuA6ISACxnXyoOU2hYAQEAJCQAgIASGwmAScz59CyksPIK76NLwnBuFE6TJNCzsvBsl5cSkqF8V3fRMjqWtNTy/YZ459Edt9AT4Ok1oMCo5VoQJqsnjBcSlYyGCXArsk+Bwf4+Uf7L7gl24WBNjVwHVLAYOdEbfeeqvmvtiwYYPWjq9Rjgt1Y8qRoQQFdVzt+RoWUR588EG88MILMwSMTZs2YUNWNibfegv6ghPwGOqCp2EqSOhUDIyNaN20VRNOWDzh/njPggUvG1HCCjtHuM5iyaSbNwJ3fRZ+a25R01iUvdPFAsS99mfEVpxEIAlMV8tKcsk3Aac+9FX0pW/CpAc7MRwXZZ4yiBAQAktPQASMpX8GMgMhIASEgBAQAkJACKw4Aq5FR5D9zP1IbCqCo5GWbdDyEdPS5ByAktU70EIxLwYoYOekX4jp6QX7rAQMfycdvLy8NIcFuyFYvOCNHRfstlAOBRYueFPn1VIMFezz3nvvNVtCsnHjRtx0003gPS8nYfcFiwPKcaFuTDkyrhQDg4WFRx55BK+88soMAYOXqORmZWGytg72NdXwKzwJ/86p1LTswFBZSPg+lFjBc+HxWJxh5wdvXOd7YiFl3NljSQQMzkriXFWCkIJ3kV6wh7KStClMVvftei9Uh6WhdvUWdGzaDUNwlNV2clAICIGVR8BmBYyRXoqY3NMxvWbQ2dsfLrRJEQJCQAgIASEgBISAEFh8ArqxYdj1dMJueFAb3PvCKeS9/ifE9lWbTYajPZB/AWX+STjxsX/GQPa2BY15oQbXGcZhZxjVlo/EDNQiwNVhOvsIOylYkGC3BQsNvLFjgV/+LWNg8Ms/Cwvclp0algIGuy527tyJzZs3Y/v27ZpYwO2VAKJiaChHhhpHzVPtWfTg6x599FGrMTDWrFmD7OxsbZ46Sqfq//JzCK44q10+xgJGXAaasigYqoszDORQYJGCxQrlxODlKNw/11nYYGfGKDlivCkTiVfmdji4esHBxU1NZ1H2rsVHkf7i7xDZWKKNpzeMwd0wApKTzMan6CoY0DnhYuxalFLGmtGkLLPzUhECQmDlErBZAaN87zOo2v8cJskKxyV2+x1I2P1Z7ZfMyn1ccmdCQAgIASEgBISAEFieBPSUbSTiwDMIqjmvTdBtoAsh3TXwMgyZTXiCxIthnR6XQjJw9mPfxnDGRoo+qVvwv+Gc+lvg2VONQAwj2GkS3q7OWlBOFiJ4GYkSLrh+pRgY7JzgjduzU8NSwOClHbt37wYLGfyZ3Q/cnkUC5YZgICqmBrsgWKywLNyWj88WA4P7X7t2rSZKGLq7zASMCXrBb3MPQFtQFHpjYzEYGWk2DxYzlIDC8+K6No+JSRi9AqGnWBg+WTfAi2KSLGax62yGW3kRnDtaSOWagB/9LK0ueQfBY11m0yBZCXyPpeE5KPjUdzFCgV+lCAEhcH0QsFkB4+QD38eZR++HYWxUe1I5n/021n/zZzMiQ18fj1HuUggIASEgBISAEBACS0RghJZH9LbD5+xh5L35J8T1VFqdCDsvjPTiyd+ct7v4ozphHap234OxhEyr7ef7oEdXBUJbixBA4RK8vb21uBbKFcGiBTsjeONjHNCT3QosILC4wcdUDAzLec0lBoYSI0yFCjU2CwnKCcFiB8+BXR/sjGC3x0MPPTRrDAzOcsLzHO/qhN+LJB6VF2jTY4fLIIlEvY4eaEzLRRcF+1RODxYreOO+eT68500JKbyfdPNByC33IHDdbZa3uyh1lZ3ErfAQMvdQBpvGc/CYGJrhxCj3iceZ3V9CT/pGGHwCAEfJSrIoD0gGEQJLSEAEjCWEL0MLASEgBISAEBACQsDWCehrLyFi/9OILT1MjovaGY4LdX/8rTklLEWDRzgu5N6KNso6MhKdDKO3n2qyoHvv3hpEdZbAl4J3skDBLgsWEdgJwW4KXkKi4l+wWKEcEiw+sINCCRksLpgWSwGDY1/ceOONmvvCMgYG96kKOx94U+4MFiJ443G4HX9mYeNKMTDy8vKmnCNDg/B8+nH4XTytdc9iETsUBu2dUZm2Fq05edpYygXCY5oKGCxkcAwMFlNY3NB5+i+tgEHiDSkusO8gR0ZlMcLPHsDqor0IGu1U+LQ9ZyVp9olGVdoW1O/4OCbCV5mdl4oQEAIrj4AIGCvvmcodCQEhIASEgBAQAkJg4QmQ88Khpw3ehUeQ++ZDWNVTccUxh8gR0OHkhaq4PJTf9iWMJudesf18n/TVso9cJAHDoLkvTJeOsOtBBeVk8YJFCuWQ4Bd6fsHn9ix8cFvTYilg8NKOXbt2aQKGtRgY6lrukzd2XbBIolwRSkjhOgsKV4qBkZOTowkejqMjcH/qMfieP6m61/ajdnpcpOwujWs2aP2zKKKcGKYCBrsuVHYSFjnsvAKWVMAwuwmqeB57HZue/glie6stT2n1S36JOE1OjD5KHWvwFieGVUhyUAisEAIiYKyQBym3IQSEgBAQAkJACAiBxSSgrytHxDtPXdV5oebU6ByIwqyb0ZK1DSOr0mH0DVKnFmWvBIwAJ2jZR9jpoIQL/qyWiKgYGCp2BbdhR4RyZVzNgaFiYLATgzfVj3JcqJtVDg+1dINFDOX24GvYFcL12WJgcN8cA4PbTFIqVN8XnkJA2RnVvbY3FTCUUKKcHqYCBgspPA8WTPi40d3XpgSMTgcP1AWuQg05MVq33oWJ0BgzDlIRAkJg5RAQAWPlPEu5EyEgBISAEBACQkAILDyBy84LL8oYkfvGQ4jvLr/imGOUQLVP747KyEycu/VLGKJvyXX0cq6zcDJcsZNrOKmboJwV40PwG2pC1CBlH3Fx0GJgsNNBCRjstuDPvPFndlooBwS34zoLDnxeCQEsMHDd0oGxZcsW3HrrrZq4wAIDt+OiYl2oW+E+3dzcpo+zM4L7ZgGBNxWX40oxMDiVKs9zvLPDLIinGkMJGE1rp4QU7p/b86aEE8u65gpx8YAfZSPxzd4BvYcvZSRxV10uyd618DDSXnoAMQ1F8JwYhKNFVhIODDtCS2bOR69F0d3fxWhi9pLMUwYVAkJg4QmIgLHwjGUEISAEhIAQEAJCQAisGAIODZUI2/8MYksOIbSzGj6GqbSps91gs3MAirN2o5nSpQ7FpMLAzotFyDqi5qPvbYR7ewV8J/sR4DABr8vZR9h1oZwX1mJg8Ms+uyb4HLsvVMwMfuHnl3y+lsUOyywkKgZGbm4ueFMChop1MT2vy64OdZydEdw3j8vHeEx2V8wWA4PFEe6f26Cv94oCBi8hUQ4Qbs+feckIiyqqrmJg8NhGewc4+IXDJSIevjm7KBvJ0goCnJ3EufIcQs/sR2bhGzOyknDQUk60WhKWizOf/R6lVc1RmGUvBITACiOwbAWMoa5WDLU3a/+IW2N+/vk/oHTPYzCMj2mnU279NNI/+U36hWi+LpFPstXPNSAYbn4h1rqSY0JACAgBISAEhIAQEAJXIzBG2Ua62uBZchxZb/0Z8e0XyFsB+t6bQ0bOLO85L7JQcsdXMJy2YWajRTji0l6GgLrT8LYbh4eHh+Z6YOeDaQwMFhnYYcGChOUSET7HTgtuz+ctBYz77rsPv/rVr9BLyzi4ZFLGD3ZGJCcnIyUlRRMiuE92WMxWTJ0Rpm1YTHj11Vdx8OBBTXBgkSEmJkbb1qxZA46Bwdfq+vuuKmDwvFkkUYIFCxjswlBCjRI0uM4bt3PwDkTI7i8gYO0tptNa9M+TNCLTcy04iIxX/oDYhmKrToxy3wTk3/S36Fm9DhP8d7+zy6LPVQYUAkJgYQksWwGjYt+zqNj7FCbGRqwS6GuoQmddBTjNEhe/qFXwCo/TFH3LC3QkasTf+Akk7P6M5SmpCwEhIASEgBAQAkJACMyBgH1jFULefQ7R5w4hrKMKfhMD9L03J0a1Xpqd/FGccROasrdjOD4dk0v0RZJbRzmCmvLhhTHNReHp6XnFGBiWAoaKicEv9CwAsDOCl5WwuMD1+++/H7///e+nBYzQ0FDw5uvrC39/f82BwSKIEgWs04ImGPAYpoWvqaysRH19vbashEUIJWCoGBh8zNDddVUBg/viOXNmExYulCNDOT/4GG9K6GDBRe8TtCwEDBYveLNvb4JreRHC899Bxrm9CBzrNsWFHntXtHpHUuaVrajbeTdlJaF3AylCQAisKALLVsA48/C9yH/slxjtn1Kzr4W6nYMeeZ/7DtZ+7UfX0o1cKwSEgBAQAkJACAiB648AfZlk39MB9wunsPrN/0N8cwmcMQkHC+cFOy56KdbFmJ6/9aZ0qSEJKNn9txhM37SoMS8sH5BbZzmCmwrMBAxvb29NhGBnBYsRLFIop4WKRcFCBp9XMTE4LgW//HNbjl/BL/68lITdFw888MC0gMHuDr5GBenkfnljcYJFhPdTeC4sKrDzQokbSsDgYKHTMTC6OuH38nMILi/QuleiEsfAuJCyRstCwidYwGCnCN8Hz4U3Fix44+UkfD+8cZ2Lo2/wshAwtMnQf/iLSyNx9D7yKja98EtE99dZFdE4K0nBjV9AD2VgmfAPhZGcGIqJ6kv2QkAI2CYBETBs87nJrIWAEBACQkAICAEhsCgE7Nsa4H/sNUScO4zIhhIEjnbRspGZS0daHX1RlHEz2ldlkmBhhzFvfwxFpyx6zAtLKK68hKThDLx145rwwOIDb7PFwOCXfH6BZwGCxQ3eq3gUfI7FCK6r8z/5yU/w61//elrAUOeUcMFCCG8sRvD2fotySPC13KcSMExjYBhpCYnvqy8gsCzfbFnPiM4B55PzUJ+7ThNAeP4sVLAowv2yC4NFDNOYGHyc23FZdgIG86PN8/jrWPP8/YjpqqKAnsYZy5jYidHmHYFLq7eh9sZPYSIsVvuZFRHj/f70SXshsPwILFsB4/yLfwJvE5TX2loZ6etCX3uL9o8Yn/fwD4KLtx99mvlPE/8STf7w3yH9E1+31pUcEwJCQAgIASEgBISAEJiFgL6qFEnP/y+SLh2BF2WAcDZOvdyq5ryIhLNAVPvE4vid30Lv+t1L6rhQ81J7LQZGPcXAIAGDY2CoYJzspGARg0UBUycGv7zzCz0LFNzGckmJas/n2GlhGQNDOTBY/OD+VWEBwtSBwWMqYcP0uGrPe76GHRHWHBgcwDM7eyq4pm5wAF6vvQK/C2e0DB36y+4YFjCKE3NQn71GEzD4vlSWExZpWKxQS2PU0hJ1jMdfbgIGz4mLM2XAiX/zL4iuPovAkU64WPxMTrUCCsLycPKeH2AsIVMEDAVF9kLAxgksWwGjp7YMXZWlmDSY/5JUvCvffg7lB1/BJP1DzGXV1tsQf/OnZvyS0U6S6u0blwrfmBStKv8RAkJACAgBISAEhIAQmBsBp0uFyHzip0irPQG9cZK+4TdfBjFOr4YD9s6oDklF0Ye/ioGcG+j7pCnXwdxGWNhWljEwlIDBAgWLCLxnoYGdE8o1YfrZcnYsXLBIwUIHv+xbxsCIiopCdHQ0wsLCEBERMf23KYsH3F4V5e6wPK7O856FjaKiIly6dEkTHtg5oRwYZgLG0CDc3nwDXqVn4DY5Rkt8pv5+ZgGjkBwxtVl5mhjCYoUSMJTzQjky1P2wYKLmuVwFDF1XK5zryhB2eh+yTr+CoLEuU2zTn8+SgHGaBIyRywLG9An5IASEgM0SWLYCxtWInnzg+zjz6P0wjE2t0cv57Lex/ps/034RXe1aOS8EhIAQEAJCQAgIASFwFQIjQ9C3NcK79Diy9z2C+K5LZhdMBVbUoc/OGc0UOLE2YQ3qt38Mo4lZZu2WumIZA4PFChYhlFihBAzlrLhaTAy+lkUQdjCwoMDLRx588MHpJSTp6eladpDVq1eDP7PLQokFLFaowuPyxv3wphwaypnB17CQ8MYbb+Dw4cOaC8M0CwlnIOGMJ1oZGYbjsRNwO18E74F2eE8MaYeHWcCIy0RN5lS2EiVgcL88HosWHBOD74PH4/pyjoGh2Km9W/67WP3qg4hqOgfvsX7NfaLO8Z5jYZzd9Xn0pKzFeGC4ZCUxhSOfhYCNEhABw0YfnExbCAgBISAEhIAQEAILScChsRJRbz2O+HPvIqCvCV6GqZdiNSYl76S8Hjo0ugWjJO/DaCXnxXjEKhgp9sVyKloMjHqKgUFpVDn2BYsGLBKwkMFiBbstrDkx+Bxv/FLPAgNfx3V+8WchgK9h0eM3v/mNmYDByzo4Qwjv2SWhrudxeFOFj/PGfbF4wGOwwMHj8Ny4zu6IV155Bfv379fSqLLYoBwYLI6wSKIV6kPX1gbHpiaEFp9CSFe9dlgJGNUZ2ZoYwsIFj8Ebj8t7HoNdGeoc73leXJarA0ObHP3HjrKSOFeVIPz028gqegMB4z3qlLbvpVgY7Z6huJS+HXU3fxYTITFm56UiBISA7REQAcP2npnMWAgIASEgBISAEBACC0eAnRetDfA+fwJZ+x5FQleZ1bGGdXq0uvihJjoT5Ts/g+G0Dcsq9oWatIqBodKoKtHCmoDB7goV84LbcZ1f5vlFn8UHPqYEAFX/7W9/i4ceemjagbFmzRps27ZNc2GwgKGEAtVezYsFChYOWLxgUUQJCCxg8KYEjOeffx779u3T+jcVMNLS0pCamjp9PfdrT9lFIve9hsj689owozp7nA9NQG18CvrtHTBoR6ITCRRKpOB7U44LJWLwMZ4Tl+UuYKisJD6HXsLGF3+NyIE6WuI0M7VvmV8SCnfdQ06MNeTEiBAnhvZ05T9CwDYJiIBhm89NZi0EhIAQEAJCQAgIgQUh4NBYRc6LxxBfTM6L/pnOCzVoi6MPCnJuR2PeToxFJmHSN2hZxb5Q81QxMNxoWQWLCLz8g4N5skjALgoWJVjMUEtIlICh6nwNbyxE8Ms9H+dr+TNvf/jDH/CXv/xFc0jwmJwdZMeOHWCBgTclSKj2al7cH29KUOB+eRzVTrkynn32Wezdu1fr33QJCTs8MjIypp0b3K+lgMHBVVucfNDiG4LmsAh0BAdpKVTZdaGcJDYtYJDwQzcCz2OvI/fF/0VkVwVcKaCnZZwWMyfGLfdgIjhKPQbZCwEhYGMERMCwsQcm0xUCQkAICAEhIASEwIIQ0JwX9eS8OImstx9DQudFq8OM0etht6MXasJWo/SWz2Mga9uydF6oybu2X0IgpVF1pQwqLFiweOHt7a0JC0qMMBUw+MWexQMWMrg9n+ONnRfsUlCCBwsNfOyPf/yjmYCxbt063HDDDUhJSdE2zlTCTg7luFDzupLzgkUNJW6wA+Ptt9+2KmBwDAyeE8+D520/0I+Ifa8juuGCNgwv8xmkOBi9eg+UxaWgftUqrR/OOKLuk69Xc+PPPC/euCx3B4Y2SfqP87njiH7naURV5iNsoIlEjPeCpao2vL/gn4qCm7+A/iRa2hMYSjfobHpaPgsBIWADBETAsIGHJFMUAkJACAgBISAEhMBCE3BoqkUUpaZMKD4A//7mGTEv1Pht5Lw4m7kbDXm7MBJNzgu/kGXpvFDzdWwugVflMbhPjmjuC3ZgcBYRFhVYmGDXAwsV7JTgOgsTLCCwuGF6nl/4+Rwf5/Z8HV/zu9/9zuoSEo5PwRu34WuU40LNi8dg4YD7sXResICglpDs2bMH7777riY8mC4h4RgYagkJt+e5oY9iZBzYi9jGKfGJA61StA4KtOqIgshkVCYkaTEv2HWhrlEOEA7kyWPyffLGxVYEDLueDujJORR28k3knHh+1qwk7Xov1AfEozpjO9pu+CgMASRiSBECQsCmCNisgFH6/B9x8dU/U5pVgwY8YfdnkP7Jb5BabmdTD0AmKwSEgBAQAkJACAiBpSSgGxuGfUs9vC6cppgXjyCpY+rbe8s5jdE3+V2O3qgNT0XpjfdgIHt5Oy/U/B0az8Gz4ogmYHAQT3ZE8MbiBIsYLC7wpgQMFhrY0aDqakmJ6k/t+XreLAWMvLw8bN26VRMvWGRgsYMLCwa8qcJjsOighBAWD5TzgtvxeRYaWMA4dOjQdAwMTtHKGy9PYZeHEhyUgBH77l7ENZlnjBkg18zxsERcjE/WRAoei9vzpoQUtZREzY/3tiJgqDm75e9H6ht/phggJfAd7Z6RlWSUOAzpHHEuaTsu3P0djEcmqEtlLwSEgI0QsFkBo6euHD21ZeDgPVy8oxLgE51In3RaXf4jBISAEBACQkAICAEhcHUC9m31CNn7BOKK9iO0uw5+EwNWL9KcF7kfRuOaGzESvgqGZRrzwnLyTq0X4Ft9QltCwoIFbyxgsHDA4oJaEqLqSszgc/x5tsLnuI3lEpLZBAzldFDuBiVcKCFDCRwsLvDGAgq35SUk77zzzvQSkoiICERFRWnxL9jhwSIEiy58vf3gAGYTMI4Ex+PCqkRNGFFj8p6dF7x0hOeniSAmN2xrAoZdVwuc6R0hnJwYWfmvIGDMPCsJL6nhuCBnYzej6HP/jjFyEEkRAkLAtgjYrIBhW5hltkJACAgBISAEhIAQWGYExkbg0NYIj/KzSN73OBKaCuGMSThQFgfTwjEvOp28UReehvO7/waDlC7Vlopj83l4Vx8DB/FUy0bYOcECAQsQvLfmxLB0XnBb3lgoYGFBxcjgDCSPPvqoJjAwF848snnzZs2BwS4JvoaLclhoFfoPCxi8KSeEEiFMBQxu++KLL2oCBset4CCeLGDwxgE8eQkJX8d9aAIGtYk7/Dbim8vVMNq+n57h4aA4nI9NmBY8lKCi0qiqPkwvtDUBQ83d++AL2PD8rxDRV08/zyxbmP9Ml/kno/CGz6AnOQ/jIdGSlUSBk70QsAECImDYwEOSKQoBISAEhIAQEAJCYL4J2HU2I/jAC4gpOoCQ1nL4j3VTtAQjbeaF4wYUZN2OhrU3YlTFvDBvsrxrNWfhdH4/PIwj8PT01MQKdl0o54VyUqi6cmJY3pRaUsIv+iwycDsWQh5++GEzAYMDa3IgT3ZHmAoYymGh+lUuCO6H++Y+TYNo8jhcf/PNN3H06FEte4ipgMH98xIS1S8LErTOBEknDiGprVINo+1ZwHjXPxqlMfGa+MKihxIueB7KfcF9mRZbFTA8jr+BzJcfQER7GTwNJNSRiGFaOCtJp1sQyjJ3oObWL8AQRKlVpQgBIWATBETAsInHJJMUAkJACAgBISAEhMD8EnBoqETy079EOr3cu1KASydKP2laxnTsvPAl5wXFvNj1WYp5sX1ZZxsxnbvp50kOaFlxCp4TvfDT6+DsOCUYKMFCCRiWTgzTPvizEjDUC78SMP785z/j8ccfn3ZgsDOCl5GwO4IFjNmWoSjBgufBfSnnhRqX67y8Y9++fTh+/Pi0gBEeHg7eWMBITk6eFjB4Xnbk0Eg6fhBJ7VWqG23fT7LUAb9olESv0tpzW+6b416wUKLcF0rA0DnoofcOgAstFQpYdxu8U9eb9bfcK87nTyHs0EuIvnQakd3VcDOOWp3y+YDVKLzx8+hLzKbUqiRiSFYSq5zkoBBYTgREwFhOT0PmIgSEgBAQAkJACAiBRSKgr72EjMd+hJyKg/T9/KTmvjAduoOcF/l5d6Jh3S0YDYuFwSdwWWcbMZ276WfjcD+MAx1w666Ff3eVlmKTxQhT4YKdFKquhA219EP1ZbqEhF/0VZ2Xjzz55JOawMBt5ypgqCUjLCbwppwUajyus4jx+uuv48iRI9MCRkhICMLCwsABQtmBodwTmqBCjo1VR96xsoRkSsA4FxWn9cnXqKCdSrxQwUB5fAcPX/jk7oJPxmY4B4TD0dNPTcsm9jrKSuLQVI2w469jzcnnEGgRC0PdRLuDFxr8YlGVvQOtOz8+lVFHnZS9EBACy5KACBjL8rHIpISAEBACQkAICAEhsEAERobh2FgJ74tnkH7giRlZRyi6AwU6tEODRziO3fkP6Np2l00KF5b0nFsomGfNCbhMDGrLR1iw4DgWylmhBAxLJ4YSGliw4DaWdXZfPP3009MCBgsLHAeDHRL8mYtyNljOievKeaEEDCWMKAHjjTfewOHDh7X+OY0qixcsYrC7gx0YLEawAMJzcybBI+H4u0hordLiPqjQ9n1GO+z3jUJRRPS0gMHLU0yXrJjOTe8bjJBb7kEguS9ssXCQfyMtk/GgrCQpbz+OiIbz8B/thKNxKnuhuqeprCR6FKXsxMW7/xkT4bHqlOyFgBBYpgREwFimD0amJQSEgBAQAkJACAiBhSBg39aAmFcfQlLh2/AdbIPn5LDZMCxe9Nm7oD5gFQp3fxG963fb5NIRs5uiip7SqXpQOlVXCubJjgt2WvBLv6lwYc2JoWJRcHveWCxg0UHVn3jiCTMBg4WL7OxsTbzgeBh8PbdX2Ucs56WEC27DYgT3y/PguuUSEiVgBAcHa+JFYmKiJo6wqMLtJ0ngyCwpQFp3A7lqjNrG4/WSgPGWZyjOhkRo8+A5qewjps4LNTebFzAoyCrdKOy72+BETozw468h5/RL8B/vVbeo7Tm8p4G2M6u2ofief8d4RLzZeakIASGw/AiIgLH8nonMSAgIASEgBISAEBAC809Ac15UkPMifxbnBTkF6GVuwM4JDb6xqF2Vg8YNt2GYMjXo2K1wOZvG/E9scXq0qyuEy8UDcKOgjq6urppIoMSLKzkxlGDBSzQ4WwmLBXyM67zt2bMHr732muZm4DtZtWqV5o6Ij49HUlKSJmCw0+FqRQkY3Ce7QLjOyzyOHTuGs2fPag4Mrvv5+SEwMBBxcXGIiYmZFjBYkGABI62sBGldDRTTxAD95eCVLGC84R6MguBwrb0SZVgwWYkChiVr99NvI+nNRxDZWAr/kS44kmxhWi5SVpLi7Z9CT1IOxsJWSVYSUzjyWQgsMwIiYCyzByLTEQJCQAgIASEgBITAQhBQzovkwn3wGWyf4bxg8WKUtlaXABSt/Qia192MCcrOYPSi+Ae01MIyrehCzHEh+zRW5cOhZB88MaZlI2GRQC0LUUIG15UDQh1TDgm1tIOvY5FBCRlFRUUoLCzUBAw+FhUVpYkL/v7+8PX11QQC7uNqRY3D4ggLDFznfVlZGSoqKjQxgwUMDw8Pbf6cSpWdGGoJCY+tIxFj1fkSxDXXkFAzDBfjuDasEjDyg8LM2vM1alzT+dm6A8P0XvgzZ9xxrKtABDkxcgtehd9En1mTPjsXdLsF4ELmLlTf/ncwBIabnZeKEBACy4eACBjL51nITISAEBACQkAICAEhMP8EyHnh1FCuOS/S3n1yRswLNeCIzgGNlFqyNioDVds+hqHMrSvCeaHub7KpDKg8CbfhHnjoxuHsYKcJESxIKLHC1IlhKdgoAUM5JFhcYLGhtbUVzc3N2mcWBHx8fBAQEKC5NdjpoQQCdf30fEig4HOqsBOCr1f9qnpbWxva29s1gYRdGUpg8fb21oQMFjB40/qi84E1tQhurkfYYCeCJoe07gfIgXHM2Q/FPkFomzSgl8bl/tXc1Dx0DrRMxjdIyz7iv3Y3vFPWqenZ9F7FxAjY+wS2vfI/CBntsHo/5wNSUXzDp9GblI1xClwLRxer7eSgEBACS0dABIylYy8jCwEhIASEgBAQAkJgwQnYtzci5pU/IeXsW/Ae6pzhvFAT6HDwxCnOOrLxNoyFRMNoo1lH1P1Y7o3DAzAOdsGOlhE41ZyFu/2k5mZgQYLFBRYxlJDBdUsBg4UOjk3BIoPpEhIWFXiJiOlSDL6W27KAoZaGKOFBzUsdV3UWLrhvvpbHV3XuW43Bn5UQosZTQofmxKC5OA0Owr2/D1mNVUgf69a6HydnTYO9O2qcvXGGUqSWX3aQmPbBDR08fOC75mb4kHjl7BcCPWUjWQmF75MeEFjA2PrS/QgZsS5gdBGjFq8QlGffiEYKYjpJDKQIASGwvAiIgLG8nofMRggIASEgBISAEBAC80OAnRf15Lwoy0fawaeQ1H7ear9jFOqxnZaN1IWnomzHJ9GXs2NFOS8sb9pQcRL2RXvhZhyFu7u7JjQoV4USMCzFC+5DCRhKWOC2fJ26RrkZWNxgIUKdVw4JFjB4U30rAUOJCHwN98HXcRuu81h8jD9rAgX1zYWvUf1anue2elpKsrHqAtZfflHn5UG8TKLFwQ37HJxRROKKmq/W4eX/6Em0Crnl8whcf7vp4RXzmWNhJOz7KyLrSxA43D4jFgYLPaMUxPZs8k6UfvZ7mAiNWTH3LjciBFYKAREwVsqTlPsQAkJACAgBISAEhIAJAfuOZsS8/MerOi/+P3vvAVjFdab9P7cXddE7EmB6L6JjY8A2NrjbuJckTre3fbv57+6XXr5kN7ubbDZZYycucYtjO44bBgw2vfcqEE0ghHq7vf7fd0YH3Xt1BQIkofIeZzRzzpxpvxkpzDPPed8Kcl7soJgX52bdCX/vAYhk9ugUMS9iUMQtKgHD4q/TBAh2SXBciUQnhhIa1MZcZ/cD92MhgsUFFiuUsKEEBSWG8DolQLBYoJwTan9KQIgVRLiPEjJ4PQsVsUIHt6nCy0rY4GOrOsfJAAXznHPmGGZR6lAumoBhsKOYssusMpGAQcFIlXCi9sfzzi5goKoM5gunMWDjB8jb9i66h+piL5/ytuistg+7Cfue/B5CPIxEihAQAu2KgAgY7ep2yMkIASEgBISAEBACQuAaCQR8sJwrQEY+pdP8/E2MLj2YdIe686I7CvuR82LRo3BNW5i0X2dr1GNh7IShqghGVwWcVovmxGAhggUEdkAo9wTXVeFlFjFYoOCJxQM1lIS3VYKGEjBUXQkFPE9WeD/cVwkkSrhQgoRyXihhQ+0jdn2igGGgISTzKGjlnKA+hIQFDA/MKDFY8bExBbtJtIktWuyLbr3h6DcM3fNu7TSxL2KvkZdZ/uG7kLp9NUaueg2DaDhRD295IycGZyU5MPcBykoyBf6BwyUrCcOTIgTaCQERMNrJjZDTEAJCQAgIASEgBIRASxAwVpWg78cvYfiuT9Gr9gKyw+6ku+Xx/tsn34Wzs5dS6sgcROkFtiuUqI9iYXiqgdP7YCzYDkvIqwkWKkaFEirUkA8lYihhQwkSiYJDU3UlUDTFVgkWLIYoRwcLIonOC1Xn/fA5KMcHb8+TWs/Hs/t9uKnoFOaG6Dq5P00sYpRFjHjPmIbtaZlau/phSs1Et+mLkT3xJtgo0KeFYmF0xqIkJEPFBViJz0ByYkzb9T79jrjiLrfOaEe1IxuHJt6KU3d9FeHufePWS0UICIHrR0AEjOvHXo4sBISAEBACQkAICIGWIxAMwHz+NFJP7sfItW9i+NldoEgHsGivrw2H0ZwXzp7kvBiFozc9iLqpCzp1zIuGK49fChzdiMjOj2H2u7RhIEqwUEIFiwg8qTo7K3iZxQLlvOA6iwncpgSMxHpzBAzuoyZ1vETnRaxgofqyc4OPzXUeOsJ9eNlOwTwXlJzBTZH4IRIVEQP+bEjD1vT44Jza0JFbn0DPmUvjIXXSmspK0vPTP2LOR/+N3jTUxki/J4aE6z3aYxQOzLkfVeTECAwejihlJUnsk7CJVIWAEGhlAiJgtDJg2b0QEAJCQAgIASEgBNqCgKG2Ej3XvI3c3avRt/QEevop40aSl7IqMzkvpt2Hs/PuRqDnAIQzu2unxy/gXam49q2BZ9N7sId9cTEwYp0Y7L5QwgYv88SCAYsUSrDgNiVssHiQWFf9m2LLogO7J1gsYZEkdugJb8vreOJj8v6VsKHEFbU9CxjKhWHxenBL+XksNMS7b0TA0O8Cs+TSbdXrmP7Br9HHXUJiX1T7fdF76D/ZiVFjz8L+Sbeh4O6vay4lHlTUtX5TYonIshC4/gREwLj+90DOQAgIASEgBISAEBAC10zAWFGMYX/6T4zf8wnSQh44onrGCrVjdl6UkvPibN+ROLbwYdTl3aJWdcm558QeuPavg7nmAuxBF0WIiGpCBKc/tVOQS+W4UE6MpgQe5ZhgiLHCBgsRPF2usJuDRQi1HyVQqO24zpNyWqj1fH4slqj1LF5wmlXen5XEjFurirHI4FG70eaJAoYW+6J7Hzgp9kW3abd02tgXcRBiKs496zD4i7cx8NQ+9K8rgj2a/H5tvmEBdj/1A0T6DNTECxEwYiDKohBoYwIiYLQxcDmcEBACQkAICAEhIARag4CprAhjXv4x8g6tIKmC0nHSC3lsqTKlYNvku1E49y4E+w5GNLtX7Ooutxxy1yJcVw7v0S3wHyAhI+jTUqqyeMETCxk8sUjAU1OFhQ2elNDBogWLCixmqC/9TW3L7dyHxQkWHnjiZZ5UUeuVu0LV+Rh8LNWfxQtu47qT9tMcAYNjX3SffjuyJ8+HNbMnLFTvSsVQUwELOVUGfPEu8rb8Cdmh+FgYisUWEjD2PP0DhHvrAoZql7kQEAJtT0AEjLZnLkcUAkJACAgBISAEhEDLEaCsI/ZTR5BxdCfGbHwXI8sOxe2bZYwQmeNLbd2x4fZvoPS2x7tkzIs4KDEVd8FuuA+sByrOweSqhNVshMPhuDh0hJ0RLGAoBwYvJxsywgIGDzdRjgoWEpINKVGHVvvhPtxXOSnUduq4set531znSQkYqs5iBq9ngcNB8VAWV5fgFiOlVI0pyoGxjcQra/d+cPYfim5Tu57zIgaJtpi6bRVuWPMmBpw7hF6e0sZZSSgWxsFZ9+qxMHJHarEwEvchdSEgBNqGgAgYbcNZjiIEhIAQEAJCQAgIgVYhYKwqQ+57/4OxOz9Cur8GaRFf3HHCZHp3GywoSe2H7Xd8HZWLlmnr1Qt5XOcuWAl76hCqLYfnyCZ4966FJRzQBAwWEJiRioGheKkYGSwWsCuChYtYp0asQ4LXq/6xDgrGrBwe3M6T2k714/W8raqzsBErdHB/JWTwufA6bmMhgwWM22vKcJs5/llQAsaOPjnoNuN2dJu8gJwXPbqc8yLxMTdUlsB84SwGrXsPeTv/gqyErCRuzkpiy8DBybfjxD3fQKSLu5cS+UldCLQlAREw2pK2HEsICAEhIASEgBAQAi1FgIc8nDiMrPxdGL3xPXJeHIzbMzsvKDeGJl6cy8xBYc5EFM1dCu+EOXH9pKITUE6MSHkhQC//Zgp0oFwVPFcChmpjwYDFAlVX6xVP5ahgIYTjVbDYwP1VUe3cxut4f7GCBK/nifcT67zgZZ64sGCh1qs678dO628lN8ktRh/F9mgYTlQZNeH9niNwYMxMdJuyEJmjZ2j76eo/LmYlWfEqZn/8W/Tyl2vcEmNdHOk+Codm34Nqykriyx0NWO1dHZ1cvxBocwIiYLQ5cjmgEBACQkAICAEhIASunYCRxu/nvPMbjN/5ATL8tUhJcF6weOGnqdyWid159+H8nLsQ7tEXyOx27QfvhHvQnBh1lajZswY1Oz6F3RDRspOwiBAbA4OFCq6zcKECacauV2iUo0IJDtyfJ1US22OdFrHCBC/zpNarOu+Hl5UAouo8d4RDmE+ZSG6ED6kRP+w0iIhLldGBVbMexulbH4U1o3uXd15oUOgHc+TSbfVbmPrh79DLVYQUCoKbGEfGbbShhpwY+yffgZP3fguRrB7advJDCAiBtiMgAkbbsZYjCQEhIASEgBAQAkLg2gnQ8ADbiQOa82LUpr9gVGm880IdwG8woTCtPwoHjcOZ2XfBNekmiX2h4FxiXnN4K6r3rIWh6jwsnmqYDJReM0kMDCVIJDovuC+LHspRwUIFuyS4Pw8JUUW1cxuvU04KdmHwpAQO5c5Q9UQBg4/DU2xxRCO4kYJ6zkMA6WEPnNGAtrqaArmuXfwszt/7jdjuslxPwLFvE/pt+hCDC3ZicNVJykoSn8lHgTrYaxwO3LgMtcMmIDhoOFl1Gu6r6iNzISAEWoeACBitw1X2KgSEgBAQAkJACAiBViGgOS/eJefFdnJeBGrIeeFPepxqkxObp92PMzfdrzkvopndtX6JL9xJN+7CjZydJEgxMWpIxKjb+SmMIRKMKB6F4sZiQ2zMi0RUKuYFCxLsmmDBgQUG5dxQ/bmN16l2VVdOi9g6ixeqzo4LXo4tykGg2hy03xsDfl3AoBSxImAoMpeeG+pIsKKYMgPWvIWZm95AVhNZScrNaShJ64f8aYtRfPuTiKZnXXrHslYICIEWIyACRouhlB0JASEgBISAEBACQqAVCbDz4uRBZOTvppgXf8Ho0v1JD8bG9+LUPjjXfzSOzb0HtVMXivMiKalLN2pODArqGSw5g2jVBQooEr44dIRFDDW0RAkbam+8jiflmFDtiXPltFD7YVEi1nkRK1iwGKIKtycKGGqdmltJwBhFQscYCkg6jESuQQZ9ew5GuWPcYpyi58LXfwjCPfurTWQeQyB1+2oM+/zPGHD2AHq7ihtlJfHT75iXAuPunLAExx7//2QoSQw7WRQCrU1ABIzWJiz7FwJCQAgIASEgBIRACxAw1FVh0Lu/xbjtf0WGrxoZkfgUmeoQNSYHtkx/EIULliGU3Rvh9GxtVeKLtuov8+QE2IkRIubl21egcstHQMAbJ1qw04KnZFx5GMnlRAbltFCODVVXQ0RUbAtVjz3LRMdF7Dpe5uCTThpG0o2cGwvclZhr1l06LG6dY3FrwFicuPE+uPIWJW4qdSJgrK2EqaIEg8iJMWPLW8igYTixhePLsAdm85jbcfDp71FWkt6xq2VZCAiBViQgAkYrwpVdCwEhIASEgBAQAkLgmgmEgrCS8yL9+B6M3Pg+RhXvpddQesmCHnhQ7V9zXqT01pwXx2nYSN30W9UqmV8DgaqDm1C5aw2ilPWF3RL+8vMIlJ6l2Bi4Y4SL9AAAQABJREFUZBDPyx1SOTQ4ECjvl90XLFqwOKFcFpcTQRKPYTCZYes5ANbuFKyVisPrxsKjO7DAW6TVOaUuC1wlGf2w57avoHLhQ1q7/EhOIG3bSgxdS06Mc4fQx10MC8JxHY90H43DM+9E1YjJ8A0dJ1lJ4uhIRQi0DgERMFqHq+xVCAgBISAEhIAQEAItQsDgqkW/93+H0dveRzdPJbLqvwYnpnispeEBW6bch1PsvOjRD8jSY160yEl04Z0EOSYGOTF4CAklIEH5thUo3/whon6v5r5g8YGnZE6MS2FjcSJWrGDXBk9qaAlvezmnReL+TXYnus1ciu55unhlpngOU975Neae2qB1ZeeAm4K7ljp6YPsd30TF4icSdyH1GAKG6nKYSosweO2fMGP7O41cTx7KSlJnScWeqXfi5P3PIpIhGX5i8MmiEGgVAiJgtApW2akQEAJCQAgIASEgBK6RAKXCtJzJR+qpg7hh3TsYVbiDxuJH6CtwEucFDQs4238MCihVas20RRLz4hrRX2rzqgMbUbl7DcIBn9YtVFGMYPk5RMlBcTVFuS2UgKHqzdmXwWiCrddAWFmwomKyOZA96WZkjZml1Y0VFzD+99/FrMMrtDo/OQHy7nBq3Y23fxtlS7+stcuP5ASiLDLRfe33/vOYv/J/mgzqeaDXeByae7+WlSSQM4qykjSky02+Z2kVAkLgagmIgHG15GQ7ISAEhIAQEAJCQAi0IgED2f97rnwNw7Z9jF4VZ9ArUEXfzylrRcIx68h5sXn6Mpy+5VGEsnogkqZnRLhSR0DCbqXaBAF2ZHBsjGhUFyzKt3xMjowPEKWsH1db2GkRe7+a67wwWu3oNutO9JixWDu0wWiGOTULlpQ0rZ4oYHAjuzAqzalYd/uzKL77a1o/+ZGcgLoPff/yv5j/ya+bFDDKTWkoS+uNI3lLULzkS4impiffobQKASFwzQREwLhmhLIDISAEhIAQEAJCQAi0PAEeOjLkzX/H5G3vIjXshSMajDuI/jXdhEpbFjbe+jVc4K/pNAQh9kU4bgOptAqByv3rUbX7c0Qo3SoXy+kTcJ49iRqLETV2+hKfqDhdzVmQwGH0+rWJomQgYjLCPvgGpAwdS46L+cgaOzvpXpMJGNyxigSMtYtJwLjn60m3k8Z4AmmUlWTIuvcwoHA/+tYVNYqFobKS7Jh0F44//h0ZShKPT2pCoEUJiIDRojhlZ0JACAgBISAEhIAQaBkChrpqjPrD9zF7z18oaCe9wCYMHeEv6dUmJ3357YNdC59E+cJlNHTErA0faZkzkL00h4DmyHBVX4xX4XjnVQz84m0ctZiRn52CiLEFFIxIFNbSSlhrauAl50fAbEG3JU+g371fJcdF5kXHReL5ioCRSOTq6kb6XTRVl2EQOaJmbn4T6QkZgPh3kb1RG8fegUNf+gEimRJ/5upIy1ZC4PIERMC4PCPpIQSEgBAQAkJACAiBtiNAmSjs+buQfXQXRmz7CKNKD8Qdm50XnE3CazDjVI+RKBw+FRfoK7xnzAyJfRFH6vpUHC/8CsNWLscZulEn7XaESL/gwSacOYYzl6gSpvWx7apOoTtpmEcEQVrPnpsw1ykWg6G2FnC74QtHELDb0Ouh55Dz9HfU7pLORcBIiuWqG9O2rkTuunfR/+xB9Hedb+TEONJ9FI7m3YHKkVPhHT6J7DjWqz6WbCgEhEByAiJgJOcirUJACAgBISAEhIAQuC4EDB4Xct/6JaZseQepIQ8NHdGHJqiTYQHDR36MKsp+sGPGMpyjVJhRyn4QTcuU4SMK0nWcKwEjGAlq98lPN4wnG4kXPKmS2K7qYXJYhGiqJdeFiyY/LfujEXiCQW0KRkjUcNhFwFAg23JeUwlTRQkGr3oNs7e/jbSIHshVnYLXYIXb7MDOaffg5LK/o3g0mWqVzIWAEGghAiJgtBBI2Y0QEAJCQAgIASEgBK6JAGUdcZDrIuvITozY/nEj54Xad5C+5Z/KGIgzg8fj7Izb4Zp8szgvFJx2MFcChimiC0+ak4IEDAuJFzypktiu6r5oCD5K2criRR1ZNNy07NHqYbhD5L6g9pAIGApjm85VVhIO6jlv1e+0oJ48vCvmtmrnc6jnWBymdLbVw6fAP2w8QEN+pAgBIdAyBETAaBmOshchIASEgBAQAkJACFwTAYPXg1wK2jl1y9tICXHQznjnhdq522DD+mkP4ORtTyBKWUei6VnivFBw2sE8UcBgxwxP/JIb+6Kb2M51MleQYBGqFy3IaUNiRQ2l8aylyU+BPL1U99KyzyZDSAjVdSmcmaTXX5dj5orfopu/GnYtr0v8qVRTZqAqeyb2T78b5+79NqL1WWHie0lNCAiBqyEgAsbVUJNthIAQEAJCQAgIASHQUgTohdRxdKfuvNhBzouS+JgX6jAhGjZyLq0fzvYfg5OzlqIm7xZxXig47WhupRgYuZ8uh5UEKI57wXEuQqROmEm94Loqie2q7iUBgx0XHlIz3CRYuGi5jp4RL8XB4Lr/SmJgVJZg1Ms/xvT9H8FMZ2LWpBTJQqLuwdXOnZR1pv+WjzCoYDdyqk7Bqt3lhr0FSaoK0O/rlsn3IP/J72rDuxrWypIQEALXQkAEjGuhJ9sKASEgBISAEBACQuAaCRgCXuS8/m/I2/wnzXlhT0iXqnbvofH1G6Y+gBOLn0REnBcKS7ubm0nA6L/ieS3trZ1ECx+JFwGarLTMdVUS21U9QENIAiRacAyMWlI1vLTMUy1N7lAYQRIywg5n82JgVJVhKD1bE3d/gNSIHw6EtMNLGlV1F65ubnDXwUiZZwZ//AfM3fgaUhJiYbCbhgeWbBh/Jw59+UcSC+PqMMtWQiApAREwkmKRRiEgBISAEBACQkAItBEBnwejlv8r5u/+8yUP6Dba8NnNX8OZB54T58UlSV3flSYSMPp98jyclEPEUS9g+MhNYTc2FjBi21nA4HqQBIwgBe6sobQkNSRgcBBPTcCgGCl1JGCEaQhD8wWMchqW9EtM2PUB0sJepGh5TcSB0VJPSL+3f42bP/1No7Sqav/rxt2Jg1/5ETkwslSTzIWAELhGAiJgXCNA2VwICAEhIASEgBAQAtdE4EoEjEXfQOGDfysxL64JeOtuLAJG6/JtT3sXAaM93Q05l65CQASMrnKn5TqFgBAQAkJACAiB9kWAhgLY8ncjI38nRm79CGMv7Et6fhz74mz6QJwdNA6nZy9FLcW+kNJ+CfAQksErlsNOMTA464jKLtJUFhIj9eGRJTzsgEaN0JCT+CwkHA+DM5FoE8W/8JMzo7lZSAxuFzK2rED/PWsx5NRuDPCc18DJEJKWeX7Stq3CoA3vY2DhfgyoPQdLQiyMo91HIn/KbagYNQ0emiQbSctwl710bQIiYHTt+y9XLwSEgBAQAkJACFwvAsEA+r/9X5i48S1k+GuRSTEKkhUvxb5YN/NhnLrzq4ikZFJGg9Rk3aStnRCwk4AxdOVymCmNqhImWJzgZZ5U4TaeOD6GipHBcTJc9VlIvDSchLOOcAYSzkRyNVlIQCKZwV0L6/lTGP+nf8e0k+u1w4uAoe7CNc7rqmGorkDuJy9h7ta3kBKN/x32G8zw0tCvbXn3o+DRf0LUKb+710hcNhcCEAFDHgIhIASEgBAQAkJACLQlAXZeFOxHGk3Dtn6IEYU7KItBhL7e8utsQwmT86IwbQAKB44h58WdqJtxW8NKWWq3BBLTqKrsIk1lIeG7zs4LdmJw8SVkIakj94WLBAwPPTce6nglWUj0PVL2k/LzGP/idzHz6EqtieOp7L3hJpzMuxXuoeMRGjBUdZX5FRCI0j3hqd+7v8Wcz5YjM1hHv8ckGiXs42iPUcifdAsqRueREyOPnBjmhB5SFQJCoLkERMBoLinpJwSEgBAQAkJACAiBliAQCqHv+89j7MZ3kOUqRXaojl54OGdBfPEZLFg39UEULP0KohnZQFpmfAeptUsCiQKGyi5yqSwkvI5dGH4O4kmZQjgLCQfxrKW6j5ZVFhJXMEwpWZufhUQBShQweFhSuSUDRd1ycejWJ1B7492qq8yvkECUgqr2/OD3mPrpC+juK0cqZREyJoiR7MTwGa3YMv1BnGAnhiPlCo8i3YWAEFAERMBQJGQuBISAEBACQkAICIG2IBAKYsirP8OMTa/DQcMMbBTzILZozguKeVE4eDxO5y1GTd4iGIxGbYrtJ8vtk0CigMExMPgO8zd3joOhSmJsDFW/XAyMALkwgnZ7s9KoqmMlChjsEfDQGZU4e2LHkm+i/LbHVFeZXwUBx75N6LVzDXKObsGwsiPkqGLfTePy+cR7cegrPwZkKEljONIiBJpJQASMZoKSbkJACAgBISAEhIAQaBECFPtixAv/F/O3v6m5Lth9EVv4a+0XMx7BiXu/iYgjTftaazDEvPnGdpbldkcgUcCIvbuxd5HbeeI2nlQ9NgaGj8QKjn+hYmBw3UN1n812TQIGH4t9AmW2LGxa8m2U3vElapFy1QR8Xhi9bgx+/3e4ccMrcDYRz0YEjKsmLBsKgYsERMC4iEIWhIAQEAJCQAgIASHQigTI+u88sAXdDm/DsF2rMarsYNzB+KUyQOJFrTkFGxZ8GUX3fZsCI5DzQsSLOE7tvWKlIJ45ny6HlbKQsOviUjEw2HXBrgxTfT+ux8bAcJFg4WqFGBj8rLELg4eRrF/yLEpomJKUayeQyllJNn+Igaf2YVDtGbr/NAYopnBWkmMTF6J89HR4xs6gGy+xMGLwyKIQaBYBETCahUk6CQEhIASEgBAQAkLgGgmEQxjyyk8xa+NrsNPQkUSbOUfBqDI5UZnSA7tuehQXbnsCRosFBhO/3krpKARMJGAMWPE8HBQLwU7ixKViYHDMC5sRF/u1VQwMxbLCko7Pb38OF+56RjXJ/BoIRF0Uz4Yyk+R+sBw3bX2DnoFA3N4CJFX5jRZsmrEMBY/9M914R9x6qQgBIXB5AiJgXJ6R9BACQkAICAEhIASEwNUToCB/zgOb0f3QVgzd8xlGlcY7L3jHIRIvAgYTjvUcjTMjZ6Js/BztC63Evrh67NdrSwsJGINXLKfYJgHNXcHxL9hZkSwGBq/j4SOqkOGC0qWGtKCd7L6oI/uGhxwYbm2KwE3DRwIc6NN+bUNI1PEqzelYe8ezJGB8VTXJ/BoIqKwk/d/5DeasXo6MsDt5VpJYJ8a4meS0EpHyGrDLpl2MgAgYXeyGy+UKASEgBISAEBACbUyA0izmvvJjzNvwR1giwSTOC3pppXgEtSY7tsx8GGfvfAZRZxpAmQpk+Egb36sWOJydBIxhK5fDRC4bFdtC7TZWrOBhHFz8LFrUOzE4G0lbxMDQjwyIgKFItNycs5L0ev8FTFv5ArL9lUgjJ05inJsgOTEC5MTYMOMhFDz5rzSOyNpyJyB7EgKdnIAIGJ38BsvlCQEhIASEgBAQAtePgHP/JnQ/SM6LvcmdF3xmYXq9KcgcglPDpqBo8gK4p9wsWUeu3y275iMnBvHkGBiaA4PEidiIB6qdD8hihhI3LhkDIxwhsSOKgDgwGFu7LZyVpMeuz5F7ZDOGlx3SXBjJTvZAr/E4Ou0OVI+YBP/IKVrMm2T9pE0ICIEGAiJgNLCQJSEgBISAEBACQkAItCiB3Jd/hBvXv0LOixC9xPAra+PCX2PXTluGY/c/S+kV02kS50VjSh2nJVHA4BgYymHBMTFUiW23UbtyYgRpQFGAhozUhKOoJWeGj5a9NNXS5AqGEaJgsGGH85qykKhzEAeGItHCc8o0ZPB5kPPOrzF//cuwkwsjWakyOlFly8S+OQ+i6AEK2mu2JOsmbUJACMQQEAEjBoYsCgEhIASEgBAQAkKgJQjozost5LxYkzTmBR+Ds0CczsjBmdyJKJy6CDXTFonzoiXgX+d9JAoYl4uBoWJjsEuD+7IDgwULPQMJtLgXEgPjOt/Uqzx86o7PMGDrCgw6uRs5VSdJqiRFKqb4qcVHmYc2z6S0ydpQEhEwYvDIohBISkAEjKRYpFEICAEhIASEgBAQAldPIOelH9OX15dgjoabdF6EKO7F2hmP4vjD/4CozYGoxSYxL64eebvZMlHA4OEhqsQYMLRhI7Htqh/HwODUqV7NfRElJwa7McLk0IiSuBGFh5Z9Ngniqdi163nAB4Pfi9y3f4X5G1+hwK4sUTUUvuecfWjt1GU4+syPJRZGAxpZEgJNEhABIwmaMP2xKdq1HpUnDqDPhDnoNWZakl7SJASEgBAQAkJACAiBeAI2yjSSfXg7hu1ahXEX9sWvrK+x8+JU+mAU5oxHYd5i1M28LWk/aeyYBKwUxDPn0+WwUhYSdleoWBdmUi9iY2Dwq2yI3mBVu6q3RgwMY0UxRr3yE0zd/wlsdEbmeieADCFpm2csddsqDNhCToxTe5BTc6qREyO/20gUjLsJpZSRxE3vHmTFapsTk6MIgQ5IQASMJDfNT/mbt//vd3Hy8/cx/pG/wYRH/i5JL2kSAkJACAgBISAEhEA8gX5v/jvy1vwBaWGfln0gfq1eC5Pz4rNpD+HYg38Lg5ZtxJmsm7R1UAImEjD6f/I8nAiCY17ExrpoKgZGbL/YGBg1pH74ycWjYmDUBUMIkxPjSmNgGCtLMPSNX2LC7o+QFvHCoQ1WkSwkbfWIRb0ewOvG0D//Cjdvea1RJiJ2Y4VJtPicHFkFX/oeYIqVutrqLOU4QqBjEBABI8l98tdWYct/fwfH176HyY/9PSY9+Z0kvaRJCAgBISAEhIAQEAI6AeuxvcigKXfHCow6s41eUCI0dEQNCtD7sFX8ZGYuzgyZjELKNlI7baHEvOiED5CFBIzBK5bTcIEALCRgsLNCy0JCc66rotpjmrQnpiEGBqdUjcbFwPDUZyEJXmEWEnhcSKOsGH32rsew/E3IcZ3VTkMcGOputO48SqmUecr5489xy/oXyJ2TPKDvoZ6jcWziAlSOng7P2FkypKx1b4vsvYMSEAEjyY0TASMJFGkSAkJACAgBISAEmiTQ44MXMf7z19GtrhTdQ7UkVTQuPHTks+mP4djj/wxYrZJxoDGiTtHCMTCGrlwOcyRw8XpYykr2THC7yj5io1EDnI3k8jEwIhQDw3pFWUhArg2EgrAUn8H4V3+M6QVrtXMTAePiLWqThcGv/AyLvuDhRSxfNS4uCujpNtmwbe7jKHyMPqDKUJLGkKSlyxNo9wJG8d5NOL/7C0SCevqh3uNnod/U+TCaTJe9eVH6Y120Yy2K92zQ+hpomz6T5qLfpHmX3FYEjEvikZVCQAgIASEgBIRAAoEBb/4SM2noSGqY7PlJUiZypgGP0Y4NNz6JMw/9vTgvEvh1pmpiEE8V20LFulDXqtq5HitweC9mIaEMJOzA4IwkFLjTQ1/wlQMjcKUOjPqDmkrPYcLvv4sZ+au1FrfBhgO5M3Bq0kLUjpqKYM7I+p4yaw0CqTvXoP+O1Rh0bDuGVBXQwBG+8w0lRDJXkFrXzXqMhpL8ADBKLIwGOrIkBHQC7V7A2PWHn2LvG79CgCL4chl3z1cw/Zs/gclq16/gEj8j9Id+2//8M/a+/Vutl8lsxcSHnsXUZ757ia1ICZchJJfkIyuFgBAQAkJACAiBeAJD/vADLFz/By04X7Iv7VUmJyod2dg992GcX/JlGCwWGM0yzj2eYueoJQoYzYmBwc4LdmL4KPNIiP4LkGhRE+YMJC0TA0ORTRQwOB5LtdGB4syB2Hv7V1B98/2qq8xbg0CYZCv6KDvk9Z9jwYaXk2YoYknjs+mPIv+Zn4iA0Rr3QPbZ4Qm0ewFjxws/xK4//icC7loN9sRl38LM537ebAFjy6//UdueNzZZbZjy+N8j7+s/0vYVCYVwbvsaXNi/SaurH6GAH+d3fo6yU0fRb9x09B47Xa3S5mZ7CvpNuZGyk+TFtUtFCAgBISAEhIAQ6FoEnHs3oNe+Dcg9uB4jyg7T99P4L6oBekH0k/vieJ9xODN6FipG58EzZoY4MDrxY5IoYCinRTIHBsfG4LgYLGVxP663SgyMet6JAgYPa/KQ7FaS0hvbl3wTFbc+2onvTPu5tNTtq9F3+yrkFOzEkOoTjZwY+d1G4MSYOSgZNxfuyTe2nxOXMxEC7YBAlxYwQj4PtpJDY9+7Lza6FdFwECxwGM0WGBIiAadmZmECBfcc/9DfNNpOGoSAEBACQkAICIGuQ2Dwyz/BgnUvakH5Eu3gTKHOYEWdKQXbZj+Es/d+EwabHQaOfyGl0xJIFDCu5EJZ/oqNgeGnISTVNHykhiY/DY3mupsCeV5xDIz6k0gUMPh4PGyh1NYNG5c+h7Lbn6zvKbPWJBAN+BD1+TDsjX/Dwq2vXUxrq47JwpIWM2fWEyj4yg9Vs8yFgBAgAl1awAhTMKPCjR+jaNe6uIchTH9USg5sRfmZAvQZNQk9RkyKW29xpGDAjEXoO3FuXLtUhIAQEAJCQAgIga5BQHNe7F2H3EMbMZycF4niBb8U8nSsx2icHDETFeNnwj1hrjgvusDjwVlIcj7Vs5AoZ0WIlIJkDozYduXUuFwMDB+JGFechaSeezIBg1+Uy62ZWL/kOZQs+VIXuEPX/xIvZiV57ee4cd3vKW5OiEQMlpPiy5HuI3Fi7DyUjpsNz8R58SulJgS6KIEuLWDwPY/QGEP+IxJb/LWVFDvjX1DwxV8x6ZG/xYTH/0/sam3ZYDRRXB0JrNMIjDQIASEgBISAEOgCBNh5sZCcFxZKh5goXvDl+6jVQ0NHNs16BCeX/R9yXdhgFOdFF3gyaMgyCRj9P3keTgrHaKfhIZeKgcExL+z0z0nVj+tBepkN0nPVKAYGxU+oC4URJidG2OG8siwk9eQTBQx1Q8otGficBYylX1FNMm8DAv3f+AVmrXmRgv/64SDPRWLhgJ5+ykSybu7TOPXU/01cLXUh0CUJdHkBI9ldlyCeyahImxAQAkJACAiBrkvAuW8jeu35AiZyb3IZdHQbhpc3dl4oQkezb0DBiBkoHT8XrsnzxXmhwHSB+ZU6MGKR8Df42BgYnIWEM5BwJhJ3TBaSlnJgqGOLgKFItO3csXc9etY7uUaWHWkkhupDSSio5+wnUfBlGUrStndHjtZeCYiAkeTOiICRBIo0CQEhIASEgBDowgQG//H/YcHa5RTrgo3+NAZXmxpbvrWV9GPVlGU4+tT3NOcFZxyR0nUIXGkMDHZoKCcGZyO5dAwMSq1KgkZLxcBQd0UEDEWijefkpiErOIa89GMs2qBnMUp2BitnPoHjz/w42SppEwJdjoAIGEluuQgYSaBIkxAQAkJACAiBLkjAcWCz9oU058BGjCw9qKVJvRSGgqyhODV8OorGz0Pt1AXivLgUrE66LlHAULEtmoqBoTBEeGgIDSPwsNOCJo51weIGuy/YhcFtHMCTA3mKA0NR6xzz1B2foc+ONcg5vg3DqgpIII0XR491G67F0rkw8Ua4pszvHBctVyEErpKACBhJwImAkQSKNAkBISAEhIAQ6IIEBrz5S8xd/TxSIz5YEl4qkuFYnfcI8r/6E1CgLFrNPg0pXY1AooAR67DgWBeqJLbXRULkviB3BYVC4IwjXp5ry2FaDqNWYmAodJ1uzpkPo4EAbnj1p7hl66uNhpLwBfNwklWzn6KhJD/odNcvFyQEroSACBhJaIUCXpzdvBJlR/egf97Nkm0kCSNpEgJCQAgIASHQmQnYDu9ENrkvcvZ/gVHn9zSZJlUxKMjIxclheThP8S5c028h7SLmTVV1knmXIJAoYFzOgaGcGVqMi3r3hZeGFSQKGC3lwBj30g+Qd2QlvSSTzlYvyskQkuv7aDZkJfkF5qx7Cc5oANaEoJ7syTiuOTFm4MLEm8SJcX1vmRz9OhIQAeM6wpdDCwEhIASEgBAQAu2PQJS+fvf+y3JMWfUisv1VSKeXiURLd+xZ84vFZ5MfwOGnvg+D3QGjmZNnSumqBBIFjOZyUI4MdmK4aUoUMNiVwY6Ma4qBUVaE0S//EHmHPtXSdppEwGju7WmTfn3e+hXy1ryE9FCd9ncn2UE1J8acp1Hwpe8nWy1tQqDTExABo9PfYrlAISAEhIAQEAJCoLkErMf2IuvwNgzctx7DzuxESsRPX0LDTQ4GKcgcSmPTZ+A8ZRup47HpJrMW96K5x5N+nY/AlWYhSebA0NwWHPOC4l2EyI0RJPGiJbKQGMvPYwQFpJ20/xP6yh+8+JVfHBjt4zm0H9iC7INbMWzvGowp2Zf0744IGO3jXslZXD8CHU/AePAbmP7t/weT1X5ZalH6g7/l1/+E3a//SutrohzsUx7/e+R9/UeX3VY6CAEhIASEgBAQAl2PQPcVf8SE1S+jR+15ZIXcFy32TZH4bCplG3mGYl5YrE11kfYuRsD0wq/Q/5Pn4UQQHPOCnRXsnLDRmI3mxsDgAJ41FLizhmJfcCH9okViYBgrijHkrf/CuN0fIyPi0UQM3r8IGEyh/ZShv/8+Fm54if7+sFwRXy7Gwnj6uw0rDPRwybC1Bh6y1KkJdDgBo8/oyeg9bgaM9IXjcoUtoBf2bcL5gzu1riJgXI6YrBcCQkAICAEh0LUJ9H7vd5i5cjmyAjWw0xfqxJcHRcdnMMNjtGHrzGU48fA/0tupXZwXCk4Xn1+pA0PhCtCHN84w4iHRIpkDg9s8lIWExY2rzUICjwspFNul1/7NGHFgDXJrT2uHFwFD3YX2MU+hjCS9d3+O3KNbMLzqWNxJRemv0vHsYSgcNlVrjxhNeuydKTfH9ZOKEOisBDqcgGGgqN7NES/UDYtQxGYOjMNFBAxFReZCQAgIASEgBIRAHAH+twJNg976D9y8hr6eU9yLS5VKUwqqHNnYN/NenFv6DH1al9gXl+LVldZdaQwMFfuCGXE8FVcrxsBQ98F0oRAT/vA9zDj2mdZUYUnHF4ufRfHSL+kZdPiLvpTrRkDLShIMYsRLP8Si7a81KaTyCYbIp7GaYmKceOKf6d6Z6jMgXbdTlwMLgVYn0OEEjH7j8tBv0jwYTPQLerlCDoyi3etwbu8WracIGJcDJuuFgBAQAkJACHRNAs69G9B352cYdHQrhpbnU8pU3bqvaIToFSLWj3G09zicnLAAlSMmwztqGgwUuJM/skgRAokCxuWykChiugODXBYkpCXLQtISMTDUsRIFDLfBhiMDJuPUuHmomjAbgaHjVFeZXwcCF7OSvPozzNvwMmzRMGwJf5PUafGQkoLsG3D6hmk4Ty4MlzgxFBqZd1ICHU7AmLjsW5j53M+bFQMjQv8HsOXX/4hdf/xP7faJgNFJn2K5LCEgBISAEBACV0uAYwxQrIGBb/9Kc15w0M5kxQsTXEYLfSHXB5Vsn/4ATi77B8CZKllHkgHrwm2JAkbzY2CENfeFj8xAnHEkMQtJLbmK60JhhGld2OFEr4eeQ87T37kq0okCBoeprSMRozh9AHYt+RqqFzxwVfuVjVqWQF+KVzJ17StID9YhI5r8b5M6YpD+Rq2e+zROPkbPBH/oZTeGFCHQCQmIgNEJb6pckhAQAkJACAgBIdA8Auy86MfOi/ytGFLW2Hmh9nI0axiOjZmHsM1GTgsTKodPhnvsTMBsFeeFgiRzjUCigHE5B4bKQuKh2Baay4JEtWQODBUDg+NkBOy2FhUwWJgL0FCEEmdPbF7yLMpve1TuZjsgYKeMJFmHduCG3SsxlrKSXKqE6f4d7jkGJ8bOQ/n42fCNo79PUoRAJyQgAkYnvKlySUJACAgBISAEhMBlCNDXbNAY84Hv/gY3rVmOtIgv6QZsz+YXg/UT78ahx/8FSMsUx0VSUtKoCLCAMZQCwZojDXFUOLZFsoCwse3s1GDXRWwMDN2NEabAnWHNlcEBPLVAnjZriwoY6tzLLJn4YunfoGTJl1STzNsBgaEvfg83b3yF/hJFyGfBT03jwq11BiuqLBnYsehLuHDfNxt3khYh0AkIiIDRCW6iXIIQEAJCQAgIASFwZQS0mBc7VmFw/jbklh+DtYnx5QWZQ1Eweg5Kxs5C7fi5lG2EHRgS6+LKaHet3pyFZPCK5RS3IAALqRbKgcGGfq6rwu1BeuvkNs6tp7sv9BgYKguJiwSLEGUnCdGwkdaMgaHOSQQMRaJ9zVMopl/PfRsw9OB6DK840uTJsYvGZXJg/YJncO6hv2uyn6wQAh2ZgAgYHfnuybkLASEgBISAEBACV0ZAc14EMPC93+Kmz55v0nnBrguOC7Bh8n04+vT3EE1Ju7LjSO8uS8BEAkb/TyiTDYV9tZM4cakYGH5yXNhID7NRP3ZeuMhpwa4LdlrUUGyWGo7RQoX0C3AMjFqKgRFphRgY6maJgKFItMM5xfYb+vvvY/6mVzUXhoncGMlKwGDCmjlP4eRD/4CoxUrqmCVZN2kTAh2WgAgYHfbWyYkLASEgBISAEBACV0rAfnALeu5ag5zDW3BD6SEtun+yfRRk5OL4yNkomTAHrsnz6TM5vQhIEQLNIGAlASPn0+WwkgODnRUsQbDTQsW6ULtQzgyus/3fRwKGj7JNJAoY7L4IUYc6EjQ8oRAJGNTf2bJBPNU5iYChSLTDOT0HKbvWkhNjI4Ye2oDhlflJT5LF15NZQ3Bm2FScm7ZIspIkpSSNHZmACBgd+e7JuQsBISAEhIAQEALNI0Bfrw2BAHp9/HvkrX4R2f4a+kLOHov4wv/4DxmM2DThThx65DuIZnaXmBfxiKR2GQKJQTy5OwsUic+aavfTSnZiBGmwSTCJgMFuDF7vJgHDyw4i3pcIGBqHrvYjQgJW1OfDqJe+h4W73r7k5fsNZqyZS06MB2koidUmToxL0pKVHYmACBgd6W7JuQoBISAEhIAQEAJXRcB2dDd67FuPgQc3YkjRXqRQgEUOhpf4UqnFvKAo/hfGzEQdxb2A3SExL66KeNfdKFHAULEu2I2RLAaGegbZgeEVAaPrPjjNuPIoDSMBCbHDScCYv+1NklspW2oTQT1DtOZY9+E4NXI6LkxeAO+Euc04gnQRAu2fgAgY7f8eyRkKASEgBISAEBAC10ig28o3MHXFC+hRdx5pYW+jf/TzP/ZDNHZ888S7cOTJf0U0Pfsajyibd1UCiQJGc2JgcKyMuvoYGJyJhF0XtfUxMMSB0VWfpCaum1w4Oa/+FDM3vEYibAQWCvJqIeHLnBATg10/Pvq7Vm1OweZFX0XxA881sUNpFgIdi4AIGB3rfsnZCgEhIASEgBAQAldBoOcHv8fsT36Lbv5Kik3AQ0f4n/cN5QTFvMgfNQel4ynmxST6Uml1NKyUJSFwBQQSBYzLxcBQsTGUgFFHG3D2kQB9bfdT3AMXzd1hnlpwCElJIca/9ANMO7qaXnEbvuJLDIwruNHXqysJFs79m5F9ZDsFdyUXWZAcGfvWYmTF0UZnRAPnUGe0Y92ir+HcMslK0giQNHRIAu1ewNjz6i+w/81fk1vKrwEec9fTmPa1H8BksV8WeIT+4O/43+9i/7vPa31NJjPGL/sWJj9NedylCAEhIASEgBAQAp2fAP3j3uBzYwAJGPPWvoCMsCfumlnG4IEkG0ctxv4n/hWRbr1hMJlgMChjf1x3qQiByxKwUxDPYSuXw0TDlPgpipXKYp+qxHYWMDhQZ02Y3BfkwqB3U/qmrjsx2I3RsgLGWYx55UeYengVLHQUHk7FRQQMDUOH+cHiWDToxw0v/Qg3bnmTnBgh7X7GXoDPYMHnc57Eyfu+jSgNiYP18u9QsdvLshBobwTavYBRcmArivdtQiQU1Nj1HJOHvhPnwkj/uLhcYVXy/J71KNm/RetqMJrQZ8Is9B4383KbynohIASEgBAQAkKgExBw7tuAAVtWYODxHRhcWUBZR/QgiOrSfBTozkNfKHdOXorjy/4BkcxuEvNCwZH5VRGwkIAxeMVyetYCWsyLpmJgcGYSXqdiYygHhrvegeGjD3Fe+truJkHDo7kwWs6BYSw9hxGv/xwT938KJ52nrX74gQgYV3XLr9tGpHNRmpswUvauo+wkm3DDwXWNspPw8LjTWbk4kzsZZ6ffCtfUBdftfOXAQqAlCLR7AaMlLlL2IQSEgBAQAkJACHQxAgEa/e31oP/Hf8DcNS8iM+yOA6C+fleaU1GR0guH85agaOmXEE3NjOsnFSFwpQRMJGD0/+R5ynITBMe24BgYAZqstMx1VWJjY9io3VUfAyMxjWprxMAwVhRj0Du/xZjdK5AVqEF61KedlggY6u50rDlnJ4G7FmNoWNBNe9/TTj7mUdPqXoMVn899Eqfv+SYiDnZiyDC5jnWX5WwVAREwFAmZCwEhIASEgBAQAp2GgHPfRnJefEzOi51JnRc8NjxIU36vMTg++VZUj5wC//CJYq/uNE/A9bsQKwkYOZ8up1grAc1dwTZ/dluoWBfqzFQ711lQ4ywkvjbKQmLwumGnzDzdD27BqF0rMKy6QDstETA0DB3uB2cn4Xt6w8s/xJwdf9aGkZjjBi9xml4jTmYPxZlhU1CUdxvck27scNcpJywEmIAIGPIcCAEhIASEgBAQAp2GgCHghcHjpi/gr2DO2heRFXIlvTYXjQuvNTuwZ9ISnLr324hk95LYF0lJSeOVEkgM4snihCqxX8VVu58W/DQWIEgDSoJtJGCo8zGfP4WJf/g+8grWak0VlgysX/QMzt/yGKKOFMr7alVdZd7eCZDrbOCbv8SkTW8hPeghV00g7oz5eePwxTUmJ9bf+g0U3f9s3HqpCIGOQkAEjI5yp+Q8hYAQEAJCQAgIgcsS0JwXmz/CoIJdGJQk5oXaQX7mUByZuBAVo/PgHZlHL2tOiX2h4Mj8mggkChiXi4GhRA12YHivs4DhoWEG+X3G4vSoWSifcjP8IyZdEwvZuA0JUCwM+5FtyD68A8N3forRpQcbHZx8GlpWkrW3kIDx4N80Wi8NQqAjEBABoyPcJTlHISAEhIAQEAJC4NIEfF6KeeFC/09fxew1v0d2qC5p/yDlW/AZLdgxbjEOP0hBO3v0EedFUlLSeKUEQuT8CXvrkP76ixi+/g3KQqIHoG9ODAyOjaGCeHrJjcFxLzjzSG0krC1zXAzOQuILh7SBAQanE70eeg45T3/nSk8zrn+iA4O/0NeRiFGcMRC77vgaqhc8ENdfKu2fAA8lGfrqzzBj259gp2fQSr6L2OIx2rBu9uM4dddXSbhN08Tb2PWyLATaOwERMNr7HZLzEwJCQAgIASEgBC5LwLl/EwZu+hADC3ZiYOVJ2KP6y2PihgXkvMifsABl5LxwjWLnRYo4LxIhSf2qCJRtWIGKdR9iyIljmFR7FmbKIMKFXx9D5N/nGBixOfRUDAxLfbsSMOoo64iLNg1QXAM/ZdRzaRlIIiRk6AIGaRuItpKAwSmFOVbCBWdPbF76LMpvfVS7BvnRgQhQ5kbn/o3oTpkch+9b0ygrCd/fwswcFOZMwJmZd0hWkg50a+VUdQIiYMiTIASEgBAQAkJACHRcAvXOi74rX8McyjbSLVib9FoCmvPCip3jb8fRR/8JkW69k/aTRiFwtQROvvhTlL75K4wxmTAtxUlZRwwkB+gBOjn+AC/zpAq3xbazgFGnuS7YfQGwUBGhHpoTg9r9JGDwFCZRI0JDnlrDgaHOrdSahXVL/xYldzylmmTegQhwVhJDbRXGUFDPefv/Ss8dS1PxxU1OjPXsxLjzKySIpWtibnwPqQmB9klABIz2eV/krISAEBACQkAICIFmELAf2ore2z7F4KPbMKQsH44mnBcnMnJxZNx8lI2dCQ9NWoDCZuxfugiB5hJQAsYoEjAmkUPCTgIGuys4AwnHwTDTxHVVuJ0nbuNJOTDcJF64Sb3wkvPCSy4ONzkyPLzMQ0hI5GBhA63kwFDnJgKGItEx51pWElcNhr32c0zf8Q450kKwJQwl4eF0hZmDcJacGKdnLhEnRse81V3yrEXA6JK3XS5aCAgBISAEhEAHJ0AR9w0eF3qtfRuTP3sJPT1lcND3aqP2Tbvh2th54TXZsHvsLTh8/98i3GuAxLxowCNLLUhACRjDjUZMcKbAaTSQiEHxLEhwCNBkpWWuqxLbzutcJE64tZgXegyMGhIsarR6VMtS4qG6t5VjYKhzEwFDkei4c46F0fe9/8WYze+hm7cc2RFP0oupM9qxbu6TKLzzGXL2pNJD6kjaTxqFQHshIAJGe7kTch5CQAgIASEgBIRAswnYju9Dt92fY+ChTcgt2o/0sJekisY26ROZQ3B04iKUUswL94ipZJVOlZgXzaYsHa+EgBIw2IExlRwSNhIwOOYFx7poKgYGt7Ohgl0VPvpK7r9EFpK2iIGhrlcEDEWiA89pGAn/ncw4ugujtvwVY0r3J70Yv8GM/F5jcWrMbJRNmgcfxQaSIgTaMwERMNrz3ZFzEwJCQAgIASEgBJISyFj/PsZ98iL6lZ9EZshN9nw9YKLqzFIGpwzcNmIh9j35XYTYeUGWfilCoKUIBD0hBN1+DlShlXNv/RsqP/gdxppNmJ6Sog0Z4RUsUPDET1/sE6ja2Z2hT7S/SwgYegyMCIL0whm2paLPw99G7lN/R3u9+pKYhUTtSQQMRaLjzw2uWgx9/eeYtv1dOCN+2Egoiy2ceabGlIKSjH7Yc+vTqFz0cOxqWRYC7Y6ACBjt7pbICQkBISAEhIAQEAKXI9Bt9VuY8cF/o7frPKz00pc4dMRHL3luskbvpaEj+Q/w0JH+AFn7RcS4HFlZ31wCJTuLULz1NMJ+/YUwcORNoOBdjLNZ4gSMxFgXav+qnYwamrDhpSEk3ksIGBwDw8tZSSx9EXAORe6DD2H4Yw+q3V3VXASMq8LWsTYKBuA4uBXdaBqxZxVGVObHnT+LvQGDCRW2LGy67esooaCeUoRAeyYgAkZ7vjtybkJACAgBISAEhEA8AY8bJlcV+n32J8z8/CV0D9bErdedF0ClOQ1l6X2RP3EBihY9omUdMZC1XwSMOFxSuQICmuPCFYSeHdWAMyuP4synB0nA4JS9Rli96+DwrscEWwQzU0wXHRixsS6aioHB7SqIp5ccHV4aU6JnJOHAnVGaAI6B4aZ5jX0CfGnTkbt4DobcMYuOTd/QDVFY0xywpNiv4IoosOj5Uxj/0g+Qd3wtvcY2DMESB8YVYWz3nTkribGqlLKS/AhzDn1MTyuLZuwBaigcC2Pj7Mdw+vYnEUnNQjQlrWGlLAmBdkRABIx2dDPkVISAEBACQkAICIFLE3Du34RB6/+CQSd2oX/1GYquzy+PDSVA/zT3Giw40Ws0CqbegurhkxDIGUNZR5wS+6IBkyxdBYGSXSUo3nIeYR8PBLHBXVQG17kLiIb5ddAKY/AcTKFCTDIcwo32Y7DUjy25XAwMM+2OY2UoAaOOso7U0UZBUkr85LhwUwYSVzhCIgY5MCIGVNry4HHORdqAvkjr35PGp7hgsofRb/Y49J024oquzHThDMa88mNMObJaO19z/UutCBhXhLHdd+asJMbaSgz748+Rt/svTWYlOZc+AIWDx+PUnLskK0m7v6td9wRFwOi6916uXAgIASEgBIRAxyFAGUdMdZXot/bPmLH2ZfQIVsedux5PwIBqowMVju44OmEhChc/RUNH+knWkThSUrkcgaA7CH+dj2JbGOIcO4WrC1G48iRCXhqKBHY6BOgzNk306o+ojeb6EKXx/vexwLKCso6EqN/lY2BwH55YwNBdF1HUkoARIfGCxQ+tjdwXHAODBYwq+1y4bPO1PUc1O0gtCRhB5N42Czm3TdPa9aPS0BbSVmzpabCmOKm9cTGVFGL4az/DxIOr4NRSbeoBPUTAaMyqo7cYXHXo99flGLX1r+hOWZuyw66kl1RjcmDDnCfo7+cT5MTIpJS9lJlEihBoRwREwGhHN0NORQgIASEgBISAEEhO4HLOC35VDNLbWkH34Tg6+TZUjZwK/9Bx9I/vFHFeJEcqrU0QuLDrLM5tOIGI1wAjvczp8gIN4Sj2kevCi2iI5QYLiRtGWk/RV+jrdoQmg9ECk9GK0b4/Y47xQzhJwLBQVxXrgpd5UiWxXTkw3KRauGnYiJf26bk4UZ1EDE/EiFrnPHgdC+qPGyLPBPmOTGGkDuiO1H70wkm/CdGon86JhY0oBs6bhX7TJqvDxs1NJWdxw5v/hvH7VyKVAjzaNckEEAEjDlPnqISCsBUcQHr+boze9JdLZiUp6DYcZ26YhvPTb4Vv3MzOcf1yFZ2GgAgYneZWyoUIASEgBISAEOh8BAzsvKitQN/P38X0z19Gz0BV0ousNVhRZcvA4XELcHLJVxHuM0icF0lJSWNTBILuEAJ1AZym2BanVlBsCw+lQTXR0CMKcMhiBQ8b0Z0XugphJLHCZLKTUBBEOOyjGLEkYFA9x/sJJkQ/RbaxBllGrzYMhLOMWGmzpmJg8DoXOTDc5LLgeBcc96KGBIsarR4l94UeA4MFDFfKfPhTbqFjhrRJd1uwc8JPwgU5R+Al94YH4UgVCRgRDLvjdgxdvBDWDCc5MeJjZBgritH/r7+n4I4r0cNdiqyIR8NTbsnA5vlPoWjhQ4ikZNAQrBStXX50fAI8lGTIm/+ByTvfR0rIA0fCMDzO3uSiYXglzt7YecfXUHHbYx3/ouUKOhUBETA61e2UixECQkAICAEh0LkIOA5sxuAv3qWYF7vRr+ZMo39sq6s9njEEB6cuRsXoPPiGjqcAdOnivFBwZN4sAqV7SlG8uQjVBTVwna0lpwUNCTGaYaSXOaPJSq91ZtoPT3rRhQ0TOSDCmhuCRQ5uSwsVUWrfsxge2YBx5h3UOYIQCRgq1oXanoeHcDsPfyK9Aj4awuG/RBYSjoHhIwGjJmUh/M4lmutDH0LCw1g4FgydB1jUcGkCRjTiAyXjQfqAPsgaNhD950xAnyk3UL+Y4vXAfuogsg7twJgtf8Hw+gwVboMNx/uMwZnRM1E6ZQH8FEtGSichEPDBkb8H2Ue2Y+T2jzGi/EjchfHzSCE/UU5ZSTbc/i2ULv1y3HqpCIHrTUAEjOt9B+T4QkAICAEhIASEQGMC9c6LPuvew4w1L6NXoLJxH2rx0culy+zEgVHzceSebyHUL1ecF0lJSWMiAY51Eaj1IUJBMzlYRNG6cxTj4hSCLs7G4ah3XdAwEnJaGMlZQUvUj90X3J8nXm5cN5BTg90aN9BQkml4nyQPPc0qNSYtJngoiKcXARIwAiRS1EYdqI04KQ5GSHNlsHvDT5O2no7nst0MPw0hUc4L8mfQMrsvojRFSMBwk7jBTgr9PGnwCWUnsSLn1ukYePMErZ2Hvtgz02FN1WNjmM+dwKSXvo9pJ76g9SyHGFFtTsW5HkNxYPGXUTtnidYuPzoPAUN1BYb+6T8waedfkZrEiVFD7qPNsx7BmUUPI5zZHVGOhyFFCLQDAiJgtIObIKcgBISAEBACQkAIxBOw0dfBfhs+wKBj2zCo4iRSo/yVuXE5kTkEh6csRtno6fBQzAtxXjRmJC3JCZTuLcb5TScRrOPhFxZ4S4Nwn/OR84L7m0nA4CEhHOuCJxpGQplGtICdJEhw3AkDB+/Uptg69+H9hcmJUYBuoeO0LdebLsPCWzDctI9iXoS0mBfHopNw2jSDho34EQgH6vcWpTkvhxAyD0TE3J92SLEuNOcFnzDF4aAhJxHaR4QuIKoNC2BXBveh7+nkxHD2SYejJ8f0CFDKVTsG3TgLfadSnBgqiQIGx5SpMTpxPjsHe+94BjU33q31kx+diEDAC8eRXcg+RE6MnSswovJo3MUFSFY7n9YfhYPG4eS8eyQrSRwdqVxPAiJgXE/6cmwhIASEgBAQAkIgnoDHTen+KtBz80eYsPZV9HFdoG/hlAJQ+5rc0PWi82LMAuTf96zmvGhYK0tCoGkCHOvCX+PHuS9OoXDVERIwwvS6zrEhyHWhzdlpQQE6efiIieNe6M4LfZ2efYRdD7qAoepeqrPzguvkhgDHokgsLGSwK0IVdm8YKejnu5iClSReUAwMclAcMtyBM867yUlB7hASMXgbdlbo++T98j64rjsvlNMiTAJGmNKt6gIKz1VMDN6ehpZE6kjkqNP2Y0u3Y8jiW5C7aA7sWWlwVhfHOTCYiIviyhRTWs2dS7+B6pvvp/1J6UwENMcOPTOm0vMY/8oPMSt/pXZ5/FTGlmpTCjbNfhRnFz2EcEYPcmKkx66WZSHQ5gREwGhz5HJAISAEhIAQEAJCoCkCVhqb3XvTxxiYvw0Dy48hPeyl74Bs6Y8vJ9MH4+Ck21A+dga8wyfLP6rj8UjtEgTK9pXh/MZzqDlRS1lF3DTgn4UKFi/IpqBN7Kwg4cJAr/E8XXRecLuJJo5ewQIBCxuqzi4I3j62TtW4wm4IFh30ogseFqSHjiI7dIx8EuyvAGrNuXDTpMfWIFEiEqAYG+ym4LUc54LdSDzxeXCd13EfCwkVfHwWUIIkVpBjQ3Ni6DEyWAyJak4mio1hiSK1by9kDhmIwfOnYkBvW5yAwb9xQZpKHL2w8a7nUHHLI7RfKZ2NAGfQMVZcwNiXf4gZhz7Rnt5EsdhPgVROZw7CmSGTcHb2XfBOmNvZMMj1dDACImB0sBsmpysEhIAQEAJCoDMSMHjJeVFXieytqzBu7RvoSwE7U+hly0yvYrGFnRd1llQcHHkjjtz1DYQGDJWYF7GAZLlJAkGX7rwoWk+xLlZRrItafk13ktPCVu+04Jd/A/3Hw0DYTRE7NISdFYkyGjU1q+iOCU1kiBMw1HFYcGCRgUWRhuedfBNamyZEkIgR78RgIYT7KycGr2e3CMt9JGZQNhLOUhIJK9GE+/P+eK+UCpZ+t8JhDyypNnJizMMNo7ph1oe/wqzz26lPQymxZuOLu/4OZYufaGiUpU5FwFhTgcHv/gbjdnyADH8t0ij4a2zhrCTsNypx9sDWJd9G+W3yLMTykeW2JyACRtszlyMKASEgBISAEBACCQSsJw6g57ZVGHBkKwYWH0ZGyKWJF4lfA09m5ODgtCVazAvvkLES8yKBo1SbJnDReVHggqeYYl0EOcYFB+vkGBfspuBMIywq8Is+uxuUw0I5K6jpqgo7JFhAUPtTO0l0cpDwoAkZ+voIiRY8qWwnl3disKDB++QYGDychAUMFmF0p4bu2uBzIdcGCRghylZiMPmR0i8LfdNDuLdsI243ntcPXv9TBIw4HJ2zQllJ7CcOIvPIDoze/D5Glh+Ou079t8GAMitlJVn6LErv+FLceqkIgbYm0OEFDE/lBXjKS+j/a/jXi0Yvdu+FlG592pqjHE8ICAEhIASEgBC4BgKp28l58cH/YuCFoyRe0Jdh7aWrYYf8FZDN/HtyZ2PPo/+MQM4oerG72i/iDfuVpc5PgJ0Xvmofzm8oQuHq0wjW8NOU6LygFKiaeMGChnJekJshykNJWHjQnzWORaE/d1zn2BLk4tCeQ+5DbogkdcqVQz1p2IYWH4OdHKoktut15cSI0JfwMA0DMVEGFM6EosfE0J0aDTExEp0YutOC10coowmLGFrRYmjwOrVexcSgdLGoQxYq8VRqIe7L4OwlDUUEjAYWnX3JWFWGIW//F8bv+gjpQRec2nCjhquuMqVi66xlODv/QQSzeyGantWwUpaEQBsS6PACxrEVr6Ng1Z9o3J8uYAxZeB9G3PFEGyKUQwkBISAEhIAQEALXSiBjw4eY/t5/YGD1aVjo63Gi84LHYdfR1/KDw+fhyL3PIjhwmAwduVboXWT75jsvOMYFB+800ZASGrykCQD0LBotVLdSnV76SVDQ6xRvImndT4IDOTloG309CwaJTgsFXndLNI6doTsx9BgYJJjQ+XAg0SuPicGCii5ksJjBeUz0c2ERhGJkaDEx/BRjowaZ0VISMApIwC3SfsgAAB8eSURBVKhVJ6fNRcCIw9G5Kz4v7Mf2IIsyQI3e9hFlJcmPu17OSlKc1hdnB45DwU33wzXl5rj1UhECbUWgwwsY2/7nX7Dj1V8iEmJLHDDl8b/HzOd+0Vb85DhCQAgIASEgBITANRAwuGphrihGn80fY+q6P6KPrzxub/x5gl+7Ks3puJA9CCfGzkPRjfch3GeQCBhxpKSSSCDoCsJb5UfxxiKc/ew0AtWXcl6wM4JdFBSGkwQIo9FOL/9BzfWgCRZanYZkUDBNFjgMlKGDM3uwCMDrDQYb1fUhH+yW0Nf7EQrFxxPQDnDxBz/Z/ISzm0N3cOh15cS42FFb0AQIcng0ODF0YaQhGwl34xgXLJo0ODWadmKwwEGxMCg7SUaEBYxDuDe9SjuW+iEChiLR+ecXs5IUn8GEP/4E0499pslmiT63SjM7MR7C2ZseQEicGJ3/wWiHVygCRju8KXJKQkAICAEhIAS6CgHHwS3IXfMnDD6xC73rzsORYFsO0csdvSLiZI/hODzjLlSPnIbggGGIOlPpy7T+wtlVWMl1XhmBsv1l4ICdtQV18FygQJlNxrzQnRdqmAgPGTGQ44fdCvzyr+osXNBDRxoBv/hzO9epHwkZyeqa64H6NV2U0NAQzFMP9MnbNN6O42FwHAzQPvl4el8WKniYCIshqujbN2Qn0Ye28HAS3k53YvCHPxcte+gaQ8imISSPO/bhnvQKtRNtLgJGHI5OX+GsJKayIox69SeYeuhTCgsbaRRImd1wRWn9UTh4HE7deD/ck27s9FzkAtsXAREw2tf9kLMRAkJACAgBIdClCGR98R7m/vkXGOAuSnrdFMKQciaYcGDoLBx44B8QzB0jzoukpKQxkUDhZ4U4/ucj8JVxQlAHOSUcMJk5XaoufOlpTJ20WfOEMI5FwUE/I1EadhGip9Jk0+rhMDstvDCb4+vshrh0oWwgJCBwPA6K4kaTh+reJjZh54WXJhY9uOjZRHCJ/hx3g+NvaL0154Ye1FOraxlW2G3h0tZnRWtJwNiNu9PKtLr6IQKGItF15sby8xhBDoxJ+1eQoBzWRIzYq+enmoXlEkcPbFz6HGUleTx2tSwLgVYnIAJGqyOWAwgBISAEhIAQEAJNEWiugHFw2BwcWPaPCHLwTnFeNIVT2mMIiIAhAkbM4yCLzSQgAkYzQUm360ZABIzrhl4OLASEgBAQAkKg6xIw1FXDUl6E3ltWYOr61xvFvlBkaow2lDm749jY+Thzx5cR7perVslcCCQlEKjzw1fpwflNF3BuTeFlY19oQ0E0FwZnFVFZRtiVEV/n2BjswuBhHJH6YJ7swkhepzgW5HpQ++NhKKrecNJNZSFRkV8aevKSykrCw0V4v7rDgh0ZKpYGnzNHLOA6r1dDVPT+PKQlEuYhJHxePOykmpbYgRGlLCQ1eMS+D3ellRKJqDbRCnR1B4aBYpgY/W4YiHdsiVjIiWMl54yWoSZ2Tcdf5mwk/T98ASN2fYIerjJkheMz0/DTyRmhLti7kwPjb1C+WBwYHf+ud6wrEAGjY90vOVshIASEgBAQAp2CgOPgVgz97A0MOrEbvVzFjWJfqIssyMjBXop9UTlmJqVOHQ2kpKlVMhcCSQmU7S/GuS9OoPaEB94Sej0P8tAOO8Ws4GCb/JJPQTa1lKkq6wgF7dSyjHDMCz/1MSfUOQsJTdTOYocmHpC1Xo+N0XRdi1lBQz70rCQU5JNjWFwcAsKnzrEq+MWYz4niaVysU7wOTXygppiispLwOXIwUT32BQsSelYRkNnfoEUs4DpPyWNhcMyLME0qBgZnJMkkAeP+lMO4PeWClj7Tqp2XCBjW6rNILc2HOSEYqytrILw9RiBq5vglnawEfLCeoXTW+bswdt2fMarsUKML5HC4pbZsrCcBo/T2JxutlwYh0JoERMBoTbqybyEgBISAEBACQiCOgOa8KDuH3ls/JefFG+S8iB9zrzp7KMtDjS0TR0bOQT45L4KDRkjsCwVH5pckULj2JAre3gNvWZRes7JgMqaQAMEv9ywSUIBOTbxIzDpCAgO98EfCiVlGGuq6YMFiQ3xhZ4UuRBg0UUMXONjlEJ+VhIUHziDSdNGdEw3BOdV+9S30LCQRTQRhcUS16k4MFjRIoNEEDHZeKGdGgxNDOUDYhRHWBBA+F47DEUCmsQZ3ph7Hbc7zSKMv7ikkanDprA4MIzloTAE3jOSsMJoa31Pt4umHs/IUsi4cgiWoxyZhhhEKdFmdlYPaPmMQMfNz1FB4HYtDkbCevtZgT4MhJVN7Lhp6dYwlI2WHyn33txi75xNk+muQQs9vbKk0p2HH9PtROO8eBHv2RySjW+xqWRYCrUZABIxWQys7FgJCQAgIASEgBBIJOA6R82L1G1rWkZ6uC006L06m5+DAzLtRPmYGvCReRFPSJfZFIkypJyVQuPYMCRgH4SdtzGTKoueG3BeayyHeeaEPt6Cf5MpgZ4WeZYRcGZypg4ZYaG4Ncm1crBvNJIbY6JjxiSXZDaE5NyidqolSqPILbFRzcnC6VbO2Lz0jCWcBiR+KEH8BasgHZ0DhKd6JEZ+FJHY/7KZgJ4buuGjszODr0TOqNAwh4b4sYPAxw0g3uXFnej4WOc8iM1iH1PpsQJ1VwLDVFSOz6gScdO8sVnbYJBcxLD43bL5qGDXHCnlVgkEEAgH4rSkIODIR5cw0McXr9cHr9cDn8cLr88IwYCzMN+RRspoO6NTweWA7cRCZh7dj7Ja/YkTl0ZgrJZmNfmdKnb1wdsAY5N/8EFxTF8Stl4oQaC0CImC0FlnZrxAQAkJACAgBIXCRgMFVCzOl5+u1jWJerHsd/bylF9fFLrDzotqWgaMj5+H4XV8j58Xw2NWyLASaJOCvDcJX7kHx1hIUrT2LUI2RXkxJ+CJnQjLnhdqRwaA7J4wkQBjo+WPxgkUJIwkWRhI/Ilwn5wSv55gXiQJGmId0cEwMGmbCAsfFmBhanWNm6Ou1FKYxAoYunBjoeHqsDBYU2A2hny+fc0Crc4wKdnjEx8Agd4k2HEaPedE4JgY7L9iloTsxeH1sDAwtFay2jvtEkGL0YlbKScxynMWQcBn6UZ1LhTkdW2ctwzn6yh7q3hfR9CytvaP9MLFQQZlitPgeFAQ4tbYIPSrykUaM7HbOLmMiZwsPCyKpqz5IcLK6Jl742UnDohDfN743DcXr9aKurg5ut1ubIoMnwEgusggJHaEQ7d9BQ+Cc7MiIF8Ea9tB+lrTro+s0F53E+Nd+imnHPycPU0N8FHWm7MTYTk6Ms3Pv1p0Ymd3VKpkLgVYhIAJGq2CVnQoBISAEhIAQEAKxBGw0nnrA2reRc2wb+tacRdpFC3xsL+BU2iDszbsTFeNmwZ87CtHUzPgOUhMCTRAo21+Gc5+fRd1JLwIVJAwEWZBgISC580LthoUJjoFBb6P0vwbnhbHemaEPCdGDe+oBP9WW+jxxfUOdnR0cvJODaLIDIj4GBoshRnJsNLSrmBg8fIOX450Y8TEwQpqgom+fLCbGpWNgaI4QLc4FHycAs8GDLFMVckzFWGI5htmOau3ivMSvkOLQnBk6GYVz74Jv3EytvaP9cPoq0NNViDRDEFabHXYSpOw0LMJBzguHgwJy0tAPFidYvLDZSISiF3c/CRUsbMTWuV315fXJBAwWMXw+n7Z9JJUEn/Se8FCbi4SNyMCxMOROpeFwfG/bf+HrMxUXYsQbv8DEQ6soPkqQIq3wM9NQdCdGT3JijEX+gofhmnJzw0pZEgKtQEAEjFaAKrsUAkJACAgBISAEdAIGtwumymJ03/kZxn3xBvqReEHftLUvebGMPPQiV2XLQv6I2Ti2+GkK2EnpUunloSN8qYy9DlluewL+Wh88JXW4sK0URZ+fQ7DaSC+ePOSIhQv+j10TbOFnMYO/fMd//WYRgMWExBgYukOi8dACXZBgB4Tu3KAdaiUYKkIwVEjLscM7aBiLsTfMpn4XhQy1no/Ljg/dWaE7JfR17LwgIUU7Xz5nPVtJOFxNbo4q+pJP87CbzjkDZnMGCS98fTzxiyUPJWlwXjTUaZ8s0NCLuubEoIwkWkwNbRvuz8fwolu0HI87duPOtHJq460NqKNzLEzpiw3T7kPZtFuR0rsnHJntW1i86LggQYpLpr8cA1w0PMZIL+AkUJjN7K4xwmKxxAkUiYKF6sPCBQ8fYfFCCRgsUihBQzsI/eA2nkKhkDZxX+bucrk0Z4av3xiEcqcgSsMveJ2RBA5jana7/jtnqCxBn5VvYNiuVehTfQbdQ3XqcuPm5fQsbmO3DgldoR70vHdQt07cRUmlXRIQAaNd3hY5KSEgBISAEBACnYOA9eQh9Nr0EQYe2YKBFM0/M0SB8+hVKf4VkpwX6YOxb9a9FPNiJvwDhiGSmiExLzrHI9DqV1Gy9zzOrD6K2lMeBCssiIYoWwgJFiwQsLOCA1tylg9dEOAX/XhRQhcqksTAaGbMCyWIVNe9herat+iFlQWBhpJivxWpzvvppTmVpoagj/pQFY6LwcMRWPRIjIGhOzFYzGBRwu3ZTvEV9iAUKaIX5zISRfrSNAA262iahlMftX2skFG/j4QYGPoQEj5P5fbgYyQXMNzE7wSF9XzLNBQnB03G0NtuRb+pUxousB0uOf3suDgLR9CtnV2KIYxsUwg2uvUsSlitVk24YMGC6ywysKCghpCouhIilJChYmBwuxIyYp0YSsBQwoUSPrg/b8tiUB0JTn5aDpLjwzx0CmyjZtPfuvhYGu0KKWUlsZw7hfRjuzB+7esYVXow6enpbp3B5NaZ0qHdOkkvThrbFQERMNrV7ZCTEQJCQAgIASHQuQg496zH2Pf/BzlFB5BO4oVKz6iukmNeVNmzcIydF0u/imDOSLVK5kKgWQQK157A0bd2wnuBxuubutELqZ5hRB8aEpt9RHcq8DAQfWiHHntCOSk0wYO2ZWGB053qMTBo+4R4BZzBQ4uJUR/zQgkYpZU/Q/GF/yaXRPwX6uzMpUhPfQp222BtUhelZyVh0UGVZDEw6gN0Rv2odX2COtcX8PlPIxA8D6tlEBy2IUhxzoHTMZ0ECBWUk+MyxDsxWCC5VAwMvb+fHBhl9Q4MPTsQp8ukxLI4FjDh+Yo0HMociQlPPoVht92iTrpdzU107+wUcySL8uf2qzuNVMqowiKFcloo54USMFiY4MKCA4sNfK+5PwsYXOeJh5bwdjyxEMGCBbcrAYNFC+7PhUUKrvM6nri/2g8vX3Ri1Ds1ojQ0xzJyLgzkwkAKBZxNeNa0nbaTHyaKYZTz/vMYvXcVMr0VNAyQn7eGwm4dNw05Op8xCDvu/BZqbrqnYaUsCYEWJCACRgvC/P/ZO9PfuKozjD8znrFn84z3bHac2I6dQsCQkBBDkoolQAqB0JSCVMr2HxSpf0A/9EMlkFqVD61UtaWV2pIuXxAiYk9KIpHEhCUQsjg28YyX2E68xGPPjD3T97nXr2fxGNtpSCboXDi598w5994z79iW7m+e93nNpUwETARMBEwETARMBLIjUPrRW9jyz5fQMHhaBPEimZbHrMztXGA1Pmt7HAPieRFf8z2kSgtbmp65dnNcGBHo/uBrnP7HCQEYhA5SttLhtVJC+K12uvoI4QWVGGLsOaPMYHURggpWCqFSwyl7wg07RWQxnhe2x4VGYT6AESxthdezGX7vDmn3yHT72/a08kKvQBUG1ROqikh7YCRTkoIwvh/jEwekusU5eUiO5ACMNjlPq5FQWcHGa6mZJ1NI5DhJpQEfvqm40PvZSgwqOCpTQwIwjkoKiW2yy99WQowzMQdeGQrh8/KbChpgBKdGUZvoQ+WUlP0U9QUVF0wZIZQggFCQoX15e9amygoCDZp6EjYQXBAo8DzCCM7JBBcc5zyOKcBQcMExVWbwHEKNaFQUQlReSOMYX4sVB5DwBJFaezucjXdSHqJLKry9VFUp6RYV3cljuPXAPrQMncxaI39WxJkFvb7lOPT4zzC086mscdMxEbhaETAA42pF0lzHRMBEwETARMBEwERgNgKOsUso7u3CsqPvYNOHr0nVkf7ZMR7woYjf2J1YuRHHnvo5Jm7aYjwvsiJkOouNQPhABGde+wLRPlbw8MgzoAAMl0+OCQoEMlheEvTA4MMhAYaYZ1rVRORBfppmmAQRNPy0lRmLva/OU+AxeOlX6O1/RRQYl3XI2ntKVqDEs1zgxYMolVQSe12i2xC1h1NgS3ojUGCb64FBgHE5ul/aQQEYHfMADE0d0VQS+3opC4gk5CFbVBkWvCDAyPXA4D3jMwDjyCzA0LWdlUv+ZjCEzyo2FCTAcInyokQUAdVTF9EQ70OZwBhN+1CAoUoMAgpVXuj7U6jAOZxP6EDIQIBBtQbBBKGEqil0vgINvY72MwEGx1iVhOoLjhN2cJyNUMMCG/WtQNNWOEorRY3BSi+5SXZ6h+u/d4U70PrXX2LjqfcECc71MxpyB3F0y16E734U8RX1SJZXX/9FmxV8pyJgAMZ36uM0b8ZEwETARMBEwESgMCLg+fII1u3/Cxo6jqEqOgBvji+AFKm05MYn12zFiSdfxGTzbcbzojA+uhtuFfMDDKaPUHlhfzdsKx+YEmIrLZz0HWCVEj7YywOwpcQQuLFUGb+dUhLD0MjL6L/wuzkAo8jpF2DiQaj0PgS8e+WetgKj2L0SbteqjHgreFDlhUAHVVVYKSRv4vL4+1YKSSyeL4VEz1eQQWUFVRlyvZQAw2TUSn2RxBnpqzJD1R5UZFCBQRPPdgEYdgqJLq7QAQaVF3WivKiaHkVAUkh8LvnkZ5QXhBWaMkI1hQ2ssgGBKif42XOckIGvEVRQZaHwg30qJzJBBUGEKjB4rCkknKMbK5Ow8Vq8ht5vdHQUbNPekKjPKkSJsQmOJoG5VolcPbuw9q6+r7HutV/jlhNvo3QqKn/b+TOU3uLy+zXgqcL5ug049cDTpipJOjTm6CpFwACMqxRIcxkTARMBEwETARMBEwF5NhPlRUlPJ2qOvYtNh/ZhVbQvKywqSb/k8qNPcqU7N2xH+P4nMV3XlDXPdEwEFopAbERk+X1SfeToACIfhBG7yModLE3qE9m/fwZeUHnBB0k+ZBEMEFDYSotMzwv6Ufx/AGMSg8Mv48LA72cBBiuMOKUlBd7R2LO0dIukkmwXeCBKEAEKft9m+DxbZU38rWBbyAMjDTDogVFCT43iRvG/EA8Mz50WgEinjlBhsRDAUOBhp5LYCgwCjGPzAozjgXVo3LkL9TvuRumqlfCWUy1w/TYqLzyivKiR6ixNiUGUOSYtAJEJLDRdhPCCjX0CB8IEwopv6hNIUInBOZpaogCDgCJXiaEAQwEHr8/GaxBgcJzncJxroCqDja9xPfHVtyLZsAVFoWoUBauuX2C/4c5OqUpS88F/sPb4e6jtP4Vl8Ut5Zw+4y3DkricRuXs3EsvqkAqJz4fZTASuQgQMwLgKQTSXMBEwETARMBEwETARsCPgkfzodW/+GY0dR1EZHZyjvOAjpoi70VW1Die2/xjDG9qQWLEG8AVMCE0ElhSBgc/60P3OaYx0jCM2JGUp40wRYVUJVvuQKjZWyggVCPy2XVUNaQ8M2/PC/radaSD89p1wY6nbfCkkLleZmIoGMCUlT1n61F2yHG5PjYgi5J7TRSgP/QgVoecENMgaRQVip48obOFe1xwTCKIeGHYKSSwWEXDRIBBjHXwld4k5qPgnWGoLKitywQRjkC+FhPdgsz0weFyRuiQA47AAjOyUL1VgfIRl8FXVoLJ5PZofefi6VyMJSknPukQvlrOCijuJQInLUl7Q64LggCDD7/dbsIDggQCDfcICpm4QTLBPgMC+jmufe6olcpUZuaCCfTZel/N5TGjB6xF88N5shBh8XeEH57MpyJhweRFzy89vSxs8N2+37isfUGFtUpXE1d+N4KmP0frWq7j5wud510eD5nCwDl2Nd6D7nr2YuKUt7zzzoonAUiNgAMZSI2bmmwiYCJgImAiYCJgIzImAY1SUF5FzWNb+LjYe/pcoL3rnzOELI5L33xdciQ4pHXj+wZ9iqn593nnmRROBhSIQOdiF0/s+RbSHdTLK5GGP6SIEFD55MA3KawQXaeWF7YEh/zrl4TavaadtyqlAwi6vOtdUMXdc+7keGF6vXXUkHo9gfPK85fECuTemBWAIxKiu2CMQ4wUUOctkTUFZaxooEGrY3h2EGIsFGPb5cwEGAYWd5pCSh2vbxJMKDVt5YQMMzlkIYJTh0CQVFykEa2tx27MvoOmhndK/9psqL6oTF7E20Y8q1xS8Xq8FC5g6QnBAgEF4kAkwCCw4TshAoDBfnwCCQIOgIXPT1A9VTHBcYYSCCc7hsQIMroXz2SfA4H0VXHDP8xVg8J70y3A2b0Gx/I10BmvgEjVGIW5FAjHWvv4HtHzytgWrg8mJrGVqVZJweQPa90hVkh2PZY2bjonAlUbAAIwrjZw5z0TARMBEwETARMBEYDYCtvLiT2g6exQVUmIv1/NCJ3aW1qN92xMYum0HplY1IhXgg5vZTASWHoHIwW4BGGLe2TMpD/uSMjJbfUQMOUW9kGveaSsx5F9LaTGjxJgpm5qZQkI/DO0XWWaf2X4J6nnhLJJ0BPHMmJaKHkmpZjI08pJ4YKRTSMpC2xDwPYBo7DCGox+JyCImTYCKpI+whYJbJfXj+9I2W1VKUqlxQQPRmXUL6CB0sFQVTDdgGdUrSSEhwOH5tnFnkqkL8l/aA4NqjcWkkDjExLMchyfFp0GuF6xddV0BRun0GFbHelGdnPG8cDstgEFYQChBBQbhBffsa0rJQuCC44QMhAqEC+ptoT+dBBEKPnht7XM+z81VYvAanKPjPJeNc3ltAgs2BRr0wxgZGQHEyNMhnhguKS/t3bBDb19Y+4koirtPIyR+R60H92H9xVNZ62NSFCFGxL8KH+59EZfufSJr3HRMBK40AgZgXGnkzHkmAiYCJgImAiYCJgKwlBfhM1j28fu4/fC/UTuP8mJcHvSGfNU409yGsw8+g1jjBusbaT5Mms1EYCkRmByewOXICPqPXUDvf+l9wZo2Wn3EK8eaBkKQkS6fmg0w+A09vTDc1sN9ponnLMBw2oBCz9M1zg8wsk08KyseRijwNMZjh3B58iDikwNIxOgJw0c7yAN2o6R+NEhlkl0o9e+R29ipHJnKC1tNIWeIUWImwFi8iedCAMMeZ/oJPTAqcBE/8R7H7kCfIKCU6FnstZ6VMqq/HarC4Xi5sA/xnaiqRMN9O7F62zYE61bBV3Ft/A2ovPDKN/3VyWE0Tg2iXHxDFFio8kL73CvQyAUYhA+ZqST8O5SpzFCgQBUHr6OeGZoawtd5De0r6FClhQILVWJoP3ecagtVehBqqBKD87gGNN1hpZO4yyX9SFohbXzPKQE3rt4uNIhhc8vn76Pycj9ylRiD7hDaNz+OcNsuxARaJyuWFdLbMGu5ASNgAMYN+KGZJZsImAiYCJgImAgUSgQ8X7Wj+Y0/iufFEVFeXJxXedFVuhrHdzyFgdbtludFMiAeBQZeFMrHeEOto/+TCDrf/AJjnaNIDDskHYPVRvIBDMIxllElxLDTS/hG7RQSjxzlL6Nqp4SkZpUaPCdz05QRTTHRfm4KSWXFbgEYzyAx1Y144iwmYkel4kS7XIqKB3HrcIYklcWHirI9Mu9ZWZdfXvPJSFp5oXMJMK6sjCoVF4tPISlzjOIJ36fY5Q/DJ8ajoi+x1toRc+KVSwIwElWiNpFytS5RPFRWixdGC1oe3X3NvDBUebHCEUWFO5XX8yIQCFjAQZUS+VJICDbYFEwQGFB5wdeo3CB4IFhQ0KEpJZxH0KCAgWCCTTeOseUqMRR0KMjQvu55Px4rOCHIGBsbQ9ztQ6LED3/rfQhuvD7pOvre8u0ZPweVGL2dKPvqGFrf+xtahk5mTY2Lr8xQSQXO127Al7uew/ime7PGTcdEYKkRMABjqREz800EviEC04kYhr8+jehgH0L1zQhK/WuzfXsRGO3pwsj5M2IotgJla1pQ5KLk1mwmAiYC1yICjrFhFPecQ40oLzZ+uA914z15bzsuvgSDorw4K6Z0HT94HvGmW/LOMy+aCCw2At0HzuGrvx9BtFdMGAUCOBw+gRRS8UPSQYrEBDGtwLBNPe2+GmKmTTzl6VX+Z/lUmSdKDKfsaeKpQMJONcns5/fI0PlzAcZjAiaeF/lEXEw4oxiLvo6R0XdEwWADDLs6SQzlZTtFhfGIpDmsQ4m7WcIgqSaWEWc6IgQYY+P7MT5xABOT5yQNIYJidz28JY1SzUSqkHhpkEizzkwTTz5Ya7nUfB4YHGezlR889gsYaCvpkBZGs/Mial1MMQFyAcY0U2FkC9atxu3PvYDGB+63+t/WP0XJBIoTl1ElZqiNyQHUuKbzel5QRUFgwU0BRqaygmoKVWoQVihEVWDR19eHcDhslTYlwKiqqkJ9fT18Pvqq2EqMTIDBY0spMfPG5wMYPT09iEQiFqQg/OC1qOJQ3w6+lg9gcA0WSFm/FV4x9fTU1Ir6pXbmboW1c3eexKZXf4GNXYfld1DARs7yLhSXo/3OHyK89SEkqMQoK0xvj5xlm24BRuB/AAAA//9mxnj0AABAAElEQVTsvQeAXkd57v9+23vTFvXee5e73CsYg8EGg+khgQC5SchNu+2ff+41SSC5oQTcQmxs3CAYgwu4YNwkuUuWrF5WbVW29773+c3s7H7ft6tiY8uydMaePWf6zHvOGX3vM8+8E/v6Kz199j52a77/t/bSnd+23u4uN4qln/5zO+tP/vF9PKKo6+9nCbQ31Nr6B75v+199xmZd/TmbdvkN7+fhnPR93/Lo3bbxoR/ZmKUX2LzrvmKZ+cUnfZ+jDkYSOFUkkLFtrU389Y9t8qbVVtFywPJ7O4YdWmXuWHvlnI9Z9cKV1jV2ivXlFw2bL4qMJHC8Etj37E7bfN8r1rqvw2KxYotZhoqmWkpKhqWmZSuc6qry8aTxU5ffiWnymSqTKp+i/Om6piu1x3p7uhVOk8+0vr5u6+npcOHU1Ezr6e1WenK43VJS0y1V+Xt6O5XeadX137ZDh29WuFntmI0o+bAV5X1R+fItJZZtjc13W33Tz1V/r0vv7m627u46y8meYpmZkyw/9xLLy7lSaT3yPo/LqD99fV0q/4g1NT9t7R27rLNrv2WkT7DszCmWm3Ou6jhD4+Ab7JSnfI/C3BPX69rs7WWcxEse6sNgftry+dOUvyDWbBNTq+1DmZttZU6D0sy2dcTsuzWl9kJnsYpKHn20YVYwbrwt+uznbcqlF7vwu/Unu6POKpp2WUVPo5XokeZnZVh2dralpaVZamqqpaenS4aZlpGRoeeW4u4J9/b2Wnt7u8sTHyZ/bm6ui6fP5Ovu7raHHnrIfvSjH9mBAwf0DvTY9OnT7cwzz7RZs2bZtGnTXN2hPuro6upy9ff1eXWKcEdHhytLndzjV69ebatWrbKGhgZraWmx4uJiKy8vt4kTJ9rkyZOd2Mjf3NxsTU1Nri/0p7Gx0flOvded6Tk24syrrOLsD71bYv696s3YtdEW3vn3tmjHs+4LTOE9i3Nt+taqcsqtctJC23HJp6xtwTlxqdFtJIHjl0AsAjCOX1hRzkgCR5JAjwC0+p2brHbHm3Zg7XNWv2ebjZx3hlXMO9OKJky3gtETj1Q0iv89JLDt8Qds88M/ttGLz7XZH/4DARiRYvR7iDMqGknguCQQa2my9AO7rGTtszb32fttQt0uqY+9+sGa+GO1RYpddW6FbZu2wrZf+ilrnzrfYlI0YrHYcbUTZYokcCQJ7Hu20rbcv1YARpuy5A8AGIARAA4xgAmBFTFLVzqgBe9mtzzABgCG0gRipKQorDImZRzQAlAjISxAA4CiF4VdAF3sCGEAjj6lA2AcPPTDJADjC1KyR1paaqk1tf7KmlsfVR88ONHRsc1aWrcqvVjpuVZYcKUV5n1EgEeuAI9c5Rt0HsB41JpbfusAjI7OQQAjJ+dMy81eoVEyRnw8gAFww/gEXgwBMAA4fDrlPOBBuNdK++rs09mv2tX5NQqb7e2K2f2NBfZ8e5FVq4kmmpE7UQBGQfthm9K42Sr6WiwnJ2cAoAC8AMTAZ2VlOSCDOIAMPMACgEMALAAFWltbXX7qIW+8u/nmm+2mm26yyspKF71w4UK7+OKLbdmyZbZgwQJXJ4AE9VE/9QXAgvtkAKOzs9O1/5vf/MbwtbW1zo8ZM8bGjh1rs2fPdvUyL9LXAFgAngBohDCgRktLq5VrLh171Rfju3zS3Kce3GPjHrvTpq19ysob9llxT0tC33r1Reorst1FE+3Fa//UGs69JiE9CkQSOF4JRADG8UoqyhdJ4CgS6Giqszfu+57tWf24jV1xiZXNWGiVzz9qDbu32KxrvmDTLvvEUUpHSW9XAi2H9lljVaVlF5cJJJpkKfoBE7lIApEE3l0JpFVutjFP/6dNePM5G1Ozw4q6W4yVtmRYYneemBcrb7DDi8S8qBhvvXmFEXjx7j6a06b2fc/uE4CxXgBGk8bcD0r0AxOmtzElBcW0QO8kbyUKuYCJfpaGEAqlZwokyFRcYF6k6N0UE8Mp+V0O2IjFMhygAagBYwLlEiXzaOHqum9a1cHvxQEYHxIg8TlLTxstgKJcrInt8ltdXWpcYMYvrLb+MdUp1oB8YeF5YmBcZpliVqSnjyfLgAPAaG59TP4Za2vfLqV43wADIydnqRgYS5UX4AYocZB5EZgVMDMYQ5+UYgdkCMzoc0AKYAf58bAyQCY6BWBUJwAYzb0x29aTba915NhvGjNscxvg0IkDMIq6am1m2w4bHWsbACpgWgAiwKwAwCAMiAGzIgAXABQhnTzEAzTwLAknA6rJAMaSJUvsiiuusMWLF9ucOXNc/eFdoD1ABsKAGAAlAA8hDHhBW+R5+OGH7ZFHHhkCYMydO9fVjSwBPwBX8JSlzhAGyGhobLKyi2+w0Vd8nuwnn2tvs/SDu61w40u28PE7bWbNxoQ+AiMC3fFvw/Mf/QurP/8jCelRIJLA8UogAjCOV1JRvkgCR5FAa91hW/3dv7Kdzz1sS278hk278pMK/7XtXvUbW/yZv7AFN/zpsKW72lqtbtcm62iosaKJMy2rsMTqdm60lkN7Xf70nDwxOGZaXsVYF+7UymedKHqth/f79LwCK54wy3LLRg1bf0djnau/p7vTilU/dFfaa6875PJnFo6w4kmzLLuo1IXb6qtd+/QHl1Vc7srRr6O5lpoqx0DpbPZU0+S8OWWjVc8sy8jNT0hqOrDH6tWfzMJil96uduvEZOnpaE3Ilz9qohVNmmlpGVkJ8Q37dqj8Zo1/jBVPniXqsP9B1bB3uxtH/qgJbnwpqcMDG/UCmGivcPxU1374IYOM6BfPpGjCDCfzhsotvm39mC1WX4rGT3dhfgjyzBrFusHF1BayLhw31YWjP5EETjUJZL35os352XdteuXLlt/dJrXQU8nDOFll69J3srlinr18/V9YCzRhp/glQxyhRHSNJPDWJLDv2T0CMDYIwGhUQcCJLL118rAppMB7AANmBpsl2EIh5o8DMEgH4BBgkKp9CAIs2CoCI8OHlT+OiZGi9ximhgcw9Ga793gw7JkdqrLfVdf9YxKAcbUAjM8IwBgrAKNCimyLwA1AF8/AaGi+y+obHpSS26r4BsvPny8FeaHlZAqQyFreX6u+G/WDvja1PGYtbb8TgLFjAMDIypysLSRLBGAsVn6xS8Q48QAGgERgYgDiAGBI2e4VbAETA9ZIAoDhmRhsp2HbSWlfTQKA0aOaWyTfjZ2Z9u81ufZSS7by9YmBMe5d3UKS0t1uae1NVtpbZ1PFCqlI73FbR2BAAEDgASgCkwJAAwADBwjAMyOOfJQJ/867DPoDuABwABhBnltuuSWBgQFwEQAMwAZYG/GO8njAhsDEIEzbAcAA1AC8OBqAEYAVQJDg29raBu5hYOBz5p5j+QsusKzycZYtfzK6tP07bcIjd9j0dU9ZafMBK+yFKTXoDmUU2+tLPmj7ll9m7WIp95SMHEyM7iIJHIcEIgDjOIQUZYkkcCwJvF0Ao/ngXnvj/u9ZzdZ1bgtE6YxFsqHxb7b3xSddk3nlY1z8xPM+6MIo3Ovv/zfb/9qzLlwwZpLSv2Tjz7x02C5Wb1nr6utqbbbZ137J0rPzbcNP/80Ob3rN5S+ZOsfmXvtlGzn/TBc+sG6Vrf/ZD6x22wYXLpu5yOZ89CtWPosfRkd2e198wjb87GZr2Ltj2EyjF58nGxV/PESp3/n0g7bh57dayZS5Lv3wmy+7cFutB1hCZRPOvtLmffyrllNSEaLcdfMjP7aNv/h3ZwNj/vVfs8yCYhePXYxNip9wzpU29/qvW0ZOIhU3VLLhZz9U+R/Z1Muut/kf/xP9gPVU0nVi02xSHTNlxwTbGuvu+Y7bqkI5wJBZH/qczbn2j1w1PZ3tLn3rb+5z4bSsbNk/+bzynKQrJK6X0Z9IAm9fAjnaJrfs3n+wmVVvSC3slTqYuHWkQ7GNaTm2bcJiW//BP7K2OSssJsUgWXF4+z2ISp7uEvBbSF63FjEwYlLazbB7AYCRJUVWV231QJH3AANgAeCZwAgHYpDP28BAaUSp98BEsInBtgNWzVFq0+W1hYRwj2xeKDxoE6NddXogQjfOVdf/S5INjEQAA8AAJkVwzS0PWXPbrwVI7JSiut3SM8stPavYCrKvkI2HQK8Xp4JtMVLEh7OBMQhgLFK1gDmMlW8ShgUgBgAO7fYzL2AIkA6Y4dI882IwP+G+IVtIACY7Vf+mzjS7uabQXmwtUK4eARhj31UAI6PlsBXXbLZy2b4YlZ9lhTlZDsDA/gVsC4ALwIcwv4QwoAXpgAkAAoATeXl5Lr8GOOBIwyZF2GJy2223JQAYbBu55JJLbOnSpcZ2kgCOhAoCUwKQAg8YQp3xwAZhto88/vjjQxgYbCGZP3++GwPvI3mDjwcwHBNDgEa3bGF0ZeZa6VkfPGltYVhbi6Xt22lFG1bb4qfvsel1W4K43LVT319dRpFVjp1rG678orUsOT8hPQpEEjiWBCIA41gSitIjCRyHBN4ugNGgVfvVMkRbJUBi8gXXWNmspXZw/RoBAdtdq5liWJTPXuZYAETAUCC9cf8ul54lhb18znKVW6xV/9lDmBhVrz9rL/7wfxlAyZSLrrVcASKUb+lncOSMGKnyyyy/wqP4Tdq/eGjDS9Zac8DVnyvmRMXcFVY6faFjMiQzMVoOVzl2wuFNr7hyGDEdzrG9o2LeChuhPfDFk2cLUMhz2d4UePHKj77p+s02G7bi0H6X/vGLd8UTZ7h+lEyZZyUwLfqZGK/9+J/s1R9/2yadc5Wd8bVvWo62kuBe+ff/Y6/c9c824+KP2Rlf/6Zliro+nFvzg/9uL8sI8IKP/ZEz/huYGqu++5f20n98y6ZfdI22/9wgma0eAH34kVSu54TccBh+I716yzoXpo7y2Utdf2G3REwMJ5bozykggVhjrWVVbrKK156xBWt+buNaPBMsDA0Fh1Xa2vRC21sxzXbPOtMOnHmFdY2fcVLavkjtbLb09gaLudVotgdI/ZMihBLRq1VqrwTKMkBWgfVkCxwlQ+ROCgkEGxgewBCTIgHAkLIv2xbaKKL48My4Jw4GBoAH4UHHtpDjsokBE8LZzBB7od+mBNdQvqbhn5NsYCQCGIP98W23tb8k8OJVa21/1hqaXhLQJxBdTMni3JWWm3m5mBtlYhaWa3sJgIvsIzT/WjYznkmwgZEIYIQx9egGoMIzL4j17zVbZuIBDMAKz7w4lg0MD4nEbHNHiv1bTbGtbityAEZhaYnNP/8cmzR/Ns0MOLbv8B8OwMSBJgOpSCIxnaTh8ue01cmewi4r7mtz4EF2do5lZchYZ2a6pWeki6FaboVTJllGoWd4AmAEVgZgA6ACyj/xw9m8CNs0KEP+W2+9NQHAALSIBzCSGRjxNi/ibWAAYNB2YGI89thjx2UDg/ooA6iC55447sN2kjaBHOWXfMrGXPmFOImePLcOGNTYM7U4t/Qn/8cW7HnRfXHJYDdMjFeXXm37YWKI1dqbtEh18owo6snJJoEIwDjZnkjUn/elBH5fAGP7Mw9b/ohyKxg72cafdZkYEWc5ObRWV9muZ37pGBpEZGtLx/hzrnCgBuHmA7tt17MPGyyAOR/5Qxt3xiVED7gAYFRtfM3yikdI+Z8tVsJVYjzMcXnYflGpbS+N2oqBKxgz2aUXCTDA1W7f4NKxMTH3Y39sMDLiHcyL9T/9oX5vZdiEcz9gbNkYzh3a8KLqedRgfMy77qtWqJMIcAHA6GwVLVJtjJx/hk0894PauuKBiFDX/ld+Z7tfeMxGL1kpJsbXB4CKdxvAKCgfpbYkczFcJvSzYAAsdv3uF7bvpd+67qVo28q4My618Wdf4cI9He22U+mHN77q7J9wGk3kIgmcChLI3PK6zXjoZpu2bY0Vd9RbVtxKMuPTerUo5hlWWTrNNpz7UWuYf7b1lI62vtw8KV9BkTx5JJEj46MjqjdZrEOrmj1dMtYoCrko5j3dnEbQNaBwNZZNt9ZR86xPq/qROzkkMGgDgy0kvFtiVQxsIfFMCmxceBCDHIAcbEGEZZcIXihCwJVOL1F+fxrJoA2MQSYGAEI8MyOEsXEgzpG2o8DUqK77B20h+X7/NhFOITk6gNHVXaPyNcZWkpq6X6mjAhRiHdo+oi2lGXNlD+MMbQ85w7FBekTDb23/rQCP1QIwtltbm7eBMRTAACoAmPDMiwDEMU6nWEqxHt4GRvIWkkQbGK68/mzRaSTfqSmyVW1+YaBIeNDy4lSbmYtsB12Kvnl9Ue7piJsgFksiW4V0QQ2uAOm4VPd8gF4G86eLPZInNkwaefToUgVOpcOwUflegTql8+bazE9eZ8Uz/bZNtpMARHANgCRAAvfxTA3XoP4EoCGkJ28hSbaBkQxgeGDIbyFJZl4APgBqkAcbGPhkI57z5s2zRYv8byuACvpDfk4rwceHsYMBK6ND+couusFGXX7y/r5gDBnb37D59/yDzdv+vL6+Xj1DYLBB16GtW9WZxVY5YYFtuuJz1rrwvMHE6C6SwFEkEAEYRxFOlBRJ4FgS4PSROin5tdvX20Ep6diumH6FUHEd67nxF7e741TLZi0RQ2KJYx4UCiCId4GBsfP5x6xo1HgrlfFPygNi4Gq2rbcXf/DfrPLlZ6x41DgrFdNixlU32tilF7r0Q2++ZGt+8D/U7j5b+sX/ZlMvuc7Fhz8BwDgkFLxw5HgHEMy46jNiB/i9tRgdfemW/8/qtDWlQODDqIVn28wPfNb1gzoAT1667f8XpTXXlv3R/7Ixi1eGqt112xMP2Cu3/b1li8mx4st/51gHCRn6A5t+dYdjRZSIkbD8y38vJsZclxIAjN6uTssfPUHjvsJtv0i26bHu3n+1V+/4Jxu77EI786s3OSYJFbzbAEbJuMkCdSY5I6wBiOjp6rBV//qX9tp93zfSYVhMv/wT7rnRpw7ZAVn9nb+yzZLNkk/9mS35/N8QHblIAu9bCcRkSydbtnfKX4d58aCYF/sSxsJPUkx41qfm2J6SybZr5lm2/9yrrUuA6Xt56khqp2jh7fVSifq0io3S6kEUTp5gNTa3rtKKq2W7plP2B/oVHOKDQkMc9w3FE61JIEaPs4WQ+AMcQXRr/uqSomIFZZZSWD7QDmmRe3ckMNQGBgCFjG46hTgcp5qjMM8dh3FL8ngbGOFdcEn647aKCIToc2ycQZsYg2FtJREzguNDe3Rcqs8/GE5VWkwARs0AgNHsqh4EMDiFBGAehX1Q0edkE7aqNLY8YI1Nj1pX9z4Z+dwnBsFIsS8qrKjgAne0am+f3mKxgto6ZP+iY80RbGCgBAMEsDWku//ehxVwDgOeXuFmiwxMDEAF8gB4BACDshjxTLSBoUjntggX+U51kb3Q7gGMYoEu52bV2ZxMjUU5+EIYIQBDkL/fOuNbO1I6lcfnHwReEutJzlc4cqKNu/QSK5g0Qe1qQ5FYGe54VX3L8S5v9Cgrmz3d0vPzHKOB/g1nEyPZiCc2MC6//HIDyMAGBttSwnwRD5AAVsTbwOAeQAIAg/zHawMjzD/h9JEQhoHBEauOidHWbgXzz7XChRf4IWpqy66YJD8ufsjv+X2qDHqOeuJem7zuaRl83mkjdGxwvONdgLVXWTDBXvjYX+hUkqvjk6P7SAJHlEAEYBxRNFFCJIFjS6Cjud7W/eQ7Ygc86lbgJ2gVPk//gGQWlmrbxm4Zgtxi2HnAdsWcD3/Rpl1+Q0KlAcA4sPZ5m3HljTZFAETeSBlm6jeqGQCM6s1rbcYHPm2TL/iI0sfK2OcIV8/xAhhsCZn5wc86gACDoOG40QBgdLU1u3QYHHk6LSAY2zxRAAYgwZyPfEkMj8Vu/KnpUHwH3XsFYMy+6pNinnzF8srHDhhSDQDG2p/dYrOvvMHZKMmTIa3c8tGuwxGAMfjcortTQwKZWzX//OJmm751lRV1NgxhXvitI6YfoePt1fNusOrFF1ivDOv25cmIopSE98rl1O+2kuqNlhvrEd3cr8bSF5QWlBBra7K+5jrZRpQip9VCp3Cpv9zjUUBQSjrTsqw7I1eKiJgZYmoku6amRmvS6QC9U5ZZbMbZqicozck5o/A7JYGhNjBQVpE7IAbPWrYRZIclKMQesBi0geHzDvbGv6eU59l7mxg+z5HD/jSSwXTCR7aBUSrQrEj9gQWC906QhNprFyCxVv4Na217VoY61+gdytEYsgVgnC0GxoWWGqtQuFgGPH+rPDAwdgno8MeoDjIwFqr3AYgIp5HAwiBOI9M77ZgXvO/ADMPawCA/7/hQGxjUgTsSgDFbAEYH1SpPhj57pNk5EGYeUO8Gwv6JdJFZLt3NE9rwMhAWCNI/dfT0ySiw+u7z8Ve2OBQmmXZS07MsJU/H0LKtROE01QWbKnnuGXfmWbbsK1+w3AljHRBwJJsYyQAGNjDCMapsJ8G2BkwLyjOPAFIE4AKggnAyE4Pw8drACEBI2C4SwtQNGFJXV+eYGSl5RZaWr61tSESMlNKzT0KbGJ1tlqYty4Uy/Lz4sX+3GTWbXH/j//BkK/PH2XMf+69Wf16w+xKfI7qPJDBUAhGAMVQmUUwkgeOWADYpVmm1fYe2DCz+9Dds0af/a0JZmBGrvvfXtvelp9xpJBiKjHcBwMDuw+LP/ZVT4uPTA4BRq1MuWMmf+YHPxCfb8QIYnA6y9A/+u01a+aGE8gHA4IfF0i/+jwHmR8h0ogAMjISu+OP/PXCyR2g/XN8rAGPZZ79hZ37tH0I33HUAwPjP22zZp//Mlv/R3yWkRwBGgjiiwPtYAp558aZjXsx/8SEb37x32NE0SVmsKhhjO6avsN0XXGft0xa8J8yLjN5Oy9UpD6n9VPX8lgNu7zxbXcK+eK7Bh5VNFAPuUXgCAyPsZY9fQSUfPrhQjlVRfOe4udY7YaGo8tqI0KOV7rwR8qUhe3R9ByTQVtNkDZUH7NCr8qurrL22S4oszIrAbPBMi6EABuo06/MhH1fi+rVk3b0Trrbh/9qh6lvjjlENW0hkyyK1uL99WCDBedYDYERXd5U1tf5Sp5I8rfex3QEb+bnzLCtzkbZDzNXRqjMEYDytFfg12kJQOSyAAfjg7V4wViwOUL9OHzki84L3OXhOJeG0iE61322lVmefyXnDPpSfaNsqGcDIE9tjRnqzTU5vs6LUHstN6R3gmABY8PsCgAJp0xJRjqExTBjuB46n4zANXTFHQzmeFOUof7QwwAdtkQ9GCOUIT1m+0lb+z7+24lnT3DYMvvXhbGIkAxjJNjAAQAEsYF8AYoR5gWuYNwAdQrwDQRU+kg0MjmbFiCfzDyATYEfwbBdhDqIOrvj6+noHYASgVUPTfJtmIy+70cacpMerpsmu2/hf32XT3njaypsEaPS00u0Bhy2MtYuvsn1LL7HWSbOtV6zeyEUSOJoEIgDjaNKJ0iIJHEMCEYDxzmwhiQCMY7xoUXIkgfdAApnb1tmMB38o5sULYl40DmFehC7tzhltL628wQ4tudB6xEDrk9Hc5NXPkPfdvI7oqrHJXVWWLVo+LiemvfOpUmFktwYFAMWD1VPAChQBFBjiuEch4Z598yGMAhKYGKRTjnBwxFEvSotTXNJzrDs92zoU1yFbOG3jF1v3xKUhe3R9ByRw4NWttv1XL1jTzmbradEWhm7YeqionnnhVV9/TGo8A2PQBgaqL0o9qjBMCFTbd84dGcAY5Y5R9QCDfz9p1feLU05k66CvUdtIxNhsfFJgxmG9Uwf0jo6ydB2/Wlh4vuVlny8A4xkBGC8dAcBgCwkqO1tIGCOed/ZIp48AF4QtNbA0BJr0M0J6ZW+jpLfaPpu32T5cgJ2RQZcMYGDXIFuQQnlKhy3OarKJGVK2AR30rYSvxYMPgo90AzDhmBZKHC5MS24DTH9hAIl0eT69buKOEaZ+mBhk7ZQ4CGfKT+0HMMplbJRvO9i8SJ6rkgGMZBsYbE9hLgigAiAG8wrhZOYFcbRF/qPZwGCbCnMLc08AL7gCYBBP+XCMamBmhHqR18kOYFibjKge0Ja99ats8VM/1qkk2+j2gOvU99iQkW+7xs639R/4krUkbVceyBjdRBLol0AEYESvQiSBtyEBbF/UbH3Dqje9att+c69VyWDjlHOvcsd2xlfX0Vhv2x6/3w7v2GCTlT555TUyZDl3wIhlxMDwp5BEAEb8WxPdRxJ4byUQa6q3zMqNVrbuOVuw6uc2sWnPsB1q0Z7/gzAvpi63HRdeb+06rehE2rxI626zrK4mKTd9ThnhmMUJXYe0AuyVkwBI8EMfsAHAAoAiOQz4gKIRVlSDYhLACsJ48lE2uAB0oFwQH3xQMFpHzbTOMXN9WQCTEimxxX6rWagjur41Cex5Zr29efcT1rpfxzRKuU9J4UQrQIhgAwNgAgOPHHcqNoaufksPAAd52D6Bsg5DId4mBuouz5a68PFh6kx2AAUhP+k+XNvwXTEwbo9jYFxlhXk3yp7FmCMAGAAw+A612G7NLc9Zc+sqa+vcoC0lm9UNbSURPFBcdLblZJ+trSNvWnvb5qMAGPSJMSYzL1Ciiffpvu/0GcdYYW6oD/3Mjx5dR8gGxufytx4TwHBV6E+xjI+ekVlvMzI6XA96VC2t4ZAoQARSxwGx4PzWkX5Aw4VdtAM4Qh42/gBg4PxWEj05hV2vQTV4kscRnr5ipV0oBkbZvFnuWwa4YI7g++XbDuFkI55HsoHBnHAk4JO5IMwjYd44XhsYyQBGmF+wiQGIQTrthnqRy8kOYDCXslUva/OrtuTef7J5OpUE4Cv5VJKDGSW2bvGVYmJcLCbGnIiJwcON3LASiACMYcUSRUYSOLoEOluabO3d/2xbf3OftWkbSVtjg+WXlOq4zqKEgtAw23S0aHtzk+UpvXDMFJtz7ZcGbGFEAEYEYCS8MFEgksBJIIF0Geyc9PBtNvXN56ysrdrytTVjOLcnV8yLCz5th5ddbN0jRllv7om1eZHfesDGNO2wfNm4yMrKtCwBGbAvsmTED6ACwCIY2UMJQEE5WhhAAk9efHAhjEKC8hCAjbAyijKBdz/S9UM9bCnpycyx3sxca2xokuLRaGlzz7fs+ReGaqPr25DAnmc22sa7f2st+5v1fAv1PHOlvgIAACKgJnsmRkynG6DYemObnFAS0lF4ebZoxPE2MVC12+UpBzND2y6k0PtjVwn3a9C68w7AIT7d569t+KEdrv4PARgtLtuIkssFYHxcWw0CA4PowXfL95m6fb/a23cLpNhtrZ2PWkPbC0IAxEVQ1/LzZmkryRwxM+qtu7PuCABGsIEBwyOeecHWEJoAEiAtMC9oV3Zg3LaRkL9dx6z68iOsUQDGJgEYDZQecMkMjJBQqLoXZNTb1Ix2BygQ72xiqPFMPRqeTrwNDMe+6C/s+qf7IGXC8XHx8RR5O+EZZ5xvlwjAKNFpJRjCDDYw+Ob5ZgEz8vLy7Lbbbks4RjXZBgb5mAeYFwAqmFeYa/j+A3DBfBDmDe7Jf7w2MMgbPAyMAJQAjBImjTrfTwAGzwz5wOqbc9+/2KztL1heX4e+1vhvga9OTIz0PNs1boGt/+AfWsui8ygauUgCQyQQARhDRBJFRBI4tgQ6ZJV/1Xf/yjb86k4r0ZGgnKBxNMcxpw17tjsEetFn/9LmX/9Vlz0CMCIA42jvTZQWSeBESCDW3GgZMjScWXvANZe3f4fNWv2gTazd4X5gcopHvGuJZdqB/NG2c/oK23XJJ6x9xuL45Hf93tu6aLayjmob015l+dpzD4UbhQQfgItwn9whFAt+/JMPH8IoMigL/NCOd0FRIQ1PGBfiKRcUFhQYlIx46rc7TUArpzZJDJXJSyxWNFKnlVTENxHdH0ECKYcPWva2LZauRQLcjq119uqavSaSkJ4dAEa2Yjkula0QABneBgbMC1RmTpwBzIjpBBnPxAiqr5Kd8/m9ugyIgZo9yKgYDANsEB8cDAcU/cT8NQ0/EIDxozgA4zIryPuYGBij+xkYye37+mBGYKeiR7YBunVSQ3PHL61JBju7BFb0dNYKoJPx7YwKgQveTkIHNjPENspInyBgY7IMfS4UQ2O+vlTeTcbhNmE4hZp63RGmDsAAjCSdsfCeN+svdi/UPukCK3t0KgpgR0msyW7M22IfKqgTrINdC/9dHAvAmCIAI0iKlnDDhT0jw6cPly98hUiM8mwFiXfYxgjq73Dp8Xm5Hy3bCvM+9EEbvWyhFU6ZKGPphQ604JtlPgDcBJy4/fbb7Vvf+pbt3r3bVTFlyhRno2LatGnGPXPGcPNEaC/MEyEc5pVXX33V8IAnzAljxoyxsWPHWrCBwbxDP0J+gArmkRB2p48IxHi/AhjII/XAbiuXzbiJb/zOxh/YZGXdieBYkFlVZqm9tvwaqxITo2P8dOst5gSfyEUSGJRABGAMyiK6iyRw3BIIAMaWJ35qs676lI42/cxRy7bVHLT1D3zfqrestQjAGHqMarSF5KivT5QYSeBdlUDanm027pE7bey2l1072R0tVtZ8wAq1D17q0sBqZ+jE3pxRtkanjRxadon1OpsXBSHphFxLumpl62K/lWnFvFDc8nRpQigfgBgwLwAWABRQRjglgLR4F1ZEyU86CgdKAeUCOJGcnzwoOvHpoR4Uj9BmADIIo3igkKCEsHrakaJ98qk6gnHGWZY6/az4JqL7I0gg58UXbM6Dd1nZ4UqX48XWXHuwqdwO92EUs1TP1rMjYpar9AK9q2wLEbNCsk5LzexfFcfgYpqOQQXoSHwX/Fq+26SgcgAgsBU6lQ+VmLA3ijmUiYF6jfoM8wJDogAcGea3kNwWt4Xk+AAMjHZyPCtgS0pKTNtIxDJpe05Hpm7VlpHtOlVDJ6porGlpebrPlgHPWr1f9XEAxlwdHTpbfWCMQA0AFZ4VJCGo7vhjU+m332LimBru3jMzHIjRx3aoTm0JabTrCyrtyvwayxOzSZuvVG7oKSQuUn+KtIVkqRgYkwVgYHsCx9YPgIcjhWFmgEvEMzUo16Hy/VW4rSfYsADwiHfYwyAf9Q+XHp+X+8yMHMvRqR3Tzl9pZ/7xF61g/Fj3bQJ0YteCb5Xv9I477rDvfOc7tmfPHldFSUmJFRcXW0FBgeXn57v5hG8+GegM7SWnhfDBgwftwIEDA+wJwAv8vHnzjG0qzEGAFMwdlAkABvfENTQ0OODj/QxgmBbzUusOW4FsYSx95NZhTyVBjq2xDKvKqbBdU5fZzss+ae2zlwfxRtdIAk4CEYARvQiRBN6CBHr1D1z1ltdl/2Kd1WzTkWc6Hmrqpdfb5As/ctRa2uprbPPDd9jBdausZPIcGzFtvvMUWv39v7XoFJITewoJp7vUiMq448mf2rZnHrGF13/FzvqTf9QPXH6Emtg1f2kv/ce3LDqFxIkj+nOKSiDW0mwZe7dakY64m7HqIRt7eKsbqTgJlqOTO7Q7PGHk2Lw4UDDOdk5bajvP/6i1zRCbQKuRUKjfTZfa1WIZbXXO1kWGjkosj7XYpN4aK0ztc6AFQAWrogAS3AcwAcWEuOT+hRVN4skDMBE8SkyyC/lDnpAeVlopQxr5AoAR8nJFKQHEcPm7BGhgG2PUDOvtBhDRGComWEbp2FDtaXGN1VRbxtbNlqF/G4NdBCw04OLDpZs32PzXn7BSGbXEvdmeY0+2VdjmvnF2MHWSdaXIYGwKLAqADG1hckyMQRsY0v2kDHqa//ExMehF2BoCi4M+obgDfABqBCYG3wbxnukQ0msbvp9kA8MDGGmpIwU8lKqvjI56vAvMi74+ASeun3xPqd7WhexdtLWvsoamdcoMwNCl97VI9ehY3+5WgSStAjDGi4ExUQyMBVLC5ysPYEzy6SP9RjydDYzQbwAOQAzGyxgIA3IoXX2BhZGr72xp1iE7I7va5qU22KR08g8FMLBBk9fXaWUpbTY9s9UqUgXcSWy05AAMXTHaiSTjwwp6I5796clhbGjgAC4olww9IX3qZfYZLl04kJM01ZCXdwE344yVdul//4aVTJviAAvmAE4j4fvlO73zzjvt+9///gCAASgKwMHcQt7k+cTXeuy/1M0JIgASuABgzJ071xYtWjQApDJPhDknHjh9P9vASJYOoPn4J++1KW88YxX1lVaUdCqJLJJYm1hUO8pn2Wsf/S/WrG2KkYskEC+BCMCIl0Z0H0ngGBLobG2213/8LdslCtyk86+xyRd8xHLKRomKePSj8npA9qurrH7XZhn1vMfqK7fa3I992cpnL4sAjB990040A2Pdfd+zN//zFmvVSkBz7WFbfMPXIwDjGO9+lHzqSSBt/04b88S9Nkl03pF1e6xI1HUcrAuUBa7xbh/Miws/bYdWXG49JaKz5+S97R/z8fUe6z67ca+NOLzRilO6raCoyIqyM6wwU4CFNJTAvIBJAYhBOKxYomgksy9oK6QHRSEAHYRZ3QwKRuhXyA84QZ6QjrIXwJJQLoTJSz58sIkR6qnv6LHGThkOpT6BGLlLr7DCJZeF5k6La/raV23ST++0Mfs2u9Vz3jRW4VFGWU0P4SwZah3RqxNwYh5YaulNsbqeNHu+baw92r3YamXIMzWV9xCwwbMmvE0Mr2gCWqQ6JgYgU5feB70jYsGg5Cc6WiaOllG9fXhwa0oyE8PbvAjMC8AAgI/ahluSbGB4AMOBF1bo+uJZI2pCLjAvvK0OWQQQ86FX2zh6BU70aNyNzb+wuvrfqW6OnWzoHycgitpTVzPSxwnAmCAAY5m2kOgkCwdEAEbw7nngAvaFc24LCUAInnF6sIa6fH4ANbWtPhBOUXxOSouNT6m163P32cV5bDUZCmDkKt/UtEYbn9am/NrOJUCD2nFIEfd2wv16vsbs6wl1+Rp9nfH1JqcDfPAu8TRhgHDFTVt2tl30ja9ZmY5TBWDEMXfgCN91110JAAZzCOnMJyGfy/wW/wB0AkwEFwCM2bNnuy0qxDN/YKgTsIJ75oww77zfbWCEcbtre5ulactiIUyMx263abVbEpJ5rrJaZDuKp9gL1/+lNZ1xeUJ6FIgkEAEY0TsQSeAtSKCjucFWf+evbPMTD9iST/2ZLfn837yF0maNVZW25nt/Y1WvP2eLZQtj3BmXvq8BjANilGz99T3WoyMDi6fMtkzRM4dzzQf3WN2OjTZCJ7BMu+JTlj9ynMv25s/fHRsYO576T50Oc7/llI+2kilzBpgVoW/1OzdZnQwV1u/eatW7ttiiT3wtAjCCcKLraSOB9J0bbPYD/2qzNz8rinibZWoFeDjHD8lOrQrvHDHN1lz7p9a0Qsq2fsy/3ZXI4doYLo5TRrK7GqxQ21nKmvdYsfS2IgEYgBWBbYFCEe65Akbwwx9FgbSjhQEj8AHAQMEgzBVPeTz1BeUjGcBA4Qn1kIcwbcczMVh5xYd6YWTgATZaBZikTl5k6VOXWPbIKZY1csJwojjp42L1dWJUbLJM2azw4JfvsldE/H18fMGOrTbt1SesomO/Wz0nBwYeUUJZTceFcIbeNb/C7hNQu59qGWM/6VhhB3S6h9teIaDCAxj+ZBEPLLwdJoZrWn9gMoQ6UW5Rf/k+GMVwYQCMzn4A406BD8GIZwAwyiwlVqxvBkWYer1LZF7w7mL80Su5pLXIDkZLyyoZ7dylbSOVoVj/VccCDwAYSwRgYMSTsgJXnPIbmBdedff2MTBUCkCBA2jwzA4HZuj77+lhKxVxjNVvpykTcPKF/N2yhQGIYravK6ZtPLm2qqNA96nWo/d9bnqdTUhvc1JDcieDw0gonncQpgY2M3AjJ061mRdfaOOWLrKRs6frdwsn2Qy6u+++OwHAYM5h60hubq5jYgwHilKaeSKADoQDgMk9ji0gdXUywCrgkjkCGxijR48esIFBHsqH+YG5BA84imcOoSz3XMN8Qzkhc1Y4/1wrXni+5YyeYjmjJrnok/1P2p6tNu6Je8TEeNZGNuwewsQ4mDnC3lh4ue1bfIG1TJ2vU0lGnexDivp3giQQARgnSNBRM6eGBCIAY2XCg+Q0ltaaA7bvpadsm0AdgIrhXPmspTbtso9r28wCyymtsNR09haLDvwuARht9dXWWnvQtj/+gO387c9ltZ0fbYNuwtlX2vQrP2kbf36brX/oDltw3ZcjAGNQPNHdaSKBzK1rbcmP/7fN371GisfQI+2CGJxl+LQc2zVmnr1x1RetedH5futIkm2JkP+duua3HbLRjdttRE+L5WmBPU8njUD1BnBAiQCwiGdecM8pAigHgAOkE+bHPkpBchgFAIUhMDUCQyIoCaF+AAqUhqCckA9HPjzx+FAuvp74dAdYaI99UExQaPDNWiDXeqSVnH21lZ559TslvhNaT9qmDTbhgTtt/I7X3ao3iiMONRhmBc7ZKfC3lq5V/hwde5uhrUr9WZ2iSfJwYUCMTLEnUEI7pWg/1SoAo+ssO6CtGYPHpPLvCuACNWALgvCRmBhHsomhInK+LNtSgk0MQAfAkcDEiA+j8AMS9AnAuFUMjKEABke+pqWV9zMsBlfhE5kXxPtVd93ICV7o3K6TR3ZaS+vT1tSy2kcP/AXAYAvJJDEw5km5xgbGcMwL/wCGAhhUBBxE32EeYeeF95x/LzllBaCjW7ZmmgRg7BGA4f8dbe2L2e6ebFvblW0P1WfYntY+m5FW4xgYGfouJVmVe++dXpmEdykwOjIysixTJyZNO/tsO+Pzn7SSSeMTOpsMYEyYMMFmzJhh48ePd9s+mBeGc8wTfO9hfgjfeQivX7/e3nhDW481B8CyGDVqlFVUVFjYQkJ+yody3OOPC8DQYFNz8i29YISVnfdhqzjnw8N18eSLa2u1VG3FLlz3vC174j9sWv22hD52CfBrSs21HTqVZN01X7HWBeckpEeB01cCEYBx+j77aORvQwLd7a22+ZG7bN/Lv7Wpl1ynLSRv7R8JFOstj/zY2c+YesnHrXT6fNvy6N1WJ0bAtMuut7HLL07oFYAA6U1VlTZVAMCYxSsT0hv2blf6T6xd9QIQjJx/ZkJ6nbasbH3sbutqa1H6J7RlZWlCOrY8tj52j+L6VP8n1J8FCemHNr7qGBZpmVk6+vUT+oeeH0hDnWdi3GvNByqHJiqmYu4Klf+k5Y9K/KGw98UnxZS4zwonTLPpl3/KcrUdZzhX+fyj2npzn5XNXCzg4VOWVVDisu383UO2XcAJ455+5Y2WoR8l8W77kz9z6ZwCE+8mnne1Tb/qRoOpQfmJ537AhaEX47boGW+XfYwpF33UtRdftld03M0P/9gqn3vYplx8nU279Lr4ZPt935GEyqJAJIF3QQIpOsIhW+BFmX40znvlYZvQtHvYVlB7ZD3A6mQ4cNeoObZ79pl2WFbhOzUPnAjbF8VtVTataauVyjggwAWKAz7Z5gVxgBrE41EAACcAOYYLoxSgRATFIgweECKU5Up56o1nWAQQgzIAJaQFF/JRf2BhcA1laDMAIfQvUMVZlW1qFYvtguut/MIbQnUn1TXWUG9ZWzZa1sEqB0rQOT9bepAid99um/7SEzamrdIdoZmK9ijXI62R1W8cLIoQ72OO/69gCJUHZmM1HQBjtP2kc3k/gOGZFh60oFf44ZkY/lnwzGDXyHaGtp4AgLhND46BJP6AwqT39QFSYD+Dr8DXFwANVOPeXtr16YAAfbIxUdd4u1XX3D2EgRFsYPjtIUkAhgyODsYLzXIS9uPoka2X7u5qMTEe13aS1Xo/eW8blQcXAAxsYABgzHLMC04coS9urK6uwKxy0lM52sCp12JbAFz4OCnPAi/6dLylZ3IIrNGWktJYs30xf59dUwhQ4yTiALeNnRl2a02ObWpNtZnpHsDwFjhctoE/DiCUvHgjgEtwWALBHS3sSzhI5qj5qFEjdvVRb2LYP1veQ15JbSCyPP1+KB0/wSafdYbNu+YKy6soc9992CLyk5/8JIGBAXixdOlSmzVrlk2fPt3NQfHzA/f48M2HMFeeQWBKrFq1ytasWeO2h2ALY+TIkQ7AYAsJR7WSlzkklA/zS2BkEMbHzyOUiXextAwbedmNNubyz8ZHn7T3zMF9klPO+tV2xk9usrmHsPcy1O3PKrfXl37QqpZcaB0TZ0WnkgwV0WkXEwEYp90jjwb8+0iAfyza6g4ap5DkaA94VuGIt1QdRkDbxAwAUMguKbO07Dxrqzlk3bL6n11cLipjUUJ9PV0dyn9Ix6i1WxbpeYWJ6YpvVXpvd5crn6zAd2ufYWvdIfcPRHZJuWVoz3q86xL6zXhwtJ+ejSX3QYfNjzaV5wddjsqnZeUMJsbdwcQgH+0N5zLU75wR5QPMi5CnQ4oU5dIycyx7hJgZaexjHuqQN/nSs/OVr3xgS0h7Q62Lz8gt9PH9AESoAeOpjK+vJ/Ef+eySUo23wtoblK7nkSUbJow/UOKxjcHJMfQpJ+n4Lv7Bpc521Z2tdyDZ/snv+46EvkfXSALvlgTSKzfbrJ9q68imZyy/u0VbR4JCk9gialu7fvDvleHOVy+80Wp16khfUZn15eQOfCuJJd7ZUGnnYZvdvtPKZRQQI3ps5zga84If/vy4B7SIP1UgPowyEBSE5N6iPAAwAFpguC8AEuTnuw/pAfhwP74VHxzfPp796zAruCcPCgyKR0gH4KAu+ooHwGhsabWClddZybkfC9WdVNf07Vtt+n0/sok6qaZdQ2bUMCpQ9QmnilFR3NtsOVq1RzXtxy80fp+XwcTHE34rjnYoj0Mdf7ql3O5qX2gHdTxpaqpO6HA2MMgho62OeQEAQDiRiaEI9Yl/D/wWqBSBB5xOgsIP+8CfVpLVDwDImKdWgAdPL6E+Ro7iz/Pk1BAMyGI4tsuVb2i+02pq7xsCYAQbGJQN748qUnnq9PW690lfHGwIk1FSxtHXp1MpBFi0tD4v/4q2kuzX+7RP6TgZqXVbSMYL4BO7MXt2/7vd/z0n2LzwJXz//T1bVHp6m8QKYbsLMgFOEIDhmBf009vEGKH2/6DwgH2kyAMYSIBtZZvaU+z7NQW2oT3T5ur0kfFprQKXpLA7+fo2+AtoADMDQAL2DO+ED8Om8f82A04hhRDOUJhyOPGkXL3cE0/7nfq2EF2Gng/gE2GMdqYnhUkH3OjU801RAcKT5iyy5Z+63kYtmGO5pSVu5Hz3zC+wuO65554EAINjTs8WWwOQgXvy8Q0zrwCe8n0HYJJnyD3p7nkqzD3+ueees+eff94Z8uSbDwAGoAj18i5QJuQP8wdzSbxNDOYw5g0c+ePd+w3ACGMAwFhx19/b3IPDAxicSnIwu8x2Tl1u26/4jHXMWhI/7Oj+NJRABGCchg89GnIkgUgCkQQiCZx+EogJMMzZ+rqVi3kx55VHbWJT5VGF0KjTDipHTLbKGctt31lXW8f0RSeEeeFsX3Q22CgpTlN0lGNx+vCnjQTmBWAFfuPGjfbCCy+4veIoIhx/WF5e7vaaYzAPxz5ywAMABRSRwNAgHAAL4vAoJsTt2LHDtm3b5kAJWBNBaWBfPJ42UEYAV3Bha0gASsKVctyHtlBEwraS1nYpy5MXWvqkRTqVZJJllo9zdb3bf2LaapOlkz6y9+91ihzqUFD949vOPbDPZr78pI1p2SklE5UR5dHBAAnhdClhrIKjLOJ6GbNyJ0LIwZKEz0M6Ln5VnhSv0nq1Ojn8ZEuZAIz5diAFI565ak/gt9qMxQAs2PrBKHCA4pxSApsC79vUjXPYowCAQA+MP62Ee4AOwHtvt0K9k/IbnFP+ewA4fHrI39TykE4N+ZWywcwQ8J+9wnKzL1T/CvV+sICQ2H6ozzMgkATwDNJyUlQYdoRsIHRs1HaSLdpOslvXqv48UsjTtfCQNkqK9FTLTJ8seausBuOvqO6wPfCAGl7OXN3JIwIwMBba29fmABvfNsZJycv4BdII1CnRd/ilooMCMAaZI8ogACNm360ptDfas2yObGCM6wcwwvNkpIwC1g3/4QA4aB0gAeffJP8eJYeDpOj18PlCrSF9aJgvkvK0SirhmctlxPPPvmpl06e475HvHLAgMDDuvfde++EPf2h79+5VbnNbPM477zxnaBMWBvMO+fnemUP4noPn+yaNK/XGf++/+93v7JlnnnGAZW1t7QCAMXPmTNcGecPcxJUwHrCCuSLMGwHgYD4Jc5HrqP68HwEM+p4uWxhjn3rAJq9/1kbV7pQtDG9DJowrnEqyvWKuvXbdn1lLEhs55Iuup48EIgDj9HnW0UgjCUQSiCQQSeA0lkD67i02S0Y752x62vJ0DOORmBdBRPuyy23V+Z+xg2ddKcpuhVnuiTl1JE+2L8Y2brNRaV1WUZBnWRlHtnkBgADYgPIB9fu73/2uHTx40IVZ1bzgggts2bJltnjxYrcNBeUCZQCPIkJ5VkwJo3CgEFBXWA0l/OCDDzp/6NAhB04EpQHQYty4cbZkyRI744wzHHMD2QWFI34llnjqRwmhPP2ARs7KqlNUFG7WUQktUvnyll15wk4lSd27x6bdc7tN3fi8s1UB1T5L4w+GNOk3LpVjNaVUpLvtBT4uXsEkhjAABrYq2PKBcxYktPqN5Yh4Rzr5cB1KJzdhch05LCOX6h+r+I+3lDoAoyqGDYwsKZMZUibZ0pEMYFCzq50WdT8IQigg55+1Z1Jk6NmgdMKsCKeXwKzoVP3pYmLAivDj8iqznqPSYGJ45kamWBJr5FepPMptpgyMTrS0lCm6z3Xetzn0L8wHz7yg/wJUHHAB48Gr3z291WqnWgpulYx5HlC8BzpSUwtUL0eripkYG6E+iregtjl5pFdy9f3kChCCxwExULc/UYcxDxoPBTxB3YdB0K56WgwGxpeKqo8KYGADY0xai+tteNJIGqACmakFx8wIaUiRFJgXPAOADRzvD65Lz1lQgLuHieHzsR2JuLCdSFCLwnyr2N0A5Ahh8vOukB8wLT48QwDGZX/+dSudMtF9+wARMK74DgEH7r//frvtttts3z7PdMFGBQAGWz2mTZvm5gzyB0CBb5lvmrJ888xHzC2AEIRD+pNPPml42BfxAAbzFMeoMgfFMy0CUEF5PGkBQCUcgBcnpP4/71cAw8QmTtVW6KK1z9myR262qXXb4ofl3is9ads2Yrqt+vhf6ljVSxLSo8DpJ4EIwDj9nnk04kgCkQQiCUQSOI0kEGsW82KzmBdvHB/zojUl0/YVTbBd05bZrnOvsbaZS08g86LeyrrqbHxPtY3ISHGW/1EWWOlEKQjKQbB5wRVP2i233GI33XSTVVZWuqcLaHH55Zfbueee6yjggBU4FAOYGCg+1EcY0CEAEy6T/oQVUCjlrMoCYFRXo0h6xWrSpEmGP/PMM+38888fADCom/qCghPqCVfi8Sgs5EPRwRNu65CSOX6epU5aYLljp+tEgcmhO2/pGlMfsjatt5zdu5yqi+KIqoj3aq2/z605bFNeetLGNW5x6qL0MCl8Xs1n5Zz7t+JQWqHxB1sXQYkEyMDRfliVT3Eh1GWvlAbGRWIY9gZlWT2X3QndYwPjCQdgzLaqmIA1y1M/2QqC8i4wS8whv9aupP4WpUrr/thMDJR+mAf00jMzWOUeZGJQY7wLzIvAxOjo3CWGxC63Ou/sa1iR+lLs+pbqABskBBMkuH62g3sqyAHpoIwHwIGnBeBAn7Ctgk2MWhfnwA3G60AbgYuWrXYlKXmwC95npOfZF5QHRvLteTDDMw8M0EYAlR837fG0APMwKNnaD2AcPiKAsU5bSKYLwBidKjaHStOqG4XencFnrJ6oP0dOF9Cgsrw7OJ4xPcHxCg7WM5iPt4J8ACQAFMcK5+qUtEIxsSaftdwWfuBKyysvdd8hAAbzAN81c8FPf/pTu/32223//v0074CLs846y+bNm+fuOY2EOSd8x1z5trnyHTOvMF9RX/jmuX/q+CAobwAAQABJREFUqafst7/97RAAI9jAAOxgPqAP1BPqD3UAXsDYCgAodeKd0/g5wSh7zBSdRHKBlSw4z8e/z/4Cso958n6bvOFZG123a8ipJAcyS239/EtsvwxJN4sR2Fs66n02wqi775QEIgDjnZJkVE8kgUgCkQQiCUQSOAklAD135v3/1+Zt/K1W0TkuFSXmyG6/mBerL/q8HTzng9Yjg3d9so3Dj/J32+W1HbYxjVttVGqnleXnWE6mlFIpFwAYyaeNAEbwwx6lH/CC00ZYNY0HMNizfvHFF7vVUwCGgoICN4SgAKAwUD4oCMnjQ4kAjHjggQfsZz/72RAAY/LkyTZ16lRbsWKFawOlBhcADKdASmmjHZSSEKZNfFBwWFkNK6/dGlNDW6c196ZY+cpr3/ZpAik6znTK3bfbrLVPOmZFt7Q7bFZwOgg2K3r6wxlSE1O7m8Ww6HBbQlKV3iGdCGWQ/OlvFcFQOa+CIgnvqKtfzXLMDJgWqjrO3gElPLMCdZVVe5TgQeU02DlAqfVK8G9bisXAmG77YWAIIJAKq6JS/VP8lpDB9/VINjGSeyk12L3jABb02Icd80KAHso9TAwvGV36nWdmZAow8DYxYC34Y0hDfbSDF4AgY7TeJkd2f2lqg3mB7ShAFzFAHDMiMC+QAvfkgZkBAwUFlvzqo/7vEVLR684HBd6RVGPqIwCMQA2TEdKh9VGWcShZwIxjXmhctOShrWDAEzmobreFpEkMDGxg+HKusP6ELSQBwBglAIPtRXQnU+8NgEQ4gQZGj3/mviVOlfFhXxvbkXj3iMPRGzzOnzzj75Pz8V5RJpQ7Wnj8zHm25PprbfSc2ZZbLGApTe+V+stzZ57heyTMtx4PYGDEc/ny5Y7FBdsqsCtCfr5tvnHmEcoDPBDHfMDcxTxCmO0j2MFIZmAEGxiMkDrj5wPqoyyeeYV24pkXpOMwqlyqk0cqzrvW0mRzLE0nkrwvnQzlp8j+WNHaZ235r8UMa9iRMAxt/LMWMaG2jV9k6z78VWudf3ZCehQ4fSQQARinz7OORhpJIJJAJIFIAqeRBGIyrpu17XUrXf+CzX1Rp+Y07Bx29BhI21s03uoLtZolxaOhfLxVSnlu1/HHJ9IVtx+0qWICjErvtsLCwgHQIpl5EWxe8EOeH/UoCsTdeuutCQDGwoUL7dJLL3XgAiyMAGCEMQWlIKx2Ug8eRSSsppJ23333ORAjmYERAAwM/LFVBUdZyuDpX3xdYUWVPtM2aeRhZRUfwmwtaenoshEXfdIqLvx46K6/qt6sjestd8dWBwqgsKEeoyzGu8ymBpuuU54m1W/0SqUS3SkgUvWcktkfRu2Frk89pCeHUTSBG9Dt2V6CunQ84XgbGLAmQrk0rRT77QCsnntuRTyzgnyYb6Q/weDjYF1a0VY8/Ximrdjub58mGxgAGAXqn7aROPaFWBgAJG4lH0CJzSqwL4KKK6DjKEwMJSa4YCMDZgaGOgfVap8tbDUJRj0TCicEAAkAABjtIHgyaPPCxw9lXiAJPOmwDOiD6upXtoe3eQFAyRvBmMk/HPMCioYAoWGYFy4/41U6p5CMsOaj2sBYKwbGlNQaAY8C5dQajpbxRwqH9zWkEwYr89AWUAoAiiLluMTnc8wgxfFOxjvyUw5oxI0+KX3KgqW28stftPLpU+OLDblPBjCwewFIyXyCh60RvmW+WUAHPN9y8Hz/ARghjvwAGM8++6zbOhZ/CgnbUmBhAEZQHywMAM4wj4S5JACh1Ee+eBfT9il3+sgVn4+Pft/dIwNOJcmVnaYz7v2mzTq0QU+TZ5roOJXkjYWX2f7F51vb1PnWWzIyMUMUOuUlEAEYp/wjjgYYSSCSQCSBSAKnowTS9u+0yQ/+m81c94QVdzRavjsucagkqkTLXXXBp+3g8ku1kqf99wIDevM982Jo7ncvZkRntTt1pEIMDGjagBLDMS9QIMIKO4pDUBRuvvnmBACD1dIrr7zSzjnnHLfNI2whCSPgxzLlAyOCtmgXRQFAgTTy3H333e5kAmxrxG8hCQAG++Mvu+yygZXXUI568O5HueqB/o0P6YAYpAfGBmG8o5GLIuFOJTnn2tBdd+U400l33WrzX37UMSk4ojRLv+5ZmY53KQIHsmW/INPZMvBKIFlQ+tQVpxQeTxjGRqbsQQAidPT6jSCeQXH0cLCBgUKJTQuUUMoRH54dciEFoKJb9yipXDt7Yw6oAPbApgEnU6CUcgIFkAf1rWovsYc6Z9qhlDKlyIhnSo5o+zCFsIPByJKZF4pyzo1adzAahrOJ0Z9t4BL6i0LNKJLdsdJDfsrSe0AMGBXB+X4MZUrEMy9gZlBukJnhbFzIzkWKFNehNi+QJPnxtIsk3yLzQsAF9j369MzLxKw4mg0MAIwJMQEYsoHhmBYSMe8l4norYXVUz5jnA3jFs3e3jjVEPdRHvWTJ1J9kchCsog7RNYinnuT0twtgYKMCABSQAbYE8wTvQvh+A9MCkIJvOXzfAYAIYYx44plbMPSLDZ2KigpnV4O6KU+dARQJTIwwfzBXkH4qAxg8ccab88YLtuwnN9msA+sc3Oc3CPn3gb+Cra02I892TF5mm6/+Q2ufvWIwMbo7LSRwZACDSSJAniexKFb/4G/tpR9/2x0jSTeX3fjndtbX/vEk7nHUtUgCkQSOVwJ9/JYJSzXHWyjKF0ngNJdATMc0Z+7cYEUbX7KZqx+yKYc3a825Vz/59A97nGuV4cO9xZOscupSqzz7amudvVz757XSK38iHaeO5HTWW3l3vU20BivRqSOAF9CvA/uCPeWEiScOj2KAkgD9m3CyDYylS5faVVdd5exfYGQTxYMf/9QFCMI9HsUBpYF4lJEAIgRF4Ug2MAKAgXJz0UUXuf5QVyhHPaFu4uIBDdoLYZSe9lbJoKracg/paGjV0alHlTFpsfwCp9jzc4ynkqlnO33NUzal9g3HnEDJk6kQKXlMlu+coy1Ww/1qN3V7+xMow3ABjhUOIEWwgYHqDrMi9DKAFcSHPF5p9UAGyihrzA7QGGBqAGCgxPbYts5ce7mzzPbYSKtLGW09KTJkiZFNsYkAMWBe9PUCdPE+A3+Elv3IJDXlwSYG4XhHmPKO66Hr8YRhdQTbEuSPDysY5zwQAbAQXKif8ozOtxuYFvST7TEhDPMC1oW7CsjhU9UQ+21e8HSc1FTP78u8kH0MbYuhPwAYXyyusQ8XCjBRC0GS23uy7I78xba5ZIpOKmkQQIpa6SVGSWYbwuRndISRDG64MEYc8/dttYz2evf+kx/HFc87wfsOHMOqPA4ADGDD3/cDHAp4NpGPzysostIx42zy8qU297KLLX9kufveeUf55lGY+R654n7+85/bHXfcMWADAwYG8wfGPLln7iA/3zbzD/UwB4V6SON7x4fvnzTAC7aQBGAiABhTpkxxIEaYzyiPj2dmEY4HRMgb704VBkYYU5q2PY5+9kGbsP45G39oixVrm1u8w/JJl/yWijn2iox6tixaGZ8c3Z8GEjgigAF4EdMME+ufGE5WWay6RQDGXd+2nm6mS7Pln/xzO/uPIwDjZH1eUb8iCbwVCfSJadsL2zb8YnorhaO8kQROUwmkHt5nYx6906a9/qSVN+7Tj78W94M/+TPCINqqi79gB1ZeI8ZFsfVm5bgf4ydabOHUkbLeFivITNOxk1kOqABMQDFAYQB8CGFADNgUKAf8yCedcLINjGQAg/LkBwghPwACjAjAj8C8gP2AcoDCAABBnmPZwMCIJyyMoAxRBlACpQVPmLqol3BQbIgPeTubBUy8sUOU6X3WrnzYDshMy7IMKeXtKsPKM6vOWfpxli3DisMxK97J58a0C/MCAAOGBCBDhhgUgBfJ4Q62GigdhgWTNek9UrJx/IR0arnSO/qZFcTDrOjozwOzgm0sDShurh2VU0HG7RVWlFep/kK0UWIdn0DXRunqm7vH2LrYImtOKVGtKKTZ8rlS8DmJQ2qO3h9/Okl4+4/EzKBXuMDMgG0Ca4H82KxgC8hwYfJrRT7BVkV8GAnEO8LxcX5rid/mEmxgwNAI+eg33odhXuBdGCEN2LwQINMH0AFLY5Cp8faYF9py4uxquCdnZWlt9rmiWgEYXcBCDsRQI7Y3Y4Q9fvEf2gGBn6l65pI40QO9dYGkcHgKQQLx4b51L1nBz26z/IMb3TsTygPmNenBc6Uc7w7bjwC8YFz4VoOEfPuhXuoYN32mLb3mapswf44VlpWLZRZz3x3fK/MK7wnfYQAFfvnLX7oTjYIRT7Z4cEoINnXwzCPkpxzvPfMQ3zfx8TYvwvdOPr55to+88MILQwCMCRMm2KRJk9zcS33UhWcOwXMPeBHAEMZEvnh3qgEY1iVguqnWil57xpb/8gc2tX5b/HD7vwazrSNm2qob/tqal1yYkB4FTn0JHBnA0IyQwhzp/w06aSWx6lYBGHfHARg3CMD4SgRgnLQPLOpYJIG3IAHZILPesITzFspFWSMJnM4SSNu3w2be9882f8OTltvbPsRop7d5MdEqp4l5cd6HrX3uGe+puApaq2xS3UarSOkYOKIQUAFgAiUj3IcrigI+AA3kA9RItoGRDGBQHsUDR3kUC+oAJKEdlAQAjKAcoDAQx7GK+GQbGCgdrJ5iAwMGBo46g7IRFI5QD2l4wnj6gid/b1u7zX5po82vqdQWCq+4s4oMswLlHkWeMEY1HW9A8Sj1flOHa9r9YT0fngOrwoAIqDlvJxxsVaAIehsZnP7g17297Yo+9YcYfwQmLWHbgfY8gOEVLP66fqqv1AMQg8IZmBX8xOSECcbXit0F1eAtYOjozf68ABbcUz4wM6iT3Fu6R9orttiaUipUU6a2DcgL9KFWnqNfHZcEBmxiICH6DdjiARfdxDlvI8NH0DskQBl1wP0gPt4wP6ABEo7lsE/BAhx9gt9Am3jKB9VcrTvwC/aF+iHP2NwJKK4seemje1q6Jtu84D1XHvc+AKLJu+1AlNMWJ/WzD9kDRCm+NwAk6gf9KUvvsE8XNdrVhR2WpTywuXAY+/3dh79hNZd+woV/3z89OzZb7MlfWfb21y2rpdrSuwCMJAm9yx16fn2p/n3D2mevvoMeDZf3CRqKAw916/vumRU8e9zIqVNsxnlnWcmYUW5OIT6AFXz74ZsM3/2vfvUrB2BUVVW58gHA4BQStpMw3/DNhvyAE4T9u6Z3OYl5Qf3EPf/887Zq1aoBAIPtI+Xl5TZ+/HiDzUV9oS/x8wNzBCAGdZA+nDvlAIz+Qabv2mxjnv6pTXzzeRtbs33YU0k2zL1Ip5KstCadlhWdSjLc23FqxkUAxqn5XKNRRRI4JSQQARinxGOMBnGCJZC+e6stvPPvbOm2Z5xKFOjWoRsHM0rs+Qs+Y1UCL/qKysxy80LSe3INAMaotC4rLi52q6IoAwAOgXlBmHuYE/yIx24FIEQ40hBFJNkGRjKAwUklKAmsasK8YLV0OOZFEAJ58XffPbwNjIkTJxoeA57Y2kDpQNEIK69hFTXUExSRZACD9Fhnt01/YZ3NPrRdbXp1GfVLw04IS3cTM0JbYAQewHwILIbQZ9TYLKUDQLQrHRCD/Kn9+X3YMysonxiWgqTGYV6kq2FkTh8AEAAOACYAL3D8pS0c95SDYYFKzHiCA2SAUcKqObEonPHMChTyjn5mBTnoj3rliquXru5utUsa5QEyOlSG/tCz7T0jbW1sngCM0eorR5aKiaCxYhcC5gUCZByeSYHNC2oCxAIGIhxGoVvnyDuYH4jI26zw+T3YADABMwPGBOBDcjieQaHkozoGwtiCjQsYHfFMDF84MC8YE0ZKgw0MX5YncBw2L2SwM1XbxnT8ht5R3lP6KeaBOzkFmxfxzAv3NFzfStN77IaSVruqQACjyoiz4Dr1TgMYfYACTXWWWrXdcna9aJn1HkDgO0/Xt5om4CAtXSwHzQsZCnNsLo74rCydBMO7JCAQ1k12Nkcvs/qhJ628mXk6+lTlXFj5+fb5TvlGmU/i39lkAAO7F8wlM2fOdIAl81B8fu7xgBgOjHRgkwcjAB3CfJAMYABejBgxwiZOnOhOMwrzR6gvAJxh3jgdAQxr13HXDdVW/MrTtuKxW2xK4y73DMMfNjVx7Pe2cYvstY/+F2ubd2ZIiq6nuAQiAOMUf8DR8CIJHI8Emg/vtYNvrlHWmFXIGFJe2ZjjKfau54kAjHddxFEDp5AEYs2NlrvxRauQAbSZrz9uk5J+7GEKsUs/+PbljbZVV3/dai78qPuxf6JtXgSRp3bKVkdrtY3orLGxPXU2QvoFgEKwf4Hiwspq8CgdKA8opCgdXMmDB+xItoGxePFiZ1wTGxWwJKgXJQBFAaUilA+KQrxSQh9pA38kGxiTJk0yPNtHOO2EOqmbNvAoM4SpI7RLOPjQLmm9nV1WtmWPjdqz38ram21Eb2sQU8IVdRvwgs0c2IJgrR1HPEwLtnyE/1CL+e9IYVRUFGDScYATMDqom7Vu2B+kEAdwgDFNgAjACl+W0h5UgFEBOEE+VH63A4GrctBP0sUfkKJHHj0/pdE3gAhfTveKC8wKXx67G9QFqEFe+uHb8XnNDvfl2e6+0daQMtY6UsZrZZ7jIzFsmS4FV8fwCrzBBkaf9iP2ic4X077omKMXo9wCSgQmhjrieu4lCUBBumdBAA74/Ilh8jBa0pEUddEzB+n034f6XY8VN7zzQIgUeO2Z7NM/fDHt4Y6lwIgYzuaFJCewSElIRRXSB4AUnhRX4uiTZ16441DdaSNS96Tc67GqrN4cHbcKiNHbBwNjKPPCj8MsL63PFuV02IrsdluW0WLTMmjvnWdguEr1p/fANktb/7ilHd7lovi2mRdycnLcN0wYFkRgaIXvP4AIfNfkCXMD8XxjgWnFfMI98Xyz4fsM6Y888sgQBgYGgQOAwTzEN5zswjdOfdQd6uWKX716ta1Zs8ZtY8MOBgBGSUmJY2Awj5Anfr4AXGGOiAdG6O9w7kQzMBhfTP1Vhy2lS6CRY+2oZ3qPUgQ6pHTzLitd/Y3p+48pHNNYUjQn4mL6pmNdHSrbYanIknEBPCpvaofKO/sr1KfvUmlF+7fb7I2/s1Ed1a588p/NI2bZC5/8G2vRqSSROz0kEAEYp8dzjkYZSeCoEtj3+tP26j3fdj/oF3/iGzZ6wXlHzX+iEiMA40RJOmrnVJBA2r6dNvvef7IFbz4pOwkdUsFQZAadVCJrFsUew52vfegrVn/eh9w3P5jjxN5lyT5H6aH1VtHXaqWFeZaV4beMoKzAxEAR4Qc8CkdgYqBkcE8eFIZgw2I4GxjsV2drBwADLAkUmpCf8igGhKmHH+TJLigQx7KBgYG/lStXun5SD30ezgYGafhkAIP8HYxTR6dmtGgrycadNrf5QHJ3BsKo2KjL9DioM8QF5gVMCRT9DMfEEP1eYZR/bFQAcsSHASrimRWe4eD1hixVSr3tagiQgb4DJHAKSGBioMp2qBNs76A/ABLk9wwJRch5oAJwxL+P9AJHmH5RLplZQbhdQIfbMqF02vNbU/y4aQ87IZ19AnJUX13KGDucNt+6UjmVhC0kAF06WjWFdwqDnl6hBMzgfcK4p3fxNi8CM8NvQRmUMj1E4kHqw4WlsDnQAFAknBpCfck2MnyrQ/9Sp+8jimwA7QLzQsJ36d7mhQAHxieARVJQPN6nH93mBW8LeYG9lB9gCCaGO22E8qQDOfGcCHvH08pI7bPxae326YJau6Lg3QUwuvZuss6XfmkxARk4vneOVeYYZL5zgAvmBq6k+efJs1GvJSfScAANgWnB98j3H8J8+8wlhJF3SKe+Rx99dAiAwVyCAU/YGJRjbqCteEfYfSOqjzkFsIEwbfGNv/zyy/bKK68MATDGjBljY8eOdf0mP3WHuYdrADVIO5I70QCGAxw0plh7q6WLNZMm0BUHUJEhxkR6S4P1SQaEU7tlH6S9xbLqaiy91edLFeCR2doog63NltXZamkAIJqf0gWs5bQ3WYbeywGncadpDsvWyTjiDg1Ex99EAEa8NE6P+wjAOD2eczTKSAJHlUDlml/bi//xd8oTsxWf/582ftklR81/ohIjAONESTpq5/0sgVhLk+VuWO2ZF2ufGMK84GcvVtsbZOBw16i5tnvmmXZ4+SXWOX3Bezrs3Ma9NvbwOiuTcUQUlKCMwJQAYEBRQAlA8UBZCSuvYWU1rJ4ST9lkGxgLFy50zAgADFgYKCsoEjjqQHGhDhQElIxkBSHEH8sGRgAwqDeAIaHeoBzRLnHBk484fGiHPvS0tNkyGfNc3nTAKfesqaMSc4ypYyAkhVE3PfNCxk5hZigfyj55fdiDC2zNGI5pAf+C/KGMZzh4FRYVn3cH8CIAErTnmRg+j2NcqC0PYCjO3fO+eaeg7onnCpuAGv1pIvHMCqc6uzy05esONi9cG6oD9odjcujK2HL0fuQKjMiR4rPbbSWZay2pGPMMhjfzJDexMJQHF54vCi5bTDzzCJaF32rgR+uBikFmBj1jNEiDeuLDlCONERLP2Hx5f08cEBG2LVix96vPujmi6+Wd5Fn1K+HhtJFQwNdDXfSHtrgfyrxQpJx6pZXs3j6lC3SSBBUHeEEZ+oaSrTD2L7CJwYq5G0t4egrGuZGpHfal4mq7RqeR4GrT8mzd7Itt97JLrWXaAusZOT4u99u/7a6rsq4dr1lPvQfxsgTGFnY1WYEeFfMC3ztzQ5gXwnwQZBZaBtggL/F8d+65qxzvwdHC2Kl47LHHrL6+3lUFwAB4AcgwatQo1y518G3jw3tFnSGe7zuk830T3rhxo/NsfwMwYUsbgExpaaljY1CefCE/efDE4xOcxpQ1ZorljJ7io/UNFM8/z4rnna1HqOcXmBECD5xTf1PEbIgJEDGBAapQJ03qKuDAMSY6/LwYUz6YEamwI3Q4Qkz9kbDFmBCI0ClmhIxr4mBQkJ6qOStdAES68rt4gAmBFeTVy+faSNG7laZ+ZLYpX39/aJv7dD3bdL2jqapfD0hfmIwUHwWocI0M8ycCMIYRyikeFQEYp/gDjoYXSeB4JBABGMcjpShPJIGTUwJpBypt9t3/aAs3PO5+/OmQv4SOorrqp6dV5cng4aVfsOrzPmJ9ObnS81hpfu9cvox3TqzdaGV9LQP0cMALFA8UlABMhBVWQIpgAwPmBKAGCg0KDPmHs4FxxRVX2DnnnGOcFHI0GxjUh8IR71BG8HfffXQbGGeddZZjeKB8oHAAlOBZPQ1KCEBFUE6CwhMADMIoKA0NDdZaW2/nbNpvK9sOi4HgmQ7xNi8AGoYLZ2r8KLSotKjR/u9g2NuaiGNOKBMMhuGYFZRHRYVZAWBAGB8c976+/vIu7BkWgA/xeYdjVnibFz4foASACI7TSCgbGCLc+/LSsxSgBP3plPJTnJZqkzIzbFx6ho3OyLTXO8rtV13TrTqlkKrkdKKOicXDNhApRrwfqSliYqgtnvPg6STIDRdsT7iRKwxsxPeBwo9yByiCzYzhwuF0kkzlBRwYzO9ZGSh3jAZ/bMf74GxcoGDqPtEhBeKo0yuNXjL0IQAk8WWU240ZmxekkwYMRR/pq9KdgizwzCmXDm5S/FCXDGBobd2qMwpt96g5tvGqz1mLQNF3wvVJCe7rVP+k+OJSa3dbrmxi5LXWOKCSbyt884AS8UyM5PZJZx6hDN8Y3xzzyXBh3gvSt2/fbhs2bHB2dqgPgAHwgvkjbG+jPEAD+cP3THm+8TCPBOYF6fiqqirbu3evmxOYGwLAwpyGJz/xIT8GhQE7Qv0JY9P7XHbeR23UxZ8YiE7NzLGUzGy3VSMmsCJNzIh0sRyc09gzGg5bhk728MwIbf3olixkKDW7ttYymzxYk6J3JUNlMsWCyFQdABMxyQuAIaej0bK6/dY25hkcwIafeQbfOWwuEe8dECmu//Sa/ugQ5/IqNdRHzhDH/fG6CMA4XkmdOvkiAOPUeZbRSCIJvG0JHC+A0Vi1w6rWr9KP7IPDtlWo1YBRMqKUXVSekF63e7Md2LDKsgtLbeTcs6yt7pDqecE6mv0/miFzyYTZNmr+mZaR43+ERgyMIJnoGklgqARgXuQ55sXzNmPtk0OYF6FEs1ahd5TNsF0zV9iBM6+yzllLQ9J7es1r2mfjqtdbecyvRqIcoIzg+XEf7rmisKA0AGIERYM8hIlHSUm2gcEJIZwcAO176tSpLg9KTFAsgmKTrIgkC4W963j2rbMqGxSKiRMnGkcgYl+DLSrUTV1hpZd2CANioOjQb3yIJw2Fxyk9us/dVWX5ew/ZdAEZU3SkLIoq/wFD8N+xwqHfrMcHGxToCyj9AAWAH6QNsiY8QOAYFooPLAzqAVKgXNgaghKM8/UpzaUDYAR2BW3CnPDsD1RkHP3w9fu2fF98/aQ7XoDqRqbIjaNic7XNI1cKWq5WlVuyS23/uGnWUVhMdp9ff8XPsTK91yWydVEkv+lwhz1T2WSNHYBfHAcsRU65vB0LxWk7DUyMXtmO6NPWFFgRhAMTo09bUYazkUGLHhgINjAYOYr/0cJHtmUR2lMFzgUbFyEcroF5gUS9tENKuNIHnqa/+j55JoUG0p+JN8aX9zYvPCDgQAv3BGFfBIaGrno39RRVNpQPbflrMoABq6tdcthXOMHWfOTr1rDyw4kF3qFQX9UWS33jccsRKwnAku+dbx8f2BdhzgCoCo408hCHD8AA6UcK8w3X1NTYvn373HdKXtpk+wrtUi60TV483zR1cw0ABvfx3zfxHOEMSMl8AFAR+hXGEfJzxTNP4IdzfCvTx8+2mRPmJCULKOgSq0JMiQyYEQBBcjAmMsSMyOho0ePV81X/UmUDBRZFpraBZMKYkGMcGQI20rWtKE3MCLZuONsUejOyBIAlb0t0hd7DP/U6Mnl32TTbNftsq9L71zVhxnvYm6jpEymBCMA4kdKO2ookcJJK4HgBjF2rH7VXf/JPVle5cdiRjFl4ni258a+tbOrChPStT91vr933bSseN8OW3vi3dmjLy/b6ff9izYf3JeSbeNZVtuwzf2sFIye5+AjASBBPFIgkkCCB1IN7xLz4pi1e/xv9uOzSmjHKylB3KKPInr3wC1Z10cdkkU+KYHbO0EzvQUxOg4xWHnhNAEa7s3mBgsAPc5QRgAl+2DulVvGEw6orSkNQeFECAD5YwbztttvspptussrKSjeaoqIiF08aCgh1oUgERYYwPoSPJIJarVDiASFQKMiPm5gEYASliDxhZZa8AcCgDGEUGVZWQ5i8XS2ttmDtTltcs195pJSK9j/ItAB86BkmLAOm6j+2IQKLAdWzXX86fRf7mRKko2R5ddyno9x6VZUwwEJ/Edcv1zfSFS+IxW3rIA4mBKeAAEqEsLd54cuzVYWTUagvOFISmRQhxV9Jh7nRqedamJZik8WsGC9AalR6plWPXGSbPvYF65w9qKhRc0yerTNs0OB++9Orbe0DP7fWailpGeVSDmFPkAKTIltXr9h6+xSwLRSW7DjRw9vIiGdmxNvIUFEnGeoKLvTgyGEUWFgUXtlluxIr84RRqsOWFfRIHx9qGrj2y88zJbwSOpA2cIPk4pkXjAGbFvHMDM+08E8nFPTPxhv3bBXRAUOe4XmFa8g7eE0GMEiB3bU/Z6Q9c+03rPai6wYzv4N3nXs2WNeLv7Sshv3GN81cgGM+CPNEABaI+3/svQd4Xdd1JrpuRbtoBMEKkgA7CVLsFKneJat3ucZ23G3ZTpyZeWUmb/Lc48TOy4w1iZ8/R5asZsWyIslFtiVLpESJVexVbAALQIDo/db5/33uAg4uL0BAbCh749s4Z9ezzzrlnvXvf6/FZ5pB3yN85viMsQ7r8/3hTvNaKSOC7yDuk/3AfAb2x+uo+861jZn+mM/+3O+F1Oef/fF4CnZomuNi36yv7ZnHY6s7Z6bTBS/yb+pokTvN9XfVcFWnU2PCnhrM8hBXmvc1JeXkay0+NWyTLOvJNvnup8BVdMl2D+eVyrt3fEkall0vkofftkvMKrxkghiFB7YAxii86PaUrQRSJXA2AKO56ohU7VwnjccPGvZEFNTCdCGQnSvZhXANNn0BmBhXdDMx9r36hGx68rviB71xKtbL+kD5bQcLI55cT6l9BUMFpn3xrMVgYlwlgbxcgeF4/qLaYCVgJZCUgAeG0EJgMNHbSH/Miw7MUFcUzZCKWSvl2KoPSXv55YY6f6m8jqRewBwwMEpqdhgGBgEGVUbIjKAioswLbhmpgLiDuw7LHn/8cfnhD38olZWVphr7UVYH9xkU/DCJAf6jcqEKDQEQDaWlvRkYzGc563MGlYqOe1aVx2akMsOoipCp294pyzYfkJUNxwAOEACgaugo50xz4YLjG4SAhVNO5Z3BzawgEEAQgyqIMi2c+sm5eBQ4zArWIXDAvpQl0cPcYB+O3YrezArO0ZPBwHL2QxVPWRjmeMk+OQ4G1qNyTPevZFSYCKUvnFkotQC0u4rGJusRKIGhPpzlWDAqxuDeJbOio3S2NF57s8RLpph6ff2r2b1fKt96R+r2HpWWitMSx/oXL9wrOgwM/ohAseVyEmPHgvcC0wQqHGYGMgy4xC2VSmVKkHlCA6DMS/fckCnBOjQKynI3o0KVT7ZlcBRW7jlpk2fuCUqVkuS95YyrJw1GD65+WaJOZnobUMu5ZrymDmjFOwMAFVqGIecIwIsOgFV1WD7T4qF7ZJaTpWGuKLYMuIZYnhInqGWMeLIOy51rZqqk+ZcOwGC1E1njAWD8Z6m76ZE0rc49K9pwEjYxtovv9FEJddRLZgKLV3APEWzQdwaPwjy+BzTq+4HPHJ9LXge+Rxica4Frijx9TtmO5azPZ1Kvn2mQbMMy5msZ22v/7ueZx3NHfQ/oscwzj/cD67jLWM73B0EOHoN9uwPvnBIwJkrxDbYEnoqW+h2QxV3nUu7zDqKxaLJz+MbiW4whhuejy5chUV9S/loOI7thLAOLJUG9BN4RMXgPygB7ZDyMPIfi6b81AR9JF9wBH5i4UN574OvShskzXHhzPc0B7b8RLwELYIz4S2xP0Erg7BI4G4BRseFV2YI19gQgLnvgURk3e0naTivA0Nj50r9K3sQyMC3+bxk74zJTTwGMxlMnJIjZ32nLbpBFD31N8idN79UPmRq7Xv6pTFywSpZ/8m8ld/JUC2D0kpBNWAngQ73muMx/6vuyfNersNYe7ZN5URsokLdv+bxU3fJRiWflSAIz2qpMDQU5GhsYDbCBgeUS6iKRgAMVEY5TAQhNp45ZmRdUAmjD4uc//7n8+Mc/lmPHjpmq7IORiometyoeqX31l05VWrRuaWlvAIOKCIEJVXAUqGB95qmyoooJlRRVVEj7vuyd3bIUShqVANYhk4HMCQZlPoQBHpj2qOUosVBC0YC1DJMC+VyG4Yfe4GZasA2ZEm5mBgEDAhg8nvZPEILBSTvghJPmeKg+0zgoFEUoIEwTPOldnynWcgLBCzIrQkAwaLNiGsDriVBYWsYukL0PfkY6lixL1mQrhydBoIZQlYFvUB8oFKfbu+ul24nBDW2ko1OOvr5O9r3winSebsF9lI/r7gBXDhODM/fOTDptXjh2L3Akc4+AiQHliQY042BFeFzMjCio9MrU0PtIx6AMCpb7/eBAJRkVWj8OoIB5qWltby4eEo43ESprXPpCWxpkTjjMCx/G9IB/r3wir8LIvBPPfCMU3WbayEBNyrgZu024aG0oq8f+1ug0OeqZasoJXjj9EchgFiCPXt5HzB2AOuyv73CpAAxjgDQMhb5qv/j2vimZLTUGuCB44QYwOHJeH74vCF6ke+5ZTpCC7fg88vnT51vbc8vnJTUQXGAb9stjsB2fea1L4MH9/LPv1LQ+72zD/dQ0+9DxKCDiHgfhgBthy+IuaYfXIT7r7tJLv0/AgvaWuvCOiAKAjGPLEAF40ZRbLJ3J5cGAFwFUZADIzJH2giKJYJkOQxzvhgjq5NackIXv/UYmw05SusClS42BXDlctkz23PNF6Zi/svsdn66+zRt5ErAAxsi7pvaMrAQGLYGzARhH3n5JNjz+LckI5cvln/07mbTw6rTHOPD6s7Lpie9I7vhpsvpz35Li2UtNPQUw+DFXPHORTFl+k8y45j7JHjOhVz8EP7b84vuwk7FaVn/hO5I3ZboFMHpJyCZGswQ8HW2StWeDFMMOzbytf5KZjYfSiqMDH44VY2ZKxczlcvyqu6QDdmmGUvDSTR7chOa118j4zlopDCTMUg8CEjprqlsqG4xMpyqPBDioiPBDn4rCk08+KY899lg3gDFmzBhhJO2cXk6odLgDFQW2dSskTKtConVPnToljDwGl4NoeWkSwLj88suNq1a21ZlVnVXllsfRSAWIUetqOg4FvHj/MZlQeVKK4FqwELOrbmaFsibcbAlTDkUIvZsxcVaSwIMfig3nPzkjzzTbUDXlsg9lSzDteAGBDFBP+2e+YVKYPKcd5UAVmGAH69IjCpdvcPlKEDELNisyodRlYZa1M5gn1VPmSlvxeDZz2uHoXLShzIp83J9hvNsbrr5R4qW9QWzT6Bz+nd57UI6t2yR1eyqk6UiNxDod8IDMBo3pmRgAT3B74FJBloROAKLgflFbFGRYqDcT9/BSy/tOQ25wRWpscMAOhwYHNCDzgpEAA2eoOVaHeTE1USuzvafl+qxauS6nEaAUXH4itjDGwObB9ejCoDvAEmlHZD6BjPcjhVIRLwITIwQmRjauntN/b5sXDvhEEIR3Ea9WuhCEclk4fbZMK8yQB06slZult1JZlVks79z2Ram56k54uoBrTbjOFHqwoDAZ+IzBA4UX9hiM54sk6OVDPT/YBOrZAoI3F8BL2wswKOpFO3fwtdRKBlyrBmFMku+EcGGRtE0rlUgo11Tjc8z3BKMbvNA++P7QZ53vAj7HfA712WS5vmOYp8+5tmee1mee7ms9fZbdW30PpD7vrOMu0zQBDA06Lk1zyzvn9sZT8pAvPTPBXTd1X5kRBjwwPeHK45zJjIiA9cDANwfL4zB4GwXgHQUoZ/Jx/0dRJwYGRZzvUT4sCHHIvBPGQ8N+eIbh9UP7KNxhR5GOg12RSAIYMQATYSzxiACwYGB+DMeIYclHHNcvgfc4g78Z3KEj+2T80Z1SdmKnjI00mfzUfzWBfHm/bKVUwvNK49JrJTapLLWKTY9wCVgAY4RfYHt6VgIDkcDFAjDy4fZr+Sf+Txk7c4kEMmFoLfkjqGO0AIZKwm6tBM6UgLeuSmbAdsxlW38r+dEOyaathDThtD/P2Lw4cQss1OfAIO4QsXmhQw02HpPC4+9JbsdpyfB5JS8EBamw0FDCqUDoDCmVDKbdTAztg1tVNrhPJeLpp5/uBWBMnz5dysvLZe7cuSYS7HAHnTHV41Gh0BlTd71169bJO++8YwzwNTQ0dCs2pUkAY9myZcbLCZUjjoN9MDIwrcqJKjos48yrlpv6YA9EWtsk3tQiM9+vlNmtp9DWARDoLaQLyinVA6qE9A7SbVwTuVEwIQgu+KAUkLuAlEn31CeYYQ5n2pOZwfYMBCR6MSmQ53gfoTLjRNZzgjPbT0YFLpuMhZJSBAVnDK7ThIAfdiv80pY/Szbf+2lpWX21NjL9UPEia0O3QsVoAMyK7k4GuBMDkyXa2SVH/rRB9v3qj9JZ1wjgKhP3isPp6IuJQQCBJikdp7W8TzBS3HvmIuixmU4NSTma7HT1k+UOw4L3BMZi2B9OR05+emWUBPw7vbvkI7mHpBAMljxEBTAUsGgEu8MwMXCxqO63Ik0Qoy3ulbqYH0yMCXLE44BJ6W1eOONIvdKay23OuPFS/shHpHxOiax6/gey4sQmd7HUZBTKpqs+JjVLrhE/DHP74cXCAyBDgQkfrkdmA571tmbxEZjA/cpAg5L5zaclM+zYg6GsEwAUglgGkwNvFxkp7zdjqwH3ubHRgEtRhRn9vYtWSvOESaY/PsdcVqLvDZPp+sd81tFnku8Pgh0ECviMKgCiaW7dQZ9fvjfczInUOizjM836jPqe0TTL3e25zzp8/xDo0KDH0zS3vAM/CIDBZxlXRDqx7CKKGAOIyMClHc2hYmnNLTJpPqUEKqLBbDAjxkhXXtKgOsCGCH5LyI6I4z2aSC7JiwYypb1ogkTzxgC4AmiF8/UQbMaSEIIhZsCmZ+zzey/5DLEsxpPB1oBNAEEZsva/J/Nf/LHMOrpZcsESgu8Yk5/671B+mbxz16PSuPxG3KCwb4TrasPokoAFMEbX9bZnayWQVgIXC8AYM22erPr8t6WobEHacVgAI61YbOZol0AnXN0d3CaFezbJrM2/l5k1ezBPm8A8raOEqnjIvDhaNFsqZq+QE5ffKm3lq8y6/HRr97XNpdhm1FfImMoNUhBuNAwKLh9hJMDASCWEigWVDfdsKvcZ3cCFe/ypAMa8efOE7IhFixbJ4sWLjT0Md30qLYwM7FMVDHc+y1599VUT6Z2AxjxVsSktdZaQrFy50rhqZXttS4WE+1RIdKaW7VJnYpkmq0MNfUZb26Vs7xGZ0VBlFFJlQ5A9YebJocBx38zZ43hkWHDOlHoBmQNGMUCawAQwD9RzDG46/ZCr4YAXTppKLwEQRqeNeikhw4IGNQugiOQDrUjAXWbV5DlGWaSnEaobObjfQuBi5AA4yYUiNIbK0YQSOXXVDRKZOZuiu2Th4O/XyI4nfyUdtQ1QaENQkjBDbADzvpgYlAxnv93lnGV2FKuBnkgPo8JwVCBdKqS8WrgYJrI/d58OMwKZrpCQkngtbF7UyjWZNWBewB0m5EvGC4EnGnRtwYVvxf3UkgQwwrjYDtBFVkYcbIy4NEBD3BiZKAc9EwZl8yLLE5dZgTaZFnBAtmBWnkxatESmjC+QObvWyJS2E66xirTAYOrRCeXSOG6K+ABc+CKIGJdhW6CmNwoGDjxgBJhP8ALjYwiAaZEVgacMBSpwbriJTR0s8Djj/WYauf4dhR2VLfOXSfOkyQbk1PeFvif0eVSgQt8lzOdzR8VZQcd0aX1+WYfgR+rzy37cgX3o8+9mV+h+ujI9htZhH32FrCmzJQd2Y2Y21sn8FjATjNLPewnj4MOemtaOkJ/A/RPFuzMG4JDMCDIsGGhzoiu3AOBEnpMmAEpmBBgTMTIjsPTQBMggCqZFHMBGAmAlXs6oifZ4P0RhuyzGCSkCPpAJf2/O9ptDyWlUaJHHydy1XpY9+32Zd2Kr+Y2jIVJ3aMCzXDFhnlSWXyEnL79NIqXzcN49SwTdde3+yJaABTBG9vW1Z2clMCAJWABjQGKylawELokEvE11UgLbMPM2/06KWmukMNbWSwXSQZ0GrfatW74gVbd9ArNk8DSCj9ChGBTAGBNtllAo1M28IIjhZmIo84KKA2c0qUiojYx055UKYFx22WXGveny5ctl6dKlxh1iajsqITpjqktSmHYvFXnxxReFsba2Vk6fhoFIfqgjKIBxxRVXmONQCXG34z77YlBlinVU8VEmBusxMh0BgDEVAMY0eCPpz2YF9RX1BsJ18AFE1icw4QAdTppgBwMBDbe3EgIfVHiViUE7ClyKQCADu5ID8GIGgKQZsFsxHdtEbplsvPsvpfG6m5wO8Z+qEzkVzja5z5lXMl2gLF3KcOC3r8n2J54EgNEE5ZPLh7IR+2NipI6Wi16STIzUon7SDqOCTAu2py0LgiJkWDjMi57yvjshDHWz7JAHQodkPIyZjAtQxXPkzcvJ+6ceim4TYgdsXrSDcdGMC0o7GCznch8CG3VIGwBDJgzK5sVYb1g+nl8nd+frcgbHrkQAAEoAAARUeQ6nOxA0A3/BzOZz7Axm6+wm08k8k3L+OfeNq1KyrK98V1OzezCQJ+/OXCCtk6eYZ5sABsEKbgmC8jnl88f3hjIzUvvoL61MCwIfbE+wgf2xX32G3e1Z9kGYF+yLUcEMd5/u/eLrH5bJt35SPABHfZGubqZDnCAsASOCu3j+NK1tnXwu52AO/ulWKwDEIMChgSuceE25VMxD67smgA+ENI1zYoPAZWT4z7po2xeobJoO4l8WDFOveuo7Mv/UDmeYKW0Pg3nx7r1fl8aVt0gig+8Zy7xIEdGoSVoAY9RcanuiVgJ9S8ACGH3LxpZYCVxqCXjrq2X2M/8oS7f+RnJIq4WLTXfgfDgtslfnTJD1d3xZTnPpyBCelVIAgwwMAhKMZF5wSxCDygKVBioijFQY+HGvygnzWM58zljqDOszzzzTawkJAYzrrrtOVqxYYSKPwX44m6ozqkwTVKCyosfUPN2+/PLL8tJLL/UJYAzUBgbHqrO03CpoQaXHgBekkHd0SfGegzK+Fp4XoNs57IjkHD7ABYdD4SiqLHcYGFQCHaYF52+VtWG2yCd4YRgWRi1JJFkVPsmG4uIJhOQEmBVNMJgcNb07yin4ClKEe4p2K4pgCNM7dqJUX3mdhOeWu2+9IbtPryTH1q2X2l0HpOFQBbySQPE2AAbdqhJcoeJDNg9kYGajexQ1x0aGpllvMEyMdIyKvsXkeAhxZt2p9E+M10mZp06WB2tkVVaD5IL9kovrRPaFz9EcUQusB9w/BCk6cE904DmgPQwyMrj8h8CWYycjLgcjuVIRy5VTUb80GJdeHIuBuLDtDUSwhKHY1yWfLTgtDxX0fs84pUPn//v+XHm7bJ40jp9o3hl8L/C51ueY7wUFGrivZdwfSOAzyneDO7A/fYY1X9P6vtBjMu2OboCDbfW9o/3wfcaYGgzzAuzV/PLVUrjgSmOXhbZWlOnA/TPSGLsJOFetl9pvX2mOgPcYpdQDazh3C/M1pJZr/gfZcnlk/vZ1UrJ9rcze/7ZMhG0kd2j05cjR8XOlovxKvIfuAvNirrvY7o9CCVgAYxRedHvKVgKpErAARqpEbNpKYOhIwHf6pFz2s7+T1XtfxUcl58Z6B85+Nvhz5GRRmey4+ZNSf9Xd4sXHvAcf80MxKICR11lvAIocGAmksU0CDFQuqIhwnwqHKhuqFDBNpgSBDioHBAGosBD8ePbZZ3sBGAsWLDDGNQkwcJkH27nrU4FgmkqDu3/KjGmCClQ6fvvb35rYFwODNjBWrVplZnrZVgEJ7ZPjJECiQftVAINbHof1PbDhkL37fcmrPm6qU5lQbyLaXrcGtkAbeivhsgKmWd+xkeEoIZyND2OGPpJUjDKhEJNVMRMgEZkVGdmT5d07PiV1t95t2jutqLzwjwoMLVcgjWuRQH0IG6mhH2KQeawrLAd//5rsevaX0llPWxhgRXiUWUHVjMpdMJmvqhoBDqzxN8s+yKRgGmymQYYeLyI8Xhb6Y1/KzIBXlWQA1IA9h6VD2V+X2Cd3ZB819i5yAUJmYVjZWAKQCTApA2kGXmfawiDzohNUnA7cAwQsmKYBT7IxeD+QcdOO6fS6mFc2duTLwbhj7FKvsekszb/hAmDsx3KCt6bNldoxReY9wfeA+73B90gqE0PL05x22iw+k+mYGHyGFWxgOdOsq8+8dsY0n20FL/Q5ZznfCwQ/NKS21fzi6x6SyR/6S/Hi/eWF3ZmRGLL2bpalz/69zD+2WfyG5eOGSkSO5E6TdXd9RRrAvJCcXACvI1MOI/HaXqhzsgDGhZKs7XfQEmg7XSXNVRXSUo1YcxQ0OAf5Lpg6A0YfF0pW/nh4wcAPVfJHvK8DnNq3Uar3rMfHS6fE8IHqx4euPyckxTMWy4RZy/HNkvr531dPoyffAhij51rbMx0+EvDAcn8IM1ITdrwts7H2vKz5aK/B8xOPpgdbfFnyfslSqcTsXP1lV0rXjIWDnnXr1fEFTgROH5a8I+9IQVejWULCZSQEJAgw6Oype6sghg5Ly5ivH/3c/+Uvfyk/+clP5PhxR/lfuHChXHfddUKAgTYweAx3fSoPVD40T/tnPsEN5jO+8sorQhZGfwAGQRIGtqNiwq3OqLI/RlVYtI4CGJqmEhQmIFNVK8GaevHR6GEXvZE4rArDpNBBJre8B7qguJKZofYqcsAo8Puz5PikudIwdTpo32CqGGjDcA7AqCCrguyKoAQKxkr1FddJ14JFKT2PjKQyMU7v2S+Nh47CwCe9exCEcaIHcvDCJoCzJeCnZYSCyEAgaJBka5gyYzUEeY6Ni6QVkWSa9YwVEWx5ZRgZ9JsjNc0yHiMKV8KnZWKiXpaBebE8sxHeXWjzguwLr+QAdMoCUyQTTAwG9qLGPMnQoYHWVtwDZGS0YNhtxjsJwQ3cc6jbBADjrY482Rt17Bywj/7CBwUwOC6+j4w3HGwVao3jPMIAiqK4L+m1AlYuzOHpzaIzmNPt6ULHRBsNYRiHjHN5gLHrgBIAcHFcqxgUeNpxiKJtXf4YOQ7mUAsMfkZOvC++cLsBMpWhpe8JPod89viOcJcxn88l81lX63HJiTutz7MyOFhPn1l9V7CORtZj1P5Yl/Xa29sNYKrvG25Zp6+Qyrzoq95wzneYF28nmRfrzmBecMlKBPfLobFzZOODfy2tK27CPXH+lqwMZ9mN9rFbAGO03wFD6Pyrd2+S45vXyMldb0ntgU1ArDlbITJt2VUy97aHZUzpIsnHB5kXbpz6C9t+9U+y8+WfSLi1Rbpg2T2rsEAygdDPvfnjsvjuR/Fj5cxi9NfHaCuzAMZou+L2fIeDBLz1p6T8iW/Lqh2/wawUPD8klVAdO5eOdEDBqskqko03fUZO3fQR8YCJ4IFruqEcfDXvS/aBtTIm2iLFxcUGWKASoTYoqDyostHXjKnWJ/OCygHBgOeff15+9rOfyYkTJ8zp03jn9ddfb4x40qAn3amyPhUK1me7dEEZEsoEURsYNTU1aW1gECAhgMF+dSaW/VKhSce8UAVGAQzWpSLT2NgoLS0t0Imh1HBJy+FKCZyGNxKUM9ITiXoTYRsGqj9hngdcp9JmxaxM2KwIZkhe1jh559ZPyal7HjH13Mq0w6ww/AMDdBmPApD5SAxxGJCMQpaHX31T9j7/H9JeR2o62SX8juBzwu8BekJwMzHSAQ5B1CKTgmwJfpsMNM26bKP1NY0sE3hlRZbE9srVwSPw7JKAZxcnjxBJARThfFybdABGJ1k3qEqWTSPuAXok6cANQQ8l9EbSniynR5LX2/Nk9wUGMKhs8n3E5WwxEx2gIgLZtmXmS2dmLrxbwC4I7DQwhLPg4nXsJAmHegMrERiL7Mwvllg2ZtqT92UCz1LcDzsUoTESQ7sonuNEJvrCu6798DZpWfeCSP1J83yTdaHvDb4nGPSZc79X+PzzeeX7hvX5vKZL89l0t9f+UpkXyrTQ9wbLlXHB9gQwGPsDLcxgk/96mBcZYF70/93rbjec9jP3bZFlz3xfyis3AR6kA1fn3tdziCCnxZMphyYtlJ0PfFXa4TLVBisBSsACGPY+GDISqNq5Xio3viEnt78pJ/dvlDBmnxhmrL5B5t3+MSmavkQKSuafFcDY8twPZOeLj0k7AYzmZskeN06yiwBg3PoXsuT+vzKu5vC5MmTOeygMxAIYQ+Eq2DFYCTgS6GZebH9LZu9eewbzQuVEryN7JyyQo7DIXrsEs+hzlg9p5oWO22sAjDVgYDR1gwpUOvjhr5FKBffdW1VGtB8to0JAZeFXv/qVATBOnjxpqtAGxrXXXmsAjPnz50t+fn73zCrrU2Eh6MB+dMaUaZ1JpdLBY/7ud7/rdwkJgRKCGKzP9hwPIxUkRvbHrbtvPbYei1uCF21tbU59jE8AYPhrT0mh32cUWTIrglDijk+YI/XT55hz5Oe+w66A4gul0WFXBCQrVChVq6+VzkVLVVzDYxuBgcK2FrhkTC65gRwFy2o8MFrojWC5BVgFDB7U84Ka7wl3weMF6ibz6brTDy9e1hsAAEAASURBVNaKFwxOL9siJqDQN+/bLw3bt0q0HQCRCcq2cBTco/E8eT9RBJObXN5BerqTTxsZ9F6SSEC1QnfUhz1eSp3tqVSS4wAgAWW4/H2Ug5WBpRwYhiEUOO3RDCEOI5x0b7oysV9uyKyAzQsw5LlkBLPMWZhsycJ+FvbzofTnclYffVACZFcQwCADgwwd2sRoBmjRinG0oxINsnJpURixHXlHI1lSGQUrJxqUukR6ZTgAhlLR7HkyadpkWSL1Mt8L96Y4thEFD8wT5LjgKrX02A6Z0HXanAO9kBwZN1dOT5guUVD7o1C2E5hoSkB2DHGMPULQAWBDAgyKBM7D5KNuJJQvsQzIHP3q8x1HfiwLHjACWLKUrIsHCG1hDwdeMAQgSBxsLVOGduHa49JxaItEju8TOXVUvGBi8JnWSEYFgzIruOWx+LzqM95fWp9j1mXQZ1rba5rPN/cZ2J8+43wXsC7fOayj/ZiKaf4Z5kUp3lfzV0kB3u0qlzRVh22Wt6FWcuBVawJ+48p3vS4l7VVpz6UOdk72T1sGduE10rD8eolNmZm2ns0cfRKwAMbou+ZD9owvNIAx51YwMO7/KgAMzLQYKuiQFcVFH5gFMC66yO0BrQT6lAA/7sp//k1Zvf0Vw7pIdSWnDflx9/qtX5KTd30GNi+wPjo5s6nlQ3WrAEaorc7MmNIGRm5urgEs+LE+WCaGKgQvvPBCLwBDbWBw+Qj3uYREFRXKhgoFWRAEStzMDCocDJyNZfmf//xnE+mBJJ0XkvLycrNERRUNKi7sWwP70ZlY7Zd5Om7u67G0bhdmamOVJ8R3ulZmYKZ5FgAe2qwYB9eRb9/wSTn5yCe1e2wdxYqapsMnQBaZhjgvaHGuekN/19vaLMFanDfcbppAoAJuhP3tzeJvbQCI4cjV3wn3m02NktHRIkEAFj4AAQxM57Q1SEYUXmug5PsQE1AqM+CuMzvadgaLyTTCvxebQ/JUe4k0JQogRbqOdJRsnw/PFRT0RJzKKUAUKOZeL21pEOBgdGQfB4gSi+GYqE9bG05ZTznbxk17lvcwpNiGZZd7DslNWScAXIAXgsjlQIzsgde0GK4ri5DmAhUCFrqEpBOKMW2kqDcSxwaGMyoyM7i0hHUzAX6FAVy81pYNJgbP78yQPbZYFn7k4zL7jtsAAEHuAD88ZLwCRElwSS+OxXTmsb2y/Om/l8UnNplOTmYWy9oPfUlqr70fjAnH9o4uH+k+CuWly0E0E6BIHNeGsmQ7ghgm8KS75auVk1uT75yfPm+OEcuotGH5cMu6X0kMgAYDn2tlYmg6nU0Mgg58PglispxpPocEQJjm88xyfS9wn5Hl7J/PurucaUatz/YKYugzbwbYx7/iax+Uybd/RnzwskGvIiMxBA/tklkvPCZzD6yTgmgrYMPexlL1nA+Hpsq6e74mDas+BHYh3t/D7H2m52G3518CFsA4/zK1PX5ACVxoAGPurZ8AA+PrADBolCv5Q/kBxzrSmp0NwKg7slsq3v2dtNVXSYD+vvExlC7E8dETwYflmGnzpfSKOyRUPNlU2/fqE7Lpye8if56s+vy3pahsQbrmsvOlf5Utv/i+TFiwWlZ/4TuSN2U6aKOoyg8aG6wERroEoKjlbX3TzErN3PO2TG8+kvaMOzHTfrh4nhyde7mcWHmrtC9Y1a3opG0wxDI9rafFV3NIQq3VYGE0SMgLBROKApUBRu5TOaBCwcAt08rIUMUl9bQGAmC42+gMqTuPCoZ7RpUzpn/4wx/kj3/8owEv6uvruxWT0tJSmTZtmmF40E0r26qywj6otLj7cu+zXyo9VJR0HNzXGGZ+fYMkmpqlCDYBxsImwKTJ5TIGTMTqVVdLxzLH5oZ77Od1n4olQARPuEM8ZvbdAXU8UMa8ABPIfGDw4JwFyj2BBV9HG2a/e4AbUw5WgBfsCbp99KKtJwkO+WJgvnRhwUHS1pXpDP/8qJcJEEKBCrIovKjL9v5IZ097/Nb4w50SQHtfHMfmOBDIxiB4QUOAXpOHfCjiQaShmuKX36lnKrv+/bIpRx5vnSpNgmULhoHBHx4uZSJgAdewaE+2hMPIYD5tZ9BuAmbc4RWIfj2SajW2UMgNc8MpT6DcYVrA/oZp18OAiEN2LL/cc9gAGCHc8mRgkHWRjchlJAGAUfmwg8HoxT4XwND9LT3LEMwIY59LRghWNGP5UTtkRg80LGtDHkEO/oS2YinJG+25sicaQqonBLKzZOzs+TLuskUyZfXlUlw+32GusB1BBYIG6JP3N9MZ+B5Y8cQ3ZUnletNJAzxE7J5xpVQsvl6aF64a8Cy5UeaT9wPBi76e656R9r/XVVMJJsZ70nF0F+JeoDyt3e8Rvj80koFhjo3u9JhM63uG+8qUcJfr88vnlZGB5azPqOXuLfP5HlAwwzTq459hXpSVS/68y8G8WI2+R+53asb+rbLo2X+Q+UfXYyFXFE8an5+eQK8jRyYvkIr5V8qpFTdLuHSeAbj0evTUtHujVQIWwBitV34InveFBjDmwX/2svu/MaJ/FD7oZT0bgJHAR1AMH4pH1r0iW/GjU3sEVM00YeqSa2T5X/xfMg5Udq7Z1B9gC2CkEZbNshJIkYC38bSUP/7/yhVJ5kVfylaDPyRvknlxz+dAx05SqVP6GtJJfNRTqcxuOibjq7dJqKPeDJcMCfVGkm78ysxQYCO1zmABDLangqEzqjpjy9lTAgmqfPzmN78RRoIXjMxnUACD4AW9kGg7U4h/VIIYGfQ47FfTtHnR1NRkyliu4AW37CuKtvSmQeXTnxmS8Td9VCZc/4jDrFBqventAvwDWOBvqBF/SyOWcwCEoF0OBE9Xm2TAK04AywhMwG+DDwyCIJZ9ZDecliAMz7qDD/1kdDRLJmj9GQAhAgACGALRLsmNgD2RZE6429D8Y7pAOfQOZ8zzm+Le9Xr66p3fu6dfNoUAYJQBwCA74kwbGQ44oW3oktNhUriZF2RqxAGmxA0TIwN1OJvvpB2Ao6e97mm/CmAUADfJ9/H4HIUAyAALAGBIjzcSsARcxjzZTxz3TgMAJ9rAUGZGB06bNlMMOwPlBDhOQ/TvwJjngVhvmxNZY4pk4Uc/IfPuu8swLLwASvoLwUM7ewEYtFPQ6MuWSiwj2Xn3F6R19W39Nb9gZQm8U8i2adoJO2qvPyPhUxXmWAQu+mJi8Fnj+4Tl7qUlbMhnkO8GLSdowfoKRrCcUcsVlNRyAhmM+r4424mPveYBKbnjs2BewI7RhX6+zzaYC1yeCa8jK5/8liyses/c56mHq8idKm8/8A1pwESYGHaOA2an1rPp0SsBC2CM3ms/5M78XAEM4Nzmb+tz/wgbGP8iHSk2MObd+ikAGH/drVQPOQFcwgE1HjsgFet/hxF4ZNqq26Vgyqy0o6k7sgv1fi+tNcfTlo/B7EHp6tvBvCjpVV6zfzPavSo5MNjF8uwxE3qVa6J693rTf37JTNS7QzLgWtEyMFQ6djtiJYCZ5BysBS7euU7m7FgjsxoPpj1VGsc7RObFvFVSjVmpdjCVhnMI1h2WwooNkgsAg6wLRvVGQjBBZ9uoIPTHwNCZ1V//+te9lpDQcOcVV1whBBi4jIRKis6suuWmM6aqaOgMq6Z///vfy6uvvnoGgDF16lRhZN8rVqwwig3bquKjCpH2pwCFptWIp6Y5Nu4zn5HHV2XIB+bbhBs/KhOvfQgsByxVADBgAup4yGRAHtAOLtA32R7s+1ubDCsiWdHYhCAY4QezgowGBjIoPFCyPQCo/SjjsgsGbgOoF0C/ZE0oqOBDvSBmtn1JBgbUZ7N8ww9GRBDj8KPcHTwALAIAOPwAKgheOKwI6CTYzwDzINUwrbvtxdx/tjFTHm8pkcYEAQyyHKgw8R4k04IKfW/4w83AIIOih3FBsAq1Ud0LJoUyL/o6FzI4yO4o8TTJdF+LzA22ybwMsFDQPgOHJfuCMYQxhAAsBNExRmQ8lPiw79jAiBvmRQuuUyvYKq3YOgwMGI7EPmMbYn3M0wvAUObFeDAvSsC8GLegvK9h9spPBTB4d0DNl2MFpbIBLNema+7tVf9iJzqqjkjL/k3SemibtB3ZIwkweviO4HVk1PeFsif0PaNlfOY08hnU8tT3hKZZzraa1veG+/ntTwZZU+ZIDlip+fNWSMF8MC/Q10gN3rpqKQTLcDK8a816f/0ZXkcIhB2eUA6bF1fJKSwbiZSBeWGDlUAaCVgAI41QbNalkcC5Ahh0GAZCpWx77p9k14s/sQDGIC4jZy64FpXBC2VBmROpXXCtqaG8klKcJngwM+Tjmk1+vbmCaYcPK/wypy3XqmR6xPHhbT4GMY5EAB+A6C7l21Gr262VwIiQgKelQWY89X1ZvunXkgNKOfxqpD0vfty9ceuX5cR9X0oasBves1K+Uwck+/21ktfZ2MsGBsEL2qSgUsCgy0uoeKQLZGawPr2F0AuJGvGcM2eOLF++3EQCDKxHYEBBgdS+CDAocKBKCOu89tpr8vrrr58BYEyZMkUmT54sNBZKI546XhrjbG1t7R63AhfaP7ec2dVj8JhMMzCPhjzprcAdCGBMvOZhmXjlPRJorpdAu8OAILjgb603YIUXy/d0CQdtGIROHZecplqnG/Trx7s1AOAjFzYissKOjQkP3rk+MAYIMmRBEQ/it8Ader/JtaTn/Z++XOs52546Pe1Y0pPfu/6lSD3TEJSfNY6VxngWDs/nyhmdD0YnjU2LlN80LU8dq7P8hCAI3PMabyVao/e5ay7tX8S4PAYGKwNYKrTKVw1jnnVS4PeAieHU4kjG4HeV3kgY+FRkAlgJ4vlQxgWXidBYZz0AJBr0ZOCVbAVw1Ip0Jw7fAHeqbgZGFiYIFn70L2Te/feYGX+9f03jfv6lAhha9Xj2RFnz4H+Whhse0qxLsuX3DB5yqdv6hlT/4QnprK4wIASBBgUyODDuE9Qk8KBMC75rUtMEFQlK6ntDn1s9OdZnZEgt0zr9bQ3z4s7POTYvkuya/uoP57JMLB1Z/tR3ZOGxTbiPyaDq/VxU5JTIW7B5UX8F2EC4NtbmxXC+2hd27BbAuLDytb0PQgLnCmD0MDD+AQyMfz0DwLBeSAZxMYZI1QS+1yyAMUQuhh3G+ZcA7AZk73hXina9KzN2viEzT+8za4FT1wMb5kXRHGPzomrlLdJx2ZXnfyyXoEcDYBxYgyUkDUJDngQYqETpDKkbuKDyoUyMVEVL819++WV58sknuwEMMjBWr15tAIaFCxcaQIHKSKqSoQwI5jO60xTLQAEM1qWSQ4CCCo8G7Y9ABcuYJttCgQzuUwHSeu72PvRXBtsSMwAuZE+eKzmTZoIVAaZD0pilB8qaD945AgAsjD0J1GfwAqzIAlMiA3WdkDDsBxq7DEJpDgAo03zHXgSYElAm+jIYm6x8yTZcSAM1X+JJICGKZRVhuOeM0Q4FvYQkAQd6wIjA20UMYADzHXUfy3no8SIA15tJUCwGY5Qx2BWJAhiIIe725MoOgeeWg5Vyev9eiXbw+mFmnYA++vLieATojcFITJaQoUFbUAZ0x7Vxygkw0PAmn2AqtLBvAXnHyTwx7VnfSaPQBJPGtaIBXtrUuMpXI7eHGh1jnjhXtzcS7mvaj8kAP8ppCyOCe6AFS3yUgcGtWT4C3TDMfZSTgdGKiYdT0Wyp9RVIy8wl4lu0UiZfvlLGLxwY80LH3BeAcSIJYNRfYgBDx9l+8pA07d0gYQB+DOHaY8Y2Rgw2XBj0vcF9PrfKpOA7wJ3WZzP1vcF2DFrfSZ39v2PrYmG3cp43Cx724HGE99dIDb562DvakmReHDyTeUEYg0yeirxpWDryN9J01V14dM/dLspIlac9L7ydv7Yl1hv+SkrFg3evlzZqnN/CISurd3/6X2XT0z806/M5yJUf/Ru58ss/GLLjtQPrWwLnCmBoz325UbVeSFRCw2drAYzhc63sSAcvAQ/sBkx79keyZP0Lkhdrk9xEj9Lr7q3JmyVv3Ph5qbzni+LBDKGH3iVGQFAAI6e93syEErBwz5CGQiEpLCw0Z0rlX2dMuU0XaKfimWeekaoqxyUfPY9cffXVQiBj1qxZ5hjp2hEwYP9qY4PgA9OqsAwUwKDS4wYu2C+jBgUwmGbfagND05wFZiSQoTO6GWBI3A4WxZ0+9JNU3gfKXOi7Xs9nX991dNRDY8slCs2wMREB84ChC2BEa2aBdME2SAxuNRWY6MrJk45CuEOF286YH0yIJGDRlY26BcVCF50MdNMZyymSKICzGGZ6jctOPFd7X/gP2fHMk9LVQCXXzcRwbFrE8YzGCBYBACEzgzYvYrDn0dv7iDmE+acMCx8YFo5NDIdxgTugp1Jyj8yK6zIb5N7cpKtXXBx6IilEZOC1GoN7Px+gijvQBkY9ALAmxE4AJDTcqd5IWI/eSFoAkhDIKEL7jMyxcuTeL0j4oU8Z+XCpy2DCcAEwzPOLc8bDZk6v7r3XperVn0tXyhJYBU35/DIy8B3DfT6H+h4wBefh39ir75WSOz9vbF2Y7gBGjeRlIzzHjAPbwLz4tiyq3Gju41TmBcELwoJHC8tkPVg8zVfecR4kbbsYyRKwAMZIvrrD7Nz6AjAmL1ouU668UQpL5wptI/j6+HjV0937m1/IgT/+u3S2tUu0tU2yx42X7KIicRgYX7NeSFRQw2BrAYxhcJHsED+wBDwwhDj759+VVVteMPYAgikzBmHM2r5fXC4VsHlRtfR6aSvnLN3I+diNH98t3u1/lFx4IikoKDAMCQUwqFQQUKBNDM6UUonglstLGFXBoLKvCsiGDRuMrQoaxuRsKj2E0MVpWVmZcLkH27vrU0Eh+0FtT2j/yoTglmXbt2+Xbdu2GcCBy0NUoRk7dqyMHz9eSktLZcaMGSZfZ2vdfei+HkfT7IvLRTTNcgIg7F8VKQIYdwLAuMvfA4R84BvuPDSkogGzhQASnOU9ZD7EsR8FeyAMw5ZxsBXcwSmH3LEUI0xmRFIZj6N+DEp9jG14fQnOQNmM+jOkKzsXzDvwQZLKJO95to+AMRFPto+RUZGZ49QjE4JABWI8IyhRLLdJENRAX4aZgX7iMIwYB7iRSLY3gAXzWCeIMaMtDSee2rlbTmzcKKe275DTe/dItNNZ2uMwMZKMCyi1NHTpMDC4/BKeUkyaMtFIRRiMHlw/d7mznNI9M0jl2kkTRrgmo1HuyG0DuwJKNCJtX+RiZp5sC9rCyMVxcpHnwz7zGNgDl40QpOiEDDtwvuqNhEtHOowxzxi8lcDuiJFRkZx68CvieeQvTfvB/hsuAEbqebWdOCjNezdKJGmAtrP6iLQd3iVx2HrhO4TPHSPfHYya5vZcQmbJLAlNv8y4umY/uTMWSf78y3HPpAdiz+VYQ7UtAQx6rlHXu6njpCebg1OWSsXCq6Ru+Y0SmTYntYpNWwn0koAFMHqJwyYupQT6AjCKMHM2pnyh5ONjNB8fod6zABgVb/wJ3jLWSgzW0ONtEcnGByYBDGvE81Je3Q92bAtgfDC52VbDQwJeeHgo/+l/k2t3vJR2wM1gXvwZNi+OP/RVTL9CMUoqLGkrD8PMriPbJLzhPyQv3CwTJkwwYAVPg+CEAhlMq00MAgwMurSEYIEyM2gD4/DhwwZs0DyyN4qLi2XcuHEGaCAowDJlWhAwYFrBAtO5658yKMjoYGxubja2LVShyc/PN8ALj8NIpYdB22lXTPM4GrSceWRcMJ3KvNC6Qw3AoMeJFk+mdGL5BgOBA4ILXcFsac0ZA5AiW4dutgao8GUaUKJ9zFiJApBiiAWzJJJTCBYEmBAEHKhAAjCKhvIkMrFU4mBQJHB9gI9A8XMABtOw+19SeQdAkIiALkylk/WohLIdPHl4MS6z5MOkseSD3w6uZ0ivV3eX2FGldffzL8jOp2A/oSnpbcVdybXP5SIOU4M8B2dpSfo0xmnKtb524uQzxTO6MqNZbs3pSC4hga0L41LV8UaSCUAzE6+BbORx3+2NRG1hGLACMuiAXNoRycRgjAK8IIDRBV28LVgobY98VTIe+awOYlDb4QpgOM8tBJDEI05v+oNU/f5xCdc5jK1BCWEQlYuuvFtK7vqC+HFPm8B71XUfDqKrYVv1bABGZQi2L2g/hQZgKZtRJp9he2Ev4cAtgHEJhW8P3VsCfQEY2fgAzcZMV0ZuHmII7zXOcPQdmo9XSv3xoxLvwvrTCFz1WQCjb2EN8RILYAzxC2SHd04SoPHOBT/9234BjNdve1ROPPy1czrOUG18PgAMGrtUgOPIkSNnABhkSfQHYLC9AhKpclKgobq6+qwABhkkqpRoO+2PQAXzNGj5sAYwsHyCSoZhX2AmORzMkRYCGBkpAAaWfHApRxeWbLQXFfcAGGA/dOWPlVh2PhgQAEMAYABJAqiRL5EJUyWeDWUPaRNQprJVGerWXDtXPZOfkqZrTdO+n360P93u+uWvZMcvfj4IACMVoDhbWo90dgCDoEUW5JgFJsZAAQwCGrqUxA1gkJHRHiwAgPG1UQdgqMR1W7vxVQfAgFvgCxnGXnUPAIwvip/39CgNAwEw1j78f0jjJfZgM0ovz7A8bQtgDMvLNjIH3ReA0X22A0VkU+h+FsDoluCw27EAxrC7ZHbAg5CABTDOnYFhAYxB3HDnoWo3A4MAhln6geUhUKxpk2IwAEYUtis6C8cBwMh1mBEEMBCiOQUSHjdZEpdY2bMARt83y3BlYKSekQUwUiVy4dIWwLhwsh2tPQ9ZAOPEtjfl2ObX4V4qvVEzvWBVu9bJ8Z0butdKliy4XCYtulqL+91yKcKUpTdKybIb+q1nCy+OBPoCMHyZWeKHka2AH+tASSE+C5DRhdmuDlB9SSVlUADD2sC4ONfxfB7FAhjnU5q2ryEjAfyu5W/8o0za+qZMP7BBypqP9hoa1+13YMa1PnOMvHPr56T6zs9cXNsXcMXoa2sWT1fPsgczGx6GKUV4tvDA7SODJ4qZbbruxDIIHwwZ0iUngw/t/R2t8I4RFnrJoKtPtqdrzyDceHqwVp/B01wr3pojkhXvkjwYUwyPnyiN02dKJC+/ewmJ2rfgkhK1faH7am+CdVhGmxK1tbXdLg/p2YTLPLi8hMtOUmfx1fYEt1xOwiUq7DvVNgaXjtCuhtqwMIPHPx5Tl6PwGO7+dF/baFq3zKerVdrA0PNgGffd4WxLSPgrx4ULsAQCvxcOI4LteQ/RS0cENiZiABgStDmBZQdc0hHm8o2kMUvze0rwgCAEmBJcDkKbEwmk6dGDNinCqBuDbAzEgPbGywfqkTVhln6AFRmDLAhK0DaFBo6NDA0eOw77FwlcjzjqMd/Yp6ANi4weQ5vsP4F6cbAwEqh3KUPNLtjC2LRZImDouEPd/v1Su2eHRLv02xTsEEiGXtBwtogDS2ufPe3wPCBzsrdLSgNdMjsYkXkZUdjA8EgInz20f8FImxgh2MEIoLbjiYT2MDzd7lSj+O6hZ5IWPIvGLgaG1I5nj0tKyMrgk9cJBkYjWF2BUbaERGWu27YT70vzvs0SbWvSrLTbdtjOaDu8E7Yyet8LmZNnSGjGYuP+NG3DZGaoDEug560wS5r6qzcSy+h9pHDT6zJl+1qZfniTTOxMunVOnixdgx+atAi2L66U2hW3SKRs3kgUgz2nCyCBIQtgbHri27IJ1tm7Wvt/sXxwmcBbd3aOrPjwN2TlX/73D96NbXneJNAXgJEBam4mrNEH8aETzMSsj/mZ7/uw7XV10niqupsWrAAGvZAsuv9R/OjDGrhZt9p3H7ZkaEjAAhhD4zrYUZxPCUC1bG3B0hHYvtj+YtqOqYjW+0NSU1Ai2274uJy+4WFjB4BGBs8eaIgufS3HakD6MneuB247g7UnxI8lLiagwwRBjXa45WyskUC74yXBB0Aj2NggQbglzOxqE1/MUeqCXe0SomcRuvuE0uTB2nto5pJBoCLeCeWLit6ZoSp/orx/5Q3SNqnEFBJMYCRAwaBLRdQWhsl0/SNIoUCCm5lBsIDLNRRs0LTavtAlHSxne7VLoUtLCCzQuKYCKAqEaL4OQftxp3lcDe5y9k1QhFFDnwAGgB7jhUQrurYEKgAhSRdAgqgHIELSS0ccYAW9dLRnFUgMcokaA5qZxkYFbVGEARKZAKWYtiFiWNLRmTtGIljqwTSBhGhmHpZ85EoEyzcTkAuvgnMlXAPoY1fVeb0Vee9pW7366dKa10e3lzx759PPyY6nn5QwnmEGvUfO58Aog6tgC+MW2MLIh8mMAtjzYOD/MViuk4/rw8B6tIURBNhEcKKTzxlCDPdWI0DCRtqIgbA7YcSTBj7bk+VhABi1D35VvAAw9F42DQf4b6QwMAZ4unJq3UtS/eoTEmk63avJmJW3GtsWQSyFsuFMCfDZyDy4Q5bDeOeS4xvPrICcYzmTZM09fy31192H9w5gueS7Pm1lm2kl4JLAqAQwvHhIyNSYvPgaw74oWXK9SyR291JJoC8AY9yMuTJu/hIJjZ0sOYXj8YLr/yP+2OY/S+X2dRLGzEkMM4MKYFgGxqW6sh/8uBbA+OCysy2HoASgSOdv/INMeu8Nmf7+xjOYFxwxvTy0QxHdM22FHFl0vTTNWixdU+eB1dABBgOMEyKQ0eDraAFDohPsQ8ypQkFh8EJJ8cMwqA8ABIMHH5AetPEAaAgQYCAjgh+IyPcDXPCBEcG+vEi7A4+TAcv8AbAlGEwxjxmLSIDtkkCFFwqSH2wM9hPAOMi2YPBjPxjrFD8UJvZtXOahzI+IuXwAyL2PZxrh3/Hc8bJ35dXSPnmKAS6UEUEAw83ESFW6FOhIzdd++9qS7eBmSOgxlBFBgIORaUZNa3+paQIQGt39ap5uU21faD/84Oe+OwQAWpWPnSpziwDq8NqRgcg6vChMI0YRY2Q6wKtHAu49GciMiMAeRQwxQUACii/ZFwkwJ6JZYEMAoND2ZFvQu0cCIEaCrkcxw8/lITGwL+LoE/QVWlY1CrSjSrtHmH6fV1gja7CdttWrny6teWwzFMOpHbvk5OYt5tuC46vZsxuMjJ0SA8B1vgK/cK7JaJHbc9uTxjxhtBPXMxsxE9df9zNwzR0WBhg4uB8ieL7ogYRsi94MjISEkU83qmHECJb/RKctk+hlV0jnyqtEFi4Z1NCDh3fLoqe+I4sPr8NUELwDJZ/nE9kTZQ0MMdbf8NCg+hvqlVsr90vLATA1ku9VHW/OVHjGm7NcfDBCa0NvCXjrT8mYTa/JlG1rZPqRzWcwL/g7R9fIlQVlsuG+r0nTVXeZ99Fg3+G9j2pTo0kCQxfAePK7suWXP5LO5ib8Tvf+QT/XC+THj/TKj35DLv/MN8+1K9v+PEqgLwBjxuobZN7tH5Oi6UukoGQ+mK79U0u3PPcD2fniY9KOGZIuUH+zYYHeeiE5jxfqInZlAYyLKGx7qAsvATAY6HXk+q0vpD0WFTsuB2jy58jaGz4rx+/+PACIqPjCYC201IP5gKVxCAQlyITIaK6XBEALYUQgIyK35phkN9WZNAEMHwCHIKjPoY5GyQQIQqXVh9/UrFgHGBHhXgqIaXQJ/1XmFMvOpaulo2SqZGLZoDItFKBQJkbqEJU5QdCALAfWJ5PibGmyKli/r1l0Ag6soyGVmaH5qVv2ea7MC9MnlFUqR+Nv+IiMv+6Rbm8c9NZBw5TqncOdPhtLh+fqrk+ggl9YvPcAW3SzJJhOl49sG1wS2PGLp2XHM7+Actt7eYGrSu9dgI1xgk99BC+uuRfLRq4PNstduWTvOABgAQCmQkQGgjxjcI/n49q5A3HMerwLmnBvdAJEVCOezclVSfSTQiYGgY6xaB/MKpbDD3xZIg990t3NWfcDR/fKgme+Lwvef0uyEgArRziAcVaB2Aq9JYD7K+PgTln+i2/JksoNvcuSKdrSoZetinFzZce9X5LWVbemrWczrQT6ksCQBTBO7lgrxze/IceB3h3ftR7otrPmtq8TGUy+BTAGI62LV9cCGBdP1sPlSBbAGC5Xyo5zQBI4C4DBPjgz1YllAIfHzpGqqQBsoZX4oIwEwHTwg+nAQNZEkMszYHcCmmwPAwMzr5ldrbAz4fxeUln1Is8wIghWQNkABcOwIriPOXl8RlJBGhrhEGx+bJ6/VDpLphiXqgQmuGxDmRHcMqbO0ilTg+wFgg5aP7WeMiu0XOunAhjKiFDmhUqHfTNP6yujwl3eH/OC9VlOuxeMehz2x3138EBZzZlxmeTOWiq5MxdLqLTcsGeMfQrUpXtQ7qdLu/tJ3TdjpwLNiPZkdGDPBN4Hei8wL12+U9P+VwlUb98pVVveGzAD49SO7caGRjzW+3qzP96vxeWXycRFi6Uk0SnT4+1SuHO9FNfsgstUL7yQIOL55ZYsDLpYDYE9Q7eq8JQKbhOXi8SNzYtWsjAAVrQwjWMR4gzjkFxG0sV3Ci50JGOM1Dz0qHgf/jQPP+AQqDwgs5//Jynft0byAIRm8r2CMFIZGAMWjK0oXoDnudvekonb35I5+96SKe1VaaVS7wvJnumrpWLRtdJ82ZUSmzozbT2baSXQlwSGLIChA97482/CFsY/SRgGxc5H8IBCSdsXyx/+urV9cT4Eeh77uPAAxidl2f3fwEcC55lsGA4SsADGcLhKdowDlsAAAIwB93UJK6pi23sIqvo6uenr9G7BlFPPaXsgI1/WT58v7RMnSW5urgExBsPESO1dmRlkUZA9kQoSpNbXtAINClRoPrfuPGVkaL7bxgXz3Gm20zS3HJMCIqybGrxgio6/5RMyGdGGkSOBLT/9N9n9y6e6Dc/3OjMAGAse/pgs+8JnTXYCYFfuj78rs956yqmGx8TYwEgyMfglMy7gN3ng5GAZiVONNjAaALQ1AcDgchJjwBNltIXRCRCjA+WtKGvOKJDWh74iGQ9+2gBhBtByuuj3v//4IZn+8k9k3s43pKCrXnIBjjIQwFh73zekjktIALAQHLNhdEnAX/m+zPklwK29bxh7R7Ck00sAvEUJmR/LmSxr7/0rabj2Xmv7opeEbGKgEhhVAIbxOrJwtUxecp2ULL0ONjCuG6icbL2LIIELDWBYGxgX4SKe50NYAOM8C9R2d2klMMwBDBoX5aKTSFIxIX8jikmBCJb1ReHxgt4mGKJQsMJQwCOwt2DsLiRB4zjqdQWzjb0F1ovDHlUsgE/cINbkw9ZCPQx8nujEspig13gPyc7ONktByMJwMzHY1h2UmUGQgCAFZ7I1j1vNV8YDy8na0LQblGC/zFe2BkEGDQpsaJpb9kVAwr1khPXcUW1eaHtuCWDouNz96b4FMFQSI2t78r1tUg3GRuo9Z84S99LEpUtk0jLHJgUBjGwAGKVrn4KtC5ghQaQXklywLrLAuiATIweRW/7xXlRbGM1gXSgLoxX3M8ENltEjSRsit51ePHelSySB7+LIqmvFO0BbGJ7mBsnZt0nG71gn87f9Uaa1HjfDb4Dh4T0zrpRKfGM3LVgl0ZKZI+vi2bM5qwQCR/fLwqe/J4veXyu4u8wSRXejJngdOTB1ObyOXCV18BgZLoXXEdy/vHdtsBIYjARGFYDhw0fSyo/9jaz6zLcGIyNb9yJJ4EIDGPRCsvj+r4I6GbReSC7SNT3Xw1gA41wlaNsPKQm0t8m8n/13uXbLvyeXbpyf5RvJidfkqfYsCUg99971WNr3R+OZdUE5h8cDrlsOwzUnA8ELAhKdcIfZlZUPwIKuSkFVh+vrDnq6yMmD+00YjFRgIysknQXjjGFJto/TyGSoUOK5BYi50nroPWl++98l2FIrBXl5QjeotGVBFgY9jBB0SPehqzYyqBQSGGA9Ah4EIbhk42zptMokxqcMCy1XBoWmOS5GehJpbGw0Y+Q4tR3PkYFlZ/M24tRM/scHvS8jRybc8jGZeNPHehXZxOiRAAGM4P/8rkxZ8xSWkMDjCCMesByAF7SJkQ/X8gxkYmRi2VkQ9w29kTiMC4IVCamHq+MWLEFjQBKgRhTsC3osIaBBQATPVPZ4qQYTQx4cHNsn4/3tsuLJb8riY46HCTx5Uh/IlcrJC2T3nZ+VtuU3muPaf6NAAnj34oUrGYd3yXIYeF1Sub7XSaPULI88HiqRtx/4G2m4/oFe5TZhJTBYCVgAY7ASs/UvmATOFcDA3Jv52/rcP8CI579KR4oRT4eB8VcAMPCDbX7yL9ip2I7PkwQsgHGeBGm7GRoSgPHNECyzT8Aa4clHtsvkukOSDdeiuoZ8sIOkvQwqDWG80xjoiaLTny1hepJg4IwsygzzAWABWQ4MCeTTM0WMzAmwJGJeGEaG8mPwDMzMxqDUdMF9ZhTsCBOISqCc/UQwERBnfYQ4vFXEA2BZoN8Y+qGnC9ZLIC8KRT5OTxfYN2YI0S+9ZMTRL/MYjMeMIDxfoM8E6ocbqqSzYrckqg/CjesRyCViAAKCEQQp1E0q25LFQDDDzcwgsEDQQtkXTLMeAQxlYrjTqUwL1uNx3OwL1mckEMKQCqDwGApYKLChbbTdQGxemM7ZP2SfA88zubOXSd6sJZJbtkCL7HaUSSAdA4Psi1w8dwQyaBcjgEcziH0FMLh8pMcbCVkYTmReBGX0UkLjngQ3aC+D30IRGPNseAQu5h/85KAknApg0DBjoy9HKibMlV13f1FaL79lUP3ZysNXAt6GWhmz/lWZtv1NKT269QyvI02+LDkwZalULLha6sCAj0wvH74na0c+JCQw5AGMzb/4rmx9/p+hjDZLDC7gziVYBsa5SO/Ctz1XACMGuhpNVW177key68WfnAFgzLv1U7CB8df4AOV8hQ3DQQIWwBgOV8mOcTASiFPxbm2Skld+Jgs3viwFHfWSH+3xYECfEDSsOZDAJR0tngzpABWcIAGXbDTkT5K2/LFcaAzmgw/LOYJYzhEyjIgIWA4MceQnwJKIwsNFJDPXMCI8UNypnNNDBYGLcPEksCMKTH0PFCYPlnsYF6wmp/c/GpRMAJxhYD2G/tJ99WMa4l/X4fckvOkV8TXVSGYwAK+eDsWYjIyCAmdMZEMQcCADguDEuQSyNtgfwRD2p2kFJBSgoE0NliuIkcrIYJp1tZ2WM4+RgAbBkT4DrosPQNP4mz8qk27+eJ/VbMHokAABjNC//EBmrH0GLooBzMFMJ5kXjAyGeYF/NOaZDQYGAQ0G+JlJMjHoUhX7AC7IzGgnOwPvBdrCoA0MLjEhE6Mts0jaPvw1yYQtjMGEVAAD5nOlGe+jyuLZsu2+r0jr6g8NpjtbdzhKAPeW0CvWkT2y7KnvytLKd9OexYnM8bLmnr+S0zc+1O9vSdrGNtNKII0EhjyAcXLnOjmx9U05/t4bcmz7unMCMSyAkeYOGEJZ5wpg9DAw/hEMjH+xAMYQurYfdCgWwPigkrPthqoEqOwLvGplHdgqeUd2SbCtRQIdrYa5EIC71Jl73pLpTUcGNHyCFmEoDfRa0oGlGo05Y+XwgmulbvZiOCeh00QELOsg8yEKxTsOBZygBnxR4yPSDxYGllmQOYGtAwJgOhcztaxDpoRhUKALo7AnQYR0AzMKO8+LQcGEftIKADgNzvwfbayWyIkDEju+V2KVeyUA96+6lIQAAoEL9sGt7p/Zy8BzCCowkpHBc1EGhvagjAqmFZzgvtYjO4NR01rfvT2bzQtekxCYF3lzlsPryCIwLxbyEDaMZgngfvRsekcyt22QMdvekXE1OyQDEzD0QJKd3NIuRgDPQh6ehRw+2wj9ARgteEzb8G7oACBCUON8AhgOI8wrlQVlsuGBv5amq+8ezVdvVJx7N/Ni2xtSWrHtDOYFfxUIbB0LTZJ3abTz+gfNb8TZfgNGhfDsSZ6TBIY8gKFnt+Hf/k42P/sjCXe0adagtgQvMuB9ZMmDX5OVn/rbQbW1lS+OBM4VwNBRbnnuBwAwHpP2lCUk8261XkhURsNlawGM4XKl7Dg/qATIyCDrwYvZf19Lg8yH560rtr+M7ggmwN0hGBngS/TbPT8QaUCvqqhUdt38F1K/+g4AGGREoI+sbIAYAXOMBMEJzt5C2eFcLUrROxUefFMmIzZDJrTuXieNa56TjPYGw7wgA4JBmRAEL9zB/VFMoGGwaWVasG269sqkcIMY3Nd2HIummafhrMwLVPQASJpwK7yO3PYpbWa3VgKOBMAQCsGY58y3nzbPLO/rQtz7eXiW+Qz7utP9AxhcOtIEf6vNiBEAGGG6VMXD3wZ3qm0PfFmC98EGBhhPdOE7kJDKwNA2x3ImydoH/4u1c6ACGYlbvPdAKZOMo3tkKYx2Lju6Lu1Zgm8orZ6AHCuaLu/d+1VpucqCWmkFZTMHLYFRAWCQrjptyTUyBQaFJsHq7aTLrh60oGyDCy+BCw1gzMXH4ZL7v44fe/xAm8/1C39O9gjnJgELYJyb/GzroS8Bw8jgxyCUEA8YGLnvvSlj9r0HJQLr27vaZdbutWdlZHDmswssjGaCGONmSv24aRIG66K1cLzUL7teumYtMmCIUbzJkOCxkqIheMHAtOaZjCHwr2nHWqn/89Piba41Bj1pA0PtYajhTm51CQkBDZYrE4L5faXTGfdUoIFl3Nf+3EwK7mvgvtbVYzJNJgfLuFVWB7f9BQtg9Ced0V2WAIBhvJG89bQx5JmF55fLRbLAwqA3kmwwL/IBOtA2BrAJLDTpWULC/TCWjLQAvGgBy6iVDAzci8bQJ/K4oCkK+wRdJZdJuPxyCV9xg/gWLR2QwC2AMSAxjchKHoDtuWDFT9zxlszGb9S0tuNpz7MRXkf2zLxCKpZcL03lqyU6ZVbaejbTSmCwEhg2AMbmX3xPtv3qf5hlAdGujkGdpw+zTyvgfWT1574zqHa28sWVQPXuTXJs85tStestOXVgE1jWnWYApcuukjm3PiJFZYskf9JcMG2dNdZ9jW7br34kO1/6iXTBbkpXR5dkFxZIRlGRzL7pw3LZPV+yXkj6EtwQzLcAxhC8KHZIF0wCBBgSUDK6GRlNp6X8iW/L5Tt+a45J/yJ+sCjIyugvqG2MqrwS2XbbX0r91fcY0ILLO2gwUwY4w9rfMS5GWePOtVL7+rMSqzspQb9XMgBOcCmJLhtRmxUKYCgzg2AC7U+o4U+CCWREMM0lKAQZWK7tU21eaJrlBE2YZnsDALlO/HwwL9idB79pXtgjGX/jR2D7wnodcYnY7kICBDCMN5K1T8PziACscABI8i0cbyQALwBi5NClMd4hdJfaZRgWNGwOQAPpRjwTjbB70YkM2sWgNxLaxCCAyd78MPbblTVeTj38FfHcPzD7KxbAgPBGafCdqpQZv34MbnT/JIXhZslJ9DDOKBIubyQz8ETuZFl3H5aO3PDQKJWUPe0LJYFhA2BU7XpHTm5/Cwrua1K59S2JJQ2GDUQwFsAYiJQufZ2mmkppOH5AmrD2ubkK65+TRlsLSxfI+HmrJDRmkuTkjevTkJyewbEdr8mxra9JFLOZcfzw+7GWm3Hi/NVSgvXhXsxaWAaGSmtoby2AMbSvjx3d+ZeAm5HhBVgf2rpGxhzYZtYNB2ErY/buNVLWfLTfA5ORQY8ALfBIcqJ4ltRPKAMjI0taiybJ6WWwAD9zeNhX6Kw5Jm1Hd0nb+1ul/eBW8XS1meUj6o2EgIWCGRQI9wlSkGLP6AYc+ksT4CDoofW5z6jsCS1PFbrmc/tBmRcElXJnL5W8uSskNAO2L0qtdf5UOY/2tBvACAG1yAbrgt5HGHMBWoRw3+fgPqJtDGAVBrRQNlUUgGcXMmmws9XYvnA8kdAbSTtiF8rD2PKd0ZE1ThphzNN/P5aSDCBYAGMAQhqhVfwnDkv5M/8gi/a+JlnxiID31utM6W5739QVUrHoGqlffI1Eyub1KrcJK4FzlcCwATD0RDf87P+Rjc/8SAbKwvBjViMTa4AXPfCotX2hQhyi20isXcLRFol0OpHG5Bh8wUIJYGYgCEp0ED/U/BDtL7S3VUlbSxUQYCVH022qX7JziiUnd3x/TW3ZEJOABTCG2AWxw7moEkhlZPjrq6X8ye/Kil2vmnE4jIwY3m79MzKcdcgZcrJgmmy79dPScMXthpFh7GHQ1SmU/qEc6ra+IdV/elqidSdAHsG7PDNDcnOyJZDm94C/DwQ2CHIQWFDmRWrazcQwcqbmh6C/L8xjW2VeKLiRTk5kc5CloUGBDU2nbumpxYtoAtgX42/8sPU6kiokm+6WAAEM////Q5n0xrOSDdfC2Z6ocaOaidVgWVg6lgX2BfczDHCHbya0zASwQeOedJXK5SJkXXTgnqbhTrN8BLe7YWKAlWGWlKCsHe5U2z7yNcm4/5Pdx+5vxwIY/UlnhJbhfvIAWM84uk+WPvv3ab2O8E1alVksb975dTl9y4et15EReitc6tMa2QAGXt6lmG2auuJmmXjZVTJp4ZWXWt72+P1IIJ6A+754WOLRLonH8DGY/KD0wrq+158DV3qwOI8ZhrOFSKRNIuEWiQMAiQMV9uLz3gu7F/5ANj5sYdDOhmEjAQtgDJtLZQd6gSTQzcjAu8/b3iahbWuk8NBOw8jIaGuSObvWSGlLRb9HT2VkNIyHjQy8C1uKJkvd8uslPGNBv+0vdWEH6MotR3ZIvK0ZDDy4hq05KtGK3eKLdPRiYJB9wWUf3JKNQQYFmRRcYtJfmuwJRtZhe7ZhWoEIZVikyoGgBo+h9bRc8zXdawsGYO4cMC7gbcQEnE9o+kLLvOglJJtwS8AsK9u6UQLbN0nxlrUysXq7MdxJBobjfcSPJSAOI4P2MLJ4v5Nnim9gLh+J4FuoGewL2sFwAxh0p0oWRju2NPDZmjXWAhhuwdv9MyTgbTwtY9f9RqZt/bNMPbZTJnad7lWHvzVduPuO55bIhrsfxdIR63Wkl4Bs4rxJYNgBGLSFseOF/ykd7a0Sxsdcf4HGO1d+7D/JFV/4Xn/VbNkIlUAMK/DwaWkADBCLR+hZjuzTsgDGyL6+9uwGJwEqxrSPQYWGXkv8tSdkwVPfk6W7/2Q6AoFcAgCCB8LIaPFkyInCMtlx26elceUtzkCS5LYEFPlEZo7xXjK4EV6c2g1w2Vf7xnOSaDgFFgbcqUJRY1CbFmoTwz2a/paQqI0LtZGhaSNvyNyddvepS03cef3t01CnNyNTxt3wYZl8y8Bo+v31Z8tGlwQSHR2S+7++JzPXPQs1ERgm7vsCPKup3khykcfAOpwNJ4jRgHdGk7GBETO2L+g+tQvABW1itAPgaEV5c2ahtN/7eQne/RHjvcgTcLz+sK90IePgDln+5LdkceUGFNPqgROsF5J00hrmecq8qNwvS579QZ9eR7h0sd6fK8fGzZbdd3xWWq4E288GK4ELIIFhB2BU7X5Xqneuk4qNf5SK99Zgtr7HIniqfCyAkSqR0ZXG3Bj+Enid0kTV2Zkbo0s6w+NsLYAxPK6THeXFk0AvRkZbi4RgGyr/8G7jtSQLluHn7HpTprVU9jsgZWQ0g9l2snimNI+dZOpTBYkD+G8uniLVqz4k4ZmX9dvPpSrsBCOjrWIPbGNskbaD74EL32aWfrhtYLjH5mZkkC3BesrIYJpBAQ6CFgQmmK9MCmVYMJ0azuZdpLs+lM3cOcskj/acppeDcTG0WS/d47Y7Q0YCBDCyAWCUvv1stzcS2r3I7PZG4tjBCCIdQCSwRxsYYcQWMDBaoYS2AMRowZbeSmjsk8tHyL4w9jC8mZKYvECi5SskctUt4l28rN9zD+K9s+DpH8jCw29LJoDTAL64GCyA0a/YhmWhp7leCte/KiUAj2cceU8md9SkPQ96Hdk55zqpWHaTtIJpFi2ZkbaezbQSOFcJDDsAQ094/c/+VjY+/UOJhXtbvtXyYHZIgrR9cd+Xre0LFYrdWgkMMwlYAGOYXTA73IsqAcMQoAIOBcUDzyKB6gooFH8vi/b92YwDPAoJwsDa2RgZOmiq57SXcSw0WTbe8SWpB4jBkMCSjERWaMgxMk5veU2qX3tG4o01WGYI451QzuIReAuBUsag9izUmwiBCLdNDC4TIbuCAAdtZGhaQYnBMizMQV3/OIPthe0mE6BMjrv+EZl861+4athdK4GBS4AARgYAjKlvPQvWhQfRYVngzjduVLmchAFFWEbi2MDowrPQAY8ktHfBpSINUbhTBWDBEAeA0QJvJAQxuJyEoAaXoCRyJsmpRx4Vz31gYvQTArCDMOf5/0/m718rebEOycTbg8ECGP0IbZgW0evIPNhfWrL7D5IFoCqYBKv0dAiKh2GPpQpeR96956tSf+PDWmS3VgIXRAIjFsAoxbre0lW3yYQFV8hERBusBKwEhp8ELIAx/K6ZHfHFlUAvRkZrk+Ruf1vyKvYZGxnZzXUyZ+cbMq312IAGRQCDLIxmX5Ycg/eSxrFTJIaZ3ObxU6V69e2wlTG0vJd0ALBpObpT4h3txs4FWRkt+zZJFLYyGLiURKObYcH9dGkFLNxMCwUzTIeD/BcC4yIf3q9MwDFDZfMt82KQMrTVeySgDIwyMDAykkY76X2ENi8y8JxmJL2T0JhnNgAM5nOZCBkYHYwAKuiJhIAF3a0y37GB4eTR4Cf5ql3ZE6TxI1+VwH0f7zl4mj1/xQGZ/esfy7w9b0p+tNUYGGU1C2CkEdYwz/LjXbvw8W/Kiv2vwcJFwkT3KdHryN7pq6RiyQ3SUL5KIqVz3cV230rgvEtgxAIYl38cti+++PfnXWC2QysBK4GLJwELYFw8WdsjDX8JpDIygicPOxTvA2vMycGsJRgZtJERg5py9kB1hnO1lXlTZdMdX5T65TeZKd8EbDkYRsZZ1sif/Qjnt8bpzX+U6teflUiDQ2/2YKbZi/ONY0kIwQmCFmRbcJ/ARGraDVx8kJF5wLbwBpOMC3Qw7rqHZPJtn/ogXdk2VgJnSCABbzehf/sfMvXN58QPg+eBRBjMC9jAAGuC9jC4UJbeSDIBZOQBwMhBPhZCGbCCxjsJVpBp4fZG0gHUsscbCYx8It0Gd6ptH/26ZN7Xv50WutKc9rvHZTZA0rGtNZIf7zBjPpE1Qdbd8RWpveYevCdgS2eIvSfOEKzN6FsCfE/C5mBG5T5Z8ssfybKKdb3q8jfCMC9Ck2T9PY9K3U0f7lVuE1YCF0oCFsC4UJK1/VoJWAmcswQsgHHOIrQdjDIJ9GJktDRK7o51knvsgMPIaKyV2bCRMaXlmFF2aPSzv6CMjCYwMo6DkdEEryVkZDROLAMj4w6JwJbDUArtmCVsPbpbYh2tZljt8FTSum+zSStgQdCCIV3aFJzDv9DspZJfDsZn8hihUjAuyqyti3MQqW3qkgCN93q2b5bMnVukaOMbMq56m9DeBW1gZCeZGI43Ehr39Euuz2uWhdAWWDoAox2gRgsAjTasHemEDQsyMAYDYHhaGyXz4E4p3vmOLNz4ipQlvSHRDsLBSYukYsGVcnrFTUPuPeESqd09iwS8sH1RvOY/pHTr6zLlxB6ZkOJ1pAV2U/aUrpSji6+XRnh6jJTNO0uPtthK4PxIYNgCGFuwznfHrx+D7a5W6QJtNjVYBkaqRGzaSmD4ScACGMPvmtkRDx0JpDIyAscPyvzn/0nmYc26H/RxP9bGZxobGYNjZFTkl8JGxhelael15mTJyIjn5A25mdbaja/KqT8/J5GmurQXJQF7GXHa0YIs+gsOsyKzG5joq27x1feBcfFJ4+q1rzo230rgXCWQgAe+vMe+LzPefc4wqXq8kdB5KlhGANDonYQAhmMlwwEwuISkE0tHDAsD4AWNdzYBvGhGjADA4JKSLqCW7XSnevdnxX/Hw+LJzhFPZla/Q844sE1WPPFNWXxik6lHTxT0clQxbo5sv/cr0rr6tn7b28IhKAEyL7AULwPg9xIbQK68AABAAElEQVT8Ziw7+vYZgyTAXRMslDdvf1RO3Y73HkAzT9IOyxmVbYaVwHmWwLAFME7t3SDVuzfI0Xd/J0c2vd5ttEvlYwEMlYTdWgkMXwlYAGP4Xjs78qEhgV6MjOYGydm9XkIAMqj0hOqqZPbuN6Wk5fggGRnZYGTMlJaCCeYk6yfNkJNX3y3hsiHGyKg6Am8leyXW2Z72YrQc2iYtezdKvMuhvqethMycGZdJAWxpefw9y0PS1c2ZNkdC8C6iLI90dWyelcC5SoAARjYAjGnvPAc7FzS86TE2MBwmhsfYvqB3kh5vJGIYGAQwYmBjhMG6oCHPZjA62oDd0SZGB5ZakYHBJWMxMK66JsyT8Lzl0nXNLeJbsrLfIacCGDQE3OYJSGXRTHnv/q9LyxXWlWa/AhyChR78Voxb86KUvfe6lJw8k3lB8CKMX43qrGJ5586vyunbYC8F95x99w3BizlChzRsAQy9Hu/+9L/KJnojiUZMVkYoXzLhfWThvV+UFZ/8b1rNbq0ErASGoQQsgDEML5od8pCVQC9Ghj8gQSj38//9n2XO++sMI8OL2VkG3yCZGUfzSuXdO78sjYuvNu0TsAURx2/xUF/7XvPOy2BoPC+xdsfopxl8mn9Fq+6QSbd8QnyZ2WlKbZaVwMWVQAJGawOP/1jGv/G8FEmnFFCVxMooxxsJmRewiQEQgd5IMjw+41I1jGeaDAsGGvBsBHjRhEj7F2RktALAoI2MhFlWhrb468qZKDWPfFU89/Zv1yAVwCCfqwMjODamTDbe/w1puequiysge7QPLgHcA7R5ETxxUBY/90NZcWRt2r4IUjX6s+UErvGOOz4rTdc+kLaezbQSuFASGHEARtnKG6Xsijtk/PzLZcL8VRdKbrZfKwErgYsgAQtgXAQh20OMGgkYI5UEKRgxW+ZtqpecPRskB8b4yMgw1iEwMxuqOSZz9qyVye1Vxtq8YzWibzE1Yc07vZa05I8zleomz5KT19wn4SFuib795CFprdwriT7csesZZ5fMktC0+YYirXl2ayVwqSSQgDvg+M73JLBrq4xd/7pMrNoK1gU9jxCwoDcSLwx4wjYGYsAAGXy2nec7YpaJELAA+wJLR9Q7CdkZBDC6WA5Ag0yM9uxJ0vSRRyVw78f6PdVUAIMwKBVcGv9998H/JI3X3Ntve1s4dCTgaWuR/HdflZKtb8iMI5tlCn4D0gV6Hdkx+xqpWHmLtMxaItEps9JVs3lWAhdMAsMewDC2MF78X9IFxLCjuVFWfvRv5Mov/+CCCcx2bCVgJXDxJGABjIsna3uk0SeBVEYGjU9SOcp4f7sseOkxmXl4s2FmBGAnIwteDwbqveRwfplsICNjoePCPJFBRkbhkGdkjL47wJ7xcJZAorVVMh/7nkx593nJA90iFwAGgcgAEMd82CMIwQsJjXrSMkYQ+wGUOYY64ZHEgBdqGwNMDKAOhokRizpLSgBytmN5QPtdn5bAhx4UTwg2bsBuThdSAQytcyxnsqx96L9Iw3X3a5bdDnEJeOtPycyn/1GWbH1FQvEuyQIU5Q70OtLpDUh1aLJsuOvLUndz/+wcd1u7byVwPiUw7AGMU/D5fmrvJjm67hU5vOFPsuIj37AAxvm8Q2xfVgKXUAIWwLiEwreHHvESSGVkmBOG4uJtrJGc/Vsl51SleABo5FUflbl71sik9uoBMTLoheD4WNjIUEbGlDly4pr7JTJt9oiXqT1BK4GLJQECGCEAGGXrn5eAYV94DPOCHkmMdxKwMnIBXOR0G/OEW2Q832RhNMfixg4GvZN0MALAIPOCLIx2bGngM+zLED9sYcTnLZXWaz8k0octDAtgXKwrfuGP44VdpPk//7ZcvvN3YO/EAFqTT9MT2rwZsqNstRxdepM0z19pvY70iMbuXWQJDHsAQ+W1+anvy67/+Fcpv+tzsH3xXzXbbq0ErASGsQQsgDGML54d+rCXgDI0MvdvkfKX/kVmVGwVPxSbADwWZMa68IHL+bgEYv/hUMFMzNZ9SZrnrTAVE/BqEMsdYxkZ/YvNlloJ9CsBAhh5ADCmb3jePINkXxQa7yOODQwyLor8PuORJAo9lBYwaOOC+7SB0QK2BcELulNVAKMTlToAcrSiLIYWhWByCGbbK2ELI373w2nHYwGMtGIZXpm4B7xwu50BA8+LfvXPsvzIW2eMn2/72mCBrPnQV6T6jk9ZryNnSMhmXEwJjBgAg0yMGvh7H/e/2XsP8Liu81z3GwzqDHrvRGEB2HuRRIlFXe5VLnHsOMVpN7nnJrk5T3xPzmPn3NwkT25yE9uxIldJjmxZttUliuoSRYq9NxAESPReB4Myg7n/t4cLGgyBAVhAzAD/0rOx29prr/2tAYX9zrf+f8la5FRuupUa6r1UAVVghhRQgDFDwmqzqsA0FDAOjaiuFjglVaKjtd7vyGipRfmZ95HT34gE38hV39IFN91td4ojoxyupEzrVGvJUjRK0LeRooXBVXVfFVAFpqkAAYZDAMYCOjCEIhJYOC3XRZTEw/AvCbKOk2MMyknAQfcFF04lYbwLZiLpYzDPKw4MAgx/qlWPTB4QN5b0ZTgxH51f+DPYPzbxdAEFGNMcsDCuZuvvRbYEhS0/uBv5zeeRO9Q+rreEF/3ivmhJyseBB/8AHXd/XrOOjFNId261AnMGYNxq4fR+qoAqMPMKKMCYeY31DqrAdBVgSlafvOzEnT+ChS8/hgW1R+Ec6UeCx23FyIiRTAfTcWRUpS3GfnFk9C1eY916VObWjyZnqCNjugOh9VQBUWDU5ULMY99D9lu/QqJ3AM5R+T0UWEGIkSJOjMQr2UgYB2N8NhKBEhLHYEgylBBg9IvjYkgABmNgcM1sJRImVEAHM5dImtWUXLg/9TuIvvtjH+ou1/gBJxB/4Tg2/Nc/YoUEEw0sGgMjUI0w3ZZ/06P6uhArAY1XSUaqDRffmbCjwwLAmh3ZqCtYigs7H0bf5vsnrKcHVYFbpYACjFultN5HFVAFrlkBBRjXLJleoArMmALGkWHraoOj5hTi2yRLydAAUiXlXtmZPch1NU3LkdEVnYiGjDIMJMo0EinNpSvQuP0z8BSUzVjftWFVYK4pYGUjOXUM0aePIG3PbuQ0HJbAnRL3wm6ykTAziV2gRpSVYtUuDgymV/VGxaIxqxidJYXwSfAMn4AOZh0xC6eajPI/gRQ85klwwl22BJ68QktC/jvg9ch5Sb3qFQDiuCwvv7ufxsKOauu8+aEAwygRvmtmHUl79xksOLQbJfWnUOBunbCzfVHxOFR5N2q3PAR36TJ4C0onrKcHVYFbpYACjFultN5HFVAFrlkBBRjXLJleoArcMgXGHBlnD2HhK4+j+NIxcWS4LEeGQyLYT9eRcS6jAgc/+g30l6+0+j4qL0zeFJluEhN7y55Fb6QKRKoCvt5exH/v71G872mBF4HZSCSdakwCEqLjJcGQTTKS2CwnxmisE5fWrULn5mWIj49HTEzMpI9uwQqBFENDQ/BIQF/uj8q39tzmMjw8jPjGBiz64F0UdjcIIPkwJk5dQi7e/sgfov3OjyMqPglR+vs8qc6zdSJKYPTix/4OG448JxlHRhErS2Dh1BEG7mxNzsf+e7+ONpk6YpO4KDaBXlpUgdlUQAHGbKqv91YFVIGQCijACCmPnlQFZlUB48iI6mpFQs1pxHU0I5qOjLoqiZGxBznX6MhwOyXVqpSm8lV+R0a+fss3qwOsN48IBQgwkgRglHzw9LhsJPHRcXDnlsGzsFQgRTRiomMYCQO2uFgMlxXAV5wLO6eZiENjokJQQUBBl4WpMzIyAi48x+MEG1H19cjf+x5yWmvEgeWxgvuyvTp7Et5YvhUN63cgfvFGxObo7/NEOs/mMcY3Wvrjb+G248/LZ8Mny/gyIPDi2MKtqN14H3oXrsZw0SKNfTFeIt2bJQUUYMyS8Hrb61fA3teDmJ4e8TjS3Hh18aSkwpOcCvnK4eqTeiSiFFCAEVHDpZ2d5woYR0b86QNYuOsJFNWdgGN4QBwZAzCODP83tDKxPkQ5m1mJgx/5BlxiVWYZdSapIyOEXnpqfitAgJH8H5KNZP+vAEl9GhUdKzEwYuCIc6J301oM3b4GcXFxltOCTgxCC+5zmawQThqHBUEF6xJiDA4OWlAj0Jlha25B0sEDSKs9j5TBLiseDtttsMXh7byFuLRsPWJX34OYwgp4BXyIFQN2R7I6MiYT/1YcZ9aR7nbJOlKNlb/5LjbUjI99YYJ2tknQzv0P/j467vvSreiV3kMVmLYCCjCmLZVWDBcFMve8iUqZ7xk7Mjhhl86s34amnQ/CFy3pv7REtAIKMCJ6+LTz80yBcY6M2jOWI8M+5LYcGWXn9yK7rwlO7+DYN7STydMpMTKaJEaG25FiVWlctBYNOz8Hb07xZJfocVVg3ipAgOHgFJIDz8AmmX2iF5fJ1JE4OJ1OxC0R90VZkeWY4O9nbGysBTIIMbhMVOiuoMvCOC8IPbhtlkCwwTZsbvlbrK4OMefPIvvUQST3t1nNtkU7cbRyLeoXr5Df5TQM2mLgcg3Al5gGZ+VtiBN3iJbZUYCxL7Jf+zkWHngF+W3VyBnqGNeRAVssjpXdjosb7kN/xTqMlFSMO687qsBsK6AAY7ZHQO9/zQosfvJHePC9H8OBiQHGK8s/iyNf/zP4Yif/duGab6oXzIoCCjBmRXa9qSpwUxQYc2RIjIySt36JwksnkTzQJa6MPiR4hxArWQ6m48g4nb0chz/yB3AVLbH65ZPAhIi2Y1S+xR1NkmknCqtvynhpI5GpgK+/D47H/g0F+15G7JYNSNi6DgkJCXA4HEhJSUFiYqLlnCB4oJOCEINQgo4KwgoLPAY8Oo/RaRHovAiMgcF2eJ5rxtBgW9y3VZ1H9su/QXprrdVaT0wiqpdtQFNpObplKkrXwAC6urowkpSFxA0PIKFcshDFJcImU1u03CIFZGzt3a3ivLiIFRM4L9gLui+6opPw5r3fQOMnfl9jXtyiodHbXJsCCjCuTS+tHQYKKMAIg0G4RV1QgHGLhNbbqAIzoECgIyO+/gJiO1tgH3Ah/dIZlJ0TR0Z/k6R/HJQMCeMDxwV3pVP+mG7KKMVgQpJ1ajjOgQFxZ7QuWYeuTfdhNC0r+BLdVwXmjQK24UFkHn4VRdUHkFBWDKdkFyFY4EJgES2Aj1CCi3FemOMEE3RbBBb+3hJesD4hBfeNW4N1TVyMwDrcjr5YPQ5guMVx0ZKci9a8YrQXFaMrKdECGIM+O6KzixCVJ/EUSlYjKi0/8Pa6PZMKDPQjZ/eTWLT/FeS1XUDOcOe4uxFeuOVf5Pb4DOx94A/Q9pGvacyLcQrpTrgooAAjXEZC+zFtBRRgTFuqiK+oACPih1AfQBUYU8A4MhIkRkbJ208j//KpK46Mfjg4tUQCADKQ3FTRiwbkxcj6drd8Ey5s/yyGsoslFaS8pCUmYTQ5Q+IA6PTBMdF1Y04rEOMbQZzXjfzeahS66i3nBd0XhBd0WlhTPMQhEVjomOB5gg06JyYCGIQXhBKEFdxmWyYGBusTahgnButwP6bm4jiA4ZHf5H6Jg9GZnI2GtRvRWVSEXpnuwnta7afkwrPkNthyFwOSpUSdGIGjdJO3ZQyjejoQ21SDFb/6d2yqfmvCG3jEE9ccm47G7EU4e88X0XvXJyaspwdVgdlWQAHGbI+A3v+aFVCAcc2SRewFCjAidui046rAVQrwJUfedMCsJZYjQ9Z2twsZNacsR0ZWf/O0HBn8I3vYZkd3XCo60grQn5QJtzMFLRUb0LX5foymCMTQogrMcQWifKPIGWlDnqcdScO9SPS4rGkjhBOBTovgNKkEGAZsGBdFoFT8PTVTRgg5WJ/1CCzMOtCJweOEGFEXqsYBDH6bT4jR4UhH7fo70LlwkVWPdXn9kIDIIYmN4clYAHViBI7ADGwPupH27nMSJ2U3SuqOo8jdNOFNGPvi4JKduHjbR+EuWwpvYfmE9fSgKjDbCijAmO0R0PtfswIKMK5Zsoi9QAFGxA6ddlwVmFKBMUfGqQ9Q+vavkFt/FsmuTsSM+i3tcZ5BJMtL2VRTTPhHd0+MExfKN6N6+2cwnJkPX1Q0vEkpAjNkeonGyJhyLLRCZClA50X86CBKhxtQPtI6FtMi0HlBx4TZv5anozvCxLjgVBPCDjPVxDgzjHODsMOCETIVBefPIeulXyOtpWbc7briknF+4zZ0VSy1nBqEIGyPbQxIXIxBZya8i7fAlrcEURLXxiZZVLTcXAVs/b0oe+IfsX7/00gcHUKCoKXAQtjUZ08As44cuOeraLvnYY19ESiQboedAgowwm5ItENTKaAAYyqF5s55BRhzZyz1SVSBYAWMI8Pe1WIFlYvtbkOUODKimCJbXppyzx7AyhO7keHpDb503D4dGSPiyOgSR0Znaj5cEh9jKNaJxmWb0Xnnx9WRMU4t3Yl0BWwSMyZnuA0F4rxI8bqQiiErs4iZMkLgYIJ1cpsg41oKfy+NM4PuCkIL0w7BAx0Uxolh3BTcj6qSNKrPPoWUpgvjbtcdn4KLm3ega+nysetYnwCDU0pcXhuGElLhyylD7OJNiM4oHHe97ty4Ara+blT8+Fu4/fCvLSAcLVP1AgvjlRxdvA0XtzyEvtLlGCmS+CTivKH7RosqEI4KKMAIx1HRPoVUQAFGSHnm1EkFGHNqOPVhVIGQChhHBivZxDWRvO8VrH7+EWR21VnXJYy4p+XIGJRIGq6oOJwt24ILMo97KCPP78hITvUH/NQYGSHHQU+GrwJ0J8X7BlE20oRF3rZxzgtCCxYT48IADZNthNDAgAjjpAjeN1NGzHQRQgZeZ6alcJ/QwlzPfRMTA1XnkPz0z5DYOB5g9AicqNmycxzAMNNUenp60N/f73d8pObCvmwbogsrYRcIaYtRJ8YNfxIJnzqaENcosS+e/T42XHx7XJN0XvSK86JdnBcH7/s62h/48rjzuqMKhKsCCjDCdWS0X5MqoABjUmnm3AkFGHNuSPWBVIFJFTCODKuCfPMX3VIHx/ljiBnokUj4duSf3odVJ19Fmqd/0jZ4wit/lNOV0RWbinaJkTEgL1BDkrmkYcXt6Ljr4/Ax9aoWVSDCFLDJt+Y5w60oFOdFVrQHmdFeK7gmwQUhBWEEi4lxwX2mUuV5TtWge8LpdFrXuFwuK6YFA34yRgbPE1oE7ht4wd9LtmViYRBoTOTEGD13JiTA6F62wppCEgg/2A+3220tjInhSZT4Nbnl6sS4WZ/NwQHk7noCi/e9gNyOWmQPd41reVA0P1y+FdWbH4Jr4Sp4SvypqsdV0h1VIAwVUIARhoOiXQqtgAKM0PrMpbMKMObSaOqzqALXpsCYI4NWZnmBSn3/Rax64VFkdDdY9uaE4QFxZPRPGSNjzJFRfhuqxJExkpYtjgw7PCnp4sjIsaarXFvPtLYqcGsVMM6LkuFGLPJIEFwJ1GmyjRiAEWj3J3Cgm8LEwDDOCbNPaEDnBAEH6wXu81rjigh+ykD4QIhhpptYsOPs6QkBRq8AxNrb70HP8pVj9U3cDF7HdrgYkDEi2Unsy+6UVKsl1u2j4hJgT0xDVEx8cHd0fzIFjPOi4SJWPPfIVc4LXsZJJL1RCXjz7j/A5c/8ica8mExLPR6WCijACMth0U6FUkABRih15tY5BRhzazz1aVSBa1Eg0JFh4zfMjZfgqD6O6IE+C2gUnNgjjoxdSPUOhGx2zJHBrCUp4siQQIGDEiOjftWd6JQ0gb7E5JDX60lVYDYV+NB50YZk+awz5gVBBBfCBwIHQgyTbYQgg2CC5zndg84KTidh3cB9XkeQQChh2uE+F17DJbCwHkGDuYZrEwOD53znJQjvBFNI+iTTSP22B9G3YtVYfV7LtggwuB0YE8M9KjFtnDLdKybBun1M9gI4l9+FuOyiwO7odigFhgeR+/LjWLLvOeR0XLrKecFL+e9ij92Bt+77QzR85k8tKBwIwUI1r+dUgdlWQAHGbI+A3v+aFVCAcc2SRewFCjAidui046rATVcg2JGR9t7zWPXSo0jrabHulTDiQsrIdBwZ0RIjIxZnFt2BC3d/ASPJGfDJy5wnJROj6erIuOkDpw1etwLR3kHEet0oHWnGYol5YZwVBBZcWPjSaZwVBkQkJiZa5zlFg26K7u5ua+FUEcKCgoICZGVJhp4r1wc6MQgWjNPCqnDlB48FAge21dTUhK6uLgt2xNVdQtk7u5HbJQ6pgEKA0UCAsXK1BSsM+OB9uBCUEFaybRMTg24M9pPFnlOC+NU7Eb9gGWLkd9Ue6wcbAbfQzWAFBvqx7NFvYvuRXwWfsfYJLzpjktGSVoQT93wFHfd8YcJ6elAVCFcFFGCE68hovyZVQAHGpNLMuRMKMObckOoDqQLXrcDVjowaJFSfRMygy5oGUnjsHayWGBkp1+jIcCckiSPDgbo129Cx7VOAI/G6+6gXqgI3SwGbvNSnueqQ7WpAVoxXFh8IJrgw5gVhhSkEG1wIIkwcC9YhHGhvb8fjjz+O1157zdon7NiyZQs2bdqE8vJy5OXljbkq2CavMzEzTPtc8/ePEINtEjzU19fjjTfewIkTJ9DX14f03m58aqgbW6SvgYVBPJmFhACD0IXtsH0CC0IKtkeowX0CF3OOx1k80XEYjEtCTOFSpK+7B/G5CwKb1+2JFJgCYDD2xaGKnaje+gkMFFfAU1g+USt6TBUIWwUUYITt0GjHJlNAAcZkysy94wow5t6Y6hOpAjdLAcuR4RmxAnwyRkbaO89g1cs/RGpfm9xCbPTDfUgVR4Zd0k6GKoMSRcPvyNgqjoyH4XHKlBKb2NjFjeHNyrfaD3W9nlMFbrYCUcMu2If6UDTYgJLBRgtKEE4kJycjJUUydIjrgqDBFOO8IJzgQphhztMl8dd//dd47LHHrOppaWnYsWMHHnjgAaxfvx4VFRXTioFB8MDFAIzz58/jySefxL59+yx3R6Z7AN/ITcFDKRJUVO4UdSVVJ9OoVm/abgEMwhUWA0hMe4QXXOi8MNsGYHCf6VaRWYSUTQ8gsXQVYtOy1IlhKRn0wydZR1obEN94Ecuff/Sq2BfMOtJtd6I9OR+H7/2qZB35raAGdFcViAwFFGBExjhpLwMUUIARIMYc31SAMccHWB9PFbgBBfjyI29UVgtWjIyGGjgunpIXvwErRkbh4Tew9tSrSBodDHmXwBgZnSn5GImOhVeCfFavvxet930ZvgRnyOv1pCpwUxWQz3RC2zkktV9AQaJk33HEjHNWMJ7FVDEvmG2EIINlMoBx//33Y/ny5Vi0aNG4GBmMa2GcEYHPRfcFnRd0S3C7urp6HMDIGB7CV3PS8EByPBy+EcRcAYecQlJ35/0WwOB1pm2uubBNAy0mAhjsD4HHsC0ao4npiC9dicyN98ORVxrYPd2mAiPDyH3xJ6h8/1lkd9chKyjryJBoeGjhXai67WNwly2HZ8Fi1U0ViEgFFGBE5LDN704rwJg/468AY/6MtT6pKnCjCgQ7MtLf+jVWvfIjJLs6raadQ73TcmQQaHA5UnYHzjzwNXgdSfDZ5N0gI08cGYXy1fKH33zfaJ/1elXgKgUEYKTV70de83ErTkV2drYFLAgujLOCcMIE5uQxTivhMZM9hPvG7RAKYCxbtswCGKzP9ng9IQJBAxe2TbcHt800D64JHmpqasYBjGQ59omcLOxIjEe+14U0m38KSL8zHc07PoK+VWvGYl4YkGEAhgEXBmYQWPA+LHRiEGKwDvsXK7Ew8h/6OpLKVl4l3bw9IJ8Ze2sd4gXiLnvxUWysfvsqKfhvWl9UPN7a+fu4/Nk/1awjVymkByJJAQUYkTRa2ldLAQUY8+eDoABj/oy1PqkqcKMKXO3IuAhHzWnYJSK/vImh+MBurDv9KpyjQyFvRU+HT/7Y74hNQbtkLTGOjKqND6KVlutYTecYUkA9eWMKyMtoTttxLOg4jczMTGRkSODKK1NCTPBO7nMJjHnBfQMXmJGE+yyTAQxOIVm7di2WLl1qxdMwMTPYBmNREByY4J4mLgXBg5lCUlVVNQ5gxMq9VhcWYqMzAbcNtGORbdi6vwEYrjXrxkAIgQTb4e8soQXvxzWBhpkywn0W1uFxAgzG2oguqkSugMXE0hXWef0hCshUutwXf4yle55BluW86B4nC/9NGxBPTHdsMt6/9/fR/KlvaNaRcQrpTqQpoAAj0kZM+wsFGPPnQ6AAY/6MtT6pKnCzFbjKkfHGL7F610+QOCB/3Mu3ys7BHnFk9FkxMsRgMWkxjozD5Xfi7ANfhTfOZEGQq6SdYYmT4WWKR9nWogrciAJRwxKzZVBiXwwx9kWTFe+CcS9MzAu6LLhwn4CCU0UMZCCA4Is+FxOMk9sMtvnNb34TTzzxhNW14BgYlZWVVn3jtCBcCAQYhCHcJ0AgTCB4IGQIBhi8/4IFC7AqyYkHOxuxJsoPIAbiJdvFmtvRv2I1vMlJAgRjxmAF2+P9CCYILAhIuDb77LC5J+/PWBi2rGKkSSwMpzgw4lJzYY83v483onxkXUtNotubEN3RbHU8amQIS597BFuqXp/wQbwSkaQhIRsNOYtwYftn0XPXJyespwdVgUhRQAFGpIyU9nNMAQUYY1LM+Q0FGHN+iPUBVYEZU4B/5Mvbz9g3jfaGi0ioOYMoz5Blny7evwurJUZGojgyGHBwMvxgHBnt4sjoSC7A6JUpJFwzVsb5LR9D64O/LS6PD7NCzNhDacNzWAGJfdF6Dslt55GfGIN8Z4w1rYMAwcS8MGCC+5zywXM8RohBsGGcEgQbrMP92tpafPvb38YvfvELSzsDMEwMjCVLlljTTdgeQQXhBCEFYYK5H/fpzCBc4JrngmNgEIAUFxdjhdOBj/Q2Y12U34ExJCmLu1Jy0S2ZLvqXLcVQvgA/ASuBbg7CCd6XMIPtc2EdFh7j1BE+C+sxK4k9JQvOhWuQufkBOHJLrXrz6ofon/b6UyjZ97L12DafF8VN51Do9gONYC0Y++KDZffjwo6HMVRQBm+OAFctqkAEK6AAI4IHb752fS4AjFH5H/NQXzc8Q25Eix3ZHidLrPxPWb6Z0PKhAgowPtRCt1QBVeDGFLjKkfHm01i2+3FxZPTALtH7E91dSJuGI8P0wjgzDi3ajrP3fxW+K2kth+XlwJO7wFTTtSowTQUY++IA8pqOWdNGOH2ErgYuBljQdcF9worAGBhmnxCAL/8GYBBIXLp0yQIYTz31lNWPQIBhYmAwXgbbNhCB8MAABNN5AgfTPs8Fx8CYDGB4BA26bLHoSc1D25atcAswYVtsg30lFGG7XHMx8MQCkHJznqPzwsTnINDgtY6Fq1Hwkd9F0nycSiLPv+Cnf4/N7/ldNYSvnBqXAH/ckbExE+27YpKsqXBH7/4K2u7/sjmla1UgohVQgBHRwzc/Oz8XAMaI24WWM4fQ23QZiWI/dmblwZmRi/jktPk5qJM8tQKMSYTRw6qAKnDNCkzkyIi/dA5R3mF5KZQYGftewppTu60XAfuVFJChbvKhMyMVnSl5coU/uOfJrZ9Gy0e/rlNKQomn5yZQwIfcthMobj+FpKQkK2UqIQXhhQEZBlQQNpgpJDxmnBEGdAQ6JxoaGqwpJD/72c+sexqAwRgY69atAx0YhAIEB7wf2yX4YJuBhb8/ga6JyaaQrEx04MGuJqy9MoWE3iZCjN6kLLRsux/9S5eNc1rwPoFghP0wkIL3J+RgfwgwCDJYXwGGB4t+8LfYttc/ptRJcrTIdDj+q/RhGbHZcWDJ3aja9hm4ixbDI+4LLarAXFBAAcZcGMV59gxzAWC4u9tR9frTaD13BGkLKpFeugwZZRVIzlVbX+DHWQFGoBq6rQqoAjdTAZ98i+nzyjeWAi9s8tKW/sZTWPbaE3C6JS6GODKSBjrFkdFrvRRMNr1kov58sHA7Tj/4IcAYylugjoyJhNJjQQr4UNB1BuXd58aCcxJOGHdEoPOCF9LxQKcFzxMA8KWf9QkxDMBgvVBBPAkwKioqrOsJDtgW72OmkvB6UwzAIDzg/SYDGKuuAAwTA8Nc3+fMQOP2B9G7fOUYcCGcYL+5cJt9MK4M3oeFx3g/AzA4jYR15qsDI7qxBvFNNVj68o+x8cJblkbBPwiNOsV50ZaUj6N3/xbaH/xKcBXdVwUiWgEFGBE9fPOz83MBYPRJuqvDP/snNBx5G1mL1iF32W3IW7UFmeXL5uegTvLUCjAmEUYPqwKqwA0rMJEjI+7yedgFatjkJa7k/efFkfEqHL6Rq77ZDHXztphUdIgjwwT1PHHn5/yOjFAX6TlVQL49nwxgGDBhnBhGLJNthNCC23Qn8IWfYIPXsEwGMEwMjEWLFo1NRzEwgW0YgGDuRfcF2yc84PZ0Y2CY63scabh0x73oXbZi3BQS3su0TUhBWMF96/dTLjYAw8TAIFwhxEgoXzX/ppCICyb3uR9g+bu/RGZPIzJHeoy849b8F2u/TG2ruuvTcC+ogLdo4bjzuqMKRLoCCjAifQTnYf/DAWBwLvWgxLAYdvn/52GTb/Dik9IQ60ya1oj0NF3E3kf+BrX7X0NexSYUrLwLRZt2IKdy3bSun6qSVyJSD/V2w+sZlhgbMrdVonRHS4yNqAgLMqcAY6qR1vOqgCpwsxQIdGRESTyL9N1PYtnrP0PCkEscGT6JldGJ1GHJgnDFpk27drBle6K+7F2804qR4T9nkyCGJerImEioeXwsSjKP2Id6USSZR0qGm8acFMaBMRnAoNOCiwEWBACEDMaZQUlDAYybHQOjqKgIKxwJ+GhfK9bZ/UE8zbBOBDAISQwsIbgw2UcmAhgm0CfXhBkxErwzfdODSBSQEZ+RJ3/nOMyt5uQ6WlwX8Q01qHzlx9g0ifOCDz4q/0INStDO1+/6Omq++BdWwGI6zLSoAnNJAQUYc2k058mzhAPA8Mo3EC1nDqLjwilLdQbgzK5cO20HxUwDDHdXO1pOH8Jgb4fE1yi4EmcjF7GO6QGWcPkoKcAIl5HQfqgCc1+BCR0ZdVWIGpVwneLIKJKI/8tOvo7o0RFLjHhZOzAirwuhCx0ZncniyJAyaovCie0Po/Whr1r7+kMVoAKOljNIkgwkBcmxkoHEH/OCsSgMwDAxL0yMC14TOIWEEIN1jDvC7LPeZADjemNgTDaFhP0JBTDMFJIecWDQ6UFwwd859plAwrhHjAvEOECMA8MADIIOLl5mI0nNQtIi+dtry0clG8ncDpyb8+yjWPnOU8jobZrUecHxZk6l/qg4vHv376Hu4f/DcoJxbLSoAnNJAQUYc2k058mzzCbAYPaQnoZq9NRXofXsEXTUnLVUj0lwYuG2T6LktvsnHAU6Inhdf1uDdX6gsxlnXvoJmqSNjOIKZJatQNaS1RILoxLxiemIk2CeDOgZk5A4YXtTHexrqcflfa/B1dGClKJypBaWIjm/FAkp6VNdGlbnFWCE1XBoZ1SBeaWAcWTYxLnGbzBT3vw1yt/+FaI9AjDkhSB+qB8JwwNIdnUgVWJlTBREL1gwfjv6gQTVO3fPl6xTPgmyN1RQqo6MYKHm2X5K3QfIqT9sZR/JyMgYS48aCCwoSaATg8Ai0GlhJOPLKgEG13Qy1NfX42/+5m9ggnimpKRYwTs3b96MyspKLFiwwAIIBAmMgcFAngQKhAQTFTPdo66uDq+99hpOnjxpxafg9JbrARgEFyZoKNsmsOAxAzAM4CDA4LYBGAQdrO9cvFamkvwekkrm5hRcu/zdGCPL0uf+E7ed3TXRkIwd478vTXEZaMouwdltD6N75+fHzumGKjCXFFCAMZdGc548y2wCjJFBF04+8x+48NavZfpIL4bdA5bqcTJ1ZOWn/hjLP/77E46Cu6sVJ559BJc+8P/PZ1SAhqu9Ef3dHUhISrWcEbGORDjSciSY5yqJi7HWmk6SWnx98xYVYEw4DHpQFVAFVIFpKxDsyIhqvIyYxmpE8ctMQo1h+a5TlgV7X8Sys2/B6R2EUxwZoQpzBLSLI6MrKceq5pV2jm//Ito0yF4o2eb8ufSGg5I+9agFJEwGEq6DY14EOzECnRZGJIILgg3CD77019bW4lvf+hZMGlUez8vLQ3Z2NpiRhPchCODC9nk94QH3Jys8z+kqhBgdHR0W8HA4HCEBxmRTSAgrjLuCUIKQgvc2MTB4vqenx4IaPM59AhbWY53EJevmNMBIefs3KJV/Y4rqTmGBq36yIbGOe8R9sW/p/Th375cwnFcCb05xyPp6UhWIVAUUYETqyM3jfs8mwBge6MGe7/01jj37A3EzSMwLiS/hdvfL37J2bPjCX2DtF/9qwpEZEIBx8pnv49J+P8Dwyh+9ro5GuLo7/QBDnBaMn5GQlo2MUgIMCey5dB2uF2Dwfi2nDltTSBJlCokzm9NIdArJhIOjB1UBVUAVmIYCwY4Ms5/29jNYuOc5ONy94sjoR0r/9B0ZXnnh2F8hjowdD/t7IC+QgwXl6siYxnjMpSoZjYeR33zUyj6SmJhopVCdDsAI1oBAgwshhQnqSQdGIMAgFOE96LRgHda/nkKIQZDAha6J5OTkkACjOyEVNbfdjZ6lyy3wQBjBNggk6KrgmgvbUoDx4YgU/vxfsPHNnyJppB9Jo0MfngjYYtaR9phktKfk49iOL2vWkQBtdHNuKqAAY26O65x+qnAAGCde/CmySpZK2tNSdNaexID8wRoKYHAKSXf9BbgCp5C8/NOxKSQZpSuQzSkkkko1TqaQxCfJIoDkeqeQ8H6DPV3+IJ5x8YiOc8gSr0E85/Rvhj6cKqAKzKQCwY4Ms0+Ld2xLnbgxBmEbGkSJwIzl4shwjA5Pw5Hhf/HoTsy2uj4i8/pP7PwS2u7/8kw+irYdZgoYgJGamgpO8TBggSCCoMGUwCkkBBGBxTgvWIcggA4FXt/Z2TluCgnrEVrweq65fz2Fn3/eg8CB2+x3qCkkBmB0Vy4bc3gQVLANAgyuua9TSMaPRtmPv4Ud7/wY0ZLamdPUJioEofsW7bBA6JBkHPEWlk9UTY+pAnNGAQUYc2Yo58+DzCbA8E8h+T4uvve8OBqKJV5FJppOvYuetkshAUbw6MxUEE8/uOjEyIDLuiUj6UczA4lE52Y2Eu4HllFJF+iRP7g5HWawpwPDA72yPwCfHGfhtXFJmRKPg0AlXdrwp2UzbVCPwZ5O648XQhJIsLsRXu/x52+PkuCmMbx/nH9hILxrKRoD41rU0rqqgCowmwoYR0Y6Y2XsfR4J4s5zDLuQPCCODMleEu3zTvoCYvptpT+svAcXtn3GOuSTKSaDhQvVkWEEmqPrYIBBuMDpHIQXhBDc5zIdgMH6JqYEp5J0d3fjm9/85lgMDEILuiU45cPEvKCshBBmIdTgYvYnkp3AgVM7eC9u09VBgLEs0YkH+9qwzjZkZemJupK1pys+FdWbt4MAw7gsDLAwQT0N0OA+3RksbJv3MJBjvsTAiK6vRoLETqvY/fiEWUcktDAILlg8Ekvn9a1fQ82X/1KzjliK6I+5roACjLk+wnPw+WYTYDCIZ6/Mge6RVFbddRfRfbkqrAAGs480n9qPvuY6a+SjJbhoojV9hFNI8q5K8zosf2C72prReemMBCU9aAUnHehqEYjhByCOtFzJrLJG3CEypWXZejgycsd9onoba+V+h+ATcOHMKYBXvvXpb22Q9ZXYIAI+EuW4U+6dmJl3FQAZ19gEOwowJhBFD6kCqkBYKsCXPXnjg+XIaK33OzIYI2Pfi6g4+25IC7h5IAbh6xQreLczyzo0LOD5+E6xhN/3RVNF13NQAQMwCBUIHYJBhXFiBMfACJbCOCv48s+F++3t7fj2t789FgOD9ygtLUV5ebkVwDM31///dcIDAw4MNDFgIfg+3Kez4/Dhw6iVGBsEDIQtBBhLJabGDvlSZI1vAA7fMGKvuAY645JxZt1WdC2ptGCMuZ+BGcZ5Yfat3ye5D4FFb2+v5SrhNXSXzIcYGMw6suat/0JafysyJEhwcHEhBu4ovzvHI2lT9+78HdQ//OeadSRYKN2fkwoowJiTwzq3H2o2AYZR1iNW4Ut7d6P+4JvXBTD6Wutw+Il/RP2Rt5G9eD1ylt6G/NVbpp2G1fQjeN0n8IDZR9ovnMBQf6cFDNJkqktG2TJklFfCGQwgWi6j8ei7aD13VGDMOQz1dQq9l29d5I8Eujns0fGSESVD4nIsR9H6Hf4sKQFOjObT+1H1+q/kum6BFPJHkPz9PtTbLdf6A9nR+RGXnIK0BUskKOl6ONNzLDfGdJ0YCjCCR1j3VQFVIFIUMI6MNHHsLTj4KhLl31enq1uylrRbjoyYaTgyhuXFZH/lvbi49RPWY/vkW/FRccJ55d/W4ZwijGbkRYoc2s8QCgQCDDoZCBAMRDBODAICAzAIJjjNxDgzJmuadQgwAmNgcKrHypUrsWHDBixbtgwLF/qDhRMYmBgUBoQQJhCEcG2AAtcECYytsWvXLhw/fhx9fX1WXwgwKrKycIe4R1aOuJHW3QSH1211rzM+BWfW3hESYBCYGGjCe7CwTwwYavrGNaEG+8U6cy2IZ3TjRcQ11qDi1cex+fzrgjSvLgwGXOfIR0O2f+xGxal1cevH0bPtU1dX1iOqwBxUQAHGHBzUuf5IcwFguLvbUfXa02g9f0Re7isFDBAyVEpMjaIbGj4DMJpO7EHHxWPyP3ePuCc2Im/5bShYcztSCsrGtd90ai+OP/0dqXsK9ph4OV9ugYoEya3OFKwdF0+Kw+J9OReH/JV3STtbkLt8AxwCIlhq338BR37xr+iR/9naYyQgGIORJqQgSrZZPIMDMi2lW55thaSZ/SwyFq24JieGAgxLRv2hCqgCEaiA9cInL3vRHU2yNCNK/j2Mkul9JftewJJz712TI6PXmWEpMGqLQp8jDd3pebh0x8fQv+m+CFRGuxysgAEYJgbGVFNIAp0SgTEyAtvlFBDCEE4hCU6jumnTJmzduhWrV6/GokWLxi4jECAYMPEoCDIITcz0DVZkHe7TefH8889bLgw6JHg/Aowl4u7YJG0u5d8f+99BSm+L1X63TCG5MI0pJAQUBCJ8RnM/AhRmKiHIMM9uAMZcS6Oa8crjqHjr58jtqkPeUJelQfAPOrX2LH0Q5x78mnWKYHMkuxCjWfnBVXVfFZiTCijAmJPDOrcfai4AjBGZutFy+hB6m+rEuZBvTe9wZuZKrIm0Gxo8t6RlbT1zyHJ21O57XpwRXcip2IKC1dtRvGkn0ksWW+17htxg3bqDr+H4r78DpnnNYi711dtQcttDcMp0D1d7MxqOviMOiyelbru4NwoEYGzGors/h7RifztVb/4SB37yd2iXqTR0W6TIsxBWxEsgUpbuxiq0Vh1BSm6JAIzPI2/l7ZbLhCljp1MUYExHJa2jCqgCkaCA5ciQl7L0959H8aHX4bziyEhxtU3bkcEXl76oeHQnpOH88m1okn+zvQKMvQKPh3OL1ZERCR+ECfpoAAbdEVyM02KiGBg8RsDAF/hgJwaPEZwZAEIXR1dXF/72b/8WTz75pHVnQpLbb78dd911F1atWoXFixdb9QkgDMAgoOA278V7cJ/gwJwnZLh48SKeeeYZHDx40JriMQYwliyx3B2VYh3IefkZpLVfsu7bF5uI+sVr0LFwCQaT5PMqbbM9wgm2zTX3CSrMlBFeyOfhc7EPPM66xgXC+nMNYCx47P/G1jd/INNvvGPTbywBr/xojU1De1ohTtz1sKZfDhRGt+eVAgow5tVwz42HnQsAg7E0OO2CATPtMseZGULsEvDSHh1zQ4PEqRuEFo3H38PRp/5FAEnthACjv63JipXRcOhNXDrwMmJkTuzyj/4BijfeY8W5oBvDK9NkOBWlZs+LaDt/GH0ttdZUlLVf/AuZDrLR6qcBGL0Shd+RnIncyg0o3/5pJOeVWudrJXf56Rd/KM/nROHqu8UFchcKxEKakOL/NnGqh1WAMZVCel4VUAUiRQHjyLCLIyOmswW2QTfsA31YIECj4vyeaTkyaB1n4L4RmVrSG5uEfklN2edMRXdGIWru+Dhc8m+4lshTwACM6cTAIJzg1BAuxo1AUMF9vuQTBJh9bjc1NeGf//mf8etf/9oSxgCMO++805pCUlZWZsXcIDQhmDBtsjLvRTBBgMCF540749KlS/jVr36FQ4cOjZtCsmQSgDFoi0GnIx0duQvQUVGJ/uysceCCfWX7fAYT44J9MPflcS6sw2P8feIy16aQMOvI3W/L300yJ9cEQKUOLEyXuq/iHpx54GsYEh29Mo1MiyowHxVQgDEfRz3Cn3kuAIyZHoKmk+9j3w/+B7ounZsQYHDKx+V9r4tT4y00n3nfcoBs/Or/wIIgO3LXpSpc/kDqHX0TLWf3Iq1oETb97v9E/oqt1iMYgEE3R3JemUw/2YnKB75sTUVhhXO7HscB+TbB57Mhd+kdKFq3DcWb74YjzZ8y0GokxA8FGCHE0VOqgCoQ0QpYjgzJApX2/osoPvKGODK64JApd/LGiChJmZjZ04jMkZ6Qz2gcGZ0JGahafhdaVm6FV2C4R1xuw/KCozEyQsoXNifTGw4ir+molRWEEIMAgu6Ha3Fi0CkR6Mww+83NzfiXf/kXyy3BBybA2LJli+XAMDEwzH0IEbgYpwXrG7cDoQHPcc2FAINQhA6MwBgYdHSsW7cOFeLAyHv1OWS0X2Yz8MjL94BAjI6kbFxaswHdxQvGAIZpm/DEQAzey7pO1nRl8DgXA1jYR5a5AjBi6qrguHwei9/8OTZcePsqeEHnRVtKAU6Km7Xtoa9az64/VIH5qoACjPk68hH83Aowph68Ww0whgf6kbVwvQCKHSjd+hCScgqtTirAmHqstIYqoArMTwUsR4a8hNnFjREt0/gYI8Mmzjex4iFa1stf/hHW1e4JKc44R0ZMIvod4siQpUvSfNdK4E+XQGUt4a9ASt0HyKk/bMWsYNwKOh+4hMpGEuzE4At/oDPD7DMGxj/8wz9YbgkqQYDBGBh0YKxduxZ0TBAKEAgQmrBdOh14jIXHjeuBUISwg+cnm0LCmBrW1BSZAlH8xsvI6mqw2qF7gIlRWyUWxumVm9BZWma1zX6yfa55L0IYts99Fq7nA8DIee5RrH3jZ1bWkTRPn6g1vry/cCfO3P8VDIvD1Vvgd7mOr6F7qsD8UUABxvwZ6znzpAowph7KWw0wPPItYu7S21G4xu+wSLwSSEoBxtRjpTVUAVVAFaACliNDXuCYpckm6VfzX/gRlux/0XJk2OVlMLO7YdqOjA5HBi4s24YWCeA8Ki+dnoQkDBXKC2OmBvkLx09b8qV9yKo7aKVQZRpVQgKChFBODNYxU0UIAPjiP9E+AcY//dM/jQEMxtgwQTzplCDAMNcTgJgpI2aqCN0RPE+4wPOEGNwPBhjUtbCw0AoKyiwni+UzW/L2LmR3NY6TvEWmPh1fvhFtpeWWk4OAgrDEAAyzz/uy8BynlBioYVwm7A9LpDswYiSGmEOywC1+6ymsv/AW7EznFlDosqISu7d8BRe+9n9JprhoWewBNXRTFZh/CijAmH9jHvFPrABj6iFUgDG1RlpDFVAFVIFwUsByZMicfk4hscnLmV2m+sVIamxbtB3RbhdWvPRDrL28N2SXxzsynBiQ1JUDsQ50pheheufnJGvJvSGv15Ozo4ABGAZYcM2FEINLKCcGgQM/O3zhJ9QwAILHuN/R0WFNIXn22WethzMA44477sCKFSss4MBrjfuB23RisB0DNniO7bEvPM794BgYPJ6fn4/y8nIru0mFzRcSYLQsKLXaDwYXZt/6fZAeE1yYoJ7mOU1/+UCRDjBynn0U6954AmmuVqR6+q9yXgzJZJJBWd6582uo/eo3/f8+iNZaVIH5rIACjPk8+hH67Aowph64qQAG0602Hd0rMTDeQN3h1ySoZhrWPPyXkqnkHsRIwM0o+YaFpaP6FC6+8zyYbrW7/qxkManA+q/8d3FbbLbOmxgY6sCw5NAfqoAqoArcNAXGHBnyEmqXoM/Lf/BN3Hni+Wtq3yuvQ722ODQnF+LwR76Brns+f03Xa+Vbo4ABGCbrB4FFQkLC2M0ZF8PsExTwPJfplPb2dvzbv/0bnnvuOat6IMBgDAwCBxZCCU7VIEDgvei04L6ZSmJVCvgRCDCY3pT9ysvLs9pbs2aNxMAIDTCai0ssOEFHBQvva1wWAbex+tDT02PBDh5nP+cSwCj78bdx79s/kKCdfkdJ4LNzm/BiQIL2vrfta7j82wIwtKgCqgAUYOiHIOIUUIAx9ZBNBTDcPZ3orD0raVRfR/Vbv7CAReVDv4fi9fcgMTsfJs1p8+kDOPvS4+ioOYmRQYlzsWg1Vn76j6w1e6EAY+qx0BqqgCqgClyPAtY30FccGfbeLgUY1yNihFyjAEMBhgKMCPll1W6GhQIKMMJiGLQT16KAAoyp1ZoKYIxIsDhXR4sAjNdw6rnvY1D+OM5fcYekON0ugTi3IyE1CwMSVK7x2HuoeuOXcLU3IS4pDXnLNmHJfV8UJ0al1QkFGFOPhdZQBVQBVeBGFbC5+pD30k+x6NAr/pgYXo/EFqibVkyMAflutysuDWdX7JCYGFskJkasZClJxlDRQnivxCu60f7p9TemwGQAIzAWBqeUmH2uTUBNTusIVabrwCAw45QRuhvMfeiKMG4HOh9YuOZCB8ZvfvMbHDlyBMaBwSkkCxcutKaQLBFHQagYGJxCYmJf8B50YtCBYWJcmCkkrONyucbV5f3N+UidQmJvuIj4+gtY8vrPsen86xL7YmIHRr0jB/U5C1F9+yfQdffDoYZaz6kC80YBBRjzZqjnzoMqwJh6LKcCGL5RSYc2Moy6A7tx6PG/R+vF00hIShZAsRmrPvfnkla1GM0nPkDD0bdkeROsn1G6GgWr70TJHQ8gJd8fAVsBxtRjoTVUAVVAFbhhBQRYRDVdQkxHsxXAL8bVg5Uv/hBrphETg0EAR+T1qC9GYmLEJUtMDCc6MotRtfNhuGTaoJbZV2AygGFiX3B6BqGCiZHBl3zCBbMf6gmmCzDYhoEThAaECgzayfuaWBisw+Pcr62txQsvvIBjx46NAYyioiILYEw3iCchBJ+F9yO84MK2DUjh/ViHz8o6ZoqLgRc8H6kAI233z7HkzaeQ11GD/KEO+S0dH7yTz8ayp/IBnPrkH8KTkatpkf2S6E9VQKeQ6Gcg8hRQgDH1mE0FMEwLLWcP4KSk7mo5tR9D/V1wpOeiZMuDiHOmoL36NPqaa+HuaRVHRjYKVt2FHIl9kb1kFRLSMq0mFGAYJXWtCqgCqsDMKhAYEyPK1Yv8Vx5H2aHdliMjxjsijozL03ZkdMan49zy7WgVV52Xjgz5N3+weJFmKZnZIZy09cSL7yP90gdW8EzGwSCYILww23RZGIBB54UBDQQMrBNcjIOCsKG1tRXf/e53LdjAeiYGxu23347KykqUlo5PyUk4YAAC2yE8YTvGjWEABh0YL7/8Mo4fPz4GMJiFhA4MAoxJHRgxSTi2fANaSz5Mo0owEQgwjAuD/eWz8t48xjq8f2CJVIBR8PN/xW2v/QDJHhfi4RGAMb40x6ajNb0YZ7Z+Bm0P/fb4k7qnCsxzBdSBMc8/AJH4+Aowph616QIMV3sjmk8fRMvZg2iVZaBD0p35JECWR3Kyyx8KMZJ6L6VgEXIq1qNow06kFi9GrDMJ9ugYqxMKMKYeC62hCqgCqsDNUCDwW2ebvMRFNV9GdGeL5ciI7evCqhf/E6vr9oe8Fb/jNY6MfsuRkQSXrNuzSnDhni/AJf/Oa7n1CsSffxvJ1e9b4IIB7UKvOQAAQABJREFUNAkmuJisHwZmcJ8LoQUXcz6wxwQOBCC8ni/8zc3NeOSRR/DKKzL9SEogwGAAz+Li4sDLre1AaMBt0xeCDToheKy+vv4qgDGdLCTNMYk4UrkODOJpnBUGXgQ6LMznneDCBBPlMXPcdDpSAUbJY/8Pdrz5KOJ9IxKm82r3xfuLduKkxCbz5BVjNKfIPK6uVQFVQBRQgKEfg4hTIBwAhtczgrZzR9FedVwCXB6XeBHNWCxzE8vv/GRY6NklOcXP7noMvY2XkVGyHBnlq5C1ZKVMDckf1z8rFob8AdxTV432C0fR23QR7u42eIYGrHoOcV6kl6ywrg90XphGCErO7foZvCNDSF+wApkLV8p9ViM+Oc2qwiko53b/l0ARm9UOz7OdWPm2bzrFJ8lQRpkQJfirielcrHVUAVVAFZjDCgQ6Muz93cilI0Om/MlX5oiRf5NzxZGRMdITUgHCDMbI6EjIQBUdGUvWwScvyB6nTDUpXQZvdmHI6/XkzVHAdvxVxJ99C0lJSdZCOBEY48IACToieNwAjOC787xxX3BNQNDW1obvf//7FmxgfQKMDRs2YNOmTVYK1QULFoxrhnCCC50OXLjNtghGCDAIFAzA2LVrF06ePDnmwGAWkrKyMn8MDAnNUfzu68jprJcXdIy9pDcJwDi0eDWaihZY7ZvpIwaOGLeFcVpw3zgv5gLAiJUA6okSGH3he89gbfW7V2UfaY7LREtmCc5u+Sha7v4ibAKibKK/FlVAFfhQAQUYH2qhWxGiQDgADPkKAMMD/bL0YUQW78ggnJl51lSLcJBxRADEgLgrRgbdklEkyXJSxDoTYY+JG9c9H/9IkT90WW/E3S+uCxdGPcNWzAtWjBJrcYwEe4tNSERMgPPCNDLk6pb7NMsfM5I3XurFSD06NKLs/jSsnJbCAKAsMQnSjvSBro6oaf7PWAGGUVrXqoAqoAqMVyDwm2gbY2Q01yFaALRY5BDf3YqVLzyK1Q0Hxl8UtMfvfcccGdFODMU64LFFoT2jCCcf/Dr6Nt8fdIXuzoQCQ/ufge/ILmRnZyMrK8sCFAQVBBdcjNOC7gzjvOCxwELAYMAGoQABAJ0TTEH6ne98B88/70/BS4CxatUqbNy40ZpCQuAQWHgdYQLBgbkXoYUBFzxPoEAHxhtvvIHTp09fBTBWrFiBRVE2FO57B9ntdTJFYlRe1P0ug0b5nO1fuAJNhQvG4luwPbbPzzS3GRSUz8DCYwaocM0lsESaAyP32Uex4bWfIHWgHcletyBEvy7mmfZW3ItTH/9DDMsXTl6Z1ksgybHVogqoAh8qoADjQy10K0IUCAuAESFaRXo3FWBE+ghq/1UBVeBWKGDBDHmxI5Tmt7X23k7k7XoCJcffsV6AYocHxZFRK46M3pDd4auUV16pOmOTcXL5PWip3CgGuiiMJKVioEwcGVnqyAgp4HWeHDq/DyNVB5AVPYqs2NExEEF4wekjga6KQGcGb8eXW543C8EGIQM/EwQYzODx5JNP4u2337aOsc1ly5Zh+fLlKCkpAad9GIcFrzMAg6CAAIPnAp0XBma0tLTgwIEDqK6utu7BvhDAsL1FixZhgUxHzd//PrLbLsPpExhyJctGg92BfWXL0VhQNAYm2CaBBde8F/vMbRY+h4EY7JP1WZfj9sRUxGcXIUlcQxnr70WCbIdzib58Ho5Lp7Hond9gfdVbiLmiR3Cfd2/8Is7+wf+yQKRii2B1dF8V8CugAEM/CRGngAKMiBuy6+6wAozrlk4vVAVUgXmmgHmx42Pb5NvzqNY62Lslu4F8kx/f0YTVLzyClY2Hp1SFEMPDrCXRDrhjHPAKwGjNLhVHxu+if6NmLZlSwOuo4B3ohc/dA0f9UaQ2nrwqFoZxYhhnhImJwVsRYHCfsIIQwDgnjHuC8SP27dtnTfUgACCQoOuCU0fMlBU6Owyo4D0MNDAuD+4b5wXBAvf7+vpQK5lIGGOD9+Axp9OJtDRJuS5TSXLESZFz4ANkt15C0ugQ4gSNsdQLwHh/QSXq8wqsNg1sMc4Lrs0x1mefzT7vYYpD2si681NIkimyMckZ4jC9OpipqRsO64xXHsPy1x5HTk+jBNvtG5tSE9y3XQIwzgjAoJN1vMcmuKbuqwLzVwEFGPN37CP2yRVgROzQXXPHFWBcs2R6gSqgCqgC/m+p+W21LJYjQ6aU5O/6LxSf2mO98MYOu5HbOX1HRntsKk6tuBtti9dLjAwbhpPSMFC+QhwZBar2TVKAL+fRp1+D8/x7FpAglOBCeEEYQecFX+a5ECwYJwbBA88TYPBF30AKHufCYzU1NWhslCDdUnhdamqqtRBc8B6sRxBiAEngI7FfbMPEpuCahbCkq6sLvb291jbrsA9sjyDDKSAipaoKGQ21yO1tQapADJa6qAS8V7QEdbkyReJKf7kmuCAUCQQorM/785kC4QWPJy5ei4KP/C6SSldwN+xLwc//X9y5+xEkeSVAujVxa3yXm+Il9kVWGc7c9gm03iexL6LsCjDGS6R7qsCYAgowxqTQjUhRQAFGpIzUjfdTAcaNa6gtqAKqwPxUIPCFzyaBp6NaG2RqSZc4MuxIEHfG6hf+EyuajkwpTqAjYzA6AV55eW7OLsepj/we+tdr1pIpBZxmBWu8jr2C2NNvWhCAcIEgIDEx0YILhBYEBIQVBjQYZwbPEUBwn4CCLgkuhAncZxwMTsvgeQM62C2Hw2HtE0ZwYR8CPzesQ3hAuGCcF+Y814QZvI7nCCHYDx63wIQctw+4kdZUjyVnj6JgsIvN4bItHu8WLsblnLyxumyfz2TWvN7ch9cEbnOfJdIARvET/4i7X39Eso4MTwgmrNgXn/wTiX2RB29ajjX1S6eQ+Mdaf6oCwQoowAhWRPfDXgEFGGE/RDetgwowbpqU2pAqoArMYwWsF0B+iy0LHRnRnc3I2/1zFJ3eJ9Ga5cV3yIWcDr8jg0EFJ3txMjEy2mPTxJGxE20L11gvWkOpGeLIWKmOjBv4jHGMTDDP5ORkcAl0SBinBYEEwQHBggEaPEd4YNYWQJDzofZ5P17DhfXNNcZhYR6F9QwQ4TYXFq7NdYH7vN7U53ZSYwPWnDyI0sFO67pWiYZxLCUPtelZ6BTA0idHDbww103kuLAuDvgRKQAjpuYMki8cw8K9L2D1xffEfeGfSmMepUmyjjRllqLqto+j7f4vWbEvzDldqwKqwMQKKMCYWBc9GsYKKMAI48G5yV1TgHGTBdXmVAFVYN4qYF48+U09RuRb4LYmREmmKJvMtU9ouYSVLz6KpQ1HrGwR8j36pDoZR0a/uDEG7ZJfQmJkNOYuwsmPfQOutdsmvU5PhFaA49Pz9n/Bve95KxgmA2IawGCmkRBIcPwMmOA265jzdEMQGpgpJ8H7BAWEDqzPNgL32Y5xWgT3lH0jVGDbXFi4z+u55v3YF+PGYH3CCLo+EhvqsfnCCSwc9qf0dUt8lVa7ExfjU3A0JQ2N8XHWfU1fpwMveP9IARg5knVk0+4fIU2yjiTKNJrgrCN7Fu20so6M5BTBJ9nsREg+nhZVQBUIoYACjBDi6KnwVEABRniOy0z0SgHGTKiqbaoCqsB8V8CCGfLiybVNXlyjJchnzhu/RP65A4gSQpHg7kVeRw3SJWvJdBwZbXEZOL18BzrKxscjGErLRv/i1fBm5s93yad8fo5Fv8QocZ/dhzTbINJtMgVDIAMX47Sg+4LbhA0GXnCbx3iOL/9ceIxLqH1eT8hAoMHreR/uG0AR2GHTTqDjgsfM9bzWQAtez3o8RyCS1NSIzVUnsGjEDzA88oly26JxMcqJdyRNaI1MYyEIIcDgdWw3VImW+CvxuSVIkhgY6Wt3hm32kdiaU0iqEufFvpewSpwXsUHOC+owIjDnnY2fx/nf/iYQn2C5o0I9u55TBVQBvwIKMPSTEHEKKMCIuCG77g4rwLhu6fRCVUAVUAVCKmBBDKnBF1k6MmztzYhy9VjZDxIaqrHqxUdQ2XR82o4Ml7gxhu3jM0HU51bi2Kf+GK7Vd4bsi570K+Ad6IN3oAf2qj2IrzlkOSXoljCwgjExGLfCvOgTWhAemPMmmCeBAEFCqH0CBn4GuPB6LmY/cDx4jG1x4XZgMWDDuCdMXd6f7fN8WmsLNp47ivIrDgy2IBOZUC15SXanF6A6KeWaAAazj2Rv+yySF65GdGIKoqLHf+YC+zeb2znP/QCbX/0hUidxXrhkMokrKg77b/8Can/rr+CL8Y/zbPZZ760KRIoCCjAiZaS0n2MKKMAYk2LObyjAmPNDrA+oCqgCYaCA9WIqL5tcW46MtgbkvPk08qoOS8DBUTgkzWde20Wke/pCOjKCH6VFnBknJXtJp7x0sgxm5IkjY404MsQqr2VCBRinZOD9p+E5/Io/m4dAC4IIggyTlcRcaACGcWRwn9sGcAQ6K3iMsMOcN3U45uZ6Agfrs2BuIGvus65xVgQ7JLhPWGHOc22ABq8jwNh84eSYA8M0fWE0Bi+l5qMqMdm63gCP4Pub+maduGi1ZB/5PSSVrTSHwmodfbkKTnFflO95Duuq3kSczz/lJriTNc5C1CxYhfoN96L3zo9p7ItggXRfFQihgAKMEOLoqfBUQAFGeI7LTPRKAcZMqKptqgKqgCpwtQLmxZHfxGN4ELauNnFkyBQSiZHhvHwOq8WRsaT55JSOjMCWaZF32eMwEuX/lvxywTIck0wLA6vuCKym2wEK+Ea9aN31Y3S/+2ukpKSMLUlJSRZoIGwg0OBinBcGZBgoYUBFoLOCoMEsxrVB2MBjBnxwn0tw4WeD9YyzYqLzvM6cZ32zn9rWgq21Z7FE4FdgOe+NxnOJ2ahyJlkAg9eYz2BgveDtcAcYqa89hWW7n7DSFGePdMtvwHjXinmePUvuxfFP/Sm82QXwpWZaY2nO6VoVUAVCK6AAI7Q+ejYMFVCAEYaDMkNdUoAxQ8Jqs6qAKqAKhFDAepGUF1auLUeGpF3NffNXyKk+KjEyfJYjI1uylqTJtACJvDDpS1rwLZol48IpOjKKF1unBiU2Rt+StRgVZ4YWvwI+3yi6j76FPomHEefugmOoF4QXXAgeuNBZYdwV3DcAggCDiwEagc4KAohgwGGAgQWt5Pbm+sCxYB0eJ5AIdFrwuAEh5nyg88IAjLT2NtxVdx6V3v7AZkGA8awjE+cFYJh+jasQtGPFvsgvRdIiiX2xZkfYxr7I/fX3seWV7yNdfjfiwUgX40tjQrYEvV2Cqg33o33bp4G4eI19MV4i3VMFplRAAcaUEmmFcFNAAUa4jcjM9UcBxsxpqy2rAqqAKhBKgXEvt5Yjox22gX7rZctZV4WKXT/FwrqjSPQNSoDC0IEXzX3oyBiwHBkx1qHawhU4/un/DQPLt5gquhYFPBILw9PfjZHju4Gz71vpVJlSldNIAmNicNs4MShc4JgRXhiwQZjAJfg8AQTrGQBhzgcOQqDzgudZl8E5eZz35/UTOS8MzMjoaMf2xmosHXUFNnvNAMOKfbH9c0hetAbRjmSJfeH/DI1rNAx2Cn/x/2H7ru8iaXRQpl9d7b7YW3EvTnzmz+HJyseoBCT1XYk/EgZd1y6oAhGjgAKMiBkq7ahRQAGGUWLurxVgzP0x1idUBVSB8FfAerENcGTYW+uRueclZNScRPzIIBL72pHTXnPNjoym+CxxZOxEd8FCSwR3diH6KtZhVLJTzPfCWBjD1QcwcvEYbJ31sHW3WCCDcTAIHrgEOzGCNTP1eNzAB64NuAg8TiARWEx9444wDg7jxOBxwgsWwgrus7AenRo8RrCR0dmOe1ovYbnPbZ03P8557fhNQgaqHEnWNdZnzJwMWPudF2XivFiD9NXbkZBTHHA2fDZjq08i5dxhlB3YhZW1718V+6JRPusNOYtRvelBtN/9OSA2Pnw6rz1RBSJMAQUYETZg2l1AAcb8+RQowJg/Y61PqgqoAuGtgHnB9MfIGAZ6xJExOGC9xDprTmOpODLK6o9fsyPDLZlLPDb/t+nVxatx7DN/hsGlG8NbjFvUu1HRd1RS2vbuexbuI6+Pc2IY5wUhgnFmTNQtc55uDOOUYH3CD7NvxjbweoIIOi0IJngvQg/uE04YuGFcFtwPbIPbrOdyuZAuAOOBjnqstA0FNo+znij8Ki4dF2QKSeC14yrJjqO4Atk7Pi/Oi7XivJA4IGHqvGDWkS27HkW6uwMJo5LVJ8h98f7CHTjxqT+BJ7cYvrQsSf/jhz/Bz6v7qoAqMLUCCjCm1khrhJkCCjDCbEBmsDsKMGZQXG1aFVAFVIHrVMB64ZQXXK5t8mIb3XQZGe+LI+PSacSJI8PuHeFX8XC6upDTcQmpI72SNHJ0Qkt9YBcYH+AMHRm5pdbhAXnZ66vciFG+8M3TwqCerjN74T5/EPaeZkT3tY1zXhBMBDoxCCyMa4LQwZznNqECz5n4Gdw3xwgsWJeAinWM04LHWZ/tBtY357lmfX4WeA9ez2OEGwMDA0jv6ca9vW1YJQ4MfgZMUEsLYMSmWUE8A4fWnpiKhIJyxCSlW4fjcxeEvfMi9ewhlB3cheW1exEflHXEpI19Z9UncPJrfwskpWrMi8AB121V4DoUUIBxHaLpJbOrgAKM2dX/Vt5dAcatVFvvpQqoAqrA9BUw35qPOTJ6O8SR4ZYXXbs0InDD40Xi+SNY9vrPUNp8WpwZQ1PGymCMDLdkLPFabQBVC9bh+Of+GwYl0Of8LRJ3gk6MgR4MHdmF0dPvjWUfCYyBYZwWdEsYpwSnmxinBseJY2ZBpytxF7hN+EAnBqED2yPEMNeHqk9AwcLreD1BB69nP7jPxe12I0NcGNskm81KARiMC2HipUwGMBKKliBnx+eQVO5PkxoVHQd7QmL4Oi+e/yG2vPyI33nhG7nKeeGVIwzl+d7aT+Ps7/xPjIrjxPqdmb8faH1yVeCGFVCAccMSagO3WgEFGLda8dm7nwKM2dNe76wKqAKqwHQV4IsuHRfWy7F8C8/i4zf9DReRceA1ZNSfR+ygC8k9rcjuvIwUydAQ5/MKrhgfdyH4fg2OXJxZvgO9OQusUy75Nr5PppfMR0cGnRjD1YcwUnMMI00X4W2vt5wRBA5c6H4grOA2YQLHgs4J7ocqrGdcGIQPLLyeC+EG1yysR1hhzpnzPMbruc/rTXsEHDyeIVBruyyrRt1I9g4gDv5YGcEAg84LR+FCJC5cjbRVd8JxxYVj3TyMfxQ89W/Y+cp3kCzPN1Gpd+ThctEy1K6/D513fUJjX0wkkh5TBa5RAQUY1yiYVp99BRRgzP4Y3KoeKMC4VUrrfVQBVUAVuDEF+OLKYr5d5r7NI99IM3PJkLzcDQ/BWXUEFW89jZLGk0j19F8V6DC4Bx6ZdGI5Mmx+KHKudCOOf/6/YWjR6uCq82Dfh1HJBjPq6kHPnt/AfeyNMa3ptKD7gdpz4TbhhdmfShyOFWEFoQPXJuYFXRSEECzGqWHOG6fFROfZHoEGz2WPeLDTM4zVFsBwjY15MMBIKFoszouHkVKxAfZ4Z9g6LoK1nApg7F1yD44//JeSdaQAPnFfyKAEN6H7qoAqcI0KKMC4RsG0+uwroABj9sfgVvVAAcatUlrvowqoAqrAzCnAF1rLkdFYg/Rj7yBV0rDGu7qR0tmEnM5L4sjotV5sTXyEyXrSIN9mn1mxHX2ZRVYVb0wsRhKcGMhdgEGZcuBLTJ7s0jlz3Of1oP/MPrirDmGw4TxGWutgAEZgzIuJnBc8T/BACEHAwDqEHNynk4ILtwNjXphjxmnBsTTXBV5jtrk2AIP3SJL2Fst0oooRN5Z5epEX5bHGomHUjn32JFRlFKK7eDFsMk0okpwXsdUnkHZ6P0oPvyaxLz6Q2Bd+0GM+aA3x2ajPq0DNxvvRvv3TQLzDnNK1KqAK3KACCjBuUEC9/NYroADj1ms+W3dUgDFbyut9VQFVQBW4uQrwpdYmL982twu2YTd8Q8NIPHsAlW//EgsaT0mgT3FkXJleMNmd6cgYkhgZnisZHAaiE9CbkIaLy7fi0gNfhaegbLJL58xxCwZJoNSRvk60vf4keg6+agEME+8i0IkR+NCBzgzGuCBoYF3CCDotTEwLa5wCXAIEGiYmBtsLPs92eD3XLIQWBmKwLielxIz6UDw0gI8OdmFltL+eWyYQtdudqC5cgbP3fQXulXeI8yIhYpwXORL74raXv48MyToSJ4E7g7OO7C3fLhl1/hyePJn+lJwGIUeWPvpDFVAFblwBBRg3rqG2cIsVUIBxiwWfxdspwJhF8fXWqoAqoArMkALWSzi/6ZcYGZYjQ9Zx/V1I7Wy8JkfGsLwEu+xxaEorweUlm+DKzBdHRiJc+aV+RwYt+3O0jMr0nJ4T76LvzAcYbq6xYmKYqSN0WnAhSCBQCHRmcJtTOwKdFqxnAISRi2PEOrzeLNznYgqPs55pj8e5b46belwXSSDST7o7sfYKwODY9dkduFi4Csc/+Udwr94aWD1sty3nxakPUCppbSdyXliQTZ5t3+qP4cxX/jt8KZmadSRsR1M7FqkKKMCI1JGbx/1WgDF/Bl8BxvwZa31SVUAVmF8K8EXXcmRIgEdccWQkn96HpW89haLmM0gTR4bkIwkpCqNuSCuS5UFSfEZFoy/aiV5HGi6s2IZLD30N3lx/8M+QjUToSQsUSGyRYcn+0rz7cXTt32W5KTj9gzEsuDbOCO4TbrDQicFrTTH7gcd4jhCC13PNtgg+jHOD51k/0GnBY6YEt8XjVwOMKPRFJeBi/koc//SfwL3mTnN5WK9zXvgxbnvpe5M6L1ySLNYVFY9DGz6Ji1/6C/jovtCiCqgCN1UBBRg3VU5t7FYooADjVqgcHvdQgBEe46C9UAVUAVVgJhXgC68VI6O+GunH30WKxMpgjIxocRlApoukdtRhQdt5Kw1nqH6MOTLSS3F58Ub0Z+TBIzEyXAXlGFy4Cj5HYqjLI/Kcd2QYXcffRu+pfRIT4wKGWy+PAQfCBy6cJkIAcS3FAApez2tNrAw6MHiOx816Ou0GAwymF3UjGhezl+LQ5/53uNbvnE4zs1YnpuYMkmXKU6lM2Vl5cS8cPn8a2eAO1SQWo3rhRjSu3YbejfdK7IuE4Cq6rwqoAjeogAKMGxRQL7/1CijAuPWaz9YdFWDMlvJ6X1VAFVAFbq0CfBm2HBlDg+LIGIRPXswZV8Bmj0bmvldw+4vfQ8FAU8hOBToyPJYjw4GehHScX70DdQ/9DrzZhSGvj8STFkQQzYZ72tH06k/RdeBV6zEIHBiw08CH63k2ts3FQAvT1mTOi1D3CAYYrEv3THVaOfY8/Nfo23RfqMtn/Vz6q/+FNS89gpzeRqSMDskkkQ9dLIGds7KOfOmv/VlHGLgzIJ5IYD3dVgVUgetXQAHG9WunV86SAgowZkn4WbitAoxZEF1vqQqoAqrALCtgvTjLN/18CWfww/jzR5G/53kktzfKvg1pbZen5cjga+aAxMhozChD3aINcKdmYSQ6Dv0LlsC9ZJ2ktZw7jgyvQJ8uxsQ4f9gaPQKgwYYqcWTU3dBoGojBNceDC7fpwJhOsTuT4ShegsLhEdx/+j2sGm4fd1l1ajneI8DYfP+44+G2k/3co7jjxX9H5lCPTBIZFfQyvjQ4clFXtAI16+5Gxx0fA+ag22f8E+ueKjB7CijAmD3t9c7XqYACjOsULgIvU4ARgYOmXVYFVAFV4CYr4JOsGXD1IGpUoEZ0DLL2vCCOjO8g390a8k7GkeGVGBkemz/gZ7/EyTiz8h7UfOaPMDqHHBkWVJDsJKMSoJPF09cljozH0HVwd0iNpnOSbZtiAIbZn2odn1eKnLu/iEJfFG775T9jWfvpcZdECsDI+833sf3Ff0Oap+8qeMEH2rv4bpz44v+JkZwi+NR5MW6MdUcVuNkKKMC42YpqezOugAKMGZc4bG6gACNshkI7ogqoAqrArCnAF2jGyBhzZJw7gvz3X0Bih0wpEYdGZmstStrOwSnW/lBlREDGoC0aDWllqK3YjIHUTHij49FXUoGBivXyrbkz1OURdc4rWT+6Tu5B34Wj4/o9Kk6NgUtnMHSDzoxxjcqO3ZFkOS1iM/LHnYpNy0Hq8tuR2dmCzY//HZa1nhh3PtwBRtyF40g/8T5Kjr2FpZcOICEo9kVDfA4uFS3HpXX3oHPrx8TVM3cz34wbON1RBWZRAQUYsyi+3vr6FFCAcX26ReJVCjAicdS0z6qAKqAKzLACjJHRL9+E+/yOjOy3f4OtL34XuUPjpycE98LvI7BJbhPJXEJHRpRMJxFHxqk196P2M3+M0cy84Esidt9yZHiGJJaI35FhHmS4pw2Nr/wU3YffMIduyjoupxi5d38JaSvHp0O1RdkRFRMHx+kPIhJgZL/4E9z+wneQ5W6/MnXkQzcKhdtXvg1HPv8X8IjTxMYpSQLUtKgCqsDMKqAAY2b11dZnQAEFGDMgapg2qQAjTAdGu6UKqAKqwCwqcJUj4+xhy5Hh7JYpJRKjIaulBqXiyHBM05FRL46MS+LIcCdnwCMv232lyzBQKY6MBAnCOMeKx92PbnFm9F8c74S40ceMSckSp8VtcBYumrCpBHExRJIDI/bCCWQc34OS42+h8vIB+SyNzzpiQTABYXuXPYBTv/1NjErGG9s1ZnqZUCg9qAqoAlMqoABjSom0QrgpoAAj3EZk5vqjAGPmtNWWVQFVQBWYMwrQkeHql9dJf9aS3DeewtaXv4fsoc6QjxjoyPBKutb+qHi4oh04ue4h1H72TzGalhXy+kg86XdmjMDHFLU3sdhEv6iY2Elf4iMNYGS99FPc8fy/I1ucF9FW0M7xzothmY40IC6eg6s+gnNf+Rv40nNuopralCqgCoRSQAFGKHX0XFgqoAAjLIdlRjqlAGNGZNVGVQFVQBWYUwoEOzISzhxA/t6X4ZDUosxaktVYjbK2s1fFLwgWwcTIqE8vx6UlmzDMTBIyJaBz4Sr0r7wDYHBGLdelQPypD7D2yX9EZf1hxMokHpOGNNxiYETXnkPymYMoObQbKy7uQdKoBJCdoFxyFqBaMts0rN6Gno2SAlazjkygkh5SBWZGAQUYM6OrtjqDCijAmEFxw6xpBRhhNiDaHVVAFVAFIkABnzgyMDAgPfUhSrKW5L76M9z18neROdIzZe/5PTuzloyKo8Aj6xGZknJgzcdw/sv/fU46MqYU5CZViJNpPkuf+lcsqd1vQYE4UZkl3ABGypu/xsoXf4CCrhqkel3ivhjvvDBy7Fu0E8fEeTGSuwCQzxinLmlRBVSBW6OAAoxbo7Pe5SYqULDrBWx46xnESbqwicqhzQ+g+hOfhy86eqLTeiyCFFCAEUGDpV1VBVQBVSBMFPCNjoKLyVqSIN/+5+97BQn9XdaLZnZDlTgyzogjI/Q0CmlBJg8AtanlqK68DSMJSRgVR0fn4rXoXy3BKmPjw+SJw78bUa31SD38NgqPvYPF1fuQO+yf3hNuACPjlZ9h83P/jtyBFhCyRAUBjIaEHNQWr8KltTvRdftH4EtMDn/xtYeqwBxTQAHGHBvQ+fA4qYcPi7VvH6KHxwdUMs9+adU6tG25TXJ62c0hXUeoAgowInTgtNuqgCqgCoSRAj75e8Hndllfktvk2/J8iW9w167vIWOkd1q9ZMDGUXmV9YgrY0S2P1j/SZz/LYl7kJw2reu1kiggQAleDxKPvoPNv/hHVLSfsWQJN4CR8/yPcNfz/4qs4S4Z6avLvrK7cOQLfwVPQTls8QmadeRqifSIKjDjCijAmHGJ9QY3W4G4xkYk1l1GlOSEn6j0FxTCXVRszXud6LweixwFFGBEzlhpT1UBVUAVCFcFgh0ZDnFk5IkjI94lU0rE+p/dcA4lrWfhEEdG8Dfugc9kHBk1aeW4WHE7PBITY1RiZHRUrBNHxl2ABLHUEloBx7H3sOWJ/4XKtpNWxdbYNJyuuAv1Ekuib/kmeLMLQzcwQ2fjLhxH5pF3sODEO1gicTqcQRlshiVqhzsqBodXPIAzX/gLeLMKJg1YOkNd1GZVAVXgigIKMPSjEHkK+GQ+Iq2hk/Tcx3mImod7EnUi67ACjMgaL+2tKqAKqAKRoIBvRBwZgwPCLqJgk+mmuS/+BHfs+g9kDPdYMQ8m+/vCPJsfZNjGHBn7Nn4W57/yTficSaaKridRIBhgDAkY6LE7UF20Bqc/+Udwr7p9kitn9nDWS4/hzuf+FdmDHQKxrv4bsy8qDl2xKTglGWpqPv1HGE3PndkOaeuqgCowqQIKMCaVRk+oAqrAbCugAGO2R+Ba7++Dzyt/+HH6loDG0VGZP2xnLBoGQZMEhz45Jy8M1togSAGOzCBg6lhz1mWf/1mF56QO/+Oa7fov9W/zWusanuNVen/VX6cP+n939OekCgQ7Mv5/9r4DMI7q6vqq996LJUsucu8VbFNNCwFC72kQSCAhPQHSSE/4QkJCSyAhfwgQMISeELrpxl1y7022mtV7/c+5M7PelXclrSzZlvyerd0pr82dN7PvnnfuvfSRkbb8dQlvrJUAvGLS926Q/LKNEt5FN572u8hLbRaQIeIwMjpCLQZG+YR5UjfrdJiyGl9c3cXWHcAg56U+IES2Z06WtZd+Q5qmL+peZFD3Q3asl/iiD2Tkmndkwu7lPqOO7IjNlU0TT5WyKSdL/VT4PzFRRwb1vpjKjQR6koABMHqSjjlnJGAkcEwlYACMYyr+wxon8MC5fAAZTgQZCEgEIhgejyMRnLAABoIJ2ASAwfN6TI/gwwVYMAMVA1YIeMIN2ND6WBfO6XG7HtO+kf9AjD8MLJOMBDwk0NXWJl0tzfpuCwgOkswXHpYFrz0kCW11fWZk4G3F4anpndmXy6Yv3mXCrnpI2drpDmC0Q26NYGFsT58gqy//ljTOAPBzFFP8m0/J7Bf+JBl1+yUKgJUT3rV7F9T3xTW3w/dFvkhYOH6brN+57vnMvpGAkcDgS8AAGIMvY9OCkYCRQD8lYACMfgpuEIpxxZJAgwIOnKUr6MCG3JgV8Euj4IYetkEMBTewTbCDoATKaXQAx8zLZlRYTAqsdTqgB+twS6Z9I//BG39GEXF71E7ITXdGBhlkEUUfSsryNyW8uR78AJGMXes0aklYL4wMR3jb4kcjasnJrmhopZNPAiPjDJi3Gufi3QEMsljoGHVbcoEsv/K7Uj/rTEeMR+U76dV/yoIX/iBpTWUSQtC8W6vFkRmyI3+m7J16ilTOPlO6YhKM74tuMjK7RgJHWwIGwDjaEjftGQkYCfRZAgbA6LOoBjSjmnTYphkKNmBC7w46WGACmyRzwg2QUDDC2nc6pOcJYtiMiq5Oy+SDbAqal3TRGa/OGFEXQI1OeKl3Z3JoPrI86PeGoIe25zjwtcqwDiodTh6rTmsaatq3AKYTQf4O+8can2733xk7GJ86tjGGMFggEmvMuMYfj5tkJAAJdLWTkdGigENgSLBk/vsBWQRGRlx7Q58YGe5sDAr0rTlXyaYbf2acfEIW3QEMyodpW8Jo+eCq26VuzlnWgaP0mYKINKc+f4+k2WFduze7bPSpsuazP5Y2Mi8QQtf+weqezewbCRgJHEUJGADjKArbNGUkYCTgnwQMgOGfvI40tzv7QcEAggVckQL44IAaBBAc0ILggnXOBheUmUHd0FEOASygLIEHVSpJsNbyNtNCQQ3kJ1iCiWEngAjLZwbrIEjBySLP2YqmDZA49Zn2jfz7Ov4IbuhY5pjiuHZADY4wjj89Zps8HemDZMoPeQlwfDhjhO+b8MIPJHXFWxLW0qg6bPquQskt3SgRYGQAhrUwsB6umsr5jnEnIWKJJwOjZNoiqceq/omUjhcAI2xroaSsfltGFr0nY4rXHBZ1pDgiTbbnzdToKFXzzjEhc0+kQWqu9biXgAEwjvtbZDpoJHDiSsAAGEfn3iu4oIvP+HBAAqdpKnbKcCDowBVsKs1kPrgxHnCM2ISeI0sC+TjL5z9VBKyTCmRQ4VSWhc2Y0GZs4EPz28qkSyNwtY+Jv90+FdBAtN8JJSOQ4Ih7+6jXtH/iyl9ZPa4xZI0/Dk4LJAOYZo8/Kx/GLZM9/hQwcx//GGcmGQlQAp3t7dLV2oqIJYESGBwi6c8+IPPByIhva5BIAfDVi5g4xCxA1jPja/Ovk603/cLz4DDfO14AjBSYjix6/neS3lzh9f59MnKhrLz2dmnLKZCAkBD93Rvmt8ZcnpHAkJGAATCGzK0yHTUSOPEkYACMY3PPCTCoggeAwNLuMPUmYEHljsohQAPm0aT7h1YVddVSwQkwKGxlkGADQQb38o6ZBxVKAhKHtEhEL9H2YY3s3r4NTOgqOspYDA0WQ3/YHrqmtRBgUXClQ9tTsEX74yirVv9N+zTdAWQ0FOVv31//xp+lQnKcKYhBXyscd7h+/dbKrA/X+OfYMiCGm2RO3E0FYvE+4rtPGRnwkZG0eqkyMoIR+ShtV5HklG2SyM5WCe0DoOFIcmviWDAz5usuQ8CXTD8VPiDOcE4Py+9jDWCEbQPzYuVbMnLd+2BerAXzotlDzoSj6Fh0xdjTpPDaOy2nnbzv5l3gISezYyRwLCVgAIxjKX3TtpGAkUCPEjAARo/iGbCTNN2wdDmyGWyQQhVbqHt2WFKuQCrIgIwOQKDKL4/S3IPH8U/VxC7UFwhitbsZCEEH/iNrgr4vqEBCEWWd2j6vBpNERSKwSSWBSQERD0DCUiC0vK18m/aN/H2NP45RB/TiWHONPxvo4njj+MPQxbi0QTqOP44t1/iztvWA+TAS4LjAmOlE5BKCp4FYnU979iGZ/dpfJaG1SmK7Wjic/E6Ay+R/J39Ott34U7/LDqUCxxrASP7f43LKc7+TjOZyr2JrhdvWxoAgWTNhsWy65vvSnpnnNZ85aCRgJHDsJGAAjGMne9OykYCRQC8SMABGLwI6gtMWgEDAgpVYq9IKXjjbABisfSpzmI4zn35D4VPwAUodV6gBNKgCqKeZj+qipQCynGVygjwsY5cnUNFlO+vU5p02rc6o8qighFUT2mA9dj9QtwIeNtih7AsqoEysn2WYl+fZvr1qqmYwdhnT/tCWv+ueYlAQdOhx/DljyxmzOk7s6+d4UFXz0PjXMWqPGx1IOM/xZ+VjYZOMBDAy8F5xxiHfNaHrlkki/GSENTdISHurZO5YKznqI6NNwsDI6Eviu3BrYoHsGjtXs3cGBUrJjNOkfubpfSk+ZPIcKwAjZNdmiS98X3LXviPjdy6TuM4mrzLbE5kpm8fOkwPwT1IH2TPqiElGAkYCx5cEDIBxfN2PE7M3qvBYSoc3ATgTSm/nPI9Zdaj6xJkA55z6hQ1VbKx98zl0JGAAjEG8V3zu8MfJt07EqbTpMcvHBU5YjWOizmdJ82JD/U3wDJ8p5tfzUPKcZ4z10LyE5embwsmHZ5J5WI+udLOchTRoHZYvC7tPfGSVncEq7DqYVx9qfjMhE4+hPt2222cbNAHQ8qZ9iMa+TxDTcJa/AlY6LKzxpywhsix0HHNcWdevWcjo4dhAUoYGti0zJ+azxr9r/KGgjm2rAi1jPowEuktAfWSQkQE2RmBwsKQvuV9mv/E3iW+tkZgu+M4gkNa9UC/7cGksry36gmz/3A8P5eTzbL/rDh0cWlvHCsCIfe8lmfb8A5JduR0smWZ1vtpdcrxLy0cugO+LO6QtF74vcC/1+e+e0ewbCRgJHFMJGADjmIrfNE4JHFj3oez95HVpbarzKpDE/GmSPulUiUxIkLCoSJ8/JjX7N0j1nvVSe2Av/g5IUESkBMfESMa4OZI5cQHK2QqZ11bMweNRAgbAGPi7ooobdX4FDwg8AMCgksYjqrwdYkvQaSdPMI/6vLCVOAv0sFYVD5mN0McF8xKkgHKI7cDgQ6vjjnNFi1mhjelE3FEctUcoZznnBFsD/7SbOOFqj/3kqjn77DA6VEG3+o4Tpn0jfw4uHYsWUMZxCqevNFviAMUYc40ryIoMHj2AcU6wzDFvYhkMJg5La0wRSOP4Y36TjAS6SYBjh398P/EvrOhjSYCfjLCWJgnpACNj22pELdkg4Yha0ldGBmqTbUkFsnvMLG2tE2Z5+2eeIQ0zT+3W+tDajVj3sUx78m4ZV7xawsFOwa+EXsBgh1FNeOsZmff8vZJVu0/9lOBXxENwxZEZsm3MXNk3ZaFUTz9NOuOT9B1gAAwPMZkdI4HjQgIGwDgubsOJ3Ym1z9wrKx+/W2rKD3gVxOgFF8uEC74pSXn5EpuaggmmdyBi74rnZc/yF+RA0Rop3rBBwhITJSo5WcYvvl5mXPJNVYy8NmAOHrcSMADGwN4ay2zkkAKmQAOaUJaDDQ4oWGGDA2qqocCftTKtSh2VQLuMZdpBpRBnXGFQLdDCmfQpOGKzPNwVQFc7mKR3wWeGnsPKOCf/jomIZfZh+cnQemyFUttnf1VZZ+M2cEFfHlAwLXDDek+Y9i3Qx3WPbQV8uMmfDB6OG8ekRIEughAYnLxWfnMsEKBgUoYGxjnHDuybrGOQDfO6A3ku4IP1cKyyrNs41ILmw0igmwS6MzIynv6TzHrjUYlrqwcjo1lHJotYEBnHVu+pDar+66d8UXZcf7tbZpRkYY7xITIuwzaulPHP/EnG7vpE4joaXYDOoAEY/M3CM578+pOIOvJ7yWwuc5Of/nzp071i1CJZ/dkfI+rIGI/zZsdIwEjg+JOAATCOv3tywvWoNwAjc+JJkjX7fMmeOh9/cyU4NMyrjHwBGOMWXyvTLrlNggND8TvvHfzwWqE5eMwlYACMgb0FqsRyskstjMnZ5jd3sDqt5hfOZBj5lDGhE2MqggAn2tusciyBfIfyM69Fv9eJNCaN2gyOcUuZFQQYUJeewUn1LcBMOKbHmZd10OcA+4B0qE5mIzgC8xD2lx/I4/JtoZlZqV0XJ63MY9of/vIn8MCxgnuu4IKOHR1YPMoTALZsx7E6eDhYrHGiY4R7jlkJ2UMor8wglsb4ITiiCWW5z8YsUMQ6bD6NBLpLgONE310YO8rIWA8fGes+kZCWRglpA4AB8IznM3YWSk75FolA9JKILrxbe0jkpZGRsXfUDCsXxmNzRIw0w0dD1cR50jLOPt5DHcfDqcCK/RIDhkrW2ndl/MalktFSod0aLACDUUfSlr8uOes/lFH7CyWmW9SR4oh02TpqthRPO0VqZp8pnXFgXphkJGAkcFxLwAAYx/XtOTE61xuAkZA9WmJzJkr+yZ+SCWddIaER0V4F4wvAKFh8tUy95KsAMMKxfmGtvnmtwBx0SYArkOoYj5N596QTe4YT69tqj6seUkQdfQIgEhWCvtRhAAx34R/ZtjOZpjKnShu+LSCBN4Y6nvXN+2L5sICWZudlWd5AR0HkYc1DxU53nDqYj9odDqOM+rVQMxSyLCx6NfNrG6pYslqUoVLJKliU7bM9bDNx8m+1b++rAgo/F8oSsRVLLYzzdn+ZU9tXNgYqpuJp2ldZDk/544ZzvPD+Y0OZQRhDjh8LBcdwTk1KOP5s/yrIjGSNRy1rv5d0rHCc4c8ZjwqMMLuOP26YZCTQfwk4DI3Mlx6R6e89JXFNlZLQ3oBfR4xH/eu9boxwqQqKkoqYNFl7xnVSds519vjE2OW7mX/HcYpa/a7Mf+IXMq58g/Zye/xo+eDy70rt3MUMCXTkzxp/WwCaJ7/5tCx8/h7JavJkXjiiWZEzX1YgXGpr3gTj88IRivk2EjjOJWAAjOP8Bp0I3esNwIhOTpeIxHQZc9olMu3iW+AHI86rWHwBGIaB4VVcPR6s2rNZdn/8H/gS2e2RLyYtS9ImzJTYzFESHpcqwSERHue771TsKJJ9q9+SxoMHpKMFwcnCQiU4OkaS8yZL1uQFEh6d2L2Ix74BMDzE0f8dVexZ3FLk+U3ggEo+AQIqaR1gVihdHgwHG47Q/E4+B+BwlDsFM5BD/WSQMYFCPGcpkWwLSdvFOU0OM8LyScBzVl3WCiUn9HR+p8AE+8lJPIEyzcce4U/3LcXUKetcgzbBvNYGPtku99Auj9kKqZqX2PUryOGYrZj2j0/5897pfbXGg3XfodxwrLnGAxU1a8wwrwU24K5bN97a5y4YPFqeip09rjjmaXbE8RcEB4wcEzpWdMg4CqA1qvQ460d5jnt+m2Qk0B8JcJzxL2LzKonfulpCG+okpKFGshC9ZAQYGZGdLeovo6e6OSqbA0KkIShc9qeMkbIRBdICRkZTbJJUTZovLQXTeyp+zM91BzDKQ+Jl86h5snfKIqlGBJCOzNwj6mPo9nWSsvINyS16X0YXF0pst6gjlB9ZLavyFsiq6+6UtpHj9dl3gZVH1LopbCRgJDCYEjAAxmBK19TdJwn0BmCEREdLGBxyFpx5pcwGSh4em4h55OETR18ABn1gzIQPDM9Vtz517YTNtOujV+STR38KXyIrPGSQUTBZCs6+XLKmnyrxWeMlNLLn8GLblj4jq5fcIxW7t0hrfb2EwKlqVFKS5M49R6Zf9DWJS8/3qL/7jgEwukukn/u2smYp9KjDtbJMHwBQxuyoHVTuVcEnnZ7bmOGteeJe5LHbxT6Pa3kqjFAiOdlTsw67jD5nOKb5OEN0lbUYHKxJQRPmp0mJKoP8tpRLTuq1DiqoTmEqn2yPhe3jVjnmZft2efSNq+zMzb7Z+AVLYdu074BQw1H+Cr7h/jvjj6ZDHKceiQOaYx9jTk1KMG6mX/sNHVMBcJBojRnLT4Y1tu3fGYwxhcE4rp3x171uj4bMjpGAfxJQRkZrq2S9+LBM+3CJxDVWSXx7vTIyrFEIwKyXKvE2lKrgKCmPzZS1p18jFWddhVcgX56oge9FvGOPp9QdwMASh9SBKbste7qsu+RWaZq64Ii6G//Os3LSs/fIiLq9GnGku/xgjAgAKEgKC06ToqtvN74vjkjaprCRwNGVgAEwjq68TWteJNAbgBEUGiqB+Bu14CyZcOkXJCG9QKJjc8GO9PwxNgCGF+H289BgAxg5c8+WqRfdKvEAMOiXRJUDL301AIYXofh7yFbaqMBTeVPFDBLnt6PQapW2gs9JnuIOyEt/F2ue/JOENe52YQHKvNB6WEpzol5uWvXzqB7GF3U85nCSq24c8GibGZns6rhpVWnV2b0cQRM25ypgZbYadA6zDhwfVu3zerjyz+vndfKzCwd5/dx2lyOO6PUzI8/bp53b5JK/ntcKNI9Vl13EzmwXd92ToyZ/vUq8HdAB93771X4P198amSvTrvqaAhp6xWhIJUGAg4AaARH+U1MkgmWWOZN+s0N4hhQwgXhNMhI4EgkcYmSslDj4bAhtqJXQ+mowMtaAkbENjIzmPjAyqJAHg5ERIcUpo6U8e6y0RJKRkSyVk0+S1rHTjqSLA162O4DBp60Vhr5b0ibK8su/JY0zT9Pn0X47+d1+wtvPyMJn/0+y6vfbpjmeVeyNypTN4xfIAUQdqQPjoxOLYyYZCRgJDA0JGABjaNynYd1LnwAGJ5NAxzlJpLI1Yt58GXnO2ZKef5KkjzhJgoLDcf7QT1tPAMaMS74BwMP4v+jrQBpsAGPE3LNkykVfAYAxSv2SUEnwlgyA4U0q/h1TJYxgHxUuJFVcsc1niqvWNNngOU6gqSnymVJHnXbetU/dh4n0LtlZvF/Lm49jJAHnEbFuo4UwsCvd9/3tnlPe33JHI79zzYPQVl5WprRF58mUy2+xAAw8AwQjnCg2FLDFVCFDyWYdYSXb9ZuD58QB8wahe6bKE1wCZGRIc5NkvfyITPnw3/CRAUYGIpg4CUZP+O3s+eElIFANRkZZbJYUnnGtVJx+GfBOPFQ0+QsKtVgZToXH4Ls7gOF0YXPiWHnvyu9L/ZzFAB7QXedEX7/5mwY2SwoYGAteQNjUphKPkmSqgKsnq0ctkDXX/UBaR47zOG92jASMBI5/CRgA4/i/R8O+h74AjPC4RImAv4u2lgZpqKqU+OxMiRs1Er4TzpT8mRfDFCEdfhjiXRNKXwBGAaKQTL/ka3DiGYafLb9/Coe9/L1d4GADGIcYGKOoJug/b/0wAIY3qfh3zGFcOL4lXKvHVMBgOkLQgol3gXkUyLABDh6nCUlY8z7Zua+YuyYdKwk4yryjs/ja97d/Tn3+lhvM/Lw29su5xkFoKz87S5rDsl0MDJqUEMjThGdCgQpS791ZFnxW7GeD4J9uo6PeTBoHocumyhNIAvoexvs5fPNqiYUvKTIyQpoAYGBMBne2y6h178qYqq09SoTmI2Rk1IORcQCMjIqM0dIaGiGNiLJxEIyDlvEzeyw/2Cd9ARhbAGB8AACjDgAGXwH+vgY06siy/0nuhg8l78D6w3xfMOrIljHzpHjqIqkBy6MrPnmwL9XUbyRgJDDAEjAAxgAL1FTnvwR8ARjx6TkSkzFSmqrLpPrAXukMbJOA0CDJn3aWjDn1c5KcP14ScvNtm3cR3wCGiULi710ZbADD+MDw9470Mz8VMgIV6lfCAu/or4KKV2BwiB0S1QIyHN2NChsnyczT2d4qhU8/IKFNew2A0c9bMGDFnFm8Azj42ve3Qac+f8sNZn5eG/vlXOMgtJWfnS3NoZky7erbMNQtUxCCe0zq2BPPAJ8VPid8FhTos/tl5bM66P5sDUI3TZVGAioB9ZEBVkYAHB0HtbVIwd/ukoUrlrikE4wxTFZGT4nMg7qAUCmLTJPViz8n5YvhI4Njmu97mOmC1tpT8QE/5wvA2GoDGGRg+JXwvAaAeZG09DlZ+CKYF40HDivOp3Z19hxZfs0d0jJ6iok6cpiEzAEjgaEhAQNgDI37NKx76QvASBs/SdKmz5aGknKp2bZdWhqqpLG5ShKScyUuc6zkL7xAxp19tQSHWZEwfAEYjEIy/ZLbJCgwFD/f1gR1WAt0AC7OABgDIMTjogqahpAab417y+8BpnD4r443bfaFA3JQGXNYGZaDzQBZDQZGeHOxMSE51vfTUeY5A2fytW+d7funU1/fSwx+ThsocF3jILRIE5Lm0CyLgQEfFxbjwgL7lF2BNmlOokCK9aHAhuM7xMk/CF0zVRoJHCYBZWTYQFoAmHLRK9+WpPWfYIyCkQGgefS6pTK6atth5dwP8FGnjwlGLSlOBiMjc4wyMhriU+Tg9FOkZdwM9+yDvj3QAAaZF+kf/1dyNnwkeSUbDmNeIM6WtOD6C0ctlKKrvi2t+RMVmHSZhQ36FZsGjASMBAZKAgbAGChJmnr6LQFfAAZ9XuSdd57U7yqTitWbpb50p9SU7Ua4x1Zta+oFX5R5N/5co5JwwukLwDBRSPy/NUcDwJiGKCR04tlTMiYkPUmnD+cw4VX9lN9kViBZkzVL+1WwAuCG4/OC5xTEAJChvjHwXfjMQ3DiuccAGH0Q96Bm8QVYOACEc97fTjjl/S03mPmPAoBBE5ImMjDgxFNZFAAr1DE0V6PtxOeBvy3K0FCTEcdPDJ15ws6eCXmMCYklCvN59CTgMDICEfo3sKlOxj36Mzl51XOuDoR0dfTKyKCPDGVkRGcoI4M+Mjieu5SRESYCtsdgpgEDMOijprVFkt99Tn1eZHthXvA6GsE+qQqOkY0TT5VtF98i7TljBvPyTN1GAkYCgygBA2AMonBN1X2TQH8BjFHzzpWCs66X5FETJQFOmIpXvyR7lr8gB4rWIPznBglLTJSo5GQxAEbf7oN7riMFMKg2d8JN1valz8qaJX+Qg7u3eoRRtXxg3KJOPE0UEnfJD+y2KmYu9gUJxJicEsiA0mp9YwPnybZwGBkKYCjYwfCkbWpCEt68HwCG8YExsHfHz9ocgMIBHHzt+1mtTS7wt9Tg5j8KAIbFwCCAcRuuxRKqOvG0nxeajihwoYAGOqR9siMP1u4AAEAASURBVBx9Mr8TfcQ48hzcoWBq9y4BvqcxQC0ADYs60auWStKmlfo+D2lrljFF78io6u3eC9tH6SODoUutqCWj4CNjlLSGREpDQqpUzDhNWgsGN2rJQAEYwbs2SxIYKTlF78nYvSsloaPB63XvjsqW9ZNOl1KEZ22cOFe64GfNJCMBI4GhKQEDYAzN+zaset0bgNGw56AcLNwidft3SHXpDulosxgY6WNmSPqEkyRn9hky8uSzZf/a/3oFMMYtvl5mDmAUkoB2oP1Q7AKg9AVwpRq2o12hWK0YRulIAQxIBfBFm+wAgLF2yZ8OAzAYhWSqHYUEBFhVrL2JzzAwvEml/8e4auyYlOg2mRnOSjI1NAAXFoBBhY4+MNoBYNwv4a0HjA+M/ot9YEpSgWY6UQAM62oH7TOPTjxDMmTyZV9WP0oBiFJFxoWaWwG00G9HQbSPW2AFGBoKZqBr+Dbsi0G7RaZiPyTgzsgIQvjV8X//ucxb86JdQ5eQkYGYUz3W6DAySmOyZQ18ZFSccqE11vFsdIWFg5EBfzADmAYKwIj++DWZ+tx9MrJsk0R3tQi8O3n0khFH2hDRrnDkfFmF8Kytoycb3xceEjI7RgJDTwIGwBh692zY9bg3AKO9vlMa91RKBVYXSjaukvaWRpVBEswP4tLGSP6pF8n4C6+XA4WvegUwnCgkQYhCEoh/R5QAWkTu2iWxe3ZJ/P7dElFbIbvmLJKqmfOOqNrjrfCRAhhkYJCDYTEwfi8VXhgY0y66VRkYnCHxn7dkAAxvUvHjmJvpCEs5q8X8PhQakqwM7rd7fHN1j2YkdOIZ0VZiAAw/xD4oWZ1HxAAYAyJeMjCaAGBMu/KrClZQrDQhUdYFvgPp/wJJAQoH0FD0yAI5CPyRhWGxnHhznBukxcyHkcBRlYAHIwPmFNFrlkriljXKyAjFnGlM0dtgZOzosU8OI4NRS/anjJLKtJHSFhSijIyy2WfBR8bARi0ZKAAj9oNXZO4z90hu1XYJxbwDnEKP6yyOzJBN4xfK/ikLpHbSfOkEw4SmMpY5pUdWs2MkYCQwRCRgAIwhcqOGczd7AzBCQuKlozFIij96Q3YvexOmCLXUxCQmOlnCIxJl9OIrZMrVX5WyTW/BjOQVmJCs9TAhKVg8cFFIAtraJHnZxzJyzXIZvXOFpCK85OtzrpTNF111fN0i2Gt3hkdIVz9tWH0BGOljJsqYMy6SDFAw4zLGSSjC3PaUdrz3ghRiZeTgvh0eJiQmCklPUhugcwQlUJVLrcKETSe5PIrtToxlOimk/wvrODKzDFkYtClGQYeBEdFedsQAxuyJE6Qgb6RU1tTI6k2b5UB5Ra8XSj8FE0ePksTYWCk5eFAKt2ztUzn3ihNiY1y7VbV1ru0ht+HcSGdu7mvf3wtz6utjucjwcLnvzu9KWmKSPPLv5+W5N9/uY0k/sjnX5kcRf7M6TjwnX3qTPg8aeQTmIw6jIhDvTodxQTYGtwlqWMcIYhzatkCMIwTH/b0Ak99IwIcELEAabDsw6OgjI6imQiY89kuZveYVLUEFP6SrvVdGBo0OmwAHlESmysqzb5CKUy/R8l0c++GRIiGIXHIEKXLNezL/8V/IuPINaOnQi6jPUUjw2xXQ3CBJ770kC16+T7Ib9nv0hsyLVjAvivJOkjVXf0+ZFx4ZzI6RgJHAkJWAATCG7K0bPh3vDcCIThop4eHpsvOt52XHG/+WxroaaWtqklDYaoYhpnna9FMl97RLpLVmhzSXbZCS9UUeAMZARiHpDmCMaN4p26NGy/7M0cfVDalKTpc9806VxtEF/eqXLwAjPiNbksdNltjsfIlKSHNFgPHVSMXWQtm3/F2pPVgmnVgVCo2Pl6ikJDEAhi+JDeBxBTCwpobVYwe44MQWGpgqbM4xIhXqzBNN87zjC4PgBbfXPfvnIzIh+dZnr5WzT54vwfaKtnOFO/buk+/cc6/4AhV+8pWbZOGMaR6rZO0AVl5651350xNPOdX0+H3q7Jnyo5tvdOV57OX/yKPPObRq1+GhseEo9c4839e+v1fj1NfHcryX3/vCZzU37+ENP/l5H0v6kc25Nj+K+JtVGRjB6TL1ylttcAImJFDMmDR0qq1QBWIFGk+GPjPq5BbPCMEOfU5swOOQrwwtbj6MBI65BPT9znc9x2hzk0SvfVfi8XtMMC4MTj/HgpGRX7Ozx36itCAwqUYt2Zc0RipTc6ULUU/qEtLlAHyQHSkjI6LoQ5n2r/+TguI1Eo6WABlqf/oKYIRuK5LMD1+W3I0fSm7p5sOijhRHpMmmggWyf+pCqZu2CMyLlB6v15w0EjASGDoSMADG0LlXw7anvQEYySOnSXzqRNnx5rOy+T//lPqyA1J/sMK1apw0epokjJsnocHNEhpQK+VbsLq7efOgOPHsDmDkNW89Lu/LzvA8efOim6Rs4Rn96p8vACMkLg7ARYKExsRIaGQUJvrWhN9XI81YNa8u3i+tdWDNIKljVRvAMFFIfEltAI8TkFCqO1aHMTd0V7ScyCMWqOH4vqDtMNkZrXi+wMLAZFdNSOgDA/fR33TXLQQhpruKtYH1EYIVQSeVV1bJzT/75WEgxq++fqvMnTzJyXbYN1f9ewMxyBR47Fc/lQSwN5z05H/+Jw8/e8hTv3N8SHw7Sr01x+dtslL3fX8vxinfx3IZKcny17t+KCHBwfL820vl/ief7mNJP7I51+ZHEX+zOgAGfWBQqSPQx2913gnfRhqBxF5hpjIYhG19fih4MJg02TR0nmdZk4wEjkcJdGdkBFeVgpHxK5lR9Kp2F3wKCe1s65WRwVcFAY1SMDI+OvtLUr7oM9blkskXEe03IyNs0yoZ88JfZPT2jyWptVYi4DeLqVcAg8yLxnpJ/vAVOeml+2REQ7HVD7dP9nVtxgz55KrvS/P4WcbnhZtszKaRwHCQgAEwhsNdHOLX0BuAkZo/R5Izpkv1nm1SvnW17P7gv7J7+dvS1mh5mo5ISpfI+FRMMLskKKhdmiorpaakTMJTUgY8CsmJDmAEhoVJcGQkmKOhqsC4JvI+xmB7S4s0wWSgo9VyvOoAGCYKiQ+BDfBhKlxMSosHmEFFyzmm35jlOcoXAQ2uKiODlukE24EgRuGSBySyo9xvE5IJo/Llvju+q3WVVByUnzzwZ9mye4+kAAD72a03y9iRuXruz0v+LU+9+ppu84Osi7tuuVn3K6qr5eFnnpMPVq+Vc7Dy/8WLL5KI8DDpQN8+/8O7ZF9pmatc940ffOmLcvrc2R6HDYDhIQ5rx7rdXk74PkRwiKmxudl3piM5c1QAjCxpDEqVKXTiSeeEdpv0gxEAhoVlH09TEdvnBeRE5oU+H8QwSKPH86T7CoAchU4fiUxN2RNaAh5jtakBjIz3JX7HOgWpwxtqpKDwLcmr3dWjjPiqoJ+MevgT25s8RqpScjR/XWK67D3509IEHxmE8fr6JARWlkrk5tWSCX8dk9a+JpnN1vu8NwAjdHuRZL3/suRs8s68oDNSQByyLmeOrAKA0VIAEN0GG3u8QHPSSMBIYMhIwAAYQ+ZWDd+O9gZgpI86SdKy5gFBp2/pNilacp8UPfOgNNZWw5SkUZUuXRkLD5bAcORphTPCxjaJSE1VAGMgo5Cc6ACGtUJpUa0DMbHvDcCwzBCoGHOif4iBYaKQDP7zrMAEJm2WJwy014kbwH0yK5AccxILyHDADeQB6GGBGZYfjMKnHwSA4b8PjM9d+Gm5/oJPaVtfANiwa/8B3eYH/VIs+d1vQPAIlOXrNsj3fv9H17m7v3WbzJwwXvd/9cij8vpHy1znvnjxhXLNp87V/RdhSvKHx55wnXPfmDdlsvzytlv0EOufPr5ATVh6AzAmwd/GyMwMqYeJ2vptO6S8qsq9Wt2m8p6dlip1jY3qj4PXMgv+PZjWbdvu4aPDqa+usQn+O7YcxjRhmbG5OULTmB37rFVEAjgxUVG6v2nnLmbRNDIrQ8YgbyCYAuvRzr4yG7yhVsHUTWtgv2aMH6enVm3c5LVtnkyJT9D7wWulOQ8BphkTxoG50ym7cc8IOnVPvH7Kofs5XksrWDbu99opy3rZJ6cd57jX64/G9e8tlk1wmOwk3hfX9W/H9fcAXjllevrOyzoEYCgDgyAEAAqOSSshwDNYJtY77pDDP+bls6NgBjPagF9v70K7UvNlJHDMJXAYI6OiWCb+89cybf0b2rdDjAxCAb2n/eEpsvScm6R84YVCTqbO1SJjpAuspb6Uj1r1jpz0+C+l4OBGbWx7fL58fOFXpWbaqbpPX15dZHgQPoE5TOJHr8jJL/xRcur36fnuH03gktQER8B8ZKFsvuSr0pbvm83XvazZNxIwEhgaEjAAxtC4T8O6l30FMIKCw/Dz1Sllm1dK2bplsn/1B1K6brk0N9ZIU1O1SBB+KmGfiZm3dLV1ugCMgYxCcqIDGKGxcRKdEA/nnVHKwgiybcZ9DdAm+CuprayQ5vp63JMOCYOyRB8YZGA4UUigJvgqDiekuJ3469MsyGctJ/YJB7DAkjF0rAANQ6x0dyhePEemBSeGDK/Kb4IXXF3uaLVW12lCEtlRAROSw2m6PUn2u5+/Hor9eKmtbxBvfhKe/f1v1bxjKxTkm376S1dVT939K0lJTIAC692/wr//cLfEw4Rp7eYt8o3f3uMq577xxG9+IenJScoQ+PwP7pLHf/PzHgGMz15wvlx93tke5i2sj0ryHffe56Esf/3aq+SC006R6ro6WbF+o5w5b46raTJDnv7fGwBdPpYfwvcGzRTcU3fQ5ZRZM+XHX75Rs9As5tOnLvLwFbJyw0b55cN/k+987nqZN3Wye1WyYcdOufWXv+Ets5KtKdBk5wYAPdlpaXq/nUJ7S0rkb8+/JEtXrHQO6fcjP/mh0FkqgZ7oyAgZn5/ncZ6OU+968C8eAMibjzyodf8N/kT+Cb8iTJeddaZ8+YpLdfv8W75+GDvjbz/7EcChTLS/Sutjxj5d/yP29QOUck+u63c/6Mc2w6g2BqbIpIu/pECF5cQT6heei0CExeazYfm/wOuH4AZXcO33HVkZfG70OF9OOGeSkcBQkoA7IyMQ5hhRhe9L3E4ACJhDRdRVadSS3Jpd+utMp589pQYwMnbSR0bKCM1fm5wFRsYF0gzmA3/de3s6ugMYFcGxsm0EWLeoj6k2I08OzjlTAgCuJ65eKiMK35WxcKKe1G6Zpmomt4+9URmybupiKZ2yUBrHz5bOuCS3s2bTSMBIYDhIwAAYw+EuHuk1YCIWiFUzKv7uKYATNFL/AQx0BSM4FWzXu2i/7lqhcs/d/+2+AhjBwRZt2Wlpw/N/l00v/h0K8m6pPrgf807Q392Sw8BgFJJpl3wNv8thWB2gNtz/RAAjcdVKyVq3RnJ3rpER1cenD4x9sSPl/Qs+J5XzFvbrYn35wIhNTZP4vFESk54uUTGxCAsf2mP9lft2SsmmdVJbXSldTa0SGmsBGMaJZ49iG5CTXGWzaPDQyQjqOeYkmE6qXwzMKrvIjMG3ghjIr34xlKVhsTB4jgBGFExIHIbAQHSOfhQe++VPdbX7rWXL5ed/+aur2jcefkCPP/vGW179K/zya7eoMk/zksu/9X1XOWfjlqsul0vOPF137//XEnn29Tfltb/c7xPA+PxFn5brPm0xRZw63L9r6urlM1//tuvQN66/Rj59Ss/PVXdfH67C2Pg9WCN0RMrU3cmoHuz20VNdb30C2f3Zlh3u1dSCsfKbr38VDo7xnvaSyI743h/+pOCPc/qvd/3oMKDFOed804Tnh/c96OzKW399SLcfff5FeeylvgEYf//5TyQnI13eXblazYlYQZ+uH4BaCE08vCS9frex4yWLz0MKYASkyJTL4QOD4J7tZJaghW4TyKAc+Rw5QC2BCuwTsLAADbIxuIvjJhkJDFEJKCMDTpvxQ6EspJDSPVLw1D0yaf2bEoQBzogl4eojo2+MDDrPfPfcm6XipPMsAIPzx0gwKEI953COuLoDGK0oVR8QJq2B1nO/O3uKrLv0ayIwcZz44kOSt3etxLfVC4wJnSr0GzFXpAVl1ufNk7VXfFtaxk7zOG92jASMBIaPBAyAMXzuZb+vJAQ+I6J37pSw2hqPOiKqKiRp/w5px49OZVa+1OSMlPq8fOmIJpVv4FJ/AYzdyxBW9cNXpWTjx1KydaWuLLv3ygEwrCgkXweAgYmp/py65/JvOwCKYEhpqUSUl0lkRYlQRsdjaoxPlMoJU6Q1I6tf3fMFYCTnjpIRcxZJ8pgpEpOcLaFK6/TdxN7V78rmt56VytJiABiIHBNropD4ltYAn8HEk0AFV5Zd/i44QcU/ri7rpBV5qHzR94UL5EAe9X+BYzQjKFryoER1koHhvxNPX1d0z3e/KdOgbDP99KGH5Z3lFiuA5gQP/egOPf7AU0vkmdfe1G33D4cBwf6fccOX3U+pOcZ9d35PwYoN23eAofBbPd8TgOEwOujP4acPPSKfFK3Ter7wmQtlzuSJWv7OPz4gH60t1G13AONgdY38GL492BYZCDdddrHLBIGAD/1+kMVx5blnyZcuvVjLuwM27go8zUh+9//+Kf/74CNlrpDBEQN/M0w0V/n5nx+R5es3yCyY1zBCC004aI5xxbdv1zwEm16493dqfsID/+/Fl+XFd5bquQtOPUXIMmGqa2iQC2/7lm7zwx3AYJ/J+KDPEvocoQlQrP2+d2dVDAaA4XH9k8bLD2/ycf1g9fzky27X/x37+l1X1LcNNSEBA2PK5V/RAgQpyKhQsxE+JTSR4zfBCjIw9DwJ8tZqtIIaGIMuoNCAGH0TvMl1XErAg5HRUCeR6z6SmD2bYbIWIJEIwzoWPjJyanfrDKo3RkZdYLjsTh4tVYlZmr82dYTsW3ChT0ChO4BBmKQNJfHrpbLanj5RVlz5XSy0tcisJb+X/PKNEgZQhaYu7onAyYbxp0gJo45Mni+diWnup822kYCRwDCSgAEwhtHN9HUpZA0E4QdJ2RReMkWCGp5VtFZiK8s9ziYe3Cf5VRvg6CxKtmVNl52T5kjJ7LnShtXTgUz9BTAqdm2UCjih2rH0WdkBm0j6w3D/PXMAjPGLr5eZl3xTJ6AD2e/hXJcvACOjYLIUnH25ZCF0bXzWeEQiSehRDFveflpW/utuKYcD1k7cn1BEhHDCqDIKSVx6HlUEn3UYExKfounTCQIVVLR0cup6OGwAg6wM/nU5q1gAMtoZfcQyLeE5DaP674clsr10wACM2665Ui48/VTt/+pNm+Vbd//edS1nwOnmnXC+yfTj+x+S91atcZ1zNq4+7xy54ZKLdPdmmJ64+2G4H+AFTSDasJp4012/cPli6AnAmDN5EoCCCNlbUupR17i8kfLADyyGx+Ov/Ff++u8XtE13AOP7f7hPAQ+nbw/98HZxnJNe8/0fePjD+H+/+ImMAHPJ3TTGHcCgI1M6NHWSO5NEWRtLLdYGb+N3PnednLvwZM16+hdv1u9JY0bJH7//Hd1+ael7YHo87lSl39+47hBz5Gu/vlv9dfCEA2AQQLjiO9+Xqpo6VzmCHp+90AI+CAYRqGEaDADD4/rxSrjlykNMGo/rR/t6/Qvs67/Bun7tmB8fBDAaApJk8qU3A+QLAnBBNpnFtvBgZID9wedIo5DgmQCiYf+W4EYQAOTzRWAQLA6TjASGgwS6MzJC922Tqf/4uczY/q6CBt2Bg96ueU90trx7+fekepH13u6evzuA0f385qTx8uE1d0hQc6MseOrXPsO/FqVNkY+vul2aJs41UUe6C9HsGwkMMwkYAGOY3VBvlxOxa7vkvv+GJO/f7e20hONHIaGuUsLaLZt3J1NER6PEdVRKZXDycQlgNNdWSmN1hax76UFZ/79/SAv9LMCBp5MMgOFIwv/vgQIwtr6zRFY99X9SsXebtOP+hNgAholC4v896W8JF/tCwQwqWRbjwqmvi+wL7HSBqk/lrBOAJzElB8AoWvIQGBgDY0Jy1blny42XfkabrgB7gL4v6DjSSe4K/V0PPnyYvwbmu/b88+QLn7lAi9zw45+5TFsuOG2RfP3aq/X40/97XR56+lnd5kdPAIaTiYyGiaPywTiIkigAGtERkS6gxL0+B8CgacfZN3/VKa7fTuQTb+YtjukLmSxf/NFPNb/79X73nnvVp4ZTIf1hfOM663quv+NHHk473eXoMCMuOxs+KC63fFDc8JNDcnHqo58L+rtgevDpZ2TJa5bDPgfA2LHP9jnitqg5f+oU+cXXLIaCO1gzGACGx/Vj/H36FLfrvxPX7+a0U6//EmscnX/r4f42nGvu6dvygZEqky/5ktLmybTQEKoAJYIAWijDAv1QMAPnKBYFKXCMSY/bLA0DYFgyMZ/DRwIKeuNZoMlw6J4tMuPvP5UZO98DJwKgnZ+XedQAjPSpsuyaOxXA4G+ZMe3y80aZ7EYCQ0gCBsAYQjfLZ1dhx05qXWBriwS0wAFfBxQQt5RctFoW/O+fMrppm9vRvm+WBGcclwCGcwWr//07KXz+fqmHTXxrFZw68UcXyQEwBjIKidPmcP8eKABj29JnZPWSe6Ri9xZpJYAB54tkYJgoJIM/griKxsRJHFeQsYU/sissXzdkV1BJo98LBTnUiSfNjOH3BmWdkKrqA0Mq/Q6jyrbd03kLT5ZvffZa7Q9NIm771d0uhoSTT5Xsuywlu3t4VSfPN6+/Rs6HDwqat5x5o6VcE3x4Ao46ae5QevCgXPXdO53s+t0TgEF/HHfc8Hkh4yLI9oPgURg73gCM7r4xWMYBMPaXl8u137euw6nrZ7d+WU6ePlWZLN4AjO6RWtwBjEu+8R2pgtNQTbitl551hnzlist01wEwbr/x87J43lzpwL1e/CUrAovTtvP9xl8s/yKvf7xMGOGFyQEwPlyzVn5APxfWsNFzU8aOlj98z/L/MdgAhsf1Y6i6AxiXfBPX7wZ0XbrY7fr7C2DAwWpjUBqceN6I5wCmI4xAQrYF2BRWhBEAGvZ4YHQSJu4zrzrwxHOFHfv5AgMDx00yEhiOEgjGAsSEp34nkza9o5cXhGckoqNFQrv5oPB17cWRGfLhOV+SijlnaRZGJ+mKiZOu0Ajd742BsT1+lCw7/8sShDnu/P88eFj0Eb6yCKsUZc6Q5dfeIc0T5vjqijluJGAkMEwkYACMYXAjg+G7InrHDknYvV0SindIdL1n6L/Y2oOSU71d4ru8e2zuTQTHO4Cxc9ULsmP5C1IGoKYMUQF0BRkX5QAYh6KQhGL1gDbMJvUmgcEGMKwoJF+V+PR8TDt8T/yNCUlvd6rn8wpcuJQsy+8FS9Bsp1OBToAbmIx2wXyAieAFV5ZZrqON5iTwgfHMnyW6q8LFdNCMfn4snDkdPg1uUN8UzS0tcvu993s4knSv7nU43CSQ8Pxb78gfH/+X+ynd/jWcVNI3hTtQ4R5edc+BEvUV4V7Qceq5a/9+WbNpi2zcsVPDsxL4ePLuX7p8TbAM/T9U1dZKPUKfzp40QasZCgDGt2FWQpBIfYPc6OkbxJHFmw9b0UP+894H8n//7zE97AAYHwDAUEedgwRgOCY0vpx4HhMAIxAMjEtvsgEMgBf4R1MRfQawHQiAjyCFA2RQYBbzgs47bbWJzwueIWVsOII230YCw0gCAXXVErFxuUTBlIQzqOjKEhm/5nXJrd/bp6ukT4x9iaOkOjFD81enjZR9MCdpGTNV93sDMKphxrwvZbQE4vcou2KbxHY2ebRLfxnNAcGyIXeuFF71HWlB9BOTjASMBIa3BAyAMQTur/qwqK+VQDiZ85bCsdpHHxYjthVKXsV6Seo86C1bv48NNoCx7qW/yOqn75FGmIS4p/Rp0yX3rLMlNXeOpGbMgvLj3YP1wfIiOViyRna9+arsffdtacfKMlN4XLyEJSXKmNMul8nn3zQgUUjc+zectwcbwDBRSI7e6FFFiyAFJn/WKjF9XYCphRVkshgIVlh0YfrDgAoHMENBDRzn+XXP/hk+MMr67QNj+vgC+dVtt2pkDEbB+NH9f/bwG9FdEk4I1H3wSXH9nT/2OE3A4V8Is8pwn6s2bJJv/+4Pev5r8Ktx0emneuTtaYfhSb/zu3vF3efGe6tWq9nJgXLLMW9CbIw8c89vVU5DAcC48PRT5LZrrtLLvvVXh/xVOHKYMCpf7rv9u7r7h38+AQef7+r2YAAYX//N/0nhFk/G38v3/0Edjx4vAAbZPvWSDAADJiQAKghWkGnB54JRSByggtuM1qMRSSgxAhfIYznHxbovmRj2MRWo+TASGGYSIFRH3h6/udwQtnuzTESUkvFb3sceAtXBj1J4JxgZ+KaxFbl+PaW9UVnyMRgZB2eeptni1i+Tua88KPnVO3oq5vNcXUCoVIQnybZxC2XHBTdIW954n3nNCSMBI4HhIQEDYAyB+xi2f5/kvPu6ZO7a5LW3oaDVxdVXSlxzpcR21EiEeAc6vBbuw8HBBjD2FL0u2z9+Vlobazx6EzdmjCRNmSrx8QWSGFWAaK7eQ+k1NR2U5sZyKS56Vw6se8+ivqOmoPAICYoIlxGTzpS86Z+GN21MSntY7fdo/ATfMQDG0B8ABCGEZiGw03dYFbwqBSsIXtDnhR6wPhTIIIBB9gXeKVx5Zp6iZ+gD4yAAjGIrvx+fNMu4+1u3SVREhNBnxE8RSYMhOXtKjq8I5iEDg0wMJ7kDFc8gPOoDCJPKRPDi3AUn6ba3jzGIbsLESCPF8KVA56H0keHO3GA4UobldNLVn4Kz0Istp3NDAcAYOxIRXH54h3ZfARr41HBPd3/zNpmJCCZMN//skPPTgQIw3P15PPbyf+TR5150Nb9wxjS56xbL2ebxAmDkwYSkISBZplz2ZQUjaOtPUEJNrvDsBIWEqcKmgAYBC5iX6AFeFc1F+HzZ4IVhX7hutdkYphLAaNdEcCKwtkoiNq2QSESpIyMjqrJU8te9K5lVOyUC0UFCFO6w8nv7rA+E0+TEfKmNt6KERNdVSFYFWMIdDd6y93psT1S2FM76lJQh+kjT6MnSFZvYaxmTwUjASGBoS2DQAYwO+GZob2nCbz2xW7z4gu1JQlAoaMKwK+UEoIfEyTRtsTvgVb4TlGYncTIRhBjtAYGwW8UKyVBKDMUZABp1AONuu6WATtikN8OXhaNY2OfidmyB3d/jMrG2yC13/zeJj2M9SUNUwbf6YXg5j7Uh5KgTwqoiKlP25EyS4vFTpQKsiPaEgf1xqK7cIhWla6S9FVFE3FJYcoqEpaVLVHCaxEgGetzzfXaYGAwd6Z6S4Jk6OW26Tkzdj5tt3xI4GgDGtIu+iigkNCHx/Q4wJiS+71FfzjhgBagU9ruWvi0YWtXyfUGQQt+xABcU8MC96MJ7iOWUhYGV58IlD8CE5KDfJiTZaanyx9u/I/Hwe8K0dMUq+bjw8HdYS2urK4wq85Gx8btvf4ObUgu/Kf9CdI6P1xbJGfPmyOUIVRqC9z6jjNDHBMOI9iW98bDl++HJ//xPHn72OVcRd8V6085d8thL/9EwowugcDMsqq6uI/dQADD4GP3jFz8Vyp1p6cpVLrkSXDhl5gw9TmeY19/5I93mx0ABGCkJCfLU//1K66V/kJfffU+WFa6TUSOy1elqTFSUnjueAAz6wKATTyYyLRiJhG8jMpWUjaFABectDEVszWH0HE1LAGA4wAWfF+MDQ8VoPk4ACfBJIKDBbzIyQvdslbyXHpG8jR9IdFujRLQ3SURns4SAkYFfmB5+4VH4CBLi60lDUJhsyp8n6y79ujQb05EjkKYpaiQwtCQw6ABG9b7tcnD7eumAws4UBlpuZFKCRMIWLiI+01r56EFmzfXV0sRoE+Ul0lC2H29MaxIRkZQmUamZEhGXKBGxCajBtxLUQ/XH5FQgHNhFb98u0aUlHu2HNNZJUvF2ia0u8zgeieMjqnZIUlffJusehb3stAEfrw+MkfqQWKkLi5fWYE/TjFqg12XZI6XZVjxaI2OkOTFVmgEotMLhXWeYZ34vTfh1qKWlSpoaKqBjeTofJYMiMDJSQgOj4CwqCne453tsMTEqMEQ8AYzwyBSJjGTo157L+9XpYZ55sAEMJwpJXPooTHCgLPi4NwbAOPKBRsWLingnlH4qWQQmKG/L3wUBC8u8RM1HAP45zA2e5zllYADA2LnPPwaG42yzL1fgOKJ08t4Op5qL5891dg/7fvT5FxVsOOyEjwO+AAxmp5lIYlzsYSVpPhNIBRZpqAAYdEj6p+9/1+v18Doqa2rlq7/+rUd414ECMFj/7779dQBQ47jpMx1XAEZgioZRxYOhIIVjNqLmIVS88NwQpFDQAnkskILPDKjyXDjBdESfKQNg+Lzf5sTwlEB3RkbY1kKJLNktQQDAY8r3SX7hUsms2dUnRkZ/JVQcmS7rp5wlB6YtlIaCmdKZkNLfqkw5IwEjgSEmgUEHMIpXvys73vuPtDbUqmhi0lMlPneEJGJFPyF3KlgUPSvDtWX7pKZ0txzcUiRV29dh9dB6bSbkjZPEsZMlLmOkJKSPxCziOFJOscIZXFstwQ3e6XAhcLqZvq5QUvbt9hguUQ3Vklu+QVI7Sj2O93eHLIs2qP4tML1ow5/DqGgJiZQaKPQ1calSnZAhLeHRHk3UZGRK+VQwLZKTPI6bnRNHAntXvCErn/yNlO/wXDFPyh0ro0/7jGRMOhnP3lgJjYz3KRQ6xNv50YtS9ML9Ur13u7Q3N6lZT2hiooyYebpMPO+LEpeaj1FK0x7vz68BMHyKt08nFKzAu9FiVFgKmbNaTEBDwQrUpH4vcL/0mz4wyGIi6IFtRiHpjxPPr1x5mTBaRG/JWzhSlqG5yKcWLZAQrH47iSYgT7zyqjzxn1edQ336fuX+eyUiPEz+/sJL8o8XX/EoQ+bAj798o4wdmQtrG5gKALQ4WF0jj/z7ebn5skskLiZay7As082XXyKXn73Yw4moU6HjSHMHwB6GeHVPd37pC/C5MUcdiN7yi9/oqXlTJssvb7tFfY1cd8ePPIAFAjgEctpxDy762reksaXZqg5K8/mQyzcR0aUNbL2zb7JDudqP0FiYy1x13jkyefRooR8PJkbwKNq2TZ6E3Lbs3mPVY3/ef8f3ZHx+nry57BP5xcN/s5ZV7XOs66EfWWYpX0PUmHXbtuuZ/z74RwkLDZUHnloiz7z2pkd9NAGaNg7vBjBlKEv6PXnuzbdlBoANmvK8/tGhCCg+rx/Xwogqruu/Ddfv5gNKr/96+/q7hbL16EwPO3lZWTAhSZIpl39F3z8BGGcEMDju2W+yMZx5hQVg4C2l5+1KCWhgU5kZKOOwdXpo0pwyEhiWElDwm4xiguNgRzPs6siX/ip5Wz6WKDAyItvBykDUkhCYlwwEI6MVc4ZG+ETblDdHisC8aBlnscuGpXDNRRkJGAl4lcCgAxibX3tcip7/i7TYDhrjRoyQ1IICSZ+4SDKBnAaHWbRSr73DwYo9G6R813rZv+IdKV37gU6wmTdl0hzJnHWqpORPkdS8KcfV5CG4pkpGvPWqjNy40utlBeNFH9dQJZEt9R7nQzpbJQY+LCLF08OyRyY/duolWkoiR0hJSq6UZeVKQzyZKqD+wct6O9gN7RFR0oa/Tk7U3FJ7VLTFtAjvGVxyK2I2h5kEKvYUyvZlz0pNiacjvujkkZI2ZoEkZBVITGK6hIRFer1yGCrov7Idn8juVa9I/cH90kUGAMzGguGXJCV/luROPU+iYtOgBFir3N4qMgCGN6n07ZgDVDiMCnVGiAmm5aTT8nXBmpSdgRVkmpmoXwwCF1CMFfQAkFEIHxjRnUcWhaRvPT48FxXwgpEj9QSd827YvtNDkT28xJEdIZgRHBzkASQcWY0DWNoGKLjqr8nXvluTlF8wnrkeTW181edWT3822TZ9n9Bkxe/kXJvfBftewPGBMfWKW/AQALCwTVHV1wWqUfMQBSascwpU4DjzqU8ZKms24NH3Vk1OI4HhJwF9Nmx2NB4K9ZERtm2tRJQWKyMjtmyPjCp8RzJqdg8II6M4IlXWTz5TDkxdJPXjZ0lXouVLY/hJ1lyRkYCRgC8JDDqAsfaZe2Xl43dLTfkB7UMaVoUyJk3CCuy5kjvvMgmJiPPVNz1esm2VHNgCJei9V2TPsjct53I4kzPnNMlZ9CnJHDdPMgvmWhOJHmsahJNYGVOmRZ3FLnFaiCgvldkvPS6zyj92Dg3Kd5sES31AlLQGhnmtvyYiSfZkT5bigilSDmeYrenpXvOZg0YC3SXQ2HBAKkrWwrTHU/kICUuWyKjREhkNs5xoKEduq+PudRDA6AD/p75mj1SVrpfWZs9nJCZ+pCSnT5fQUGuF2L2s+7YBMNyl4d+2o3BhiRjvR4ASZFVwG6vLzjmyMHiMfi/IbkM2PadABo4zlOq65x6RyA5EIfHThMS/3prcvUrAUep9AQ7O+V4r6pbBV33dsh3V3f5eix+dZBQSOvGcfOmXlWnB54K+LixnnQihCuCHIJ4L0ODDgTzubDHXs4RzhoHhh/BN1mEtAf6+cMHCxcjYtVFGvvyo5G39RBkZ6iOjgz4y6MUCj58Lle1ZLHxV4RdMNqZMkI+v+K40wGmnxZw6Ci+MnrtmzhoJGAkcZQkYAOMIBB5UVyMj3vyvjFr3iUctoW3NklW5U1KxajmYqSwoRVaPWySl+d5tjtuxOt6UmCIticnSCnOQzgjvq+WD2UdT99CUQBtony1NFYgc4ckGCgqKkGD4TgkOwTeooo6PgO5XiemL/mtrrZOWxkrp6DjkgJd5Q8NjJDw8BY58vUeWceozAIYjif5/W6tj1mqxA2JYx6zpoE42ycBA6gLLwTItsc45JiRRXRUGwOj/LRiYks4c3Rfg4Jz3tzVf9flbz0Dm7++1+NEHMjAag1LVB4ZGH6GvCwARFAdZFjxGZoaCfsq6AEhBUAMLF9ChFAhUJgaenQBE+tGDfrRvshoJDFcJHPp9wRXimWLUkrBtRRJeXizB8FEWA18Zeevfk/Sq3RINICMUyx19SQzS2oq/TZlTZSUAjKbJJ2n9Bjzsi/RMHiOB4SUBA2DwfgItDoR9baBjY9z9HuN8UFOjBDK0oFsKrToIpsUTMqf0A7ej/m/SVwUiaOMVTiz6UOpA2M9WxLdux7e3tC8pX1acf7VUzZzn7bQ5ZiQw5CVgAIz+30IqWpbZiK2hEqSAoqWmJZgE0rTEMRNxgRg4xkQQg2AHz6979s8S0VbarzCq/e+9KXmYBByl3hfg4Jw/rGAvB3zV10uxQT3d32vxo1PKwKATz0tu1uck0HHWSVYZZKJ+L8jKwHGKSJUkMi3IwgCwoY48sY0zyloySpQfwjdZTygJ6O8LGRlI/E0K3b1ZRrz+hORsXSGxjdUSiYWOSEQuCYWPDCsunvNS8hRTI3y5VcDx/Pax82Tbp78kLWOmeWYwe0YCRgInjAQMgMFbjYl+9LatEr97l9cbHwgnZMl7t0gCPCu7pxCEiM2q3i1poFcfSWqScCkJHyFV0Vb4O6euZvinOJCdJ/Upnsdd52PjpXZ0gbSlGPs/Rybme3hJwAAY/b+frlUwKF0EaaFnIUHZAsuCSQEMXU226O8dCGdK5ayz45D/C4a/LlzyICaXBDAQBcqkYycBR6l35va+9v3toa/6/K1nIPM71zaQdXary90HBs1ECNYFhcIcE8+Ky/+FzcrQouiT+rzAjhWBxDbHwjPC58YkIwEjAe8ScP0W8bTNyAiFWUnYwQMSjGhXccU7wMh4VzLAyIhSRoYFpHevbV9khqydfb6UTjtFmvMnSWeccTTfXUZm30jgRJGAATBwpwMAUKQvfUtGrfE0BXEGQTCAitzSDZLVttc55Nc3o3/UBsRIU5B3E47GkGjZmz1JKtNyPeptjI+X8klTpBmOT00yEjgRJWAAjCO461DEqJtSF9Qt7kNJ42SSq8VUxjptMEO/ccwxIeEkk9vMrwCG8YFxBDdigIo6Sr0vwME5729zvurzt56BzN/fa/GjD/nZ2dIABoY68cQzEQjn1k5y+b2w2RZ8HpR5QQAQ//jscN95liw2hnempFOnr2/W0VhZIs2V5VYW1M0w8ZF+hoTkM9xYUQqH6Qd9NeXX8aDQcO1HWHTPfsr8qtQtc2tjgzThutubvEdrc8vquYkV/KDgMDiAD5eQyCgJwncg9n2ZM3oWHvi99tYmXAP+mhulvaXJAoBttoF7awFwls5xFQwH6iH4CwqDGSZkfCIm/R0ieI5EMDBs5wYZ8caTkr19tcTCwX0oQAz98dIMmk33d+VMlfWfuVWaJ8yyD5ovIwEjgRNVAgbAwJ2n6UjBkn/I3OX/9ToOAmHcEd1ei5gefv7Q2rXVALxYOWqR7Jno/aXbgYlTS0KytEV5OjTsDAuT1qQk6YjqOVKL106bg0YCw0ACBsDo/010lCuuKFs6KhQv/KNpCAEMdeBJdQznNY8NbgC1UOCCLXdgdaxoyUPKwGBoUJOOoQQcpd4X4OCc97eLvurzt56BzN/fa/GjD5YPjDSZctmXVYlSgMIxIwGIoE487UgjyrhA3Wo6gmOOg0/VqghuIH9/Uwci/mx++R+ya+kLWgVZIKMWXy6jz7zMryqbEelt8yuPSfEnb/pVzlfmqNQsGX32lZI14xRfWY7oeMXmNbLl1Seketcmv+oJjoiRsLgUic2GI+hRE/CdJ1HJWQAGvC8Q+VV5PzJX79suVTs3SQ1Ch9YiXHhrfY20NsJpNd+rbik0NknC41MlLjtf4keOktjMPInLGuWW48TZdH5z9Irx/ARi7IZiHIQB0ArG8xAIAEhDeSODsqHwzf0WAHsNY6ebqCMnzlAxV2ok4FMCBsCAaALh32LGX/8gZ218zqeg+nLCYVo0Bkd7ZK+KTJKPMREoO2Wxx3GzYyRgJNCzBAyA0bN8ejvLlS4qWZbixQm1Nam2mBiWY09ODDmhVBaGPefuBHDhTDLJwIhoPWBMSHoT9mCfd5R6Ry/yte9vP3zV5289A5nfubaBrLNbXQQwmkMz1YmnYwJCJ50OcEHfF9CedGXfggChaGG1WBOULpy0Nvmp+7rr90dHa7N8eO/3ZPW/7tOyoVGxMvO6b8jsG3/kV10NZcXy0X23y4ZXHvernK/MySPHyqwv3iFjz7vOV5YjOr7nw1dl+V/ukuIi78xXX5WHxSZKZHyKxOeMltRxUyR1/HRJLpghEQnpEgjwZ7CZGI7voJa6KmmuLpfyzWv1r2r7eqnZtRmRu6qlqbrSen+6XURkUgb6nSoJI8dI0qhxkjRmKvo9TcJiEiQ81gpx75b9hNrszsjgxauzXHw74KH7vvO8nlBCMhdrJGAk4CEBA2BAHAMFYNRLlKzIXyQ7p8z1EHJbRITU5Y+Vlsxsj+Nmx0jASKBnCRgAo2f59OUsJ9zKvKC5CFeLSX0HsOEcU6DCrkjBDFB7nUk6NmTtU/dJBH1gGAZGX8Q9eHkcpd4X4OCc97cHvurzt56BzN/fa/GjD3Ti2RgEBsYVt6ijToIXTN2ddeoxG7iwmBaAM/D8qJkJng/+47PUXxDDABj+ARhBoTC9QES1CJiPhMfGSTIAjJELzgcgMAVMjGw10dAbOUgfHYgyR7ORfWC6EISpP7BHGg6WSSui0rU01EkbGL3t6hDeebCsjoRERsPsJULC8R0WEwvmyBiJz5sgWTMXyojZpw9Sb4dGtQ5Yrr11wECHweJl3zjMHRr31fTSSGAwJXDUAYyUvDxJGVuAl/aZkjP3Qtgw9mxfWQabuNLNK2Xfx2/IvhXvS0ebFY4xZ85pkrPoU5I5bp5kFszF3KH/FM6BAjAqAxPk1TO+ILsuunIw75mp20jghJGAATD6f6sZAlVXIzEBVMCCE0H8dYKiq6yMwGC118ZBy98FJ4xUyAB48Bi3O+D/p+iZP0t4aw8OPJ2JpnaVbWj11gfa013kcSadDmBisULQFvvl1GFvu+fltqcqoA0d+nDK6pFh3D4FyeQIw9mHE0lNut/t+iF9S6Q9yN++13qj7O1jLn8NS8phYfVbb7FzvbxcZ5sX3tP9R8aerp9hVKde+VVWolR1y3knFjUQ3plsC7KSuG2dd+YYlLFVLzvCZ8spjw2/02ABGEEwTY2GaWpodKzffWKB6PQcmXzZV2Tkok/3q3xvhbozMELhayMqLkF9W3grq+8NsMXaMQdshd+MtpYW+J1oltTR4yTnpHMkc8YiSZs0T8IH2bFjfdk+qSvZJdvfXCI7335BGupqpQ0s3hCYsITRtwXkzj++3xQsxhjqxBjhu5R9b2tukdaGWonPyJHI1GwZc+alMv7Cz6sD2aAQOJE1yUjASMBIwEigVwkcdQAjLiNDojMzJW3KbMmafao6Yeqpl5Wg5VVsw9/6QinbsMGefIsYAKMnqZlzRgLDQwIGwDjS+8jVYipnlsNBrQ0ABXU+JxqJHiNIQEUM5xTgwD4BECp/a54EtV3LOCYnLE+lzT2xPOyWtWIqnWiPmjb/I6+GolRFUw+gIPPDiRsbwJ9Sgm1FlMctcMOt3/Y513Vwn+VYXpXUAGmuOQingGWWKYwet/uHvCxXA1v16n07pMMOhx0LBSIht0BX0l31ogj7y3rDYbNOZ4qa+tC+Y4qD7mj/rXJ2fVzBRx0E4PevXCplsP9noiPCNDikS596kp7nsf5ef1/aZ/3sB2XG/JQh/7rLv6GiRA6seV9qsbrMFJWYImmT54P+XoDi9j1kOZa35T+Q7fd6/9mq03dL4Hrf9Nq0Xwx1imtk59Hf8s2rpWTth7pCzkMpY6dI9qzTJTQmDk48b8UYIPvCuh6OW5qRsDBBDEtaPE052SYkqFNlxj4cYRosACMqIUnyFn5Ksuec2a8eElBIGj1FYvCcDEbqDmCk5o+XvNM/gzE2zmtzjI5E9kMN/EyUbVwtdft3Sz2e9wj4DovNGonrXCwF518PvxJ5XssP1MFd770i29/+t1RuK5IqvFPaAAh3YVwkZeRKYu44zG9HSmz6SDBBwhTIaG2sA2BRg/7ukroDu+AnY4dU7NkuNBUKA5Mk/9QLZPwF1yt7JAqAhklGAkYCRgJGAr1L4KgDGKEJCbADjJf4/HxJHjcenph7Rpzr9++Vun34odp3QKr27MOky5o4DzaA0YFo1O0SbP0F2JMWW56YukhLYLi0BB3yWs5TtREJ8gkcb5Wcfk7vkjc5jASMBHqVgAEwehVRzxmgaDlKnAM6OEonCyqIAeVeQQsqplT68I61/ixTEip2mo/nsJpIQITJ5SeDLViHtDxXrVV5pIJsH1ell+2p4kelz1YW0RYVQQssoYJoAy3si12YSqMq9fzW4yitfWY4WLyNHfYd+qflFXhBX/V67GqwveGFR/D3qDTVVuOgSMFZl8m0a74pYbA/70v7rI5KFFdP2xFBob2ZEQfACES7TIyEwKgCwVHREh4Tr8fYH15GV1eHNIJmXl+6V7bCceHOD1/T8yGRMTL6lE9LPhQ3K/JEqvabMmw8aEWUsK4PgEp8IlZpw0FVr8YqbqPWG4gV2wissmukCLsfbVidpqLXhj52gMrOurrQbyfx3gSAfcPf3sCQcNDZ41AvV4zp/8GSMR0sFj55rxQXfqzFkvMKZBKcXaZPXYDrr0O9TdKBPnQCtOL1BwYiugKUsRD80ZkiIyzwugPIpNBbaY1DHYMYF6TYU7FjBAreWwc0C8RvagDAhJDwKGUO0KGlBSZwnBy6/wTEeF36B4d/7S0Nel8623itGH+I9sBrCYuOlzDIbct/n5ANz/5Z6nAPmMYuvkTm3PhjlTlNQRqrSqX5YDlkb4FXlAlX8lsgQ8qbKQhRJMLjAWolp0sQAA72WyNQ4FraGhGBAuPC6tMhWWtBloXZA6NlhGFc8I/jwhm3gwVgxKVmyvTPfQ8mMrc63TiuvrsDGCNmLJA5N/1Esmad5rWfdCjc3lwvFVvWyp6P3pTSdR9jcasIoFSThMKcJHfumTLz898H6DLRa/kjPcjnva25QTY8/zfZ+MLfpKH6IMYwDIcBFEUmp0ry2KmSNnG+JOaNl4S8iRKqz0K40LlqMyLDcDGuGtE2SgqXSfmm1dKCsd+Md1H+grP1+U8pmC4p47w7ej/SvpvyRgJGAkYCw00CRx3ACMSkKQg+ISIRWSM0JkZX5noSqoamAj2vualZWmoOeXYeTACDAEUdIofUh8RJbViC1EdYk1Gnn+2Y8BzIGSVV2Z4rE+2YoNTBu3RLpkHRHVmZbyOBI5GAATCORHrQHW2AgLWockYwguwJVar5jQ2CBvR7QcAByqgDYFDRpcKueRXUsH1jqEJqAcmsl8o/CqEqnuAm9tGOVZfFprBO4Dx1fXxZ7AzuU/mFMsoyaIN90Syoi87b2BdX0vpZxgIFVDm2+6vXwYzYP1SPda2Oorj++b/I+ucekaaaQwDG9Ou/A6ZFYp/aJ3DBv4rNhVjRL4JiCwWmDs76eEFIYdGJGh0hbeJ0MAznKNgQRIUc/aXytePNZ2Az/z/Qz/dKTek+LROE35K4zByNTJB78vmSc/I5ev0tiGSw853npXjVu5ovBIBA6qTZyJcHJsEyqdq9BewN+AFISJXsuWdI+mTL7xLZJbUlAPxL9mClehu+90K5hl0+AQ+3FAxgIBxOD2Owapw2cYbEYNWYYAiVdMq/DIyFoif/KPvWfKilkvPGyviLvoDV8fFSvmG1VO/ZBsZLuYaO5PWHAojh6nHciFGSiMgQ0dgm2MJrd+TPitrbQZ+Hr4CafdukAkpcXekeRG2otVgxEKMCFzGJiNIwVjKmzZPIxDRV/i0giHJ2u/9BgQqmNFaV62p4xcbl6FMlQIUW9TVAUCcRfhGyZ50iu99/BYrno1KHMKNMBDBm3/BD9DkLQEcIIoC8JNvfWqKgD8dQ6rjpkj3vdCnHSv/+1R/gEEJ3AlTJmb9Yxiy+DH0KU0Cldv9OZfZUblsndOLYgpV2AjzdU2RKpkSljJB0ME8zps5XgCc4BCAPkgEwLB8YvQEYNMMgy6vxYInUFG+TPR+8Kttff0aqDxTr2M0/abHM/tKPASRM6S7+Adm3oo1s1OdyJ1gYTQBO+ErKmT5f8k69EM/mKIlJHwFAlCBVggJvBNHUdKQVZi9w+tkCMOPA2o+kpOgjObgVrOKt6yR93DRJGT9Lck8+V/JRj0lGAkYCRgJGAr1L4OgDGCEhEoDVnhCusti0zZ662YGJLcOMdeAHoL0Fq112GkgAIwArKGOf/afMssOokg5YG5kodbHJUhufJnXxKU6z+t2ByUvphEnSOGaMx3Gzc3xKILiiXMIwceXE3FtqSU6TtmTPe+wtnzl29CVgAIyBlbmCC6oDgvWgQANUQgIQ+nBY4IWFMEAtxeyc+RVE4Db/KVjBLZbDajf+AYJAXioXFnBBZZMr0ayS+TShPBVZbRPbzko/c2j9quh6PqEWi8MNwEC9Ck44fSUQg1Yc5dZaPbcUZu0LwQy3tOG5v8g6rMI32gDGuHOukBnXf09CwJggKML23JPTvl4b2iIgUA3goKRohZRxBRWrqo1V1oo+y0UCTAhHeMfM6fDLNPNkiU7LleiUbO1fB+z1V//jN7L+pcfcm3Bth0PpmfSZG2TSpV/WY01Qylc/drdsef1Z3Q+DH4MsKPRxOQVyYOU7UrZto67kx6SOkHGfvk79FBDsb4EZTcWW1bpKzb4SLKFjwRaABO4pGI4EYwAOxCCkY+a0kyV1wgwo2FnKWGA+mlys/efvAWB8oMUSsnJl1BmfAeiRBhOYD6Rq10ZprK5AvTV6PgK+C2IAWiTADCB9yjw1A4hEfaERVkQuiw3TJY3lYFLu3ioVW9dK2fpP0L89YMRUgYnRpPXQbCECAAbryZ5p8XUsAABAAElEQVS1UOuJSc+RYAAkZGAEYOxwYFGZJQOo7sBeDWFZCoWwbP0yqQeA0YGVcgIh4ZBZ6vgZkrvgHCldv1x20F9B1UFth+ybOYjyQWCBqejp+2Ttv/4kDZUVup89dR7KnYtV/mWye/lSZaqER8ZKwXnXyIzPfQc4W6CuxpeuWw7T1mVSAaCDK+zNALgaq6tQh2vka32Ov4MRc05HvWcD4MlRkIcnDYDRNwBDBUl5kV3UVCfb3nhGCh+/Vyp2bdVToxacJ3O/8jON7OHkHcjv/QAe9n3ylhxYtVT2Y1wo+ykqXvLPuFhZLmRCBWOBrvt7pHsfysEgIYtk/8q3pbTwI4nCeyIW4VRHYrzlgY1lkpGAkYCRgJFA7xI46gBGCMxHIqKjJTIODpvgQToQqyg9JU68mvlXU4fVk0OhqQYSwBBMyiN3bZeYvbu0K12YyHbAGVMHVr3aw8OlE/RP99QFVL01MVE6wCAx6fiXQOZrL8uUD/6Lya+bMuR0G/e68KRzpfhsM3FwRHI8fRsA4wjvBoEH17i3GQkEAnic7AaMf/4jEOGEqSP4oMkpy2+7G8rUQH3KKoBTOn5bYIEFADimADxmrbyzHfxpGbYPIAOKKJPmQV+sfOySBUg44IMqAshPcIFJAQXuozesg21ZZgVWfwhkWKYuVp80/B77zjL4swCMvwDAoIIpQgBj5uduVxMS7vtqvwNmIlQyd779HFbqsfIKkKARLIJ2HOsAO9BJwRFRMCEJk6ikVCglmZIz7yzJnX+2KjpdHV2y+u+/lvUv+wYwJl98I8w0blG5NAMcWPWP37oADLIZ4tKzlS1SBzOURtDXIwCsx2XlyxiE6M6Zf4aCAZU71sv+Fe/KwW2F0tZQL8008wBAz0UA90SfD4yKEEmwAJR3RnLImHqy0t8p/3KYkKx9/B7ZB/YBE2ny9H/BKAq1xbu0fbI6HKfa7F84gCAyQuKRLwUMhsyZp8AXQI7KnyYsZEaUb1whO5e+qKBDC8CGJprhwBylE2YgTOoAEW1EwMyC7IvUCTNl5CmfktiMPGUtOGOKTBNS8w8Ufih7sBpOJkSjAiHoE+oisyUQjMi45AyAPqMRJaJEDu7aAplY7AgCGGRgRKcBYII5SOG/7pU1T9zrAjDiMnIkFqyURvgCqd6/C2MEoTvjUmUsxszUq25FmMxyqQagte/j1zQaBZkfTWSSYJGFZiXdE4EZ+jtIzJ8A09kZMgJ+KXLmWb4pDIAxNACMHe+8JFvB+KjEs1W5Z6skZOci8skMZeWQgRGK54nAbG9JTUowXqoBAlbt3qzAYXRaDtgb/BvRW3Fz3kjASMBIwEgAEjjqAEZMWppEpacLV3SSsPoTDG/NPaXa8v1SC6/PNXv2yEH8OROdAQUweuqAOTfkJTDp0fvk/BWPYZ34cACD68cvzbhW1n3xa0P+OofjBRgA48juqgIGThVkGFChJwAAUMAFADgsC/s8AQLHdMMy7SCIgFVvZVVYIIjW64AXqF/NSOx2HOYC62Fie/x/iClhAxBQaunfwAJFeMwdzLb6qKACq+E5MjwUzLDqVfACIIyrPfYfiddmgRG6q31n++uffUiKljzoAjDGn3uVzPriD1SRt4AV7+1TiW2qLpFNL/9Dtr75AroAfw9hoaqQcqWfsmH99D3RQqWefQgKkDFYTR1z1uVQxDPgsC9ONj7/V9nx1nMKgDRUWSv9BBKi4pOxGp8JIOIqGX3WFdrpJjA7Vv39N7L5tSW6z3xkJtBnRhBAfyrnZHtEp+fqym3iqHGgpn+IFV38FS2TSjg6JEARCjAgBHlp8sCOEpwgc4FgQhMADukKkOjEVJh9TJQx514pmQAxmMo3rpQ1j/1O9q5+X/cJzkQCVAgGsM8xBCFbpj+4h/QD0AIggn4xQpAvKjEdwMMsKYB8GdqS8qGZSFN1qexb9qZs+9/TUnVgt5qYhMFfRghMSgPhk4OJdPtW1NXaBseIHZ2SPnayjD33CgVEwuNTIfdI5OoCIFEKwGaX7IUZwW4AGHWV5bgvwcq6IFuFCyMKXOnNCYDPkjppIDvDjmI27uzLAWD8SKIBVHCs0N+HOwOD/kbCoxARIwS+QiD7MMg6LCFD2RNjFl8M1sUKKYYz1v0r3lEzG/r9IEARAvkwGgXlw9SJ9nhNzQBOGqsqNfJEVBpYM+dfJxPBuGEaLAAjGkAaGSOjTr9E2+nrB0GqaCjSgx3Nw18fGMq6AZjYXF0pDRX79L5veeWfUl1SrGZGefPPlFk3/lCSOeYGIW186R8wQfurOuJsqCyVLJgC5Z12EUyw5uh4J2jmT3J8Y9CEjQ6DTTISMBIwEjAS6LsEjjqAkZw7Ek6WRkn6hPmSOeU0TK4siqmvLlfs2SDlu9bLgdUfyv7C5ZgQWCtJBsDwJTFzvLsEDIDRXSJDZ98AGANzryxlzlL8CU4oW8FW/qlhKjhBkABZHGYEjxFcULYDnTVScSUAAoWPPgHo10HVNChryprA3qF2bK2euq4yI2CK4lwKy4NBYdVjgQZaHsCDxbKwz6N/qFhXyK367QpYJ1Y6FYBhGTiKdNgkmgN91Da1v1Zell+35H4AGA8dAjDOuxoAxp1qr95T+2UblutK/wGYU+yHA75oROQIh2lh4ugJMFGYZvUZLBM65ju4bQOYARVgaFRLNvxSZM1aJCkT5gAgmCw1uzaDebAREQyekz3L39GuEpDIg1+FXNDfyXAglZxyYUSVFY/8f/a+A6Cu49p2i957rwIEEqCCUJcty7Itd1vuca+J7ZfEcV6Sn7wkL705eenFJe6997jIsi3L6g1QAyQBovfeu/5a+3CxQFggDFgSMwmge8+cmXPWmYvZa9Ze+9f9BAaVCZ70UcBOLf0ZAhDY29k7i6OnF1QO0Uip6JL97zwpRds+QnAPbwoEzr5QEHhHJmDXPxEESbjO1wv5fWt1GRQLeUir2AH/gGIQHe7iFzkN6St3Iw//Qp2fCox0EChFaev1PJpQumAuD1xDwPQ54hMZr6adVFEwjaIeO9ItDdWavuGA8UJgSDjzmq9LCMpa8lk1wvOiDIag5bs2KsHS0dEKM+9A8YFvQMD0ecDTFzj2atUTeow0wyOEJoneULLQwyJo5iIEiQtUgcI1VgPPiSKQIRVIG6nO3QPvinZcj6sExCSh/woYgHqCOGiX2rxM9Sxpqa9Eyk+DrhneEAmMhfBLcKcCA2tx13N/GaDA8AJebgHhIE5SJBzmko5IH7EHOcNdcla8yH7rSRg5Pi4tVdhcqSgR/+h4KE8SxZ8KC3ow9BEYrQi0W6tLpTRtgxTDu4RKDBfgMwvYzLvth4rteBEYJFV84a/i3pcmo5ON4Bt9QaZfeBOqepw9gt6j73K8BAZTpNrxueKzpy9JJdI4qg9mSBeMW93gORG5aKWk3Hivfo5Gf1Wff+buF/8lu6HSIVnGFJb4FZfoGveJnG55qfQpyz5/hIFHbN4YDkipNuVTB2JjXhkEDAIGgeEQmHACI3jaNAmdOVMi510g0Yuvxu6L9zGvsTwnTcoObFO2vRB/sNhK4BkC45iwmYNHIBD91ouy+NM3xLe9WvwOWwZ+tsMMqtYkXim7r75NupHa1AtzWdNOHAQMgfHFnwWJCJU2Y7Ez+LMpHTRlpJ9n4EH6WPC4VeFByQMSDSASbGNYSguSGDqYEhoM1kgA2EgR9udxq3Qqx7QICXS2+vcpLZQc4bm4xb7L0ONKblBtgSOU92MwVXhQhaBExSBIrHktogUd8H+OyPtAf32NMq4Iove8+A/Z/dL9/QRG0kU3yHykETCoPNb8uR+9qv4JjSiZWIuvQBACvigvGT73NAR5K/S+aXbKILUkbZPUHsiQahAVgVBF+MXNlKhFSCVZdrFi0gW1ws5HfyN733pC74KGf3NAHMy57l7FgffK+2+tKZMdj/1G9q+2FBhUNvijvKR/wlxVAYSlnN6PAu+/ubIElTaewfzr5DDIC1ZE8UZJR18E9CQwvBF0Uw3QgdSXxtJcBIG7pGTbWqnK26/jBCAAn3PDt2FGeLnOXwF/ivQn/9BPYDhC1eDpHyr+UGrQaDQQqR2uCBqbQYYUbv5AiQmaWDbC44ItNHm+0CA1LPUMfWYsOZn78SvwqQDZAd8LZxAiPri2YPhlhM87C6kc8KI43A1vjWwEp5vQbwfIh33ihBROpriEwKcjZvkqccM1kOQqwrUfeP8FqUOfZmBF1YN7QJimrcSvvAYpH956v1RJ5G94T5pKcnFtpVqVhdfH9KGFd/5cU0i4+na98FfJePav/Skk/iCT/KbNUXVL3FlXQPnhoqVWdS1j/sw3n5As+Jn0dqKKClQB3lHAGiQVyZ0gkB78nFDpUl+QDe+U/fj75T3J37qWU2McR1lw83dl0Td+o6/Hi8DQwUfxzT96msy7/UdQidwyirNHfspgAoNmljMuvU3VNkONwoo17TDCrIHxZVnGZk0baoaSyR2fIXrDkHCh6scLpM14NHrSpEGVRIKQLeliEKBf+1+su3CsD3ddl0fOy88kzXT5O2gkzUojMSkkI8HK9DEIGAQMAobAMGvglEfApfCQeOBr1qfvyuKSdRqs2G6aoU5a0FLJmr9SauITpCUhwXbI/DwBEDAExtg+hH7iQSN2rn7lBxC0WioGm9LBloahZID+Af4ZSWEjMTQNhX4ZJDNIRPAnmhIXJCD4nhIRcNmwHUcf/o/nWufxAtAVBAP7KkGBvkqycLgj+jMQ6CdF8H5/9RL8u78/TrHGQtCAcdT0k+8hyKfPwS7sorZCgs6WdPGNsgBGjiy1eaz5WXp1/7vPShtKbTbXVspUeBfEoOypH3bcfVB1g/fN8RvLDqk/RM4HL0jep++rZ4VnaKRMO+dqpEHcqP2YbrHj378YSGCgPOncm7434P7pgbH94V9K9vsv6rWS6Iicd6ZELjkfEvlZ4hM1bcD9k9inJ0NLVYn2JznUi+vqRRoGYaA3SAc8ItogfW8qLdDqH6xQwvthC0TVjzk3fgcExmWKW1V2mqQ99jsp3PmpHvfGrnwIlAihKWfo/J6hMNZEOkcHAspGVIQoTV+PKiuvg7ixCJEwVEyZe8v3oV5YrueX7d0sWa8/DDVEBpQadcAmCsajF6Ms61LcSzyUFQG4/y5gXI1ry0eqycfwHHlTOqHU8AgM1XESoAqgYoPrIHftW1pRph7KDlaG8Y9OkDAoL6j4CIbyhWkpXEv1UL3QNLF05ydSkrEeyhaLwCaBseiuX0KBQWUKfJCwNtKf/Ut/cBqZukyiz1glwbiPoMR5uFek4JDcwzrlmqwrOigNMCPVlFaUyO3FMX4RazZWm6ChakNJHvDO16/a4jw9ZggMhQEVed6X7fgslOzZpm+4+wWoZwoJxaEa1zC9XFjlhZ4j7fCfYSnfgJgZErX0IpBXZ+D5Q6Xj7TfU6V/4vcEExsxVt8ii//q1uCIFS0v9DprhwLtPy36U71VfoEHHhnrJFLKky+4Y6pB5zyBgEDAIGAQGIWAIjEGAmJenLgIzH/27XJL2DP5ctQIt3in/dch1uuRHzpLcBadL1dLTTl0ATsI7MwTGWD000AYMsBBoMwAERaABtTW69XnQQFw/HexLMsHqj3/097UpIHhc/SsQzOkOI34yELQIC/6zjzywRXQ4xnEQdWsAeNRd6fl95+GHbUyqEfCiv7tFFnAeqiqgsOAR/JvD6/zaF9fNa8G/lZhBJxIou5//6wACIxk+BCy76Ozl+9kcfdd75Pw0d8xE+dV2kA9dCJanLj5bq364wLvC2dPbmgfzdcMDQqsjfPCS5G38ACkSfuKKsWdccgvSM76u19yFPjse/JnsASnCRmIiBekEKTd8R1/bImCWijySwHDz8ZeE874iced8BXL1UKRz+Bx1//QIYEDXAlKC57dgB7gV/25H5Y0OlHtl0MfAuh0VTlqbGtWzowcEAZsSGDd9F5VGrtLXVGAcSWBwV54EQgSMSd2hgnByc9P5exBUdrc3qR/E7uf+IeWoxsGmBMbN31eFCh9JyfaPlSSozN2LdI9W8Y+MlcilFyKdNNkqOYnqZHyYltlnG3bYN2pp09bGRph+eyIVZwXk+t8UXxA3xOjA6hfxPP8ptSWHINzoEJbgTFx1h3puuAeGwEzVRZ9LB1IOmqEKyfvoZSWhWiD/Z0s8/1pZePcvocAAgYHxdoG8SEcaSUuN5U0y7cxLJQnEki9ULB7A2w4pUzYSjj9pFApTJWlDdasWqFB0tx14s3pMB8gnpgAR61Z4dTSjsgkNT7tQYpVtoggM+p54BYRY61tnHtk3GpsmXXEn1CcXjeyEUfYaTGCMdBjiZw+fDqpznOA5whKk9I4JQNoSn5XDINP1kY47XL+jCIzLQGBAReMKbxaSqYPb9od+Ljue+bNVmnfwwSFeL7j1e7Lknt8PccS8ZRAwCBgEDAKDETAExmBEzOtTFoGhCAzebLlDmFT4RssuSKOLV154yt7/yXhjhsAY+6emqRtUPPQF+zRLJAHAoJ//p8KC5ADTIlSJQYIATQkBBG+fnYcgDq+VReD76GcL8kgA2P6oV38N9lO/Css81CqjqqP2n6djcRzOjyDFZjKKCXV+frMIGBAT6PfZuBgbF845VdmBC9JrwT3ynmz9dr/wd9n1/N/hrWApMGZeeossuOvnMNj00vE+b/60x++TXa88qEEo0zC8kO7g7h+M3XgHXCdNqDk/YICCgGRKM1QQDeWlGmQ5I9CfecVdKNf6Pb1/BrLb7v+x7HnDIjBYPnXONd+AAuO7OE6MrPtn8LvlgZ9I9nsv6Nju8N2YdeVdkojAkoGpIwJ0233pOcCou70ZAXi5FEBpVo5d7a7mehhsNiGVokMDbqY09MBDqgtqjW6UJee9HKa3CVogUiZSbvkeCJKrFX+WUd3x8K+kcMc6PR4UlySzrr1HU2Hoh+EAwoHzs/VCgVC6fa2kPfl/UobUDzYSGPNu+xEIjLO0X+HWNSjL+kepgL8FVQsuSPFwR0lWJ3eUR+VzsgWAuA/iSL+KxqpyrehBMiJ6wZmq6PBH6g7X38H3n0OZ2T8jpSefrIfEnnauzIGKxS82Gbgj3YNPHuPSp6WrtUWy335C9r7ygDRhTDYauC5GyU2WUSVRR3Ir/TmkkNjSA5BelIo0Cpap5Xhcb8RZf+L87q42EFatkg+sC9a/D2NJEBdQo9DXgLhy7fJnJ01J29pBsuA1roVtoggMKhrioKiJPu0CnXek3+jL4ot0JXphjGcbLYHhxAo1ML6kH4l/HCreQCETMud0reZBk1nb752xvvbBBEbSxTeAAP2JeozYO7r1rw3bvIbAsCFhfhoEDAIGgbFHwBAYY4+pGfEERSDh+cdlxaYXxa23RVylvf8qG6d4SYOzv2w8/QrJu/za/vfNP758BAyBMXbPwKaeUBKAZIOmjVhkAMJGJSKooFASAMEaA1QlGkAoMHizna8qC7xWYkFTQUiAQNmh5/YF4X2kB8/TfgxM8cVYX0kRG9mg79nG4hyWl4VFUJBIIdFCAqWvD/6lrzku3+1LReE9kXjhT55ru9Z+xQfmUaNGEBi2FBISGNyF71dg6HlHz78NMvf05//R77+kE4/wG3eK5113j8yDWSjvn5VKtj/4U9mNagZsJDBSQAwwhYRY2e6/DcqJrQ/+RLLefV77saJEyvX3wvzxm0Pefw9UCA3w56g5uEcrnZBI6OpAMA0SygXpFKz2RZKAQT29RIhVe2M9PAUsVQAJjLm3fl9i4ffAfjQu3f7vX/YTGMFIW5l76w8k5oxLhpy/ZMcnshOeHbZ0gPBZCyUVJpURi87R51+w4V0QHPdBoZGh93O832JQjnYB/AZY1YTP/8B7z2qVlNriQzpU/IpL4UfwUygmpiuO6GRNwfWBRvKCVVVsHh2JqLiy+Ou/UQKDayYD5Eo6PTD6CIxZl90mi7/5Wy0LayPGLOIOg2H+RngbNJUfQkWVV0BioLRue4t0tzVrFRYXKAMUa6g2SFYQ7zak77TUWOk6E0VgeKOyDZ/Z7K98UzE40b4NJjBcvH3FE+kYrHhzrObg6ikuvqhkB9+WoCQYyiJ9yDMkRrE/1nlf9NguEFxU6tDEk9VsEs5eJbPx2faOiEcaSchRxAlVW9lvP659h5qb6hwawNqaUWDYkDA/DQIGAYPA8AgYAmN4jEyPUwSB0I8/kKTNH0t41UGJ6Crsv6sucZSOKc7yweLrJfvGr/W/b/7x5SNgCIyxfQafBWNWGomO3h/wWyQG9PIadNpIAD0HBAJDQiU/lNBgnEiSA+SCkhAcySIVbEE4Az2OYe2u69kafH5GYPSREAiYtVwrxtEdbozLMfsDRmvmvmNMQbEIjX7fDtt5g+e3KTF0d7+PwGCQalNgrLpVFt39K1TA8D/m/Ezl+EIExvX3qFko8aEiYhuUFbtfe0ShJ4ExF8RECgkM233g/pkCshVER9Y7z2k/Ehhzb/xvmfWVbw15/21ID8lf97YUb18HY8vdUs/AyMEOZVRdUMkjAj4SIVBueIgDSquyZCNTXegLYfOsCIwBgQHPimkwwCSRUoEKD1RgFPRVSyGBkXr7/8BIEx4ZQ+BfAgXGzsd+CwJjq15vOHwoaAQZPv9Mff4FG9+RnVCy2FJMtNNxfIslgXHXT2FimqLzZ7/zNAiJP6JcbJ6OwpSPRXf/AmaOCbg8i4RCR11D3JFn+dxd6N/QZzJqKTB+A5VBmCpp0p/6wwAPjFmrQGDc89vPSokCE/6P2PA5Hdrwvhz69D9ScyAdGGajQDdSrkAO+SGFwTt0KsrNAmsE4s7e/sDbD54eH4AM+lSv1RAY1oMfTGAEx8/UVBAa3x6rsWKHppDAK8MVaVpU8Ti4eqhq7FjnfdFj++DhshfrqBlmsK31dRI1/3SJRcpVEFJYAqanoNwu0qCOaA3wpKkr2K+KpiPe7v8nSbgDH77a/3vOEBj90Jh/GAQMAgaBYREwBMawEJkOpwoCftu3SXTadonL2yEJzXsH3BYE77Jh6jmy59wrpC0kTDqDQwccNy++HAQMgTE+uPeTAwjGGOixkWhg8MemxANVFQhW+f6Ragyeq32VoOjzwug7D6PwbHzxJ8dBMInAjkFff1O1BkkINNv8DBD75u9XUOjued9cUIMwZcN2feyLF9Z1Hvl+3zlDzZ/x7J+x0w6fg34C4zaY8EGBAUk6A9PPm5/Kgp1QYHTDNJApIp4gA9x8A7V/P+HSh+Hg+2cqzvSLb5ZEVFdgo4nntvv/V3a9+rC+VgLjum8peXDk/PRV2AavjMx3ntV+SmAgzWQW0k3YBt9/U1kB/D0ekMKN70lbE8qZIk3EE8G0Z2iE+jh4BkeDvPDUcqA0LW1Bac/92B0uQVlYtkAYIZKgoAcG76kcRIRFYKzV4yQw5n31xzIV1VTYBs9ftGWNVlfpJzBmL0KJ2v+1CAw8/4L1IDCeAIGxf5cGdJpCgjKqLPVpqWZ0VHzrWyefLSHOBmPOJVrm1Ss8hh1RgeQ5KDr+YBEYWENxp58vqbf+UFNI7FCWkrizdQMHpnNkvfZv2Y0qNExLYVMCA/4FWmIU52fA/+JIE8+ZIDCW3PM7JbeUcOsjRWwE4J6XH5J9IKFaa+F/AbLJOyRc3EOANQxJae7q4OYNrD1Q4cZHFT6ZUIBkvWepaQyBoY/gKBNP+pgsREoX/U5OxJbzIX1UnlFj2PrSAlQFQvpK8gKJRJpUNPxCWCnoeNJXtvzzh7L9yT/q7zDeryEwTsSnbq7JIGAQOFERMATGifpkzHWNOQLHIjD493KlfaBUeoZJGv5ILzkfO42mfekIGAJjfB6BpYyw5PVKIoBvsKklLLKCJUj5qcAXf+Abj2uATuKA/0Kga2u2wM6WxmF7nz14jOdZZAhTPAale+hx7aiBsW1+jmEjWjgOTSqtObmz3jc/DnBnXOdQYgSEiZIYOIGkhI5hzc80gQz6HNRZHhiaJvCN34oTjDi1YVAG5oPnp7Ig7ZV/wYQRJTORlhF72nnwirhcSyfSD4Lz9F2QhaENF86P8bwjYvE1Ta+xq71Ntv3rxyAw/q1TUjY/FwRGKqT+HMM2P6uQbAHRkfmfZ7SfZwBSSG74b5l97b14ffT916NMaNpj90kOVAE9MAp1g8fE1DMuQtWSlQjCg2D66a8EEPHr7miX2tzdakxajJKlbEpg3PYDYSUE3n9l1k7Z/tAvpGDbx3o8OGE2PC2gwICnwlDzF8HjYsejUGDs3qL9w0lgoERtJEpb8tmw1OrOp34v5UhxIYYBSPWIxVj0rLBzdMB9961FXN+AtcnR8KzdYEZJDLnbzjf2g8Ag6VBTki+H29slet5ymXn111H6dBZICahNsEvPe21DxROSQTnvPy/733lCmvpSREhgLEGKiEdwpF5f+jN/knQYLn6WQnK7LPnWfZ8pMDirqoms60x7/PeS8cI/pK25UY1TY5edJ/GobOIeGC5uTCeANwpJFBq7Eu89qHKy723rWRoCgw9VTjoCgyWKCzevQcngDVASpSlh4QafkdgzV0ny5XeIq3cQlCFHe2FYd3v0d0NgHI2JeccgYBAwCIwUgXEnMPa+/ZCkv/QXaYUpGZtfRDTqpCdLRMo5ErVgFeq38w+Sz28Vh3ZJee5OKd68WkrTPrWkvugeih2ZiKXnS8i0eRIav0D/WPn8UcwRg4CIR1aWBGZnSmL6WkmpsXYeB+PSLfbyzsKbZd8tXx98yLz+EhAwBMb4gW6T2isxwUifVACCPgawSgJYLAECN0uSbyMg2JNGm9rQn+exDwM27UuCgufoMR4+IihFEKikBI8hsFVVRV9KijU/vTRoDopAsX9+VhyhOoRqELzP8Y46x5pf50Uf67jelF4Hz9313J8RpEKBgaocbJbPwe/UxFOZDr3/o+dPh3IjE/Lx1uYG6ayvl4SVV6Dc4W0IfqPEIyQa1+aop7O8YxuqT3Sh8gWrftjBd8IeBpQeQRHi5h+i104FxtZ//FB2vWZTYPjCA+MbknLz/8NxB7FXDLtR7rQSJp4kMCwFhmdAsKQghWTOdd8e8v7rCw/Itod+rpJ03psPqmskIkUmDpVLWCWBKQ00lWzHf4dZWrQyc5vkffKmVGRnsHu/AiP+3OsU/4p98MAggYHUELYQEBipt/9QYldcPuT8RVtAUDz6Gyk+ksCAAoMeGFwHRds/goHq32DimQFsmiQwdoZMv+BmCUs9Q6uqqJknyB4qR9pRSrUTFTuIIQ02GfC7+gaJe0AoVA3wR8B4OR+9KntAKtUVHtTSqIEgQqJOu6iv7OlcVT9wPdblH5AqzFm0+X1cw8fS1qe+Sbzwenhg/FqNILk20p+FBwbXRr8Hxu2WBwYqzei6o4roiDbYoJGGjik3fhtESxjUOcGq/CBxUY8UAj6b3A9ellyoUNgMgWEBOTiF5ERXYNQVHJBqEHAFn74l+SgB29EJc1asixiU3I07+0rxCo9Dqla4kmxcp1Pwe4GKDP5O4u9F+t/QULarFRWA8LuCviz8fCv5CkiMAsNaF+a7QcAgYBAYCQLjTmAcXPei7HsPJeiaLQLDC3/w+cYkS+iMpRI+c7k4grE+VqsuyZaqwj2QtK6Xqr1b+gmMgBl0nj5DAqNmSVDUbP0j6VjjmGMGAfvGJnGEu/2cV5+Qsw+8OSQghsAYEpYv7U1DYEwM9JoWQmKAxAP+6GbjvxEt8l/6HkkB7P0zzNdgnH94M7hjQKlBHskFS5qBnyQwrD/eNUWApAL79pESn8nyrR1t2/y2Cij982Nsjskx9Hx9QaIFr/vmtx1TcuMY86cj5UB32fsJDOyyI02AJp7Hmj/zzce0BGdzRaE0lBVJaOJcCZq5BME3/hs2fznIAUjH7Z2lKjsdyoV0+CJkSP2hfVoe1Bk5+jHLL9XAn9dPkmMrFRivPMRbVBPP2dfcLbOv/xbIfOTx2yHoQeDdWlsum0F0fKbACNZKJbOvhQcGMBl8/w0I5Lci5YQ59WxeQSGqloiGmswPFSVcQaC0VJZK3aH9COTXSBUIisbSfFTlKNP+QUwh+eqPEIhdrfOX47+12x74Wb8CI2T6HBiR/gj3smrI+YuwM73z0V8fQWAshunmTyV8wQp9duV7t8m+Nx6RSqSmNCHlwt3TSwJmzJXQucskYt454hlG3whXaSo9JJVIM6kBljUHd6GKSIfiGDJ7Me7ncvGEnwfvv3jHWpXzV6MfnwmrvXgGT5WQufAlgKGneyDIDlc3KdryEUxN38R975P6skJUakEaEFoSqoyQwKACgy3t6f87SoFBDwxXbxAY+J8SetrT+jaYwJi2/GJJuOhGKEqSUMEjCc+P5VUrQJyshi/JWmkqyUO6S46ebAgMC8OTjcDgZ7cDprfZ/3kS5pxPSXNjja4nL3y23EFSBiTMUQLNByWHfaLisP7gg+LkBjKrXSvWNBbnSX3RIWkAqdVQfBDre7dUHEAqK38nohkCw1oX5rtBwCBgEBgJAuNOYJRlrZeCne+jlFiTXo+rTyh2pKaKb0QiyIdElXoe60Iba4uloboAOxn4A6QgU/9oZX+viOn6h4J3wFTxwRf/qDHNIDASBKLeflkWbHhb/FsrJKDXItZs52EfVzbCC2Pv2ZdKK/LHO0PHt5ScbV7zc2gEDIExNC5j+a7uAGrgz1FJGJAxQJCMVzbCQokFB6ogmL5gpXIwqGNahx3JBbxHbkGPcbca/WgGyv8xcOc4/ePyPFuFE/bjMVvj7/G++T97i6oQKiwwP4+RCOH18TVMRKfY28gNnMrr4fsYlyaf7KfEB86j6aP6HPQRGLMh+14EHwT1wPhssqPmL9iwWvJhQlmdvUNNKL2gBHDFVwhKhYamLIJs3EP/O1aJChtVKBPakL9PqvOzxV8DmXgtTRp/Lqob4RqYQrIVue8ZfQSGk7uHxC67ANU/mE4xC4EPqmigHwNgTSF5+2m9Mk+aeN7y/5BCAgJjiPtvhJnljsd/Lbko69kFrw4nBPShiakSmDQfO8MxuEd/qAtQ9QCpJhW7NksNCA/uBvegFCibEhhQWEw7z7rOChAODNLzj0ghmY8qIEpgDDF/EVJE6JlhU2BEgHBgVRCmsBD/mrx9Wq2jPGMjdrEzpAvlRT1QccI3JlFCUQKTahaWS22uKgYRtAspLnuktnC/OELF4gVyI2LBWfASuQWeHtF6/zUH90px2lopRfWTchiOtqNUqRM8CAJQ7jVy0VlQvASDEHKRin1Qb25bC5+KCqhP6tQPg/dLBcbSb/4OVUgsr6MhU0jogQETzqH+tqD/xh6kkLS2NOtuemhyKtbCaZrmwlSX1rpKPMNKKUtbj9Ky2/W5d0FVwjZRBIa7f6AkoCxu9LJLdN7j/UYPD+/IWE2JOfJcBvI0qGyrq9K3nTy80G8alD4BR3Yb9t8TRWC0gKRrLMnB56JNr4lKHl4vCbPRtNyPX5eDq1+Auicb5ZKLUFKXxK49ShHHgsSYjTUQg/GnKiFp7+SOKiRtUBa14hoKQFzkSyOqBTWV5Us7DH3bUAnIDQSqCzBMuvxrVjWiIy6Kv08binJx7iF91w6/67wi4vCZiDmil/mnQcAgYBCYfAiMO4HR3lgtzbWl+gcr4eUfKZSzOrl5ibOrl+7OHAv2rs42lINrgflZA/5QaLT+gMUJlJw6uvnAZd0NKg7ISk0zCIwQAaeSInEvLZI5H74hS4vXDjgL4ZFU2QVYXhgoGVhsvDAG4DPRLwyBMdGIW/PZfCCUMOgjGKxdf3xCEGBbqgcqNPgSMn+oNpSkIGEBskHJBSo4cIwBoB4nydHXX8ficRIRIDSYZmAjS/jzcDdKuCJ1oJ/8wHnWGNb8eIFj1vzW9ViqER0fAfNQ86fBgyH9afgcHKHAYKlMBqnHmp+VBGoRrOQgaDmI3XwHBMaOqO7hAhWBC0waVfmB9I9O/PepCwFtO8j6TvyMXnCGRC1diV3ZpRIyaykuEwoMpJBs+cf/QIFheWCwxCbLXVKBkHjJrTL9opsVh5bKEq1Csu+tp3hLwhSSVCUw4IFBTAfdf0t1OSTpD0rBxndBVJRreVR3rdDghWt11XSWHuwEdyGtoR0pLu1I46C5pabrYPyg2ERVWNADg/iXZWwCIfFLyd/6kc7PFJL5d/6krwrJ0fNTZbDj4V9I8S7LAyNizhJ4YCCFBB4YfMb09KjJy4QiYg0UEa9q4EfTQxd8UQHDVBvi2IPrI0YdCO5Y4tU/KkYiFp8j4XOXQ3G5zCIUcP9tjVVQNRyS3LWvycH3XoQ5Z4WuF3p/0FfE3gkeGCCweru6oeLoli6k/zQjNYXlL9ksBQarkITrvOlQYKShzKothYQmnkvv/b01n54x8FvmG49J1ttPSAsC2IaKYqSN+GE9+GhQ7IC/SYhtT2cnduzrpBkVKw6jnC3fY5soAoOVOnyCw5B+Ezjw4kf4yiMoUmasul2ilpw74IzaQ5mSiRSoyn3b9H0G1ImX3SHhqcsH9BvuxUQRGPkb3pHstx6TVnxG2GgSmnzl3SDNoOYZRSMJWJefpYRc4cYPpAVrqxvpIc6enuIKo1wH/G5wgPqHCjRVs0GNxt9j6ocCv5YukIYdIDL527AXHGvYtGQJTl4Iv5iV+H1x/oAr6sGa2fvy/ZK75iV9n94qR5oCD+hsXhgEDAIGgUmEwLgTGJMIS3OrJxkCcx7+i1yUYZUpHHzpXeIAL4ybJNN4YQyGZkJfGwJjQuHGZAhfGSCjkVhg4x/fFmHBP7lxrI90UALBRj7Qk4KKB9Vt8JzPPDH0vb6gW8fHeP1+GUxXwTEqLHiqkgFHzo/3LJUF1RZ9KSycn4SJEiB95McI5qeJpwapNgIDCoyl3/q9KjCONT99E0igH4Rx5KGP39Cd/FaYN3YjGLEpGIgTA9MpCDCYHuGMr+hlF0os0kc8UAHEPRDBEgL0rrZmSYNXxH6UUGyHAoKSdDaWg0yFSWcqjDKJRysUA1vu/7EcSWDMRRWS2dfdC/iPvv827Pjnb3hPaDRYizz9+rIC3XHuvz6cw+CHu85uUH3Qe6SNRAZ2gNlUgYEUkfjzr9P5WYVEFRg2AkNTSH6MNI5VQ85fuGm17HgECgyoO9iowFhw189BYJyFVzAORepGe3M91B8b5eAHL6HUaxZew+cCu+I92KDoBcnARh8Q4ujk7iYuCAaDklJBmlyiShKPgEixx/Xz/ukr0NXWKIUb3oWk/xnsbOdJOwkkHa8dY8BPBKSIF8uaYre9FUaeNfAwILHEpgQG1Df9KSRPgtx6dqAHxmATTz2x71sBUmaKtqyG4iZdanEvnQhI6WsA8LSHrgV4ILh5YS1gs4VY28iRiSIwjrze0fw7YGoCKsn8SBIuvGnA6Syxu/WBn0r+lg/1/eDps9DvJ+oDMaDjMC8misDIfPNRGNz+TmqLD+kVTT/nClVe+aDk7mgaFRXdICGozDq07h1hall7faVWKeqAQS1JQv3c9f0es81BUo1r0hlrmJ9DKlec8HsieOZiCUMqFa/HJzLe1l1/9kCptPlvP5A0VEFio7fG/Bu/o58tfcN8MwgYBAwCkxQBQ2BM0gdvblvEEBgn/iowBMaX94xIXFgBGQNmKCSUuyBJgXdBGCipgTc/IzcQgCK45A6+1aiuwGsbAWI7B8dJjqhZKIJ6q7v1npInJDJwnAQFX2MUixzpm5+jWyoP9oH6Q1NJLAKEwXLfgIyGB8yf9uR9AxUYIDCW3HOfEhg8R+fUmxw4P4NrXmsNKnfU5e6V0ozNUrE3DSqAahAN1q4u79fR3RuqQjfxh5ohAD4ZYXOWIq1gGdJLoNhA4MFr7unulAPvPo3A5y01eKwpzOOpSmDMu+k7MvdmVCPBLdDEcxOUGpm2FBIoMObe/D1Juf6/Fa7B909ipLHsEIJpqBw2rYEXxw5NGWmpqdTxGTQzJcAdahP/hGQoID2lfDdSSQpy9bhNgTGNqS6Ynykk2+7/SX8KiXpgoIxq3IorhpxfCQwoNo5MIVlw58+QQnKuPn+uCVZHaakqtXL/M3eimsM2ldK3AMduqC7YWMXBEekvPvCm8EuYpTvTYXNPg1kqPDKgKFCTU/RT9ciUw3gemTAk3SllvJfsNE29aUXlEfpfuLgzOFyAaiwXSsWebXIQxp8tNdU6T78HRkikrusMKjCGqUKiJ/Z9ayovAN558Lj4CB4Xn0grFDP1FUVYRtba51pwRJAaGIe1MCNFmDpTsttSLBgCwwLxZCUwmDbH301UdDRXFmM975GanN1I8yiUJihy2huqpAUpYNbvz89Wjauvv5JyVFvR8NM7Kla9MtQMGClUjq6e+Fx6fHYC/mUIjAFwmBcGAYOAQaAfAUNg9ENh/jHZEDgWgdENBcZHSZfJvlU3SJe3t/RAHmraxCNgCIyJx3zwjEoigIjoYzD6gzRE/Ppvi3SwyArbufwDX1USSJtQEoOBnY2AICmhxIWl9rARIDZCBH/5YxiSJn0KEPYfg/kPrn5RyQOmInL8mDMvlcTL7xJn7IT2kyx6nUPPzwoeHVARlKZtRIrFFlTLKJcO7Lzamj3MPB0QhAQiYA0EgeGLUqH0nzjy/qn0oCF15b4dSjY0FloEAiXn9HhgCgfvvwM79pTpF0D+zuaE9ARKx6fB08AWJB+Jfy+uuxc7wyRUSnZi/EwE8/CT6EDqhjaQPA5ItXT1C9Zrc/bw1KpeDQUH9TC9JWZcdruEz1uh89dCIp8N89LKvdv1uHfUNEm89DaYbp4x5PxV2Tsl683HpTZnj/b3jZuJnP47gEXqgPtnikgbzA9pJlqavlFoPsrd6154BOgzR/lTB5Sm9Y6Mk8DkeVpmlZ4SfEZDrb/2hlr4W5QreUFlAFNviJ09vDOcvf3gU7JQolFOtgIER87ql/UYBkLFkgtwfXda6UNYZwc/eBFlVp9CqgnXhshUKGeSr7gTu+R9JXb13c++dfftwpelbQKhtQmpJIWY+zMCQ9cC1kMAlCvByQvwTNaq9whHYHoTVQ1JSLtgY5rAPlSkyF3zsr62w049U4lm4HkfT2vDs8587SEp3Pj+8Zz2uX09QO7MuOQ2iVy8ckAfej+wKk8lSC42LzyrRKSahHFtHEerQApK1huPQo2TqWf5T0+RpFV3ALOU4xhl+K4FUCZl/+dxfB4sw1qmkCTh2Y42hWTwjEwxq8nZpyogkhjtdWW6ptWr5ojOTkg3c0Z6k5IX0VRboLwyzD6ZckSzz6Ea05/2vfogqti8qIftnJBCgrUzA+lmphkEDAIGgcmMgCEwJvPTn+T3fiwCg2aeGYGL5cDcM6VqRpI0T4e5nmkTjoAhMCYc8mNOaFNeMHhWw05u15NcQNMdR/zbCthBApCwQHDIfrZ/a7++19ab7G8RFbYxbOfgCE/TpmNSEWIjP0YxP3f/G0vz+nwQDmt5U+bv2yO1wtY+IzJAvAyavxslPrUUKYJmBs6Ukfd2tdtOhUrEHgoBR3Hx8RdHSMOdXLCjivSBI++fZE9bfZV04HwG2lRO8N6Y0uEVFotrsjwZuiEdb4bHA3d42Sg/94QKwT0wzCJ2jrp/KBz6fBfaUcq1HWkv3Ri7t6cT8/NBYAzcpz3IAZIhTOFpR9pJJ7wmOD8rJniGx6LqBvxA8Mw6kN5Bj4n2BosAoaKAZpouXn5Dzk+vhwaYE3binjAgAn8fqz/GO/L+mfrRA8w6mxqBA0rOQnlBDDUdiDeKc1mWlkSQK4I9R6goqByx7/NDGfz8qY7phtdEJ+ZnGg0JEqpcOA7v1w1Goa4w9GzHXCQY6IFBPFjaluSSPQgTtmb4WHBt2Hwq6P/gFRGrOGmHQd+s0pi9ik9bLRQk7S2aJqNrGn35PFlRxtmbQauftEMJw+euDddGE0abESPxYYUKqjrYSPx5Yi14hUbp65F+4249zR5bqkpGesox+7G6jhcMKVkW9sjGNJxG+ED0rw2oeby4dnyOz8SzA6aqDbjezuY6Hd7Z00/nc8baGctGUo+48PPKxpLG/KyN1sRz8LXRs6UTnxerTGorvE9o2knDUNtvL+sMpnCRWHOkfxv836g04trm86a/z1CNa5VYN5XZ1oZd39qIHqq7ec8gYBAwCEwaBAyBMWketbnRwQhE/ucVSYXpXWBzmQT19u1U9nVClr4c8JwpudPmS2HqQqlLnTf4dPN6AhAwBMYEgDyGU/QrLxCo4i9za2T8Ea6kBAkI/FHPP9htjcGbjZzgH+v6hzwCvH4igWknTBHBMT33CFJBg1n25TH8tMbpU36Y+Sc9/keuM9t6Mz8NAgYBg4BBwCBgEDj5ETAExsn/DM0djBIBp7IScS0vkZTVr8ppRR8PGMUQGAPg+NJeGALjS4PeTGwQMAgYBAwCBgGDgEHAIGAQOOEQMATGCfdIzAVNKALYFU555K9y4a7nB0xrCIwBcHxpLwyB8aVBbyY2CBgEDAIGAYOAQcAgYBAwCJxwCBgC44R7JOaCJhQBQ2BMKNzHO5khMI4XMdPfIGAQMAgYBAwCBgGDgEHAIHDqImAIjFP32Zo7GwkCyJ+f/vzjsmzLa+Le0yzu0qJn0cKvxClaioPiZd/Ss6Vy+dkjGc30GWMEDIExxoCa4QwCBgGDgEHAIGAQMAgYBAwCJzEChsA4iR+eufQxQAAERujaD2X61nUSWblfIjvzdVBaBraLizTZeckny6+VnKtuGoPJzBDHi4AhMI4XMdPfIGAQMAgYBAwCBgGDgEHAIHDqImAIjFP32Zo7GwkCIDD8t26R6PTtEpe3Q+Jbswac1SmO8s7imyXrprsHvG9eTAwChsCYGJzNLAYBg4BBwCBgEDAIGAQMAgaBkwEBQ2CcDE/JXOP4IWAIjPHDdgxGNgTGGIBohjAIGAQMAgYBg4BBwCBgEDAInCIIGALjFHmQ5jZGiQAIDJ/duyR0725J2LdekhsyBgzULfayNXK5ZJ5+njRFx0l7ZPSA4+bF+CJgCIzxxdeMbhAwCBgEDAIGAYOAQcAgYBA4mRAwBMbJ9LTMtY49AjC7cCvIE5/8PJm5/h2ZX75hwBz0wqif4iPVroGy5cwrpeiiKwccNy/GFwFDYIwvvmZ0g4BBwCBgEDAIGAQMAgYBg8DJhIAhME6mp2WudVwQcKytEZfqSkl581lZlr96yDnaxUneXXKLZN9455DHzZvjg4AhMMYHVzOqQcAgYBAwCBgEDAIGAYOAQeBkRMAQGCfjUzPXPKYITOnqEruOdpn75L/k3MxXhxzbEBhDwjLubxoCY9whNhMYBAwCBgGDgEHAIGAQMAgYBE4aBAyBcdI8KnOh44nAlN5eCV/9pszaskZC6wslpLtiwHRd4iDbI5ZJ5tKV0hgzTdqjYgYcNy/GBwFDYIwPrmbU8UOgt6dbDvd2yxR7B7Gzcxi/iczIBgGDgEHAIGAQMAgYBCYhAobAmIQP3dzyEAjAzNOxplpcKssk5T/Py7KCDwd0OixTpGGKp1S7BMnmFVcZL4wB6IzfC0NgjB+2ZuTxQaC7vUW6O1vFwcVdHJzcxmcSM6pBwCBgEDAIGAQMAgaBSYqAITAm6YM3tz00AkwnSXnkL3LB3peH7NAhzvLBvGsk+8qbpMfFVXqdXYbsZ94cGwQMgTE2OJ7MozSW5EnNwT3S1dY0JrfhHTVdAhJmi72j85iMZxukpapUWqpLpbmiSFpryyQocYF+2Y6fCD87mhuA5S5pLi/Uy3Hy8BH/+NniGRI1IZdHjGoO7pb2hmqdzy0wXAIwv4u3/4TMP9wk9QX7pRrX19vVoV19oqeLf0KK2Ds4DnfqgONcrzU5u8XeyUXs8d8IRzcPcXLzFEcPL3Fy9xFHFzdxdD02uXUYpHpXW6t0tzdLJ54bv7ramqWrFQQZUh7ZiJ1PdMKAucfqRUdTPbDYJS1Yz2zWWpmDtRI5VlOcFOPUFx7QNdvTaWHuHZWA3x9zxvz3x0kBhrlIg4BBwCBwgiBgCIwT5EGYyzgxEBiOwOgUR1k//SLJOvcyaQsKli6/gBPjwk/RqzAExin6YI/jtnI+fEX2vfKAtFaXHcdZn9817uyrZM6N3xFnT5/P73ScRxhsluxcK2Xpn0r1/nSpL8iRpCu+JnOu+/ZxjjS+3RmM7Xr2r1K68xOdyDNsqsy8+usyddnF4ztx3+jFOz6WvS/+U+oOZes7QbMWyZxr75WA6SkTMv9wk2S9+Zjse/UhkAQWWRZ3ztWSctP3QDp4DnfqgOO7nv2LZL7xiDhhjbl4+Yl7UJh4BEeIV0QcvhLEPTBMPPHesVov0hqbK0qkubJYGosPSGNJLsiEEmmpKgMBVAtNoEjyVXfLjEtuPdYwoz5Wl58tu5/DWkn7VMfwDI+VWdd8Q6JPu2DUY56MJ2a//YSuCRJIbLFnXaG/P/hcTTMIGAQMAgaBLwcBQ2B8ObibWU9UBHp6JOKDtyR560cSVlcgod3lA660CwTGTnhhZC86R+ripklbtPHCGADQGL8wBMYYA3oSDpf5+sOy8/H7pK4kf0yuftZlt8uSb903prv+JDCy3nxUcta8iEDzkDRUlMr8m78ri/7rV2NyzWM1CNUPW+//X8n59B0dMjB2hsy7/UeScMENYzXFMcc5tO5N2fHwr6QsK137RS9cAYx+KSGzlx7zvIk6mP7UHyTtqT9KS12NTjn7iq/KknvuE2cv3+O6hC3//KFsf/KPOM9bXFw9xcXHT1x9/cUrPF68omZIKIibkFkLjzlmT3eXlGVskop9O6SxaL80leZJW301yIs66WhthgLAUeaBXCHBMh6ten8G1spPJHfDuzp8UGyizPvqjyX+vOvGY7oTdsyMZ/6ka6K5plKvceaqW2Ux1oSbb+AJe83mwgwCBgGDwKmOgCEwTvUnbO7v+BCgFwb+eHWuLJe5bz0HL4w1A87vhplnpu9cyU1cLCVz5krjzFkDjpsXY4uAITDGFs+TcbSTgcDgbvn2h34me7F73wl5f293tyEwhlhsk43AmGJnL3ZIP3FwckIair24B0RAjTEVJMDVMuPCYxMB3e2tkvX2M5L3ydvSUlkgrXXl0o111dPZKYd7usTV29cQGEOssbF+yxAYY42oGc8gYBAwCHxxBAyB8cUxNCOcggjYdXZIyqN/k/MHeWF0i70c8JolufELpCh1odSnnBjS51PwEegtGQLjVH2yI7+v4m0fysHVL0pbnbUDOvhM+jnUFOVB9t+sh3zDp4pPVDz8B4b2uIhacr4kXHQTPAk8Bg816tckMDb//fuy8+m/6Bic2ygwjoZzshEYh7Eu2KY4oBqNvb24ePiKK75mXvlVSbnh2OlFnVjP6U/+SQ68/5y0tzZKe3O9HAaBIX1juvsHSupN35W5N/2/o4Eeg3eMAsMC0RAYY7CYzBAGAYOAQWCMETAExhgDaoY7NRAwBMaJ8RwNgXFiPIcv8ypoJthWWyk9fcaKg68lZ/ULkvnW49JUbZU+nrb8Ypl93bcg2Q8a3FVf0zDS1T8YJU7thzw+mjcNgTEy1CYtgQHD5ynOzjKl9zAo8CmSet09svCunx8TtM7mJtnywI9l71tPy2H7KXLYTuRwe7sc7rAMRg2BcUz4xuygITDGDEozkEHAIGAQGDMEDIExZlCagU4lBGjmmfDy07Jk69vi2V0vnmLt7vbiz89Sx3ApCZwme04/VyqXn30q3fYJdy+GwDjhHskJd0G7YQqZ/sTvpaGyVK8t6aIbZMk3fwepfviIrpXESHdHmxomNpUXSzcMHK2KJ4f1fAcXD5RE9RSvsCjxjooRO3snfGFHva9VZqWpcWfex69K3sbV+i7TBuJOP19iVlyOigUpWrWAB3p7uqUqa6fU5u2z+mEcGju6B4bCqDFXOL+dgxMqVHiI37Rk8Ymcpv16e3uko7FWzRtba8qlo75Gr3kwqcPrNyTmWgAAQABJREFUcnSH74JPgFaLcPMPkin2jv1kzWAPjICYBEm99Qdq4tlYhuop1eXS2VIvPcCDjZVaXLwDxNUvUNxgOuni5SNT7HD/doimj2j0a6ACpq2uSprgVdLeWIM0mk6ZMsUOVThcxS0gSLxxL+UZG2XnY7+T8uxdevZgD4zGMqhp9qfh/Fo97gqfAe/IOK3G0Vicp9U37HBNnqFR4h83s99ck2oF4kOVTltNlT4/pmAcPmwpIGyX6gg/CuLjQVNNjOEAYsHOzgHXSUtMkbH2wLApMNxx/zR9bIP5Jj0sZl9xR78RpLPnQH8N+qnwXljNJh1+HNmrXxY3/wBxhpFoG++x1vLn+DwCg2RaV1uLVi1pwWeC5B/Xc0+n9UyZ1sL1zKoiNBN1g5LD0Y0lfwdW1BqpAoNrswdpU+2NdVi/RTpfN6ql9HZbRAvXMz9Drn5B8ACZCmIxQD8/XBtDNa5pXj/XEjHowLjd7W1yuBfqEzSORU8RN79gNUd1RHUXO6xx2zNk1aKq7Axdx+zvFR4jgdNTcb/efHlUo7Et+7PaCxvVWwHT5/VXiRkpgdEBhQyfWyvwboNfhpZT1mox1u8R28ROWH9cg56hkfp15O8TPvuq7HRUPcnQ7sTIKyIG/WKkoTgHPigF+P3gIA6obOMXN0t8p86wDWt+GgQMAgaBSYWAITAm1eM2NztSBKZAqhv86ScSv2OjRJftk6jOQ3oq/xTpEidpsPOSj5bfKHlXTYz53Uiv+1TrZwiMU+2Jjv39fFECg8Fye0ONFG37RIq3rZPWqkJpr6voD35dfBB8+oVJ9NKVEnf2xeLk6o0gyr3/RtJAnrDiBEuU2swfEU2JO0wbXaH2SL7yzv5qJCyLmf7k7zUtgAM4INUkCkRHWMppkrf2TZg2btaxXf1DJBHVJeLPu0bn6UYJx9rcvVKDryoQJnV5mdKBYLizr1qG7WJIOHiERotvTJJELDoLRpELULLTA0SEFZweRWBET0Mg/W0JnrVYCjatkUoYRjaX52vQyDGd3L1A2qBsJKqEhM07XfxiEjWAtEdQemTraGlCtYwSnL9dijZ9IPWF+zWAs0PahLO3nwTOSJHYsy9HAJYvGUizqdi/R08fTGAUbHhPdr/4N2koytXj/nFJEnPmpShPWyYF69+TjqYGcQC5E77gLElGlRdvBKdsjWUFqGySieCPZFIGKnWUKj4kjI5srgh6PUJiJGzuUlTTWIlgOljxtgXT40Vg+E+djsB4GqrTHJDaokMSf9aleLbXil9s8lFBaC+MpGvz9kr1gQw5+P7zUrh9nfhFxyNYj5AGBNt1xdZ/iz6PwCCZ1FxRjGeQK6XpG6QKVXHaUda3s6lOoSCh4OIbKp5hMRKaskSCk+ZphRRXn4GlbEdKYHBt0ly0Li9LCrd+DAJqN8iHUiGJwcbPiotviJJ4MWdeiJ+zrTUE0mGo1o5nzOuvyt6Jajlrcc85Sgz0dHVqdyqrWDo2KGk+1uQyJbNIapCIYstb+7rsAanZ1FcqOBKG2zQ69QZROFQ78N6zsuelfylhwuMxyy+VlBu/p6QiX4+UwCARQmKSn89qECJt9ZX6OSIpcWTjc/QAIRG58EyswXPx+fRUjNiHhBvJ2ExUw2EjURh12nkStfR8yfv4dSnZsU7sXVxQQckb1Wduw++I27Wf+WYQMAgYBCYbAobAmGxP3NzviBCYgj8iA7ZukeiM7RJ3aIfEte4fcF6HOMu6pEsk89wrpDMwQLp9Bu6iDehsXowaAUNgjBq6SXPiaAkMBre9vV1SsWerlO/ahIBpl1Qf3CftTTUwTKxGNGHt3rtgx9jV3VcCE1MkNHUxdnPnSlDign4lxqa/fV92PPWnIfGm+mDBzd+RhXf/Uo9TKbDl7z+QjJcf1NcOzm4SPnuB+MXPknIEm+X79yKoDhSv4EiZedVdCFJuUtVFS1WJlj4lQdAApUYLyIK2FmuX/ciJOZ8ndrqpLgibezoC1NM0SPYMidJugwkM75BwqC8uRDA7FRhsAQmwX1rqq1QlwBOcECh5BYQiYIxHpZDFEpS8QHwRdLv7heh4DLapXuE1laavVywZ+DaBcOAuOhUhrh6eGryHL1gBVUC95K/7j9QW5+v5gwmMnDUvyY5Hfi2VOZZCJQAES0jKUmmvr5Xyvduks7NL3KBYiFpyHoLMb0PZEaL4VGXvAPmzXoPoRipAEKy3NTZABdCl89i+uVFJ4hOIZzkXJMiZEhCfAiVHUj/BM14ERhgqjpAk4jorR4AbNX85CKaz8XxOB3m1zHZ5+pMERMmOT7Qkb/G2j7AmdktY8jzgngR8t4D82a39BhMYDJSpnGmDOqckbZ0+i9pDWVDEFEhbMyqXQMnARgWLG0glkje+U0FOgVAISz1d/IEFd/btoR5iG47AYLDNz1BLZRGe/UYlr6pATDWVYz6QEF0tDTqOPcgFdyh3SJiEzFkgwTMXStCMuSBkogYoMThWNxQL9QXZKN26XiqpVAJh11JTgefZAAVJX+oMiEEqR6g+CEpOBTk2TwIT5qLii1XWNPs/T0Ll81upQSljtvgVq2TRN34N8i1JXw/+tvfl+4UkZD3UT2xJF1wvi775W1Uw8fVwBAZT3EiClu/aiDW4AcQRlRJYgy2NUNwA80EEhgfWLD/joXOWYA2u0OuimohtcCoa1RbhsxcpZsSkdN9OKE8CtALKzCvvltkoa2uaQcAgYBCYjAgYAmMyPnVzz8MiMByBwXKqewPmy8GZp0n5zNnSnJg47Jimw/EjYAiM48dssp0xWgKju7NVVQKZrz0sWW89qUFSJ6Tq3Ont7dvtJZZMB5mCXWs3by8N6BPOvQYmjHdi59RLd06/CIHBAMUTagvK+VuxY9vWgHS1gDDxDI+T5Mtuk2krL5eaHOzGYxc9f93bUgYCoxuBHAO93u6efll9/zNnygZUHa6eXiANEhCoz5c4pLGEzT1NuwwmMEhQ+IDscHRxU1KkBUEuq1zYAv8p2NV2wI6vOwJeKjEYhMeddQVInDk6HqX9bQ3VGmxnA8OqnD3SCZVJD3blSW5Q1k8Sww2Sfw+k9BxGkNqAnfG2eiuYHo7AYKqAu3egpiM0ITXEwdVNPLCbHwkCY/Y1d6mcnrveRVs+kMLNq6GAqQI2HdYzpOHloODRzhHVQPDFe/aBmoREyLRzrhLnvvSC8SIwYpacI1EgigpA3hRA6RM8Y44G3lOXXYwd/0v6Hx//wRSK3I9ek8JN72k6QT3UJdELoaYBgZS//h0pTt+k/QcTGMS7raEKAfQByXr9USlGKfBOPIeu9g6tWmJ7plQHUYVhj5QRJ2cn8QyOkOlIu5q6fBXShQI1VYUTDEdgkHDoQtpFVdZ22fvSA1K+BwQTPDq68MW5bCkfU6AisHNwRtqEm7hgXQaCvEi69Bbcz2kDlBiaNgIlVMn2jyT77SelNn+/jsW1RFLH9iztUD7WwQlpSd5MgQmV8HkrJPHS2zRFidc90QSGTXlRgGdTtGWNtIO46wbmvagU0485L6yvUSVlh8+oH1KjfLEGqfiIX3m1Hh1MYPBZeeEeXbz88ZlBSg3SUzzxOSL5k7TqNv0dYRvX/DQIGAQMApMJAUNgTKanbe51xAhMQR6xW26O+OUclJlb3pO51VsHnNsjdlIGL4xSemEsO18qzjhrwHHzYmwQMATG2OB4Ko8yWgKjoSRH6vOzUeHkJTm49g31inBxdccOZyDKXYbA68FOYeMOcCu+GAx2ILhPwOd9BgI+r0ikBUTECyXolK3XYbe7Ki9bz+G5gbGJqn6IPftKDZJ5YLACgwSBZ0Aw0iyg8kCw7grvCv7bPTBCIqAQoA/GoXWva2CkqSOotuKB63OmESkk/y5elvJLg0lI9juxG8xUjpbGRigxAhGkJynZEnvmKr2uwQSGPRQgHlCPOUHV4OwBggY+CwzwSZI0V5SAVKmWjpZmYGOPfv6Q7S+QmVffjaBxuY7XAj8O5uyX7PhYCRamRzA445j0qXBGCgo9Epg609nciBSQOmkFSdLV2qLnD0dgOKNUqJsXsbHwobeHM4LswBmpErn4bCg/DiKo/4+U74aC5gDSUpi6A2zUtwM79bwXNvphdAEfenzUlZfgujxAqgQoGTP7unsVS/YbLwIj8YLrJPHy2yXz1Ycl+4OXxTcsGiTVVEm44EYE87dx6v5GUojleHPWvIJUGHhKwMxzxrlXSTTIjqzXH5GcT9/RvoMJDJ5XiRSa8j2bJf+TN6QYihqmDjm5u4sXPD/coMxhI/nFFI1mqIyoanDHuos67QKkMyC9A6lCPn2pFsMRGPR8qMPnpxRqjxxUCeLa5xpy9fSBMgZrGs+erVNxh4oCKVYdIGf8cO/xK6+S8PlnIa0jXr1B2I/rlilARag6VPDpf6SxqgLeHG5Knrnj+u1B9nEdteG6m+AxcRjrygXrNRLkztybvy/+0ywVw0QTGIUgz/I3/kcq926XCqi4XEAKukINwrXqBvWW7fcI118XUr6a4UtSV1ooqsRAH6aK2SrSDEVgsJ8LPgNuWM+u+HLGv5kCFLlwOT6HZxA60wwCBgGDwKRDwBAYk+6RmxseKQJ22Emyhwx53nMPyVkH3x5wmuWF4QgvDG/56MybJO/K6wccNy/GBgFDYIwNjqfyKKMlMDRQ2vi+yvIp6/dg6gVSLZh6EXXaObq7T9yKtn6CXeFP4Q1RAJl5oYTPXKCSewZgkYvOVf8MGg5mPP1/sueNJxRqBs6zLr8D1VDuRSBDosGStw8mMLib7IP0De/IBASQF0jU4nNAFjgg8HRW08HujlZJe/oPkvvhyxrISvdhCUK6SWDyQgnFjnxAwiydj8FrU2kuzP92IaB/V0ozM1SJ4RceLXNv+b7MuPgW7XcUgQGywRkBp3fIVImGFwd3+UkWtNVWS966N6UCaRtNSBFgWoIdguGQGbNl3m0/VNNPDlh7KBuB9stIIVinBE5rQwMCZg8JQLpD/HlXI+1hOrwQOuDnsEs9LOqKchDINiLVoVWvZzgCwx3VYjxDopF6sERioVRgKgxNSZ1gOklTxlwQR7tf+ofUFR5Emkm9EgP+CfOgFFmI1Iyl6hXAiVqqioFPnhRtXiP5UCaQlCHGied9RRb+1y9hZGmlxIwXgZF6w72y4M6fyrYHfirpL/xLHKEeYGrN7Ku/jlKo31csbN+oRNj28K9l39tPK/Hi4DBFUq69R2aA6Nj+0M/0ffYdTGAwbSYH5EjBxveQSrNP6pA6QoUNU4qiT78AfhFWsEtTz7yP30Qqy04ldVjiNWBqogQhtWPaWVf2q3WGIzBouFoA8qgEPhU0pm1tagRREgFzyWSQSyugLkjQW2pEulDhFnhjHNgtTUiFcsGzC5mzGMH3mUihOAc+JrHar2r/Lsn58FUp27UBxEiWdMNI2x1pLjTBjVl+PsgRT2ksLcDndbumUzWi6hDVPTFLVsqCu36hyg4ONNEExp5XH5A9rzwgzUx1qauVINx/AFLMQmYtwhpcgt8jVkpOU+khXH8uiL53JA8+Mbx2O6hg5n7l67L4G79RDAYTGPRm8Y2IFY+wWJkKLwyme3H904OG65+fNdMMAgYBg8BkRMAQGJPxqZt7HjECdtitmvvIX+W8zFeHPKcdhp4ZYYtlP3KaaxOSpC3GqhowZGfz5nEjYAiM44Zs0p0wWgLjAHaND7z7LAwj92uwFzx9toTOPQO+BEsRXC1TIoFgVkAaT3l8KVQGRWkbEOzFIzibIbErrtAddPYZHHgwjWP+zd+VRf/1Kx7ub4MJDHpWhCbPR0C3DOkM50hEn7LBdgKNQXPXvqqeBoc7uyEwsIPhYpS4h4L0APHBKiMkOVj9oKEIBo+5+xAAbu5XgvihgkHq7T+E3PwOHXIwgeGC1BA/TTWZB6PAc+EpMB+Bord6YJTsWAtlxVqkh6zv9xMIhXfE/K/9BAGlpeioyNwhe195EHNukhYExg4gbnyjpmt+f8yKS9X7gikpNfC0KNi0Gj4J20A2HEClhiq9nuEIDL+oaQisF+lu/dSl5/UbK9rwKYXvwKF1b6iyohcBL5UX7iA8vEgKQeHARnyakIbRAJKjYu8WpOHsVDNQHkuGx8jie1Cxxj+UL8dNgbHg1u/Bh+F3sv3fv5Q9r/1bupFecBj/m4PnMueG/0bFC1RHQXpMJ5QpbTCQTXv897LvvZfEESke7iAhZl/7TaR53Cib//4/IMke02sdTGC0gkTb/eK/FA+qhqhW8IfZpT8IgKgl5yo5xRNpuEmz1DJ4rlCB1ITzvKCY8IUPQ/LlX5UYKD3YhiMw6pDikYXyxcXbP5YmVAtxQEAeNHuJ5esBEtA70jJYpV9LGdQgXEecswtqGF94qoTNXSbTzr0WygmLhKOPxh6sJfq8NKOajDvURUyBCsVYkQtXIGD3hBKqHL4Ymap2oPEuW1BiqsSt/Io+c76eaAIjH4RR/vq3UPWkSQ5jDboFIsUjOFo/n6y6olWOcM+NqCLSUHQQPhkbpWT3VihIenm5+nti6b1/0H8P/j1ChVYYjHiD54BUhRkpCU7TDAIGAYOAQQCCy2/t7BlokdyHypQe5P7SwNv6HWuwMghMSgSGIzAOyxRpETepcfKX9WdfJ4UXXzUpcRqvmzYExnghe+qMO1oCY+8rDyH4fgg7p2Ua1E078xJJvOxraljJXX+b9Lu9Ad4UCM73vPBP2fvWU/CsCBJWCZlx0U0ILL+lQA4OPEZKYLDKRxzmnYYAjKSITb5vezpW+dQ6BEeN6gHQC1+HVqgh2lGBpItlG5EywmtrxT3QOLC5ohQpGlUIgq3gbjgCwyckQqaeeTnIiwtgJjhdq1Hwvhl0cUwaY+596UEEXNv0kgYTGKU4nvbk/0kxiIFuqBr8QBzEgNgJh6zfHyoMqjloLskUHKo1iretkUOoplDbV2VkOAIjFOaVcQhyaXbJCigsJXpkIyHEtBSWbJVelB9FEEm1CFNpSP6005AUAXoT1AIkMUgONCP9wFbRYiIJjMUo7Zvx7D8QYD+Da6zQkqgzzr9Wki7/GnCP1LQlrR6CQDcTgXzOJ2+LB9aZOwir5Mtul9gzLz6KwJiH6hqssMHWgue149HfSPZHr0L10irOINFizrhUvS38sLZI6rAxlaEW6R4kc/I+fAVqnZ3i4OEh/tjpT7nhu5Jw7le033AERjUUExnP/hUKjE+gvmgACRIkCSBZok87H0qZqUh1sNKbmL5D3IvhbbHv5YekHioKd6hCaGKZDCPKYKiJ2ApRBWcn1lIZTGB725qgNEqW6RffBKJjOa49Gv4n7vCm6QIh1YZ7aJHDfQatDlB0UOFERQ3bRBMYXGf8HNLfhT4dLCfbBtNZvk/T2naW9sUapLlss/q/YD0ilcnm6UGi8/MIDCq04uFhM+286/T3gx8IKdMMAgYBg4BBwBAYZg0YBI6NAP5gClu7WqbDFT6yMkciukqG7N8k7vLe8tsk5xpLqj1kJ/PmcSNgCIzjhmzSnTBaAiPj2b/I7uf/IS2oFNCFgImVHsKgpHLx9EPuvSfUDlMUy54umAgioKcpYgFKWlKh4IygKfmKO2X+HT/WPqMmMLC7PhOpAclX3a3Bvs3TwvYQGfxb/hZN2Hnehx3zg1pStA1BEYMjlhVlXn0nfmrghACewWtPZ5sOMRyBQSNBlnmNReqAC6pz0BuCjcRJLzw/KOff+djvpHDHen1/MIHB4HU7qoYU7YVHEJQWwfAhSEZaBFNrmK9Pc1A2NWgEqVKw8R1h1Ycq7KKzDUdgREIJM+sr30JKyCL1E7AFqXoyvul19nRqpQ3uzDdVFKoag4oUem50ohIGd8ZZgaOtsR4mk23ABzvlfbvfE0tg3Ae/lOehqHlL6g/tBYmTI7FI24lBgBqAKhr+SD1g6VBW38gDCVEERYIffFZ8Y2dJ/LlXSsT8ZccmMFA6duuDP5VMVHI5jGfhAXVN0mVfBQlwi/ox0JuBjYaYDKhJQOx54W9yaMuHMsXJSfwjYyX1lh/IjAtv0n7DERiVUN/o2ti5Ds+3Fakq/lB5LEUKSZJ+fhyQHsHW29uNz0+7NOQfAImxFt4WlfgMeUgoUlZIvlCJwUZVyPZHfiNlKEF6uLtDIuYsQonf7+D4GZouYauOop2P8W2iCQx+PnuxButxfzW5WUhXgncMlCIkNTpBYnRxHbbC/wVrsBVrkCazXXhta8ciMJiKNhPk1axr4dNCDwz4u5hmEDAIGAQMAobAMGvAIHBsBBBA2OMPEKfqKpn3yuNyRv4HQ/Y3BMaQsHzhNw2B8YUhPOUHGC2BkfbEfbLzmT9rugQDWicYEDq7uIodFAhT7Oz7cSOJgIhXOhD8tiMAoazbzsFe5l3/LaQf3Kf9RktgkLCYe/29MvfW71s58UfMy4FZqrIblR6a4UOR9dbTUoRdagblXe3YgcY19+J4b0+v/rsHZKtWfzgM+SSvGW04AiMAqovUW38Av4rrkTJjr/euJzLBAWOwNOS2h36Byhlr9e3BBAbTB7Y/9HMpyrAqY9AfJPX2H0HRcT7ux15TXngi8aHp4qFP3kR6xG+lHGaHbMMRGDFIa1lw58+QSjBP8bGRSnoyvjEwZiUMBr8H3n0RKotDKN/ZqCka1pwWPlSusCoEd8lt5AXHmGgCo3jbx0gH+lTTkUrhLxIGvEJRISZi0UpVGrDySPH2D6U8Y7OWkg2BaSrTB6Lh8xAIv5PBKSQDFBhI1dj8rx9J5rvP6fP3DgwBQfBdmK5+XdOh7PrWlkWK9ahPxvZ//xwGtm+p+al/VJz6m8yAqSTbcARGBUirbQ/+DJ4ia7FWeqCAgDcJFDKOCLqZ6mR7VkyV4XrsBnHS3tysxBh9N8JBSi342k8lYuHZOh/NWLc//Espy0zT/lHzz1DfkNCUM1QNZRtPOx/j20QTGKxm1NXeJAffe1kNgan6IWHBqjD8fB7G55NrkeoRaw2iehDet7VjEhhQ0cy74dsyH/4pdvC+4GfKNIOAQcAgYBAwBIZZAwaBESHAVJKEV56ThdvfEa/uOvE63DTgvFZxkfVJl8j+c1ZJe1CgdPtapn0DOpkXx42AITCOG7JJd8JoCYydj/22n8AYDWjDBR48PpwHhguqbMzDLvM8BP1DtW4oKVjlowLeEYUb3sfudBp20Jnb2SMeKL3q6OYBc00ENihT6eDsoiRBEyT6Dah0wDYcgREYO0PnTrjgBu0/+Bure2yF8eTnExgfyfYHPyMwImDOOP+rPwWBcd7gofT1IRiD7nj4V1KWla6vhyMwqFBYePev1OdgqAHri3M10C7e+iHSU+DFAF8EVtZwQclOV+DDVB5byVDi01pdphUgmNbANtEERvXB3TBa3Q2zzRflEDxB/OEF4YO0gGkrr5E4VKth+VBWtGmAOqOxqlymwrtiKtJA6PPggUocwxIY//yhZL7zrN6bN/rPBTk1+yvf1NeDv7GCyNb7/1cOfPS6HiKBkQqDVpYkZRuOwCjfs0VNSWmKOpoWgRQSklORi1fq6fmfvg0FBtYGPErYolGFZ+HdP0f6kKXQ0DdH8G2iCYwaKKOqDqRL0cYPpBieMe1YW91QbLmjZKwL1Fos3Wtn74TPpzPWo6s0IY1EU6j6SMbR/B4ZAQymi0HAIGAQOKURMB4Yp/TjNTc3VghMwe5J8IZ1Epe2WWKK90hU56EBQ3fCzHNfwDzJSV4iZbNTpHlG4oDj5sXoEDAExuhwm0xnncoEBtNCsuC7cQilM5vLchHUgpiAsaMz/AACoqehDGUczB9RjQDBOn05epD2kYeKJSW7t+gSONUJjMItH8l+pGVUI4BsgMcFA0dxdhR/BO++0dOROhEkjvAZcUV6jCsqmpQiwMzDTr/NI2QiCYwl9/weFV1QvhSGl3tf/Kdkvf8i0mICxQ3Go8lXfk2/0p/4o2S/A4+MFvh6gIiZccH1krjqdpAXEVA4OBkCYwS/2CaawDi45lU5gGdZX5AFf498EYokkJITAM8Rf3iPOLqTaERpVfh+uGENMj2I/iaHkVrDZggMhcF8MwgYBAwCx4WAITCOCy7TedIiADmo+6E88c05KLM3vSMpNZapnQ0PCGil3DFMSgLiZPcZF0jlGWfZDpmfXwABQ2B8AfAmyamjJTDSnvy97IQPBtNCWD2AhIAvFAn2jq4quZ8Cg95jtako68mdc7ZRp5AMo8BgZYn0J/+I8pKvSDtSRyhBD4xLFL9pyUpeeKLagb2zO3Z33cTexVnNMrNQ5cKmmBh3AgOEwI5HfilFqDJxGMoQppjMuf7bErnkPJQ69dK0AuJDYqELJp/569+W3c/9VSoO7OHbw6aQDKfAyFnziuxCWdI6VHdob6wRn5Bw8YX5IyuheEdMQxlV3z5sXLH77YLyoa/KflSeaemrgjLRBEYH0lvohZD+1P9pqgfTOhyhEklGyV16qvD9LBAyTLdhRReadzIFhGavvcDw2AqMUtny4I8lE8G04L9Xnr4BOPcbMAm9Qz0pbH4kTG3ohG8KlSAZMM3MRRUNpnSoBwYUGzMuukWfzbAKDKSQbH/4F5KP9CI+e3cfX+CeoJVimO4wZcqx0x28oTyJO+dKmLMm6Xz5G95RE9IyKI74eYxENaDU236AFJvleIZI7Rph+sRICQyWHmaqR9abj8Go9l9QLZXpdSSBNFr0zd/CiDRSX2c88ydJe+qPav7KN2auulVTx0g+se2BMemelx/EmipHOlq1BKB8rF/CbP18+mANssIMP6P8fHINZr3+b8lCmg/TmtgMgaEwmG8GAYOAQeC4EDAExnHBZTpPZgTssCNmX18v8559QM7KeWcAFMjyhajbQersfGTNWTdL/uXXDThuXowOAUNgjA63yXTWaAmM9Gf/JOnP/U1aEFAebmuT6SuvQFWRb2gpTgcXVrs4NoGhnhmQiLONF4HRAjPAbZD509egtxskqq+vJF5ysyRceAOMEGE2Cs8BBoo0SqRxZU3uXuzu/0vyoUxgG28CoyRtnex4/DdStGe7HEZAGIgdZ3ooRCxciQAwCtdoVQ1ph6FhU3kRKk28J/uRJlGNiiRsXzSFJOutJyTtid+jDO4hkDvdMnXRCph+/heIqJmKj52DFUhbJoq1IC+ekSzM34JKJGwTTWCoFwIMKnc8cp9kogRpF0q8sszszFW3yAwoLXaDUMte/bI+VzeQL7OuvltmX3ePekCQ+DgmgUETz4d/Lpkguw63t4s71iZTg6bB38QLwbibnxVwd0Olw2dBE86sNx6Rwp2fip2rq1YhSYWp5vQLblRshiMwKjK3g3D4tRzC+Xz2ATABTbzsNvXzcHRBahPSmo7V7JHS4wzfGZsxa8Gm91GF5HdSShNPXH9YUqrMvOYbEpa6HAaWAeKA9IuRtJESGCQHG8uKJHfNC/BPeQaVQSp0+OMlMEiEpj/zFzWJpQfN9HOvktnXfwOlecN1DcJYRj11WC2Hn9E9UN/se/spQ2CM5GGaPgYBg4BB4HMQMATG5wBj3jYIDIWAXWuLzH/0r3JO9htDHdaSqhuSL5L9Z10qHUFB0uVnvDCGBGqEbxoCY4RATeJuoyUw9kKpsPc1lFGtQhlV7MhPXXSWxK68WgJQScM3Jhm7pi5ij53kZvhJsLwlpf8tVUW6o++A6hr+02ahesQcRX4oAoMmn/PvspnvOWo/lv3c8vcfSAZ2bNmG88Bgacwt//ghAp6ntb+7f6AkXnSDxCMw9QhCgOQVoFU2WlFKtRrVK+iVUQJVRGWfwmG8CQzOt/ulf8Lsc5M0o3Qkg+bgWUuxa346ymQuFc+wKGH1iEYQDMUIdMtgYFmxZ7M0VFjVnL4ogZH55qOShioptcVWSh/HS77qTgmcQc+ISCV9WGGmNi9TvURKd3wiJTAmpeqGbaIJDM5JBcRelEk9+P5z0oKUkkYEztELlkto6hkgeFbj+rYK/Ss8QqKRPnIrPClu5WnY3a85JoHRWluBQPrPkvvxa9IGwmgKjCNDkhfhWSyDj8QSCYifZY2Dey/L2Aij0E14bhukpjhP3Hz8xXfqdJkNwiAOVVHYhiMwqg7sgvoFZVR3rNPqGh5QE8WceamaclJV4R4YBiWTo1YoaQZh0oxnzs9PD8qg8vPDtBh/KBXc/IJ1vmJUtNn9wj+UWGkFWeMD0iX69IskbN5yLbVKFQYNMukhQY8QmrWyeYXHSQjWGkv2sg0mMKJQWSjp8rskMGkeSLUINRftAuFSjesv37VZytLXw3djq5r58vzjJTAGe+mQCJ15zX/huqbpPbJCCyvhEE+mOhVv+0iKSPrg+bAZBYbCYL4ZBAwCBoHjQsAQGMcFl+k82REYjsBoF2fJCkiV3MTFUjpnrjQnGi+ML7JmDIHxRdCbHOeOlsA4sPol3ZFvKDog9SX54hsRKz4IvKIWny0xyy/C7qkv5N8eUpK+SUrTNkjF7o0IQNLwPsqoopwhd7dnXnmXgnw0geEkKVACpN72fXF09kTA5q79viiB4Qpz4NjTL5Spyy8FgTIHQW4kKm8UaYnVPFT4IDnQikC3vb5G5xtvAqMmbx8C8ReAz6dSX3gAlVpgXohANmD6XJT+vAZB41yUUnW1dvvffApB3C6Ura3tLyM51gRG+OyFEnv2FVrOk/h0o1pLI4Lnos1rUKnkXWlB8NuEErS9XZ2Kz5dCYCBwPbT2VS0pWwW1QeXBfeITGgmPhGAE+KWqBAiAkoUEGY09Y1dcptc6HIHB4/veeExohtkMP5BmkFruvkHiGRqtqRpRSy2zzBYYmR58l89sPYgHmJ4ilcEb6yhgeoqWUI1CxRO24QiMuvz9UJE8AQLjY2BciMohvUghmQECa6FMXXa+jucIJVNrVYWSV6xoU7lvi5YWdUaJ1+DkhSBo7ug3aC1HVRaqYyrg39KIcriOML30iZ4h4fPOlITzvoLPkKvU5OwRlu4t2vyBpkvxOiNwPPW2/xH/+Nl8eRSBEQhCMnz+2RhnOX6yoskUaauv1vWQs+Y1aSo9JC1QY7BUMtsXJTBilpwjcSBCA6engqCZg2dajvK+RZL/yVtClQlLIDdDWcXKLGyGwFAYzDeDgEHAIHBcCBgC47jgMp0nOwJMIwn+9COJwc7NUK0HbuM1MO+qD4+VpqlTpSM8fKhu5r0RImAIjBECNYm7jZbAKIVqoAQ78jR2LMbnmekObl5+4jM1AcFjsuWd4Ogs9UV50oBd6qbSPK0eEAyPhYDpcxCkXYIA82pFfjCBYQdPgcjU0xAwnakS+DDk8bMdL4GhHhgo90oPjDZ4SLAMKEuf+iLA9QgIFycvH6gJ6lBdoxyeBnulnkaW2F3uQfUStvEmMJpRurNi7xaU/vxYireskTqQKSRrvFDCMxABsUdwhFZJaUXAVr1/L4I57MJDAdADYoHtixIYBz94Cbv2f4cHRq4wTYXKBb+4JA3aPQLDpRvpGZTu1yFlpTYvCxg2STf8H2xlLL8MAoNzl2VskPLdmzWlpnDnBnHBc3SGwqCtrQUKhXYJR4WOCCgHuH5YRpRtOAKDa4u7+1QylO1cB2JkjzjAf8EN1TACoHTwxbph62xqlEr4TNRBtcK14uzuLqGzl2CuFfoViL5swxEYVCbxuZNQoIqhCWvQHSoIKoP84NPiERyO9BAX4N8IxUSuNJTAhBbr0wFVc/zxjJgaEgO1h80Dox6qiuLta1VBRLKD690dRIc3iEUqaviZaqkuhVnmQalFBZXWhjr17ohFpZaFd/0c622uXnfuR6/Jruf/JrUk1LDW3H39sR5idByfqGnIDLMDgdakyqpGmG62gjRqxLVTGcJ2vATGnpfuhwfG/dIMYqKjvk78p8bj/pN17btjDXaiDDs/o7U5e6UG67ADiowelP61NUNg2JAwPw0CBgGDwMgRMATGyLEyPQ0Cumtihz9A7JAzP2TD7s5hByc57OiAn474chiym3lzZAgYAmNkOE3mXqMlMJoRuDRXFkk2TPyyoSLoRuA4xc4RqSNO4ojPLwMdGnn2gDTo6erBV7uW6IxbdoHEn3cVSI4U7PqmKPSDCQy+6eTugXKenlrGMvXW/9F+x0tgtCNnPvP1x1CF5C3dVW8EYWAPw04HXKO95tbbqf8G5+/u6NAqJIe5s9u3uzveBAbLvNK4sHjrGuT234/yqBkq0bcD8eOoKTh2wHGKOOB3oQMqp9AjoBnqEBp6sn1RAqNg0weo2vG01BzIkAaoAKissAcRYO9gb5k+AodefPV0dSNYb9X0DSoFbO3LITAOgxTLEZYxzYYPRs7aN7HULJ8Emncy6I8/80Ls4l8FL49ZqKYyQy93OAKjB9hqic6c3ZKJ9Kg8lPXkGmbgz/QLB65pNBIoXe2dSnJxrXgHh8n0C2+U2HOuFs9gKEH6UjGGIzC6QLY0VRSokiPrjUelLDMduDto2VAHJ5T2hUHpFDx7pkr0IG2mu7NLPz/+0TESa0s1iZ+DSiwhel0d9EmB8oI+KdlvPSk1ICroYUKPDEeMx3XEcXpg8NkNjwxxAGZQZcQtPEvm3/ETCexL5yrc8qGuier9qEwD9Q2JCZbTtQdxYm/f999jYOAKE06vsFhh6k0t5ursS0k5XgLjwOoX1Ri2AYQJq5DwWVJ1NAWmo/bAgPffi/lIpvG6lTw7Yg0aAkMfv/lmEDAIGASOCwFDYBwXXKazQcAgMJEIGAJjItE+OecaLYHBAIxBS9GW1eo9QJ+GZlQiaOf7zfBI6CMB7GEeaIcAyNMvQNwDgiVy0Tky9YxL/n97ZwFfx3Wt+yVmZibbAsuMMYXBiUNO0nCatGmSMt3b9t725vWV4fWWuU3TNMwch53EiVkG2bJkyWJmZh29b+2jo54jS7IkW7LsfNs/H+nM7Nkz85/RkfY3a31LvEMjkb8fZaDppOTAI/8ruS89iPD0BjyNtppE6mR6xR1fk5X3fc/0m6yAocdYgVB/fSJdc3i3NBdrFEE7nqJbPRx0gqrH5+HtDdNAlAzFxEn9OloRtq5tugUMCybcFphC1uUfgBni00gR2G3SILraW/CUG1EWToI0HI3IiDZRK5rSUZO9a7jiw8kKGA14ql0JH4ea7B1Sl5MlHfBIUH8LW4qIMyp5KB9fVMjQEpbdSF9pri4fftp+OgQM3Fi4RxoRTVAtBx7++bC/iblgeHHz9pHMK2+Hl8e9SCuJMiVgdd2JBAwVsfR+Vl+Noq3PIzLifUQXVCNlpx4+FB24HlbRSEUBU1oW4pqvlptFNE/82o2I9Fgr7l7+JlVD93ciAUMFkz5U8VAhpuSDV8y1b6+pNClCep8PwKBUm5MzRA1U3/D29xMfpMmEzFsgCdhfePoyc356f2jTFA71ilBz0WIIdppeo6kXna2t5h6zlR1VAU/FQRPtERWLSI4NMu+SmyBGJJlxGotypHIfIlyQTlV/ZB+qg9RYDTaH0oY8/YPEyy8AUSCZEgl/EK3GUrp9C9JIrGlXkxUwNPWlytyDO6Uex9zV3oTr24TLbBXKVDzR8/eDiaregx0QIZsqS4Y/XyhgmMvGFxIgARKYFAEKGJPCxc4kQAIzSYACxkzSPjP3NVUBQ58+q/BgjBSrSyBivAUx412kY5RLW125eXKqRNx9AzGx84Uh4lJMltZJeMZyichcZQwK9SmzNh1LS3SWfPiymXjV5FnLhJ6sgKECQXeLGheWSOG7L0klJqXqb9ACkUKbTka1GopPYBh8H1aZEpalyLOvRiSEtukWMHQfeu7qKdBYdATpJDulYtc7CPPPMyH++tRcPTHC0hcjauUmmKXWyqGn/ii1BUd005OOwNDqIsqnYu8HKNH6mjTBrFMjMWyTdTdv+JVA3AlBSH/0krXwUMBkdfdWbIMJJtrpETCsVWs0fWPn774l+xC5Yt+0ZOrSW78kyz79bUz+EUkydI+dSMDQMUwkDsZVfwqrX8T7UodJfGdTFSINbKKat/iFRokfzC/jV18Ab4j1JuXGC34ZThp1hGum7UQChvXnZ9CYaer9qWkppR++ZcwxO5qrpQdikTYVkNwhlgRExRsz0Uikq4RnrDBVavT8bPuz/Tx2okJME+4fvddLP9yCqIYiCBstJnpDx/OE4agPojaCUxYg5WUD7q0lEgwDUvWm0aYlYntacU/At6b4/S3whzmEn5eyYdPPQBxHIPrHrrjAeIwUvfcCxMdfok+l2X6yAkZPWyNEs3op3rYFvhpvSGt5vonEsJVJdcPnhwc+PyLgBxOzbB0+Y94yVYJo4mlw84UESIAEpkSAAsaUsHEjEiCBmSBAAWMmKJ/Z+9Ac/OIPXh6OStAnyUnnXSsefoETOrG+LkQ0aJUAeEjUHz1oKh30wtwQU3OzvU7AVIgITJgnwcma2x+PyVecw9g6+WqEwWBjISZLyPdvg8igzRkh61pJQSM2tOlT5kJUiVCvAm0ueDKrXhrxay4170e+6LgD/d0mIqQu54CpptHViMkhUku06YTTGcemhqNByanwUgjGhHUvqn4UmvVeEDYSz73SmFrqgg74FhTi6Xx9XpZZ742JbBIMQXVCOVrTcbS/CgPa/KISTX+bYaJtG52Md2HC2qo+IfBe0IotfZh0agiGPu0PiJ+DqiTnSA8iM8qQ9qHrtQUlZ0jyedegYkOyeV+XmyVF771oRCVdoPvRazmSt+mMl4GBPvN0vqk0H0+/D4hWu1A+ln6rSaeW8tRr5x+ThH2lGfGnHhPt/iG/g6jF641JpvvQ5LcMPh7FH7xk0k10H+oLkQyfBq2aMZmmERDFiEqw3UN6D+g49k2rkei9oBFA9s0ZKYh6TySs22i/2ERRFL37LMSa98xyNZhN3LAJVT8ucuin42paTweqmzQeO2IqdvS2IW0HYo82Z1QGcfcLEW8TFYMUFXi+2Ede2AbTSJ4iGMNq5QxtWlVEzWPD05fbupivtkiMNkS21OcdxAS+EL4XMAe1i8DQqAmNPtBrEBCbgp+huGHBwWEwvNGqHV0wJG0pOSq1uJc7YTqqY2l6jTYXeKx44Pj94DVlvDZQzcQrMNikmuh62z3RjJ/DOpjGquBn7glbBIb6dOBnWH0qwtKWoc9eRJC8jJ8pq6ilFU2S9fMDkRrayne/bYxRbaKYRnwknb9Z3L19zfoB3GsDiELSaCA9f/V56UbUh0kVQQ/9GdfzD4hXb4wMpDsdwL2qTK2fLwlrL5fkCzabsfTnXe8JTaPRpgJP4vpNw58fZiFfSIAESIAEhAIGbwISIIFZS4ACxqy9NLPmwHTy0Ad/A9sTTZ0wuHl5m8n9RA7S9uTXghx1nYzoOLbJkm5vnhDjqbROLF3gI6A+Gc7Ibx/ZdCKnqQvq82CBb4a1YQKPya9WUNCm+1IvBtsEWsd2VU+LofXWbRxfjz++geFz1Z46hnlSj2gMFTRUJLHtX5/eu4KF+gho04iOfjURHKq4oOehk3PbetPJ7kXH6Uf6gZ6bNhVk1MtCS6PaN3OM4GZRQQEeBWo2apvAmaf66K+h9DhwnH/X8PHpODqejqvN6m/QjvVDk1Uctx7faLyt+9coGi1NCp8SGCyb/eIc9Xi0WZ/u67XDNYMHgj4Vt+DcbetH3ivq6aF8bOuNdwT2bx3HuseJvOo11nvS1qz3wPEiiE7W+4cm+ra+Y90Tekza32bQqlz12rrifh/ZNBJD72Hb/Wju6aGUBlDB/YJICzBXXwi9r809NBR5YRtLr4Gex4nuFT0u8x/7G8DP0CDuAd23jaHZn96j6gmB6+Ck1wLf6/GP1qxjWe8l6zW1jfXva2oiNyDEmJ9HnIe5x4aPf/x7wnhz2M4d95feDyM/P8w9B0ba+vH5Yr0nbCkhXkOfL05mvfU84bWCnxHr+eMew/nbmrl3cGzOMPh2cXdFH/Qb+vnTPvrzr59Xtjbynhjr3rH151cSIAES+DgSoIDxcbzqPGcSOEMIUMA4Qy4UD5MESIAESIAESIAESIAEZoAABYwZgMxdkAAJTI0ABYypceNWJEACJEACJEACJEACJHA2EqCAcTZeVZ4TCZwlBChgnCUXkqdBAiRAAiRAAiRAAiRAAqeAAAWMUwCRQ5AACUwPAQoY08OVo5IACZAACZAACZAACZDAmUiAAsaZeNV4zCTwMSFAAeNjcqF5miRAAiRAAiRAAiRAAiQwAQIUMCYAiV1IgARODwEKGKeHO/dKAiRAAiRAAiRAAiRAArORAAWM2XhVeEwkQAKGAAUM3ggkQAIkQAIkQAIkQAIkQAI2AhQwbCT4lQRIYNYRoIAx6y4JD4gESIAESIAESIAESIAEThsBChinDT13fDYSGOjvk9aKImmtLJbW6iLpbKwwp+ni4SVhcxdIcFK6ePiFirtXwLin39PeKJUHt0ljcY4M9Paavq6+vuITGiURqSskMGrOuNufLSspYJwtV5LnQQIkQAIkQAIkQAIkQAInT4ACxskz5AgkMEygr6tDSne+LeVZ70n14W1SW3zYrPPxD5A5518lKedfIwFRaeIbmjC8zWjfNFfkS9ZjP5XiHVukp7tbBgdFvIODJTglQxZcca8krdg42mZn3TIKGGfdJeUJkQAJkAAJkAAJkAAJkMCUCVDAmDI6bkgCxxNQAaNk+5sQMLZK1aH3pbrggOnkGxQic8+/WuZeuFkCYlTASDp+Y7slKmDsefiHUrT9Nenp7BQL1vmEhkpIynzJ3HSPJK26Qpzxzwn/zuZGAeNsvro8NxIgARIgARIgARIgARKYHAEKGJPjxd4kMC6B6RYwNAIjc9PdkrjqcnERV0gYLuMez5m+kgLGmX4FefwkQAIkQAIkQAIkQAIkcOoIUMA4dSw5EgnIdAsYjMDgTUYCJEACJDBRAoPIP7T098jgwIA4u3mIs4vrRDdlPxIgARIgARKYlQQoYMzKy8KDOlMJTL+AkSkLN90ryas2namIJnXcjMCYFC52JgESIAEHAhbLAIT1ViNiuHn5i6u7t8N6viEBEiABEiCBM40ABYwz7YrxeGc1gZMVMAZF/1mkqTxPsh75mRQf54FBAWNW3wA8OBI4iwj0drZLW3WptNeUS2ddhXQ310sflln6ehzO0tndU9x9g8QrKEx8I6LFJzxGvEOixN3b16Ef38wcgf7eLmkuPiotZfo/X/p7uiR+zeUSueCcmTsI7okESIAESIAEpoEABYxpgMohP74EToWAMSD9EDByZd8jP4ch6OsjTDwpYHx87y6eOQnMLIGOhiqp3PeR1B7eJQ1H96M8NEpDNzdKd1uzw4F4+AeJb3C0BMTNkYj5SyQ8fYmEpi0VH4gYbKeHQE9rkxRve1HKd72N67dH+ru6ZOld35L51917eg6IeyUBEiABEiCBU0SAAsYpAslhSEAJnAoBY/wIDFYh4Z1GAiQwPQQsFotJNeioq5TqgzulIT9bWisKpaO2XLqa6qS7vdkIqgN4mm/fXDy8xcPXX7z8AhF5ESb+0QkSlJQpIXMWSlj6YpSAjrDvzu9ngICKT4ee/IMUf/CSdNTXIHXEQ5YZAeO+Gdg7d0ECJEACJEAC00eAAsb0seXIH0MC4wsYV6GM6nUSEI0yqmEnKKOKsN89j/7ouDKq/65CshH1R9xYheRjeI/xlElguggMDPQhRaRVanP2yuGn/yIVB3ZIT0ebDHS3W3fpNE7Z5kHtYl4gWEDEiEqS6KXrJfXy2yRs3sLpOmSOOwaB1qoS2fm7/5YjWx43PQKj4ihgjMGKi0mABEiABM4sAhQwzqzrxaOd5QTGEjA8AwIldvkaiVmxTgJiUsQ3PHbcM2mrKpWcF/8pZfu3S39nlzg5u4pPaKiwCsm42LiSBEjgJAi011ZI2c43pWr/B1J7aK80wftCTSA9vDzFPywGn0FR4hUcKu4+/qho4S6DlkGkJnRID1JKOhurpauhRtoaamWgr0+8/QMldN5iCBi3SAR8F7wCQ8XVkwaSJ3F5JrUpBYxJ4WJnEiABEiCBM4gABYwz6GLxUGc/gbEEDFdfXwlLTZWgtAzxj8VEIHz8kOqu+lopeecNqcrNkcGuPnF2dR8SMOiBMfvvAh4hCZx5BLTcZn3efsn65y+kbO9WE3lhGegRF29vCQiLkqiF62AAuVpCUxcgRSRRtKKFBUJFZ1ONtJTmS92R3Yjc2CN1OfukuaYCwRiDEj4nXeZccoPErjhfAhPni6d/yKhgdN/639rw1fatCfhwEmvgh34dJwLEtvXQONbxhgaa5HjW47HYHYeTODs7m2McHlfHHD6+fx+bbb31K/oc108P9MTnoRvqqdjGc2RiHUN5jMZEU4Ha4Fey8w/fkdzXn9TOEhgZJ0vv/IZkwAPDycnZYTvtb3ZmHXZondO/921GGHo57nzGPxc9/uFzMENYj3m047bfDb8nARIgARIggbEIUMAYiwyXk8AUCIwlYDi7u4tfWJh4BAXDrd9X3Ly9xh1dDdfaykulpa5GpH9AnN29KGCMS4wrSYAEpkqgt7ND2qqK4XuxQ46+9ohU5x1EFEW/+AUFSnjmcgnLWCpB8WkQLpLFOzRcPOB14ezqgQgMC3x/2qW7pcFUKWkszDFjNB7LkZbqEvFEFZLIhYg8W36exK68yAgf9sc4gGomptJJZak0HDuMMSqlt70JPhy9ppt6a7j7BUtgXIrx0tAqJy5uHmYCbj+O7XsdqxvmlWo22lxcIN1NtdLb0SSDA/2mi6uXn6mW4h+TBEFlnnhjPA9Eijg7u9iGMF8rsrZK+fY3pb+327yPyFwlsasuED2/upwsE3HS19kmrl4+qLTiB0YrJWrxOuyrTdQ8s/7oQWksyBb1Chno64bYg898Hz8JmbtQQpBO447jcMO2ozWd7CvTnrYWaSg4LE2oJNLX3mhSe7S/E4QUNx+t+BIuYWkLJTg5A78fPMTFxc0M19PeIqUfvgrzznekJnuH1OJaaPMMCJLI9GUSvWS9xJ1zCcxWV5rlnfA2Kf3wNfidHDDv3eFlEpa2BP2DTSqRVqDRaBvdr1afsQxYxMnFFdVmYiFMXSDBSRlmu5EvA/1IR8J5tJYVIKJnm6ivim7nF5NstguMTRm5Cd+TAAmQAAmQwIQIUMCYECZ2IoGJERhLwJjY1mP30qeg1hQSRmCMTYlrSIAEpkKgs7FGKrK2SSUiLyp2vyP1JccwKfaW8ORUWXjTfZK44XJx8/SDeOA57vBt1WVSdeAjqdjzrvmvE1i/aHhhLN6AVJJbYeo532H7HpiCdjRUY4L7kRS995o0F+UgBaVK+jpaTD9PLcsajOiPJeuw/Q2YLKdj8h44PFm3H0wn/h31VdJSUSSVOJfy3e9JW8UxaauvhJDQabpqaVetjKKCRNzqCzH5T5fA2CQjitiPlfXgT2Tvv34hXai4oi1j0y2y7FPflOL3X5H8158yx9wOcdkb6TTeASEyDz4fi279irTXVpr9F771jJR8tAWGp61G0PALjRDPwFBJ2rBJUi7+BESgSPEJjrTf5fD3Gg3RgWNuhwB07N2XpHznVqTnVEg7BABtKiZoOo9vdIrMvfhaSTrvCvEAEzeIItr0Gqj3Rc5rj5r3I1+CYLBqqpFsvsesagTznb//juS/+4J5HxAZIykXXC1+UfGSv+UpqYYY4+nnZ0Senq5OiDq94uTpJRHJabL09m9I8nlXY7vjI0H0d6GmFamQcviZP0tNwSETzROTsVyW3fktpFReMPLQ+J4ESIAESIAEJkSAAsaEMLETCUyMwPQLGKxCMrErwV4kQAITJdBaWSwFbz8H8eJdaTp2ULox+QzEk/LwjBVIAbkOqSOrTMTFyEiFkePr0//26nJpKslD2dV9MoBICt+IBAmMT0XqyUIT8aDbWCz9YuntkTr0KfngFanLPYCoiRLpbGlE6kqLWaf9XBHBodEK/phUB6ekI5pjtcQhksMHE3j7SIz+3i5EKLRDCHGff+cAADHGSURBVPlQSj7cguiLPGlHGktXZ4v0IpLBgmgAbW4+AeLh7QMRIxLHFSMxyzZAnLkSvh4RiKTwHY7sGClgxGC/MasuRlTFQURg7MG47YgUaTXRF26enhKPdSkXfQLnfQRRGoekqTBXmsqOSX9PjxFP3H2xX/QLgSdI+HxEcyw/F//PM8dk/6LRGhrZUfTeS/AieUtay4ulFefRAyGoD2KPNo2EcEfZWi+IFkFJc01KT8yyC5Dis8ZEYnTWVZ+UgOEVGCyhiOrwwDE3HDskrQ114oNlGkWinieW/h6cfxuic8Jk7mU3S8Kay01kjUZs2DeN7KjPOwAh6R0pfu8FjFMtKuREwg9lPsSTSIhIbCRAAiRAAiQwFQIUMKZCjduQwBgExhMw9A9PJ4Qq2/K5xxhiaDH+UBwYMCHausAWgfHvKiSXowqJK6uQjA+Ra0mABCZAoLEoTw6h6kj5rrelvalKPJCSEIP0AE37iFx0DqIUJhfub56+wxsDz+WR6hCBya+jeWd/HwSHjmYp3PqiHHz0N1JXmGf6Orng8xGfk8MNURWasiBubnjq7y5Jy9bLopu+aMxB3b2R+oGUBG3drY2IiqiUgi1PSO5LD0pLDaIVjD+EC/pgPOuHrvk8tWg6ifo44GXOuZsk88bPS1BiujEotY03UsDwhWDiHRorva310o6J+CCqtRhvBxicahpN1PwVErFoHdIw9kt1YbbxBpEepJ9gwm/66Wc5zsU/Is6Mk37VHbLg+s/oQQw3Xd/bCTNURKBkPfATOfzSI6Oeg26g+zQnAVHELzBE5l/7KUm/6k6IPYGI+GiWnX+0el9YkH44CLHINDBwcXWVQERgLLkDXhjXfNosHhmB4YwUHU+/AHBzl76eDnM9fIOjIZrAhNXDHYJMB0rrIkLH2QlVZs6TaNwnMag2Exg3x7qfodeWimIpRRpOBaJ6ag9tlwGk42i6S/SSDZJ8wWZE42Q69OcbEiABEiABEpgoAQoYEyXFfiQwAQJjCRiuHl4SFI2w3/BI/DHvj3Ds0fOfbbvo6+yQBjzFbNQ/xPH0UD007KuQJK+6An/wQxDBv7O5DWJ+YtE5ytl9mmfzJeS5nQEE1LMh658/l1I8LdeyqQFhEXi6fpMkrLtCAmLnmAoikzkNm/+BGjW64rPOxdXqz2Abo626VGqP7JHyHW+aVItOREl4+wXh8zEGYsJc+F4EmK4ddVXSXFIgXXia34Gn/uFJ8yT53CshqqyFn8QSpE74m371+dlSCZ+FCggwFfvexzl0iouHJz5z481EWb0qtLWUFkhj0RHp7ugw/gxRGUskBiKNTqpjlm4YFlrGEjD8I6LFPyYGIkM9PB2qEG1SKs1VJUjpiIIwEYXIEotoKdoApF8ExqcgCqTa2q+mTJoR5eKByAlPb3/JvOE+pFF8wxyT7UUF69qcXVJ7eBciMF6U0j3vm4gVTVEJSEjBdUg2XQf6ehFhctRErHR1tGLZoCSvvUQS12/CuS4WTZNRIUrTgSrh5VGdl22208iKmIXnSDQiPzR9IzR1sVk+UsDQ6+WnKS6olBWEc/CLSjCRK04QNNRPpLUcnhb7PpAORMsEoaJWBIxd5yESQ6N07FtD4WE5+upjUrnvPWmGD4YbRDH1CVE/FBXHAhDhw0YCJEACJEACUyFAAWMq1LgNCYxBYCwBw9M/QGIWLZOoRStg5pmAkNzR859tw7Yh3/no289I6ZF9YoGhpzOeTNIDw0aHX0mABE4lgdqc3Xhqf78UffSGGTYM/gaLbvuyeVLu5ul7nPeFTtQHkR5ioiMmcCAaBeGESkpayUNb7ZEsKXgL/goHPoRJZR7SU9wgDMzBBHcN9nklvk8w/WoO7ZGira9KfW6WNFUUim9gEAwml0J0uNCkfqifhbYSHHcezEcbjh4wgoJGcXghXSQGqRqpm26DV0W46Vf8watS8MbT0lZXbsw+/SE6+GIinQRRJAMRDGpOqm2kgOEPw0rv0Hj0u1zSrrhR6guOSM2hLKnKgtcHPD9sKSrBCXPFP26eJK67TOZcdDUMTfeYfpXaD+kt2vRcV9zxdVn1+R+a97YXNTTNf/0JKXr/JaTxHJZGnG9w3FwJQtRC4vqNEJMuMV37Otrl2DsvSNmOd2CQeRQRM/USicowGikTv/YKCDHnmX4TLaM6UsBwB4MgMAmDV0XqxpuNsKMD9kIsaUaKjJbYzUekS01BLgQkXwmds0AW3/Jl7HsjAl1sXhiDUnN4t2Q/8TuT1tPR0iR+IWGSCM6xKy6CeLLIeJGYA+ULCZAACZAACUySAAWMSQJjdxIYj8BYAoYX/vBOWn2+JJxzsQREpuCPt7jxhpEWPK3LfvHPUpL1PnKiu/BXrxMFjHGJcSUJkMBUCYwUMMJTMmQJIgRSLrpuVO+LlvJjUrLtFVM+dSL7DIifOxTNYU1FqcDnWs5zfzd+Eq0w3gwIj5LYcy41T+gjMpbB8NJablUjNRryD2Gy/oaUwRRTmx8qiMSvvgTCxCfFLzLeLCuAaWb2k3+UJpRz7WqpwwQ8CZVBVkEwXmtEDI+hiI5aVBBRg8/q7I+kBiKKs7sbTDiDJeXCG2QJTDg9Ee2gbaSAoSkiUcvOhxnpGolethbVTRqNYWfeSw+gTOkTSI+wVk1JWHm+xK29XMLTl0gEBIB2jdKAseeRZ/9i+unYYwoYSLHQc8h/4wlTjaUD1UziUX42fu1lpipI6NwFujn21SP1+fDiOLIX4s4LUoWytcGxifAIyTSREEnnqqmmyFQFDN+QcESknGtEIk0NCU5KM+OpwNLd2iB1ECb02tXAC6Svvx/lWeMl/eq7JGHtpeIZFImIGy/jk1G1b5scePTXUoWIkl5EvIQiemb+5nthnnoJPEci4R8yfhSi2SlfSIAESIAESGAUAhQwRoHCRSQwVQJjCRi+QSEy9/yrZe6FmxE6mya+oUnj7qK5Il/2PPxDKdr+GpzsO0UznhmBMS4yriQBEpgigZECRsTcTFn26e9AwLjePFUfOaxWGdn15+9KGcSAibS4petk5b3fhZhwvule8uHrsv/hX0otSnf2wrQzAk/kM66925h0qmeGi7u12olFTS27WuXY28/Koaf+JJ3wd1DBIR5ix6KbvywBQ74LR178J1Jg/p80ohzrYF+nJKw4V9Ku/jRKry4Tf6RAuCJ9QZsKCq0QRQq2PCpHX3/MmHE6e3nJ/EtukpX33I+UjQjTb6SAMee8qyQDk+/AhHnGsNIWSbLrT/fL7of+15RL1Q3nX6nVSL5q0klsY+nyj379Ddnz0C/02zEFjP7uLtnz9x9Lzkv/lF6YdiLERdI23mLEAWViE1dgaiF93W2mSsl+7Dt/68viFxyGai8JMv+6e1Gt5Xazn6kKGIFRcTJv422SeP5mnGu8eKN6in3T1JBDT/7BVJlpR5lab5h9Jl94jbkmwcmZ4hEQalJNKvZslQO4xhXZe8w9FI1Ssys+c7/Er7nUfjh+TwIkQAIkQAKTJkABY9LIuAEJjE1gugWM4JT5smDTZyQJHhjOsPGkB8bY14JrSIAEJkZgpIARjoofSz75n4hMuB5lOz2GzTJto52sgFH0/ssmyqE2/7D0d3eKf3iEKZHqFRIBscFreH9qQKmeEq3lhVJ3LAcml52oIuItiWsuQ1nTb8MvI9UcUs7zD2C8nyLtohQT/z5JWXe5LP7kf8A0EhNqTLA1BU9bL7yFtFLKkef+Itko7alVTwQpHZmX3yKrPvt9pJqMLmCkowTsUpT+9AmPNuNpqoS2kQLGouvvkeWYpLvDm8NtyHdD+01UwNgJQST7uQdMlAWcQiUM1yEwMQ3mmZ6Gi45lNQXtxbm0Sz1SZhrgEeLmg0ot8K1QUUdFDG1TFTCCUVY28xOfR/WZm5BSEwC/JkcD1nb4eZR+9Jox56zJ3gmxpVPC5i1Aqdv1SHW5SnwRkdFcmofqI+9KweuPS0NpoTEFjURFm8W3fQWVX841x8cXEiABEiABEpgqAQoYUyXH7UhgFALTL2BkSOamuyVxlVYhcYOIYc0pH+VQzopFNPE8Ky4jT2KWE6hFOoBOxotQNUInyGEI919w8xclGVFj7j5Bw5Nn22lUwihz799/KJWHdtsWOXxVQ0qtOmFBioG2kREYhe8+J3v++n0YTB5w2G6ib+aed6Ws/OwPhitZ5Dz3V9n7j5/AJ6PYDJF68fWyGh4TtgiNkeMeePSXxrS0rb7GrMq86pOy+gs/ggGm1ZtoZATGouvukdVf+okRL+zHGilgLL31y7Lmyz8bFmBsfSciYPRBCNjx22/Kvif+YNtsUl+1ROlSiE6Lbvmq2W6qAkZIwhyIQ/8taUjRGa31QgBqLss1Rp5HX3tc6ovzTZnVsNQlknn9ZxGlkgZD1Q9gIvqeidJQs08/VHCJQJnX+dfehfKpK0YblstIgARIgARIYMIEKGBMGBU7ksCJCUy3gBGCCIzMTfcIq5Cc+FqwBwmQwMQI6JP8rH/+VMrw1LwHJpF+oeEwobxB4tdtxIQ0VXzgWWDf2qrLYGD5Hp7yF9svHv5ePTJ0AttUUWKWUcA4cQrJmSJgqBdGD7wwqg9sl4MQW8oP7RN3T1cJgYFpOtJ2/FFyV/0vag7thH/JAVMOPGTuImv51POuRlSM1VNj+GbhNyRAAiRAAiQwSQIUMCYJjN1JYDwCp0rAaKo4Knsf/hE9MFhGdbzbjetI4JQQ0EoU2eprgDKq7ShZ6gHPiEgtLQrPiphlG1ASdN6oXhhj7bwUhpu7//J/4X+wy3Q5XsB4Xvb8bSgCAxEfzm5uQ6kjGlGG9AxrhsZYw0vi6ouRqvE/SDvJMH1ynvsbIjB+LE2VEEwwXupFm2XV536AiiBzhyufaEeTfoEKKgcf+5Xs+9cvxERgIB0k86o7ELExSyIwYOSp5wDghomrh7uVw1DaivXN8a/eqB6y+LavoUTr58zK6YrAsO1ZDVH3PPBjKYZQZUH0iJYJT7nkZmOsWoXKLI0F2fAbKREPpNLErroYpVMvlKiFq7F+fANr2/j8SgIkQAIkQAJjEaCAMRYZLieBKRA4WQFjUPSfRZrK8yTrkZ9JMU08xQIR40QTmilcKm5CAiQwREAnu4XvPCvlu96VxmMHpbOjDdUl4lBKc5nMveRGVPNYY0qpOrvoD+OJ24kEjKL3X8Tk90dSffQgLCv6JCQuyZRQ9Q2PM54bTk7jp8YFJaVL9NINw8aWR178h2Q99DOYeJbIICqCJKNyh6ZShMCMVM0vbcet/hfdSGk48vzf5fALfzOmoALxZAHMMlfe970xPTBmIoVEvUB2/PHbsh8VPvQcnJ1dJXbRSolYsFpcXOFDAq+O8ZorhIKoxetRAWWp6TbdAkZzaQFK1z5m0kRaUM7VGQJLONJE3H0DjXjRXlsm3e2tuI9iZd4Vt6H6yMUoj5sCP4yg8U6D60iABEiABEjghAQoYJwQETuQwMQJnAoBwyL9EDByIWD8XEq2v84qJBQwJn4DsicJTIFAZ1O91GRvFy1vWvbR61KPcqROnp4SFp8iGVd/CuWfL8HENNgYUzpBxFCBwWZkaaIaUBljcKAf/wdMyoCWPd2HlJTKw3vN0YyMwChGCdasf/1cqvGUfgAlNmPhi7Dwli9JJMqUemA/Lm6eZnyLBeP19xkvjUF8VXlX9+3s6i4uMLa0VQPJffVfsv+RX0gDqpBYYNQZiyf9czbebkqZBqKEqyvORZumvjTD9LLw7aekYOtz0tvTJdYqJDfK8k9rFZIw0+90eGBoFZKdf/2uZL/0oAzgexcnF1kAz4gFN34OZprB8CIJNEw0OMMC1hYIPxYYnCp3cz1wXVzcwQWmq9pGEzCW3vlNmHzeh/7awxrmotE3O3//Hcl/9wVdiFSQ8T0wTCe8dDbWoBztTph5vifl27dIW2MtzFjjcW1cTeRFZ2O9iSKJSl8ki2DeGb92IwxB/SDGDEWU2AbiVxIgARIgARKYJAEKGJMExu4kMB6BUyFgIH4YAgZSSB75yXERGKxCMh59riMBEpgKAZ08t9dVmAlpHsp4Vufuk/6BXpTI9EeaxnxEMiyQkJQF8MOYK74RMeKFUqbOptSpk1hg1tnd2iTttRUo7Vku7TWlUp+3z5g8NkFQ0DZSwCjfu9WURa3L3SvtmOgGhEdJzEqkGEDAiJi/QnzCojER9pBOmGw2leRJc3GetKCyxSAEDTdU+AiFp0LMivPFa6jE57F3n5fDqCzSVJQr7Q21EoDtg+egMgbKt8avvkQ8/K1P/auzd0j5znekHvutK8yBIONtRIuUC66TRTd9YTii47QIGOB44LFfS94WlHeFGNDT1ipxyzdILCIXwhEJEzZvsbW87KCTNBYdkabio9JSkisdiHRwx3XyCg5HVMp5hp8yP07AQCSECgkZ19xtrp2LizWiY6oChnp2dNRXStX+bXLkxQekBlViPD28dNcm8mKgt0dcPH0kEiVyF9/2VYk75yJEwrhDdLJWhDEd+UICJEACJEACUyBAAWMK0LgJCYxFoK+7Q8p2vSPl+z6Q6pxtUge3dm3e3r6SAuf8lHOvloCoNPENjR9rCLO8ubJAsh5HBMbOLdLb3Q1JQ/AHapAEJafL/I13SfzyS1mFZFyCXEkCJDBZAvX52XLw8d/CC2OrdLU1mrKjgy4eEoAKF5ELV0r4/OWo/JFhfAxcPXxNREBfdzvEi3JpKMCk+thhaSw8jEl1uXS0NEt/T6eJCIhBRMTyu79tjBz1mGoO75b8N56U6oPbpUnTDzCp9YtMkIjMlZK47hIJgOeGG8bXNIWKLHyWHvxIaiE6OOGT0AelVuPPuVTmo+JFQEyyOcUyeHfkv/GU1B/ZI41lxyB0DBrvBRVE5l56Az47rZEVpR++KUVbX0S0QI30trVIYHS8BKBqRtL6K2TexpuHq4ycDgFDzTFzX31UihAJ0VKaayqqBMYkwhRzriSsuRApGBcigsEX9hguSNvYKpX7PpS6wzDKhMDjj/KuAXEpxkRTU360jRQwVCRKv+YumYeSsF5BEThXq6gzVQHD7AQvNTm7TfpOKaJ3+hBNY4EYpk3FCx8IR2Hpy2XxrV9i+VRDhS8kQAIkQAKnggAFjFNBkWOQwBCBgb5uqSvOlobSQ/jD/LC01haaNfqHZ0TaeomYs1p8Q6LFB3nZ47WO5iop2PG01OTvMKHC2tfF21s0RzxxyRUSkbQCf8wjjHsoDHi8sc7kdSyjeiZfPR77mUags7EaT9Q/NE/VK7O2mUl0X08vIhU8xA9P+L2CQsXTP1BcvXytvhLIQhhEqVQVbntam00kRi+iMbqRmtGP5T5+/hIYNwfCxXqZc/GNEpRorUChk2sVMcp3villSJPraGkST0QRaAqHHyIF3H0DRFNVeuGhoFEYnQ3VMBdFZAUm6pGLVsNc9AI80b8U/SMMYo3S0PE0/aV015vS1dqCaAUv8YfwEgB/DRcTLSKIDqmQlqpSU2lloLdLVFhJWL8JY54jkRkrkWpijSA4HQKGlp6tRuWOmkM7pOT9l6Vs7wfiGRgkXj4B4oOoF1+cu3p5IIlGOuuqwaRGOppqwL5boucvk+hl6yR25SUQmtYYJiMFDGUaljwPItQKSUbESQyiNbSdrIDRjHSjo288bkSVZlyHttoqM6437pVAmKhGLlonqZfdJGFpi81yvpAACZAACZDAyRKggHGyBLk9CdgRsFj6pbO9SjraKs3X7k7kAaM5OXvgj/l5+J+IsGw/8UTo8nitp6dF6qr2SluTVQCx9XX3CpSwyKXiH2h98mhbfrZ+pYBxtl5ZntdsJKDeCn3dbVJ/dL8ceeEhqTm4A+kALYgC6zJeFOq3oBNthDg4Hj5MFZxdXPA552LMJl08PBBBgcgNRAXEofqETqpD5y5Eyoc1EqK3s126mmulFIJDLgw4mxA1oULJIDwd9BhMFQ7dA8ZVrw01sFQRIipjMUxFb8AkfBXGTjWeHNqtp70Z49XJsbeelryXHpJWiB4DMMIctOh4erwaw6afwxB91SvCzR0VPtwk8ZzLJOP6eyQQUQ5eQeHmHLTf6RAw1EtEmahPx+Gn/yjHECnS39eP80AEHpgMonrKcFMfEPBWzwtvvwBJPv8qiBLXSEBsKlJ84k23Ngg1u/58v+S9+cyQj0ifWR4UnSBL7/qWzN98j3l/sgKG8cLAfVKBtCAVo+qL8sy4ARCiIhetl2hUsolbcR7EjBSznC8kQAKnnoB+fmgKc39Xu+Pg+Px08/Iz6XKOKyb3zoLPn77ONhmAOG3f9PPUzdvHVEyyX36i783vGoynn2tuiFC2efecaLvpWq+fs73wT9LPVT0em/FzP5b34feVevsoR10/m5v+bh39HvAd/n1pO35zz+Cc+xFF6dCccc/Ar0hTLO2b3gP9eo/hgYVDw+99dzzUsD0AUO8qZTbyXnHYBm/03nHXaz/0gGHk+hO9p4BxIkJcTwKTIDCIP+z7+vAD3tcufb3432f9sHeCIZu7O0zw3APEVf94xofheG1goEe6Omqku7sJT9xgkId/+JMV23njiRzCfz0Cxtv8rFlHAeOsuZQ8kTOAgH5+Wfp74cFQBxHjINJBcpDGgafq8LLohC9DFyIlejpa8cdJq8PZuMD7wNPX11SY8AmNEp/wGBM1EBCXLEFI0fCLTDTihe0PnAEYcmrVjabiI8Yroy53P/aVh31UI3WlefgPHxeUc3X19Eb0h0ZSJJsUk7jVF6GaRbIxtrRV5tD0iz6MV5uzF+kv7yKd5ZC0lBciEqMBVUeazDnpAbujAoZGNPjHJsHTI81U+IhGuVitVKL7sRmTni4Box+Tg96OFqlCOkYV0msaC8Af4k43ePegeoppQxMSD0waApBiEpSUKrHwA9EKJFazTz/TTVnmPPtXKf7gZeNP0joUGXGqBYxeVKxpqypCqs9WIx5V5ewz+w+GYJGw4SpEhVwkYfDB8A2Lsh4/X0mABE45ARUEjr31jJTu2OIwtitMfRM3XCkJ665wWD7ZN13N9aZSVTXMnu2bitJJ515pPn/sl5/o+9bKIil8F0bKiNxLPO+a4epJJ9puutbXHN4lxVtfEC+kKCafvxlCcKzZlaY5Fr33vElxTLrgWqQwzu7PsWNvPyPF2152wKQpmnoPJCGF3b7p700V/ct2vWW/GA8fvNH/Kolfc6nDcq3ipdXKKve977DcA5Wnks69ypRd1xWdKMVehH7ViCYcr/ng97pe+0hU2ppKo4AxFWrchgRmiIC1qCqeeuIfdGEjYszQrmfFbihgzIrLwIP4mBLQp+s12bukDmKGmka2IfWju7EKT6ocBQw3CBgeMPb0jYyX4JRM8z9kbjomrTFIN0HliSHDyJEYbZEYNYf2SMmHb0gzzCm7mqrgnWEVfl1RjUTTVXSSHg1DzrD0ZfDgWDjsVTFyPP3DqbWqDJ4ZO6US3hltlZj8YyI/AFFGm6d/iHgGRxuz0KQNl5k/Su0jL2zjHXz8N5INL5AePEXSloYyoMvvvh+VQKzigK2fVlo5+OTvZQAVQbTNv/ZuWXHP/xl+emfrt/sv/1eyn/6TeatP8Bbc8DlZ9qn/tq12+KoikkaTtNdWSvH7ryGy4X3paqiEEFNn3R5RJK5ISfTAuUQtXo3/a8EEBqvwDbFvGpWi1WAqISzUHdkvLRVFZrUvBKZFt35VUq+43bxvLoVhNEraliAaRps/ruHi27+OlJ9PmPcnelG2ve2NhveBR34l5fvxRytElrDkNEnddKdJ9fFDSV5Pv8ATDcX1JEACkyYwCNGzHZ8P9XIQJsBHX3/CYQT9bE676k7J2PwZPO32P+4pvEPnUd6Yp+kQKTVVLPuJ30IgedOhlw/S9DKu+YykXHw9nsL7Dz+Fd+g0ypsapMvt/OP9+GyrlmV3f0fmXHTDKL1mbtHR1x4xn4P+0cmy6nPfl9BUa8qbKdH9jx+b32mrPv8DmFpnzNxBTWJPeg/og4UDj/5Kcl/5l8OWLoh0SLvyk5J5w+dhgq2RFT4mQqILv9/1nsmHiGHf3LE+7aq7cF0/jWgU/P7GPaTRN+34/a+/G4s+eMW+uzH1TkffeRtvxT3mZypU7f37D4/rp9E2PTB/1spi7j6+EhybMqnfNQ47xRsKGCOJ8D0JzCICKmAg/tn8QyA1sp+tpe9m0SFO66FQwJhWvBycBMYlYCpNNFSZqhjdzY2ojNFk/pBR/wj7pmVNNYLBA5NUNcxUUcArMMSE4jqZyhPO9t2Hv7dFYnTUV0trWZFJoejXMOX+HtPHVK1AxJqXemNExYk3nn6pD8dY4cZ9SHXRP+LaEW2gKRQ9rY3m/SBS+7TpxF//IPONikfaSDL+iNI/uP8deWE64aUuNwvCzQ7R8GFtYWlLkQ6xFiVArZU7zEK86B/hWtlEq6No0woq2k/Lmtq3qgMfGW8LXabrIvDEaaynThrWqwJOX1ebtICJRr/0drYgdLfNDGlSalTYwXH7Qhjwi4gzTDwQXWLf9OlaW2WxtKEqTAfEkO6WBrNavTB038HJ8837HniWVMEkVf0rtHn6ByPl5xxUnEk170/0ohE5reUFRijJe+URqc47KE6InImcMx8lYL8IA9LLzB/N+iSYjQRI4NQSsEZePCWlSN/yDok01Ygc9oBJYwdEgj6IEEnnXoNIjI0Oq0/0Rj839Kl+LSIUdHwVqu2bBal66lHkhM/GZDxNV0F1Io0CxkQoTbyPibx4/0VEkETiOkU4bojfKXqNuhHFl6SRFesul2NvPmkiL3yCIyHqW1M7bRtpOXTtr9XJNGojctEa3ANPGX8sH70H8LvdvmmJc/0drnOVJNwDoalLpPoADLqLc+y74WEC0kZxn7ajb9wK+FihwtZkftc4DIY3FDBGEuF7EiCBWUOAAsasuRQ8EBIgARJwIKDRIp34Y1TNRzXSo2z7G9JYWSJuMGSNSl8ii2/5usSvushhG74hARI4dQRUqNz+62/KgWf/Jivu+JqsvO97DoNriteev38fIf3PQVD8giy8+csO60/0xmYGrOWSl975TTzF/5zDJk3FubLzD99B6ez9svRT/yVpmz5pzJc1RdC+Obup3wFSqIeMkm0ChlasWnjTl4ZTXJzd3EyE3UhfBD1PPZeBHquo7AyR1AOfMzYxWz0serBeBV4TKYdJu763oJyzNhcPT2MObROh+yHCq0m0ZSh6rnDr8ybCJDBunonAUAFX0/mObnkUy38PA+pUWXLHf0rIvEUYP+CkvTA0+rAP4q/N20h9N4x5NY5/Km3P334gh+CdlInqXMsR0WLfRt4jSz75Ldnx22/K4ZcflmW3f1VWfOZ+++5G7N7+m29J4XsvyNI7/gPRenfI9t/9F6Ikt5j3i2/7ukP/E90jKrL14lwbjx2SQ0/+UVorClHS+2vws7rRYZzJvqGAMVli7E8CJDBjBChgzBhq7ogESIAEJkxAQ8stmDSoj0nBW8/Cy2QbfEcKZACiRjD++I9cvE7mIiw8bCgUe8IDsyMJkMCECYycnI4UMDTKTctQN8HPKCJzNaLJlkx4bO14osnpSAEj4ZyN5mm9+hHZNx9UUUo+/1ocwyqz2CZg1MEwOiQ5w1TY0xV+UQkwJN583HHqfjTKoLnYGikWED9Xki/cjG2tkWQNKAGu/gwqgKRceJ2JHtD3rYhC0xYyJxP9r4PRsdVMuBblnwsh6rTXlJv1bdUlMCA+gopOq4yAoSXA1aNDjazrsNwbkWvBGCNu1SWSctF1JhrFbDjFl9KPtqCk9/OItLMKPbGrLsRx33CcceZEh9dIQK1gpdd4ZHTfyHtkpgWMDkRxFuJ3RP3RfeITGi3+8EfSY5xolN9YDChgjEWGy0mABE47AQoYp/0S8ABIgARIYJiAToh6YLSqqTrqdl+ff1COvvqY1OAJbF9XFwxXQ+F7cYnEoPqMptSo/wUbCZDA9BAYOTkdKWCc7F4nK2BEQADY9cfvSCGe1rsjQsIdKYDafJHqln71p2Hse4V4+ATCaPmg8cAog8ePJyrzqbm9NhUY0q+526QXuCMl0XWoQoWW99730M/g52MVRoIgeqj3RvSyDUhdDJCynW/Lnr98z1SaWnjLV0xFq5zn/y4t8O7QpmkQS+74hkmd6+lollIc35EX/m7S7HR9b596+bRJIiLG1AOjuSxfDjzyS2ksK8DnXaup+OQBb4ik9VegitN/DQshuu1kmkZe9OLzM++VhyTv5YfwmWn1WVLTzMxPfM74Rul52wylJzP2aH31s7obBqzZ8GpSwUQjaNQLYyYiMEzkBYw/648ekP0P/8KIaLp/45UB48+RVU5GO/7xllHAGI8O15EACZxWAhQwTit+7pwESIAEHAho+K8612vIuAVlatVDo6W8SDpQTUDL6KoxmxrGxWEioDnzatbGRgIkMD0EZquAUXN4t2hUQXj6cnPi6qfQjnQRnbRqlQ9tauLZjMgK9UIIQZltbVqmU6MiPODFo5EUNjNN9fHRCAMdQ5umJHTUVIg3qhtpP420UAGjo65CVNwIjJuDalixxodJ+/tFJcJ7aJW0V5dK4dvPwleoDVEfsTCJtpYKrcvbJ+UwKA2dt8QIGK4QK2qxPzUtLdv5FiIHoiRh7UZ4fKyXiIWrkL4yNVNiTcMo2voc9utjKp3YyrVqKp6eW+zKC2Fo+onhVBs99qk2LXt67I3HkfrxmqkM5geD5nCIykFJ82dEwNBKWAVvPg2uSC08dlg6IaaEJqWbamLJF1w3Yb+Usc6fAsZYZLicBEjgtBOggHHaLwEPgARIgASGCdQc2iW7/nS/FO1A6T31mEbKtv4RrkKFN8rRhqUtxxO+eyVm6YbhbfgNCZDA9BCYrQJGI9IutNKSemJoqz2Shc+N/5F2eOQsu/vbphT2aFVIqvZ/gH7fRTREiyz/zP8Ml/40HheYANsqVGmVKa0UpaW2tfKTVkNSAaP2WI544rMoGmaiS27/TyNa2JMveOsp2Qu/CDW7XPXZ7w2ntMxUFZJ9//q5ZP3rF5KEsrarv/gT8YZBtbaDT/xODuG/ptksBbeRFa/sz2Ei36vhdndzg/HF0FQY9cZQDw9tmrYyExEYLRC79/3jJ6hwtcXhkH1CIyFy3ymJMAhV42+tijKVRgFjKtS4DQmQwIwQoIAxI5i5ExIgARKYEAF9srrrL9+X4p3vIky7F5VZXFAi1d9UQ4lCmHYUqrCEI8/dH7nsbCRAAtNL4OMiYDQW5ZjIr6aiXANUS4Cq/46WPbUXMNR4M37NRolZfp75HNKqGfZttgoYGtGmvhxBSWk47tXHVbyyP4cTfa+VrApef0yKUe7UVKpC9El4xnLzX7edKQGjByJULQRvLdVt39TgVSNhtHKNRs/o74ypNAoYU6HGbUiABGaEAAWMGcHMnZAACZDAhAjUH82W/Y/+GiX1tpk8cxdXV4R7B8Hgbj7M7TYbczZXTz/8AW7NaZ/QoOxEAiQwJQInEjC0UpCmeWnKhf6cTjb1YaoeGKcqAiP+nEtN+U8tRZ374gMmFUFB9cOzorujXSJTFzkIGFptZOW934WAcf6oPGergNHT3myuk5b21us0shT3qCczykItHa7lUg898ydjapp5/X3G+8O+63QLGBmb75Fu3HMDSBvSsrvuqLBi3xoKDhmflJH3iH2fiXxPAWMilNiHBEjgtBCggHFasHOnJEACJDAqgS6EJdflZIm69g8ODoizs7PJ1/YKCpcAVB/xCY0QJxc3LHcZdXsuJAESOHUETiRgqPdEPnwQKrPeM94TSedeNamdn24BQytVFLzxpGi5Vb/oRDO51xPQSAyt5OGFFAz7CIwzVcAoeu9FVD15VqKXnovyojdN2QMjH5EXRVtfAKsEE50Slr5sOPLCduGnW8DQai8FbzwhzSVHTYTFSDFp+gUMC1Ib+/FfcxzZSIAESOA0EBjE38AW/TsYedZsJEACJEACJEACJEACVgJaFSjrgR9J3qsPY+J7o6QOeU7Y+KgpZs7zf5OKPe+a6hMLUOlCmxo8drc0oJpQm3jiKflYkRnttRWy758/RcTVh6gecYtotQz71l5TisiIB2GiWSQLbvwSqnxkmKfrWtZ0/vX3StJ515ruLaUFkvvSP/BkvlEW3fxl8QoONyaeXQ3V8MT4DowrbzD9RnpgeAVHyAFUsNB0BPXTCJ+/cqjfNjmEyhouqFIyGQGjeNsrcvDRX8Fjwt/4MAQmppnxij94SQ4//ScJjE81Jp4289ACGBZnP/ZrpGLEo4rKpyQ4ZT6iCkKOS/Ho7WgDz3oID97gGWJ8gczAI14OP/sXOfTUH0ykWvo1n0alFqsZqAoPBa8/bjwwlqDKiS1qQaNn9Dq5+wWZ6zRWdRKNutB++1GpJfv5B2Xh5rtl0W1fc9i7bqvHpsyyHvyxERnmQCyZd/ltDv30XI48/1dzzRd84vOm/GzWgz+Rsu1vyLzLbsb76x36d8GsM+eFB6QJRp0LbvyiRMH/aD/umdqcPTIX90zC2isc+rfBcDX3xX8Y09KFuBc0jWQqbcwIDDVnMuIFBYypcOU2JEACp4DAoAoXzqdgIA5BAiRAAiRAAiRAAmcRARUi6uCdUJebhWpAx4y3gP3pOTm5wDAzSQLi56IiyLLhah86SdVJs1b20IoQiTCVHK31dnagdOlulMLcL63lhdIJwcG+acSD+lAEJaZKGCqODPT2GAGjbN82CcE+1YNBm1bdMP1QhUKPo6updkICRgT8dHQi3HjskNm/plpo62qslaaSPAmISZmUgKHVSrQUaxNMRrWikq2MaTsqmjSW5kl05jkOAkYLyqjW5uw11VJaK4+ZKiU66fdB9RP7VrbrLTmGihvBczJl7qU3iVdgqP3q4e81+qAud6+0lOQb0ccy0GfWaUUUf1RwCk1dYiqFuMAfQlvhO8/KsbefMaWp5152K4QTV7N85IteSzXr1GofdUV5Ep6cjmos6Q7dnJ1dJemCzUYkqc3ZJfW5+6z3TE3Zcf1s94xGcATEz5FaeB/VHz2Ia1CASi+VDv1d3DxwbZMkMHGeuQd8wmJg2rpbGlA+1dwz8Cuxb64eXsbEVcUj9ebQ0rlTaWMLGFMZjduQAAmQAAmQAAmQAAmQAAmQAAnMCAFNJclCxYfclx9y2J8rBIbMGz4n+iTdvukT+yw8JdcSl5k3fF4yrr3bfvVx37dVl5mn+8XbXnVYpxN5fequ0R/abOkBhR+9IV5+AeKGiARtOiFeeNMXh6uK1GNyux+RFV0NNbLw5q9IwrqNpp+KFfsf/l/pg2fHwlu/InErLzLLqw9+JAce+aWpZmIWDL2EpS6WRbd+TSwwFD7wyK9MVRJ9H7lgtX234743kRiIrGgpO+awLmb5ubL49v+Q4KQMh+UFbz6Fqie/MQLFYlTzCIhJdlivURVZD/4UpUHXyaov/OiEJsa2SAwVkrQpvyV3fhORMAEO4x547FeIDPmzEUWW3vUtUbFgtKb71jHHa84QPzKvuw+8vmq6aSrJPkRW5L32iMNmbvCsWHDDZ2U++tq3zqY6E41T+M5z9osh1oTgHviCpF5xh8Py1ooi2ffQz6V0++sOy33DY2TBTV9C1I1jJIdDpwm8oYAxAUjsQgIkQAIkQAIkQAIkQAIkQAKzjYDFMoBIjD3SkH/Q4dCc8SQ/LG3pcOSFbaWWJa1FJII+IQ9LX4rUj/m2VaN+7e1sR+TCHmkpzXdY74ZUDB0/ENEW2mwCRg2OJWHNZcNlStWYMixtmREytF9Xcz2Od6/0d3dIKJ7y26oW6SRZz8MCQUYjOnwjYrW7dDRUmf6d9VXmve3FOzTKjCswK9XzcXJxwVP9ZeId7Fh9xNbf9lW9PerRX9Mu7JvfUMUOPV77ZiIxUArWKzhUwnEe7iOEhskKGA2IKKlH1IxGrGjT1JSw9BUQKKyRF7Z9q9Cj0TWamhOGaIWxvIW0TG09IjvGa07wJdJrZUuPsQz0W6NbCrIdNnN2czdMQxBNYt/UT6Uud4/xH7Ffrmkzeq00Cse+qThTi3umFVEs9k3Z6T0XEDvHfvGkv6eAMWlk3IAESIAESIAESIAESIAESIAESMBGwCZgnGyFCdt4s/1rT1uzdEOMKdr6vBS8+aREopT0ktv/U3wQZcA2vQQoYEwvX45OAiRAAiRAAiRAAiRAAiRAAmc1gY+bgFG2A94Xbz1hjDG1ClPovIUmesENnh9s00uAAsb08uXoJEACJEACJEACJEACJEACJHBWE2hGiolWDWkuy4fvxheMYeTZfMJqnpmNaijhSKFQ7wxbysvZfM6z5dwoYMyWK8HjIAESIAESIAESIAESIAESIIEzkICWO63Py5KeliZTUUPNO8/m1gKhpi5vv3iHROJ8l6L8KSMvZup6U8CYKdLcDwmQAAmQAAmQAAmQAAmQAAmQAAmQwJQJUMCYMjpuSAIkQAIkQAIkQAIkQAIkQAIkQAIkMFMEKGDMFGnuhwRIgARIgARIgARIgARIgARIgARIYMoEKGBMGR03JAESIAESIAESIAESIAESIAESIAESmCkCFDBmijT3QwIkQAIkQAIkQAIkQAIkQAIkQAIkMGUCFDCmjI4bkgAJkAAJkAAJkAAJkAAJkAAJkAAJzBQBChgzRZr7IQESIAESIAESIAESIAESIAESIAESmDIBChhTRscNSYAESIAESIAESIAESIAESIAESIAEZooABYyZIs39kAAJkAAJkAAJkAAJkAAJkAAJkAAJTJkABYwpo+OGJEACJEACJEACJEACJEACJEACJEACM0WAAsZMkeZ+SIAESIAESIAESIAESIAESIAESIAEpkyAAsaU0XFDEiABEiABEiABEiABEiABEiABEiCBmSJAAWOmSHM/JEACJEACJEACJEACJEACJEACJEACUyZAAWPK6LghCZAACZAACZAACZAACZAACZAACZDATBGggDFTpLkfEiABEiABEiABEiABEiABEiABEiCBKROggDFldNyQBEiABEiABEiABEiABEiABEiABEhgpgj8fwm7yF0DwyUvAAAAAElFTkSuQmCC)"
      ],
      "metadata": {
        "id": "4p8a-moED52n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us do some edge engineering:\n",
        "\n",
        "Extending nequip framework by reducing NH4 to UHN, where U is collection of 3 Hydrogen atoms, moving as 1 frmo molecular dynamics point of view.\n",
        "\n",
        "We will see how this coarsening impacts N-H bond RDF prediction.\n"
      ],
      "metadata": {
        "id": "JiDuW9HkGucT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us verify it with our data - before coarsening.\n",
        "Ammonium Ion should like a tetrahedral. (it does)"
      ],
      "metadata": {
        "id": "-NpUVEFK1CnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ase.io import read\n",
        "atoms = read('./Si_data/DES_L.xyz', index=0)\n",
        "atoms1 = read('./Si_data/DES_L.xyz', index=56)\n",
        "from ase.visualize import view\n",
        "view(atoms, viewer = 'x3d')\n",
        "#view(atoms1, viewer = 'x3d') -- Another molecule."
      ],
      "metadata": {
        "id": "20g8oLjHw81B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Sparsification by Edge reduction\n",
        "Molecular dynamics systems do not like particle reduction as total energy then changes and then we will be comparing apple to oranges.\n",
        "\n",
        "I will take max of biggest angular momentum and pass than on (instead of sum..). This way i can see impact of coarsening without drastically changing the system.\n",
        "\n",
        "If max data looks reasonable, I can further coarsen point cloud by only paying attention to top 3 biggest contributors (GAT).\n",
        "\n",
        "I also thought about just editing input data by reassigning 3 H to new type (say U) but that MD would be too different for small molecule such as NH4. For bigger molecules we may get away with that...\n",
        "\n",
        "We want to make small changes, reduce compute and still get good-enough MD data."
      ],
      "metadata": {
        "id": "nNKSZEcChhRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile coarsen.py\n",
        "\n",
        "from typing import Optional\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from torch_runstats.scatter import scatter\n",
        "\n",
        "from e3nn import o3\n",
        "\n",
        "from nequip.data import AtomicDataDict, AtomicDataset\n",
        "# Edge data gets rolled into node data.\n",
        "from nequip.nn import GraphModuleMixin, SequentialGraphNetwork, AtomwiseReduce#, EdgewiseReduce\n",
        "\n",
        "from nequip.nn.radial_basis import BesselBasis  # Got radials\n",
        "\n",
        "from nequip.nn.embedding import (\n",
        "    OneHotAtomEncoding,\n",
        "    SphericalHarmonicEdgeAttrs,                 # Got Sphericals\n",
        "    RadialBasisEdgeEncoding,\n",
        ")\n",
        "\n",
        "from allegro.nn import (\n",
        "    NormalizedBasis,\n",
        "    EdgewiseEnergySum,\n",
        "    Allegro_Module,\n",
        "    ScalarMLP,\n",
        ")\n",
        "from allegro._keys import EDGE_FEATURES, EDGE_ENERGY#, ATOM_FEATURES, ATOM_ENERGY\n",
        "\n",
        "from nequip.model import builder_utils\n",
        "\n",
        "#from o3.tensor.irrep_tensor import IrrepTensor\n",
        "#from o3.tensor.spherical_tensor import SphericalTensor\n",
        "#from o3.tensor.cartesian_tensor import CartesianTensor\n",
        "\n",
        "# First, we define a module that adds 3 H atoms as U:\n",
        "# Code for this was repurposed from Nequip_and_Allegro_Tutorial_2022.ipynb\n",
        "# https://colab.research.google.com/drive/1yq2UwnET4loJYg_Fptt9kpklVaZvoHnq#scrollTo=i12diW-ebK-w\n",
        "# e3nn is https://e3nn.org/\n",
        "# Nequip uses e3nn\n",
        "# Allegro uses Nequip.\n",
        "\n",
        "# Irreps classes combines objects as:\n",
        "#                                  A1    0    0\n",
        "# A1 + A2 + A3 = diag(A1,A2,A3) =  0     A2   0\n",
        "#                                  0     0    A3\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# E3NN -\n",
        "# Irreducible representations are defined as :\n",
        "# Irrep = (multiplicity) (times) (rotation-order) (parity)\n",
        "# for 32 vectors with even parity, 8 vectors with odd parity\n",
        "# Irrep = 32x1e, 8x1o etc...\n",
        "# Spherical harmonics are given by irreps_edge_sh (symmetric)\n",
        "# Clebsch-Gordon coefficients suggest that we multiply by (-1)^{J_1 - J_2 - m_3}\n",
        "# see https://medium.com/@vajhu/managing-multiple-graph-interactions-using-gnns-6cc6d9f040fd\n",
        "# when we encounter 2 systems with angular momenta J_1 and J_2.\n",
        "# Parity equivariance also requires us to multiply by (-1)^{quantum number \\elle},\n",
        "# which is shown as L in medium article.\n",
        "# By using principle quantum numbers (L) on periodic table 1s (even), 1s (odd), 2s (even)\n",
        "# See (Github) nequip/configs/full.yaml -- Line 45,46,47 (above spherical harmonics)\n",
        "# This will mean parity equivariant Irreps for {l = 2} = 1x0e, 1x1o, 1x2e\n",
        "# Output :\n",
        "# Potential energy (scalar) = 1x1e (even paerity)\n",
        "# see https://en.wikipedia.org/wiki/Pseudovector for vector transformations.\n",
        "# Angular momentum :\n",
        "# It is a pseudo vector as it is a vector with vectors direction not conforming\n",
        "#   under rotaion, reflection. Vectors flip direction under reflection, rotation.\n",
        "# - Angular momentum turns upside down under reflection.\n",
        "#   (Torque, angular velocity, angular momentum all act this way)\n",
        "# Cross product of two vectors is a pseudo-vector (changes sign under reflection).\n",
        "# ------------------------------------------------------------------------\n",
        "#\n",
        "# Irreps define how to transform data under rotation, reflection etc..\n",
        "# Use e3nn.FullTensorProduct vs outerproduct doing einsum (which is not irrep - irreducible representation)\n",
        "#  - Product of Tensors are reduced to irreps, by change of basis using Clebsch-Gordon\n",
        "# See https://docs.e3nn.org/en/latest/index.html for more details about FullTensorProduct.\n",
        "#\n",
        "\n",
        "# we will sum spherical harmonics on first layer\n",
        "#https://docs.e3nn.org/en/stable/api/o3/o3_sh.html\n",
        "\n",
        "# We will use coarsened allegro:\n",
        "'''\n",
        "# We will use MAX instead of SUM thus negating other edge effects in AtomwiseReduce\n",
        "def CAllegro(config, initialize: bool, dataset: Optional[AtomicDataset] = None):\n",
        "    logging.debug(\"Building Allegro model...\")\n",
        "\n",
        "    # Handle avg num neighbors auto\n",
        "    builder_utils.add_avg_num_neighbors(\n",
        "        config=config, initialize=initialize, dataset=dataset\n",
        "    )\n",
        "\n",
        "    # Handle simple irreps\n",
        "    if \"l_max\" in config:\n",
        "        l_max = int(config[\"l_max\"])\n",
        "        parity_setting = config[\"parity\"]\n",
        "        assert parity_setting in (\"o3_full\", \"o3_restricted\", \"so3\")\n",
        "        irreps_edge_sh = repr(\n",
        "            o3.Irreps.spherical_harmonics(\n",
        "                l_max, p=(1 if parity_setting == \"so3\" else -1)\n",
        "            )\n",
        "        )\n",
        "        nonscalars_include_parity = parity_setting == \"o3_full\"\n",
        "        # check consistant\n",
        "        assert config.get(\"irreps_edge_sh\", irreps_edge_sh) == irreps_edge_sh\n",
        "        assert (\n",
        "            config.get(\"nonscalars_include_parity\", nonscalars_include_parity)\n",
        "            == nonscalars_include_parity\n",
        "        )\n",
        "        config[\"irreps_edge_sh\"] = irreps_edge_sh\n",
        "        config[\"nonscalars_include_parity\"] = nonscalars_include_parity\n",
        "\n",
        "    layers = {\n",
        "        # -- Encode --\n",
        "        # Get various edge invariants\n",
        "        \"one_hot\": OneHotAtomEncoding,\n",
        "        \"radial_basis\": (\n",
        "            RadialBasisEdgeEncoding,\n",
        "            dict(\n",
        "                basis=(\n",
        "                    NormalizedBasis\n",
        "                    if config.get(\"normalize_basis\", True)\n",
        "                    else BesselBasis\n",
        "                ),\n",
        "                out_field=AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
        "            ),\n",
        "        ),\n",
        "        # Get edge nonscalars\n",
        "        \"spharm\": SphericalHarmonicEdgeAttrs,\n",
        "        # The core allegro model:\n",
        "        \"allegro\": (\n",
        "            Allegro_Module,\n",
        "            dict(\n",
        "                field=AtomicDataDict.EDGE_ATTRS_KEY,  # initial input is the edge SH\n",
        "                edge_invariant_field=AtomicDataDict.EDGE_EMBEDDING_KEY,\n",
        "                node_invariant_field=AtomicDataDict.NODE_ATTRS_KEY,\n",
        "            ),\n",
        "        ),\n",
        "        \"edge_eng\": (\n",
        "            ScalarMLP,\n",
        "            dict(field=EDGE_FEATURES, out_field=EDGE_ENERGY, mlp_output_dimension=1),\n",
        "        ),\n",
        "        # Sum edgewise energies -> per-atom energies:\n",
        "        \"edge_eng_sum\": EdgewiseEnergySum,\n",
        "        # Sum system energy:\n",
        "        \"total_energy_sum\": (\n",
        "            AtomwiseReduce,\n",
        "            dict(\n",
        "                reduce=\"max\",                             # <-- our change\n",
        "                field=AtomicDataDict.PER_ATOM_ENERGY_KEY,\n",
        "                out_field=AtomicDataDict.TOTAL_ENERGY_KEY,\n",
        "            ),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    model = SequentialGraphNetwork.from_parameters(shared_params=config, layers=layers)\n",
        "\n",
        "    return model\n",
        "  '''\n",
        "\n",
        "# We used code below to test 33% coarsening [copied from allegro_example]\n",
        "class AddCoarseningModule(GraphModuleMixin, torch.nn.Module):\n",
        "    field: str\n",
        "    noise_sigma: float\n",
        "    _dim: int\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        field: str,\n",
        "        noise_sigma: float = 0.0,\n",
        "        irreps_in=None,\n",
        "    ) -> None:\n",
        "        # Modify Edge Energy representations\n",
        "        super().__init__()\n",
        "        self.field = field\n",
        "        # noise example was used, keeping noise sigma to prevent edge features from going to zero.\n",
        "        self.noise_sigma = noise_sigma\n",
        "        # We have to tell `GraphModuleMixin` what fields we expect in the input and output\n",
        "        # and what their irreps will be. Having basic geometry information (positions and edges)\n",
        "        # in the input is assumed.\n",
        "        # We will save the unmodified version of `field` in `field + '_noiseless'`\n",
        "        # we need to tell the framework about rotation parity irreps\n",
        "        # `field + '_noiseless'` will have--- the same as `field`:\n",
        "        self._init_irreps(irreps_out={field + \"_noiseless\": irreps_in[field]}, irreps_in=irreps_in)\n",
        "        # this is just an e3nn.o3.Irreps...\n",
        "        field_irreps: o3.Irreps = self.irreps_in[field]\n",
        "        # ...whose properties we can save for later, for example:\n",
        "        self._dim = field_irreps.dim\n",
        "\n",
        "\n",
        "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
        "        \"\"\"Run the module.\n",
        "        The module both takes and returns an `AtomicDataDict.Type` = `Dict[str, torch.Tensor]`.\n",
        "        Keys that the module does not modify/add are expected to be propagated to the output unchanged.\n",
        "        \"\"\"\n",
        "        noiseless = data[self.field]\n",
        "        data[self.field + \"_noiseless\"] = noiseless\n",
        "        # Effect of zeroing\n",
        "        # Randomizing.\n",
        "        data[self.field] = noiseless + self.noise_sigma * torch.randn(\n",
        "            (len(noiseless), self._dim),\n",
        "            dtype=noiseless.dtype, device=noiseless.device\n",
        "        )\n",
        "        return data\n",
        "# NOISING fields did not change N-H bond distances---'''\n",
        "\n",
        "# Edgewise coarsened energies\n",
        "\n",
        "class EdgewiseCoarsenedReduce(GraphModuleMixin, torch.nn.Module):\n",
        "    \"\"\"Like ``nequip.nn.AtomwiseReduce``, but accumulating per-edge data into per-atom data.\"\"\"\n",
        "\n",
        "    _factor: Optional[float]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        field: str,\n",
        "        out_field: Optional[str] = None,\n",
        "        normalize_edge_reduce: bool = True,\n",
        "        avg_num_neighbors: Optional[float] = None,\n",
        "        reduce=\"sum\",\n",
        "        irreps_in={},\n",
        "    ):\n",
        "        \"\"\"Sum edges into nodes.\"\"\"\n",
        "        super().__init__()\n",
        "        assert reduce in (\"sum\", \"mean\", \"min\", \"max\")\n",
        "        # We will keep at max.\n",
        "        self.reduce = \"max\"\n",
        "        self.field = field\n",
        "        self.out_field = f\"{reduce}_{field}\" if out_field is None else out_field\n",
        "        self._init_irreps(\n",
        "            irreps_in=irreps_in,\n",
        "            irreps_out={self.out_field: irreps_in[self.field]}\n",
        "            if self.field in irreps_in\n",
        "            else {},\n",
        "        )\n",
        "        self._factor = None\n",
        "        if normalize_edge_reduce and avg_num_neighbors is not None:\n",
        "            self._factor = 1.0 / math.sqrt(avg_num_neighbors)\n",
        "\n",
        "    def forward(self, data: AtomicDataDict.Type) -> AtomicDataDict.Type:\n",
        "        # get destination nodes 🚂\n",
        "        edge_dst = data[AtomicDataDict.EDGE_INDEX_KEY][0]\n",
        "\n",
        "        out = scatter(\n",
        "            data[self.field],\n",
        "            edge_dst,\n",
        "            dim=0,\n",
        "            dim_size=len(data[AtomicDataDict.POSITIONS_KEY]),\n",
        "            reduce=self.reduce,\n",
        "        )\n",
        "\n",
        "        factor: Optional[float] = self._factor  # torchscript hack for typing\n",
        "        if factor is not None:\n",
        "            out = out * factor\n",
        "\n",
        "        data[self.out_field] = out\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "# Allegro rolls per edge data into node data.\n",
        "\n",
        "# Second, we define a model builder to add our new module to an Allegro model:\n",
        "def CoarsenPairEnergies(config, model: SequentialGraphNetwork) -> SequentialGraphNetwork:\n",
        "    model.insert_from_parameters(\n",
        "        # see allegro/models/_allegro.py for the names of all modules in an Allegro model\n",
        "        # `\"edge_eng\"` is the final readout MLP\n",
        "        after=\"edge_eng_sum\",\n",
        "        # name for our new module\n",
        "        name=\"add_coarsening\",\n",
        "        # hardcoded parameters from the builder\n",
        "        params=\n",
        "            #AtomwiseReduce,\n",
        "            dict(\n",
        "            #    reduce=\"max\",\n",
        "            #    field=AtomicDataDict.PER_ATOM_ENERGY_KEY,\n",
        "            #    out_field=AtomicDataDict.TOTAL_ENERGY_KEY,\n",
        "            #),\n",
        "            field=\"edge_energy\"\n",
        "        ),\n",
        "        # config from which to pull other parameters--- this means we can set\n",
        "        # `noise_sigma` in our YAML config file!\n",
        "        shared_params=config,\n",
        "        # the module to add:\n",
        "        builder=AddCoarseningModule,\n",
        "        #builder=EdgewiseCoarsenedReduce,\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWHyCsvbD5Hm",
        "outputId": "fd80b1fa-f54a-4cd5-baf5-3aa1ed8a5a46"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting coarsen.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before running this part we edited _allegro.py\n",
        "/content/allegro/allegro/model to substitute sum with max (thus negating effect of other edges).\n",
        "\n",
        "We copy allegro and build it so this step needs to be repeated manually!!!"
      ],
      "metadata": {
        "id": "9d_jfpClbFLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine pair-energies using spherical harmonics and bassel functions to get collective angular and radial momentum."
      ],
      "metadata": {
        "id": "DbrAqRJaHENu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a new config file with `noise.AddNoiseBuilder` added to `model_builders` after `allegro.model.Allegro`\n",
        "!perl -p -e 'print \" - coarsen.CoarsenPairEnergies\\n\" if $. == 12' allegro/configs/DES_tutorial.yaml > allegro/configs/DES-tutorial-extension.yaml\n",
        "# we can set options for our custom class using the YAML config:\n",
        "# Only turn it on for noising ####\n",
        "!perl -pi -e 'print \"noise_sigma: 0.33\\n\\n\" if $. == 9' allegro/configs/DES-tutorial-extension.yaml\n",
        "# change the run name\n",
        "!sed -i -e \"s/run_name: DES/run_name: DES-coarsened/\" allegro/configs/DES-tutorial-extension.yaml\n",
        "# only train for 1 epoch since this is just an example\n",
        "#!sed -i -e \"s/max_epochs: 100/max_epochs: 100/\" allegro/configs/DES-tutorial-extension.yaml\n",
        "# print out the start of the updated YAML\n",
        "!head -n 18 allegro/configs/DES-tutorial-extension.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1EeCeejHPws",
        "outputId": "9e10d272-8fcb-485e-ae15-7b82fe30a394"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# general\n",
            "root: results/DES-tutorial\n",
            "run_name: DES-coarsened\n",
            "seed: 123456\n",
            "dataset_seed: 123456\n",
            "append: true\n",
            "default_dtype: float64\n",
            "\n",
            "noise_sigma: 0.33\n",
            "\n",
            "# -- network --\n",
            "model_builders:\n",
            " - allegro.model.Allegro\n",
            " - coarsen.CoarsenPairEnergies\n",
            " # the typical model builders from `nequip` can still be used:\n",
            " - PerSpeciesRescale\n",
            " - StressForceOutput\n",
            " - RescaleEnergyEtc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Coarsened model\n",
        "Here we will see impact of coarsening."
      ],
      "metadata": {
        "id": "r6EpXdLsHbjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf results/DES-tutorial/DES-coarsened\n",
        "!PYTHONPATH=`pwd`:$PYTHONPATH nequip-train allegro/configs/DES-tutorial-extension.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4uMH-l28YAC",
        "outputId": "019caf80-b8a4-4f94-c14f-8c78ee5c402d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-moose-739946801467655344\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241213_002954-49wm4JT042cPI_nvXwXk2LL0rA84Kqm9YW5YrUfWoZg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDES-coarsened\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-moose-739946801467655344/allegro-tutorial?apiKey=c9642a9e6f095d80e85d42553309fed789420d69\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-moose-739946801467655344/allegro-tutorial/runs/49wm4JT042cPI_nvXwXk2LL0rA84Kqm9YW5YrUfWoZg?apiKey=c9642a9e6f095d80e85d42553309fed789420d69\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
            "Successfully loaded the data set of type ASEDataset(18700)...\n",
            "Replace string dataset_per_atom_total_energy_mean to -35.92002647258334\n",
            "Atomic outputs are scaled by: [H, C, N, O, F, P, S, Cl, Br, I: None], shifted by [H, C, N, O, F, P, S, Cl, Br, I: -35.920026].\n",
            "Replace string dataset_forces_rms to 0.02045545683041229\n",
            "Initially outputs are globally scaled by: 0.02045545683041229, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 37928\n",
            "Number of trainable weights: 37928\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0     2     6.94e+05        0.973     6.94e+05        0.015       0.0202          244           16\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    2.117    0.002        0.915     5.92e+05     5.92e+05       0.0141       0.0196          197         14.3\n",
            "Wall time: 2.11714261599991\n",
            "! Best model        0 591741.723\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10     6.59e+05         1.51     6.59e+05        0.019       0.0251          166         16.6\n",
            "      1    20     1.45e+06        0.767     1.45e+06       0.0134       0.0179          344         24.6\n",
            "      1    30     1.26e+05        0.629     1.26e+05       0.0114       0.0162         87.2         7.27\n",
            "      1    40     1.26e+05         1.21     1.26e+05       0.0179       0.0225         29.1         7.27\n",
            "      1    50     6.59e+05         1.01     6.59e+05       0.0165       0.0205          266         16.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1     2     6.94e+05        0.982     6.94e+05        0.015       0.0203          243           16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   11.063    0.002        0.976     2.79e+06     2.79e+06       0.0145       0.0206          268         23.5\n",
            "! Validation          1   11.063    0.002        0.923     5.91e+05     5.91e+05       0.0142       0.0197          197         14.2\n",
            "Wall time: 11.063626635999753\n",
            "! Best model        1 591308.610\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10     6.58e+05         1.11     6.58e+05       0.0173       0.0216          266         16.6\n",
            "      2    20     1.24e+06         1.66     1.24e+06       0.0204       0.0264          342         22.8\n",
            "      2    30     9.68e+03         1.05     9.68e+03       0.0159        0.021         22.1         2.01\n",
            "      2    40     5.39e+06         3.82     5.39e+06       0.0297         0.04          713         47.5\n",
            "      2    50     2.85e+06         3.22     2.85e+06       0.0297       0.0367          449         34.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2     2     6.92e+05         2.35     6.92e+05       0.0251       0.0314          243           16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2   12.414    0.002         1.53     2.79e+06     2.79e+06       0.0194       0.0255          268         23.4\n",
            "! Validation          2   12.414    0.002         2.19      5.9e+05      5.9e+05       0.0243       0.0303          197         14.2\n",
            "Wall time: 12.414906169999995\n",
            "! Best model        2 590337.054\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10      9.8e+03          2.1     9.79e+03       0.0234       0.0296         22.3         2.02\n",
            "      3    20     2.85e+06         3.07     2.85e+06        0.026       0.0358          449         34.5\n",
            "      3    30     3.02e+05         5.34     3.02e+05       0.0362       0.0473           90         11.2\n",
            "      3    40     6.54e+05         9.38     6.54e+05       0.0558       0.0626          165         16.5\n",
            "      3    50     1.19e+06         14.5     1.19e+06       0.0635       0.0779          223         22.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3     2     6.91e+05         7.85     6.91e+05        0.046       0.0573          243         15.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3   13.772    0.002         5.72     2.79e+06     2.79e+06       0.0395       0.0497          268         23.4\n",
            "! Validation          3   13.772    0.002         7.39     5.89e+05     5.89e+05       0.0445       0.0557          197         14.2\n",
            "Wall time: 13.772844037999675\n",
            "! Best model        3 588880.673\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10     1.35e+06         31.1     1.35e+06       0.0939        0.114          190         23.8\n",
            "      4    20     8.03e+05         58.2     8.03e+05        0.129        0.156          385         18.3\n",
            "      4    30     1.23e+06         67.1     1.23e+06        0.146        0.168          340         22.7\n",
            "      4    40     2.31e+05          138     2.31e+05        0.185         0.24          128         9.83\n",
            "      4    50     4.51e+05          214      4.5e+05        0.249        0.299          192         13.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4     2     6.83e+05           80     6.83e+05        0.147        0.183          241         15.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4   15.131    0.002         72.4     2.79e+06     2.79e+06        0.135        0.177          267         23.4\n",
            "! Validation          4   15.131    0.002         74.7     5.83e+05     5.83e+05        0.141        0.177          196         14.2\n",
            "Wall time: 15.13193359899924\n",
            "! Best model        4 583239.786\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10      1.2e+06          664      1.2e+06        0.444        0.527          202         22.4\n",
            "      5    20     7.46e+05     2.58e+03     7.43e+05        0.895         1.04          212         17.6\n",
            "      5    30     1.28e+06     2.57e+03     1.27e+06        0.859         1.04          185         23.1\n",
            "      5    40     2.37e+05     1.18e+03     2.36e+05        0.553        0.704          149         9.93\n",
            "      5    50     9.37e+04     1.22e+03     9.25e+04         0.61        0.716          106         6.22\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5     2     6.59e+05          951     6.59e+05        0.506        0.631          237         15.5\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5   16.476    0.002     1.26e+03     2.78e+06     2.78e+06        0.551        0.724          265         23.2\n",
            "! Validation          5   16.476    0.002          868     5.65e+05     5.65e+05        0.478        0.604          193           14\n",
            "Wall time: 16.476569464999557\n",
            "! Best model        5 565449.659\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10     1.14e+06     3.33e+03     1.14e+06         0.96         1.18          196         21.8\n",
            "      6    20     1.18e+06     1.15e+04     1.16e+06         1.79         2.19          177         22.1\n",
            "      6    30     2.98e+05     1.13e+04     2.87e+05         1.71         2.17          142           11\n",
            "      6    40     6.77e+06      9.1e+03     6.76e+06         1.48         1.95          532         53.2\n",
            "      6    50     1.05e+06     6.16e+03     1.05e+06          1.1         1.61          272         20.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6     2     6.13e+05     5.09e+03     6.08e+05         1.17         1.46          227         14.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6   17.811    0.002     6.33e+03     2.76e+06     2.77e+06         1.24         1.64          259         22.9\n",
            "! Validation          6   17.811    0.002     4.55e+03     5.27e+05     5.31e+05         1.09         1.38          187         13.5\n",
            "Wall time: 17.81213269499949\n",
            "! Best model        6 531462.408\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10     6.36e+05     7.34e+03     6.29e+05         1.27         1.75          227         16.2\n",
            "      7    20      4.4e+05     1.21e+04     4.28e+05         1.68         2.25          228         13.4\n",
            "      7    30     4.72e+05     2.21e+04      4.5e+05          2.4         3.04          220         13.7\n",
            "      7    40     7.42e+04     1.29e+04     6.13e+04         2.03         2.33         20.3         5.06\n",
            "      7    50     3.77e+05     1.65e+04      3.6e+05         1.97         2.62          233         12.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7     2     5.57e+05     1.11e+04     5.46e+05         1.72         2.15          214           14\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7   19.178    0.002     1.26e+04     2.73e+06     2.75e+06         1.77         2.32          251         22.3\n",
            "! Validation          7   19.178    0.002     9.83e+03     4.81e+05      4.9e+05          1.6         2.03          179           13\n",
            "Wall time: 19.178331608999542\n",
            "! Best model        7 490388.393\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10     7.38e+04     1.13e+04     6.25e+04         1.68         2.17         40.9         5.11\n",
            "      8    20     6.24e+05     9.55e+03     6.14e+05         1.63            2          289           16\n",
            "      8    30     8.84e+05     1.51e+04     8.69e+05            2         2.51          191         19.1\n",
            "      8    40     9.05e+05     2.45e+04     8.81e+05         2.47          3.2          154         19.2\n",
            "      8    50     1.26e+06     1.49e+04     1.25e+06         1.73          2.5          297         22.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8     2     5.15e+05     1.47e+04        5e+05         1.97         2.48          205         13.4\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8   20.562    0.002     1.45e+04     2.72e+06     2.73e+06         1.87         2.49          247           22\n",
            "! Validation          8   20.562    0.002     1.36e+04     4.47e+05      4.6e+05         1.89         2.39          173         12.5\n",
            "Wall time: 20.562960826999188\n",
            "! Best model        8 460465.613\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10     5.75e+05      1.8e+04     5.57e+05         2.17         2.75          275         15.3\n",
            "      9    20     5.66e+06     4.47e+03     5.65e+06        0.915         1.37          729         48.6\n",
            "      9    30     3.32e+05     7.38e+04     2.58e+05         3.24         5.56          104         10.4\n",
            "      9    40     7.11e+05      2.8e+04     6.83e+05         2.65         3.43          355         16.9\n",
            "      9    50     8.35e+05     3.26e+04     8.02e+05         2.81         3.69          165         18.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9     2     4.83e+05     1.88e+04     4.64e+05         2.19         2.81          197         12.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9   21.954    0.002      2.1e+04      2.7e+06     2.73e+06         2.22         3.03          241         21.6\n",
            "! Validation          9   21.954    0.002     1.82e+04     4.21e+05     4.39e+05         2.14         2.77          169         12.2\n",
            "Wall time: 21.954452433999904\n",
            "! Best model        9 439270.811\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10      5.8e+06     1.06e+04     5.79e+06         1.37         2.11          738         49.2\n",
            "     10    20     3.42e+06     3.15e+04     3.39e+06         2.75         3.63          490         37.7\n",
            "     10    30     4.73e+05     2.75e+04     4.45e+05         2.72         3.39          164         13.6\n",
            "     10    40     8.19e+05     3.67e+04     7.82e+05         2.93         3.92          163         18.1\n",
            "     10    50     2.58e+05     6.12e+04     1.97e+05         3.42         5.06          127         9.08\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10     2     4.44e+05     2.74e+04     4.16e+05         2.63         3.39          186         12.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10   23.291    0.002     3.46e+04     2.68e+06     2.72e+06         2.86         3.88          233         21.1\n",
            "! Validation         10   23.291    0.002     2.76e+04     3.87e+05     4.15e+05         2.61          3.4          162         11.7\n",
            "Wall time: 23.291625774999375\n",
            "! Best model       10 414727.593\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10     9.11e+05     3.57e+04     8.75e+05         3.23         3.87          268         19.1\n",
            "     11    20     1.86e+06      1.9e+04     1.84e+06         2.29         2.82          444         27.7\n",
            "     11    30     4.11e+05     1.98e+04     3.91e+05         2.23         2.88          205         12.8\n",
            "     11    40      6.4e+07     1.03e+03      6.4e+07        0.511        0.657          655          164\n",
            "     11    50     8.78e+05     2.37e+04     8.54e+05         2.38         3.15          284         18.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11     2     4.38e+05     2.74e+04      4.1e+05          2.6         3.38          184           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11   24.685    0.002     2.92e+04     2.69e+06     2.72e+06         2.56         3.55          236         21.2\n",
            "! Validation         11   24.685    0.002     2.85e+04     3.83e+05     4.12e+05         2.61         3.45          161         11.6\n",
            "Wall time: 24.68556251099926\n",
            "! Best model       11 411632.984\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10     1.34e+05     7.35e+04     6.03e+04         4.23         5.54         80.4         5.02\n",
            "     12    20     3.77e+05     3.28e+04     3.45e+05          2.9         3.71          120           12\n",
            "     12    30     2.88e+05     4.69e+04     2.41e+05         3.32         4.43          191           10\n",
            "     12    40     7.37e+05     5.81e+04     6.79e+05         3.83         4.93          135         16.9\n",
            "     12    50        1e+06     2.32e+04      9.8e+05         2.75         3.12          223         20.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12     2     4.28e+05     2.98e+04     3.98e+05         2.72         3.53          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12   26.075    0.002     3.24e+04     2.68e+06     2.71e+06         2.78         3.77          233         21.1\n",
            "! Validation         12   26.075    0.002     3.13e+04     3.74e+05     4.06e+05         2.75         3.61          159         11.5\n",
            "Wall time: 26.075384043999293\n",
            "! Best model       12 405716.134\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10      1.8e+06     1.47e+04     1.78e+06            2         2.48          437         27.3\n",
            "     13    20     8.42e+05      3.2e+04      8.1e+05         2.64         3.66          166         18.4\n",
            "     13    30     5.42e+04     1.35e+04     4.07e+04         1.97         2.37         16.5         4.13\n",
            "     13    40     2.92e+05     3.96e+04     2.53e+05         2.93         4.07          195         10.3\n",
            "     13    50     3.36e+06     2.35e+04     3.34e+06         2.35         3.14          486         37.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13     2     4.27e+05     2.87e+04     3.98e+05         2.64         3.47          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13   27.448    0.002     2.95e+04     2.68e+06     2.71e+06         2.56         3.61          234         21.1\n",
            "! Validation         13   27.448    0.002      3.1e+04     3.75e+05     4.06e+05         2.69         3.59          159         11.5\n",
            "Wall time: 27.448359223999432\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10     4.87e+05      2.5e+04     4.62e+05         2.67         3.24          195         13.9\n",
            "     14    20     4.82e+04     1.68e+04     3.14e+04         2.28         2.65         61.6         3.62\n",
            "     14    30     2.84e+05     4.54e+04     2.39e+05          3.2         4.36          190         9.99\n",
            "     14    40     4.66e+05     5.32e+04     4.13e+05         3.29         4.72          184         13.1\n",
            "     14    50     3.58e+05     3.52e+04     3.23e+05         3.06         3.84          256         11.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14     2     4.25e+05     2.93e+04     3.95e+05         2.65          3.5          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14   28.786    0.002     3.09e+04     2.68e+06     2.71e+06         2.66         3.68          233           21\n",
            "! Validation         14   28.786    0.002     3.18e+04     3.73e+05     4.05e+05         2.72         3.64          158         11.5\n",
            "Wall time: 28.786613519999264\n",
            "! Best model       14 404554.487\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10     2.07e+07     2.86e+03     2.07e+07        0.741         1.09          466         93.1\n",
            "     15    20     7.02e+06     1.83e+04        7e+06         2.05         2.77          541         54.1\n",
            "     15    30     5.24e+04     2.25e+04     2.99e+04         2.39         3.07         28.3         3.53\n",
            "     15    40     4.09e+05     1.86e+04     3.91e+05         2.18         2.79          166         12.8\n",
            "     15    50     7.22e+05     6.46e+04     6.58e+05         3.95          5.2          133         16.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15     2     4.26e+05     2.87e+04     3.97e+05         2.63         3.47          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15   30.139    0.002     2.92e+04     2.68e+06     2.71e+06         2.63         3.58          233         21.1\n",
            "! Validation         15   30.139    0.002     3.13e+04     3.74e+05     4.05e+05         2.69         3.61          158         11.5\n",
            "Wall time: 30.13949630299976\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10     4.25e+05     2.14e+04     4.04e+05         2.33         2.99          169           13\n",
            "     16    20     4.75e+05     7.46e+04        4e+05         4.49         5.59          155         12.9\n",
            "     16    30     7.16e+05     5.65e+04     6.59e+05         3.81         4.86          166         16.6\n",
            "     16    40      5.6e+05     8.35e+04     4.76e+05          4.2         5.91          212         14.1\n",
            "     16    50     2.09e+07     3.69e+03     2.09e+07        0.833         1.24          467         93.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16     2     4.22e+05     2.99e+04     3.92e+05         2.67         3.54          180         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16   31.500    0.002     3.24e+04     2.67e+06      2.7e+06         2.75         3.77          231         20.9\n",
            "! Validation         16   31.500    0.002     3.26e+04      3.7e+05     4.03e+05         2.74         3.69          157         11.4\n",
            "Wall time: 31.500505085999976\n",
            "! Best model       16 403070.289\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10     4.44e+05     3.72e+04     4.07e+05          3.2         3.94          183         13.1\n",
            "     17    20     3.06e+05     7.45e+04     2.31e+05         4.16         5.58          167         9.83\n",
            "     17    30     7.66e+05     4.17e+04     7.24e+05         3.32         4.18          261         17.4\n",
            "     17    40      4.7e+05     4.05e+04     4.29e+05         3.21         4.11          241         13.4\n",
            "     17    50     6.22e+05     1.02e+05      5.2e+05         4.91         6.53          118         14.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17     2     4.11e+05     3.37e+04     3.77e+05         2.82         3.75          176         11.5\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17   33.013    0.002     4.08e+04     2.66e+06      2.7e+06         3.09         4.26          227         20.7\n",
            "! Validation         17   33.013    0.002     3.68e+04      3.6e+05     3.97e+05         2.89         3.92          155         11.3\n",
            "Wall time: 33.0132438669998\n",
            "! Best model       17 396818.329\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10     7.94e+05     3.73e+04     7.57e+05         2.97         3.95          160         17.8\n",
            "     18    20     3.29e+05     5.37e+04     2.75e+05         3.68         4.74          236         10.7\n",
            "     18    30     6.91e+05     7.04e+04     6.21e+05         4.01         5.43          161         16.1\n",
            "     18    40     8.57e+05     2.45e+04     8.32e+05         2.41          3.2          280         18.7\n",
            "     18    50     4.23e+05     1.27e+04      4.1e+05         1.76         2.31          170         13.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18     2     4.13e+05     3.26e+04      3.8e+05         2.73         3.69          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18   34.383    0.002     3.45e+04     2.66e+06      2.7e+06         2.74         3.92          229         20.8\n",
            "! Validation         18   34.383    0.002      3.6e+04     3.62e+05     3.98e+05         2.82         3.87          155         11.3\n",
            "Wall time: 34.383675564999976\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10      7.7e+04      1.7e+04        6e+04         2.08         2.67         55.1         5.01\n",
            "     19    20     6.91e+06     1.06e+04      6.9e+06          1.5         2.11          537         53.7\n",
            "     19    30     9.96e+05      1.7e+04     9.79e+05         2.24         2.67          405         20.2\n",
            "     19    40     5.84e+05     1.69e+04     5.67e+05         1.97         2.66          247         15.4\n",
            "     19    50     2.08e+07     3.14e+03     2.08e+07        0.754         1.15          467         93.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19     2     4.23e+05     2.91e+04     3.94e+05         2.57         3.49          180         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19   35.739    0.002     2.59e+04     2.67e+06      2.7e+06         2.38         3.38          234         21.1\n",
            "! Validation         19   35.739    0.002     3.22e+04     3.72e+05     4.04e+05         2.65         3.66          157         11.4\n",
            "Wall time: 35.739863716999935\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10      2.8e+05     4.54e+04     2.35e+05         3.13         4.36          188         9.91\n",
            "     20    20     4.44e+04     1.18e+04     3.26e+04         1.81         2.22         44.3         3.69\n",
            "     20    30     3.91e+05     3.13e+04     3.59e+05          2.7         3.62          123         12.3\n",
            "     20    40     7.92e+05     3.43e+04     7.57e+05         2.89         3.79          160         17.8\n",
            "     20    50     8.05e+05     2.38e+04     7.81e+05         2.24         3.16          271         18.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20     2     4.18e+05     3.11e+04     3.87e+05         2.64         3.61          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20   37.113    0.002     3.78e+04     2.66e+06     2.69e+06         2.84         4.07          227         20.6\n",
            "! Validation         20   37.113    0.002     3.45e+04     3.66e+05     4.01e+05         2.72         3.79          156         11.3\n",
            "Wall time: 37.114221784999245\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10     4.97e+05     2.91e+04     4.68e+05         2.68         3.49          252           14\n",
            "     21    20     6.39e+07          419     6.39e+07        0.302        0.419          654          164\n",
            "     21    30     2.78e+05     9.36e+04     1.85e+05         4.77         6.26          149         8.79\n",
            "     21    40     3.68e+05     2.63e+04     3.42e+05         2.62         3.32          120           12\n",
            "     21    50     4.38e+05     3.62e+04     4.02e+05         3.05         3.89          182           13\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21     2     4.13e+05     3.27e+04      3.8e+05         2.69          3.7          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21   38.487    0.002     3.96e+04     2.65e+06     2.69e+06          2.9         4.17          226         20.5\n",
            "! Validation         21   38.487    0.002     3.61e+04     3.62e+05     3.98e+05         2.78         3.88          155         11.3\n",
            "Wall time: 38.48813869099922\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10     4.26e+05     1.51e+04     4.11e+05          1.9         2.51          131         13.1\n",
            "     22    20     2.35e+05     6.57e+04      1.7e+05         3.71         5.24          118         8.42\n",
            "     22    30     6.91e+06     9.95e+03      6.9e+06         1.39         2.04          537         53.7\n",
            "     22    40     7.81e+04     1.69e+04     6.12e+04         2.05         2.66         55.7         5.06\n",
            "     22    50     3.34e+06     1.34e+04     3.33e+06         1.72         2.37          485         37.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22     2     4.25e+05     2.87e+04     3.97e+05         2.49         3.47          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22   39.847    0.002     2.61e+04     2.66e+06     2.69e+06         2.28         3.39          233           21\n",
            "! Validation         22   39.847    0.002      3.2e+04     3.73e+05     4.05e+05         2.57         3.65          157         11.4\n",
            "Wall time: 39.84792182199999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10     5.78e+05     6.83e+04      5.1e+05         4.18         5.34          307         14.6\n",
            "     23    20     6.89e+05     3.42e+04     6.55e+05         2.77         3.78          132         16.6\n",
            "     23    30     3.89e+04     1.25e+04     2.64e+04         1.86         2.29         26.6         3.32\n",
            "     23    40      4.7e+05      3.8e+04     4.32e+05         3.11         3.99          282         13.4\n",
            "     23    50     5.91e+05     1.03e+05     4.88e+05         4.72         6.56          114         14.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23     2     4.15e+05     3.15e+04     3.83e+05         2.61         3.63          178         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23   41.187    0.002     4.25e+04     2.65e+06     2.69e+06         2.99          4.3          224         20.4\n",
            "! Validation         23   41.187    0.002     3.48e+04     3.64e+05     3.99e+05         2.69          3.8          155         11.3\n",
            "Wall time: 41.18744675599919\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10     5.53e+04     7.03e+03     4.82e+04         1.42         1.72         53.9         4.49\n",
            "     24    20     4.04e+04     5.37e+03     3.51e+04         1.14          1.5         19.1         3.83\n",
            "     24    30     2.03e+05     1.05e+05     9.71e+04         4.91         6.64         89.2         6.37\n",
            "     24    40     6.39e+07          191     6.39e+07        0.174        0.283          654          163\n",
            "     24    50     8.09e+04      1.7e+04     6.39e+04         2.03         2.67         56.9         5.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24     2     4.16e+05     3.07e+04     3.85e+05         2.55         3.59          178         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24   42.513    0.002     3.24e+04     2.66e+06     2.69e+06         2.58          3.8          228         20.7\n",
            "! Validation         24   42.513    0.002     3.38e+04     3.65e+05     3.99e+05         2.63         3.75          155         11.3\n",
            "Wall time: 42.51350332899983\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10     4.39e+05     3.23e+04     4.07e+05          2.8         3.67          183           13\n",
            "     25    20     3.26e+06      1.1e+04     3.25e+06         1.52         2.14          479         36.9\n",
            "     25    30     5.06e+05     8.62e+04     4.19e+05         4.35         6.01          199         13.2\n",
            "     25    40     2.78e+05     8.54e+03     2.69e+05         1.54         1.89          149         10.6\n",
            "     25    50     4.11e+05     2.44e+04     3.86e+05         2.34         3.19          127         12.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25     2     4.27e+05     2.76e+04     3.99e+05         2.38          3.4          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25   43.851    0.002     2.59e+04     2.66e+06     2.68e+06         2.22         3.38          232         20.9\n",
            "! Validation         25   43.851    0.002     3.05e+04     3.74e+05     4.05e+05         2.46         3.56          157         11.4\n",
            "Wall time: 43.85185896299936\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10     5.67e+05     6.41e+04     5.03e+05         3.92         5.18          305         14.5\n",
            "     26    20      7.1e+06     1.72e+04     7.09e+06         1.82         2.68          545         54.5\n",
            "     26    30     3.05e+06     9.54e+03     3.04e+06         1.51            2          321         35.7\n",
            "     26    40      3.4e+06     1.95e+04     3.38e+06         2.06         2.86          489         37.6\n",
            "     26    50     7.43e+06     7.34e+03     7.42e+06         1.32         1.75          557         55.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26     2     4.17e+05     3.06e+04     3.86e+05          2.5         3.58          178         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26   45.220    0.002     4.11e+04     2.64e+06     2.68e+06         2.85         4.25          223         20.3\n",
            "! Validation         26   45.220    0.002     3.35e+04     3.65e+05     3.99e+05         2.57         3.74          155         11.3\n",
            "Wall time: 45.220539501999156\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10      6.6e+05      5.8e+04     6.02e+05         3.61         4.93          159         15.9\n",
            "     27    20     4.29e+05     5.22e+04     3.77e+05         3.18         4.67          176         12.6\n",
            "     27    30     4.13e+04     1.51e+04     2.61e+04         1.98         2.52         56.2          3.3\n",
            "     27    40     1.26e+06     8.14e+03     1.25e+06         1.37         1.84          298         22.9\n",
            "     27    50     9.46e+05     1.99e+04     9.26e+05         2.33         2.89          276         19.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27     2     4.14e+05     3.11e+04     3.83e+05         2.52         3.61          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27   46.569    0.002     3.74e+04     2.64e+06     2.68e+06         2.67         4.02          225         20.4\n",
            "! Validation         27   46.569    0.002     3.38e+04     3.63e+05     3.96e+05         2.58         3.75          154         11.2\n",
            "Wall time: 46.56986947199948\n",
            "! Best model       27 396498.385\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10     4.89e+04     1.05e+04     3.85e+04         1.59         2.09         32.1         4.01\n",
            "     28    20     3.28e+05     8.73e+04     2.41e+05         5.05         6.04          120           10\n",
            "     28    30     4.01e+05     1.36e+04     3.87e+05         1.78         2.38          204         12.7\n",
            "     28    40     2.76e+05      3.9e+04     2.37e+05         2.82         4.04          189         9.96\n",
            "     28    50     1.76e+06     7.48e+03     1.76e+06         1.36         1.77          434         27.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28     2     4.19e+05     2.93e+04     3.89e+05         2.41          3.5          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28   47.907    0.002     3.07e+04     2.64e+06     2.68e+06         2.38         3.67          226         20.5\n",
            "! Validation         28   47.907    0.002     3.19e+04     3.67e+05     3.99e+05         2.47         3.65          155         11.3\n",
            "Wall time: 47.90725209399989\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10     3.31e+06        1e+04      3.3e+06         1.42         2.05          483         37.2\n",
            "     29    20     5.76e+05      1.5e+04     5.62e+05         1.78          2.5          245         15.3\n",
            "     29    30      3.4e+06     1.89e+04     3.39e+06         1.99         2.81          489         37.6\n",
            "     29    40     2.27e+05     1.46e+04     2.12e+05         2.04         2.47          132         9.42\n",
            "     29    50     3.85e+04     3.73e+03     3.47e+04        0.933         1.25         19.1         3.81\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29     2     4.14e+05     3.04e+04     3.83e+05         2.45         3.56          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29   49.286    0.002     3.59e+04     2.65e+06     2.68e+06          2.6         3.93          223         20.3\n",
            "! Validation         29   49.286    0.002     3.28e+04     3.63e+05     3.95e+05         2.51          3.7          153         11.2\n",
            "Wall time: 49.286814908999986\n",
            "! Best model       29 395302.266\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10     2.97e+06     5.57e+03     2.97e+06         1.12         1.53          317         35.2\n",
            "     30    20     6.92e+06     7.99e+03     6.91e+06         1.16         1.83          538         53.8\n",
            "     30    30     6.01e+05     4.16e+04      5.6e+05         2.84         4.17          321         15.3\n",
            "     30    40     3.38e+06     1.34e+04     3.37e+06         1.64         2.36          488         37.5\n",
            "     30    50     2.09e+07     1.67e+03     2.09e+07        0.564        0.836          468         93.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30     2     4.19e+05     2.86e+04      3.9e+05         2.34         3.46          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30   50.645    0.002     3.02e+04     2.65e+06     2.68e+06         2.33         3.66          227         20.6\n",
            "! Validation         30   50.645    0.002      3.1e+04     3.67e+05     3.98e+05          2.4         3.59          154         11.3\n",
            "Wall time: 50.64532721799969\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10     7.25e+06     3.05e+03     7.24e+06         0.91         1.13          551         55.1\n",
            "     31    20     6.97e+05     4.11e+04     6.56e+05         2.77         4.15          166         16.6\n",
            "     31    30      2.8e+04     1.86e+03     2.61e+04        0.623        0.883         16.5          3.3\n",
            "     31    40     4.82e+05     1.24e+05     3.57e+05         4.85         7.21         97.8         12.2\n",
            "     31    50     3.33e+06     1.08e+04     3.32e+06         1.46         2.13          484         37.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31     2     4.27e+05     2.63e+04     4.01e+05         2.21         3.31          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31   51.988    0.002     2.74e+04     2.64e+06     2.67e+06          2.2         3.49          227         20.6\n",
            "! Validation         31   51.988    0.002     2.85e+04     3.74e+05     4.03e+05         2.27         3.45          156         11.4\n",
            "Wall time: 51.98907744600001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10     9.77e+05     1.64e+04      9.6e+05            2         2.62          401           20\n",
            "     32    20     3.86e+05     1.67e+04     3.69e+05            2         2.65          124         12.4\n",
            "     32    30     7.06e+06     1.38e+04     7.05e+06         1.51          2.4          543         54.3\n",
            "     32    40     4.99e+04     7.37e+03     4.25e+04         1.41         1.76         50.6         4.22\n",
            "     32    50     6.14e+05      5.1e+04     5.63e+05         3.19         4.62          154         15.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32     2     4.22e+05     2.76e+04     3.95e+05         2.26          3.4          180         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32   53.349    0.002     3.62e+04     2.63e+06     2.67e+06         2.51         3.97          223         20.3\n",
            "! Validation         32   53.349    0.002        3e+04      3.7e+05        4e+05         2.31         3.53          154         11.3\n",
            "Wall time: 53.34972836099951\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10     3.59e+05     3.61e+04     3.23e+05         2.92         3.89          163         11.6\n",
            "     33    20     2.08e+07     1.08e+03     2.08e+07        0.471        0.671          466         93.3\n",
            "     33    30     1.71e+05     8.27e+04     8.81e+04         4.18         5.88           85         6.07\n",
            "     33    40     2.71e+05     6.39e+03     2.65e+05         1.32         1.63          147         10.5\n",
            "     33    50     5.85e+05     1.26e+04     5.72e+05         1.61         2.29          248         15.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33     2     4.27e+05      2.6e+04     4.01e+05         2.17          3.3          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33   54.719    0.002     2.81e+04     2.63e+06     2.66e+06         2.21         3.51          226         20.5\n",
            "! Validation         33   54.719    0.002     2.82e+04     3.74e+05     4.02e+05         2.22         3.43          155         11.4\n",
            "Wall time: 54.71948505799992\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10     9.47e+05     1.94e+04     9.28e+05         2.19         2.85          394         19.7\n",
            "     34    20     5.66e+05      1.1e+04     5.55e+05         1.53         2.14          122         15.2\n",
            "     34    30     4.56e+04     4.65e+03      4.1e+04         1.26          1.4         16.6         4.14\n",
            "     34    40     4.02e+05     2.93e+04     3.73e+05         2.39          3.5          125         12.5\n",
            "     34    50     5.01e+04     9.25e+03     4.09e+04          1.5         1.97         33.1         4.14\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34     2     4.24e+05     2.64e+04     3.97e+05         2.18         3.33          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34   56.115    0.002     3.31e+04     2.63e+06     2.66e+06          2.4         3.79          223         20.3\n",
            "! Validation         34   56.115    0.002     2.85e+04     3.71e+05        4e+05         2.23         3.45          154         11.3\n",
            "Wall time: 56.11613513799966\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10     4.34e+04     9.52e+03     3.38e+04          1.6            2           64         3.76\n",
            "     35    20     5.59e+06      2.2e+03     5.59e+06        0.693        0.959          725         48.4\n",
            "     35    30     5.36e+05     4.11e+04     4.94e+05         2.85         4.15          302         14.4\n",
            "     35    40     2.65e+05     6.28e+03     2.58e+05         1.29         1.62          146         10.4\n",
            "     35    50     2.61e+05     1.97e+05     6.32e+04         7.21         9.09         51.4         5.14\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35     2     4.25e+05     2.61e+04     3.98e+05         2.14         3.31          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35   57.551    0.002     3.03e+04     2.63e+06     2.66e+06         2.27         3.65          224         20.4\n",
            "! Validation         35   57.551    0.002     2.82e+04     3.72e+05        4e+05         2.19         3.43          154         11.3\n",
            "Wall time: 57.55156016099954\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10     3.89e+05     1.07e+04     3.78e+05         1.51         2.12          164         12.6\n",
            "     36    20     1.19e+06     3.82e+03     1.18e+06        0.969         1.26          289         22.2\n",
            "     36    30     5.57e+06     2.32e+03     5.57e+06         0.73        0.985          724         48.3\n",
            "     36    40     4.14e+05     1.12e+05     3.02e+05          4.5         6.85           90         11.2\n",
            "     36    50      2.6e+05      4.1e+04     2.19e+05         2.85         4.14          182         9.58\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36     2     4.27e+05      2.5e+04     4.02e+05         2.08         3.23          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36   58.938    0.002     2.85e+04     2.63e+06     2.66e+06         2.21         3.55          224         20.4\n",
            "! Validation         36   58.938    0.002      2.7e+04     3.74e+05     4.01e+05         2.13         3.36          155         11.3\n",
            "Wall time: 58.93911921199924\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10     2.08e+07          957     2.08e+07        0.437        0.633          467         93.4\n",
            "     37    20     9.72e+05      1.6e+04     9.56e+05         1.93         2.58          400           20\n",
            "     37    30     7.51e+04     1.35e+04     6.16e+04         1.79         2.38         55.8         5.08\n",
            "     37    40     5.49e+04     4.96e+03     4.99e+04         1.14         1.44         54.8         4.57\n",
            "     37    50     5.59e+05     1.45e+04     5.44e+05          1.7         2.46          241         15.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37     2     4.26e+05     2.49e+04     4.01e+05         2.05         3.23          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37   60.293    0.002     3.03e+04     2.62e+06     2.65e+06         2.24         3.64          223         20.3\n",
            "! Validation         37   60.293    0.002      2.7e+04     3.73e+05        4e+05         2.11         3.35          154         11.3\n",
            "Wall time: 60.29375404299935\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10      4.7e+05     4.54e+04     4.25e+05         2.95         4.36          280         13.3\n",
            "     38    20     5.04e+04     8.05e+03     4.24e+04          1.4         1.83         33.7         4.21\n",
            "     38    30     3.24e+05     4.39e+04     2.81e+05         2.67         4.29          238         10.8\n",
            "     38    40     2.62e+04      1.9e+03     2.43e+04        0.634        0.891         15.9         3.19\n",
            "     38    50      4.3e+05     1.87e+04     4.11e+05         1.89          2.8          131         13.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38     2     4.29e+05     2.39e+04     4.05e+05         1.98         3.16          182           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38   61.683    0.002     2.85e+04     2.62e+06     2.65e+06         2.14         3.53          224         20.3\n",
            "! Validation         38   61.683    0.002     2.59e+04     3.76e+05     4.01e+05         2.04         3.29          155         11.3\n",
            "Wall time: 61.68344700599937\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10     6.34e+07     3.07e+03     6.34e+07         0.97         1.13          651          163\n",
            "     39    20     2.21e+05     6.91e+04     1.52e+05         3.85         5.38          152         7.98\n",
            "     39    30     4.13e+05      1.4e+04     3.99e+05         1.78         2.42          168         12.9\n",
            "     39    40      6.9e+06     9.38e+03     6.89e+06         1.37         1.98          537         53.7\n",
            "     39    50     5.12e+05     2.72e+04     4.85e+05         2.27         3.37          299         14.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39     2     4.24e+05     2.42e+04     3.99e+05         1.99         3.18          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39   63.031    0.002     3.32e+04     2.62e+06     2.65e+06         2.35         3.81          220         20.2\n",
            "! Validation         39   63.031    0.002     2.62e+04     3.72e+05     3.98e+05         2.05          3.3          154         11.3\n",
            "Wall time: 63.03188636100003\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10     5.32e+05      1.3e+04     5.19e+05         1.62         2.33          265         14.7\n",
            "     40    20      4.7e+04     3.02e+03      4.4e+04        0.905         1.12         34.3         4.29\n",
            "     40    30     4.23e+05     7.58e+03     4.16e+05         1.34         1.78          171         13.2\n",
            "     40    40     1.89e+05     6.98e+04     1.19e+05         4.51          5.4         84.7         7.06\n",
            "     40    50     5.13e+05     1.01e+05     4.12e+05         3.72          6.5          158         13.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40     2     4.26e+05     2.28e+04     4.03e+05         1.91         3.09          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40   64.364    0.002     2.54e+04     2.61e+06     2.64e+06         2.07         3.35          224         20.4\n",
            "! Validation         40   64.364    0.002     2.47e+04     3.74e+05     3.99e+05         1.99         3.21          154         11.3\n",
            "Wall time: 64.36507815200002\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10     2.08e+07          257     2.08e+07        0.231        0.328          467         93.4\n",
            "     41    20      5.4e+05     7.58e+03     5.32e+05          1.2         1.78          119         14.9\n",
            "     41    30     1.05e+06     8.14e+03     1.05e+06         1.31         1.85          230         20.9\n",
            "     41    40     8.39e+05     1.19e+04     8.27e+05         1.44         2.23          279         18.6\n",
            "     41    50     5.69e+05     3.21e+04     5.37e+05         2.45         3.67          150           15\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41     2     4.29e+05     2.16e+04     4.07e+05         1.84            3          182           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41   65.701    0.002      2.4e+04     2.61e+06     2.63e+06         2.05         3.27          223         20.3\n",
            "! Validation         41   65.701    0.002     2.35e+04     3.77e+05        4e+05         1.93         3.13          155         11.3\n",
            "Wall time: 65.7015261229999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10     5.05e+05     1.34e+05      3.7e+05          4.3          7.5          149         12.4\n",
            "     42    20     3.59e+05     1.53e+04     3.43e+05          1.9         2.53          120           12\n",
            "     42    30     8.87e+05     1.54e+04     8.72e+05         1.97         2.54          267         19.1\n",
            "     42    40     1.23e+05     9.94e+04     2.34e+04         4.52         6.45         43.8         3.13\n",
            "     42    50     2.09e+07          201     2.09e+07        0.216         0.29          468         93.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42     2     4.16e+05     2.35e+04     3.92e+05         1.93         3.13          178         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42   67.044    0.002     3.32e+04     2.59e+06     2.62e+06         2.43         3.82          217         19.9\n",
            "! Validation         42   67.044    0.002     2.54e+04     3.66e+05     3.92e+05         2.02         3.26          152         11.2\n",
            "Wall time: 67.04471346799983\n",
            "! Best model       42 391614.660\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10     5.55e+04     5.22e+03     5.03e+04         1.16         1.48         36.7         4.59\n",
            "     43    20     1.09e+06     6.66e+03     1.09e+06         1.07         1.67          235         21.3\n",
            "     43    30     4.21e+05     6.29e+04     3.58e+05         3.34         5.13         97.9         12.2\n",
            "     43    40     4.06e+05     2.67e+04     3.79e+05         2.07         3.35          176         12.6\n",
            "     43    50     1.17e+05     7.67e+04     4.06e+04         4.02         5.67         57.7         4.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43     2     4.26e+05     2.08e+04     4.05e+05          1.8         2.95          182           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43   68.404    0.002     2.27e+04     2.59e+06     2.61e+06         1.98         3.12          224         20.4\n",
            "! Validation         43   68.404    0.002     2.27e+04     3.75e+05     3.97e+05          1.9         3.08          154         11.3\n",
            "Wall time: 68.40448335899964\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10     7.36e+06     2.06e+03     7.36e+06        0.666        0.929          555         55.5\n",
            "     44    20     4.66e+04     3.86e+03     4.27e+04        0.977         1.27         50.7         4.23\n",
            "     44    30      2.6e+04     5.69e+03     2.03e+04         1.09         1.54         14.6         2.92\n",
            "     44    40     1.13e+06      3.3e+04      1.1e+06         2.11         3.72          279         21.5\n",
            "     44    50     2.67e+05     8.89e+04     1.78e+05         4.69          6.1          129         8.63\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44     2     4.19e+05     2.16e+04     3.98e+05         1.85         3.01          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44   69.817    0.002      3.5e+04     2.55e+06     2.58e+06         2.47         3.81          217         19.9\n",
            "! Validation         44   69.817    0.002     2.35e+04     3.69e+05     3.93e+05         1.95         3.13          153         11.2\n",
            "Wall time: 69.81742184599989\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10      4.9e+05     1.51e+04     4.75e+05         1.78         2.52          254         14.1\n",
            "     45    20     2.83e+05     4.44e+04     2.38e+05          2.8         4.31          220         9.99\n",
            "     45    30     9.85e+05      1.1e+04     9.74e+05         1.66         2.14          222         20.2\n",
            "     45    40     5.41e+04     8.99e+03     4.52e+04         1.44         1.94         52.2         4.35\n",
            "     45    50      3.1e+05     1.57e+05     1.53e+05         5.26          8.1           64            8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45     2     4.17e+05     2.18e+04     3.95e+05          1.9         3.02          179         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45   71.217    0.002     5.27e+04      2.5e+06     2.55e+06         2.75         4.51          216         19.7\n",
            "! Validation         45   71.217    0.002     2.36e+04     3.67e+05     3.91e+05         1.99         3.14          152         11.2\n",
            "Wall time: 71.21738527699927\n",
            "! Best model       45 390616.702\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10     3.94e+05     6.26e+04     3.32e+05         3.29         5.12          153         11.8\n",
            "     46    20     1.11e+05     1.02e+05     9.02e+03         4.51         6.54         27.2         1.94\n",
            "     46    30     2.38e+04     1.55e+04     8.25e+03         1.88         2.55         9.29         1.86\n",
            "     46    40     6.62e+04     1.24e+04     5.39e+04         1.56         2.27           57         4.75\n",
            "     46    50     7.33e+06     3.64e+03     7.33e+06        0.858         1.23          554         55.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46     2     4.14e+05     2.29e+04     3.91e+05         1.95         3.09          178         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46   72.582    0.002     8.18e+04     2.45e+06     2.53e+06          3.1         5.46          212         19.4\n",
            "! Validation         46   72.582    0.002     2.46e+04     3.64e+05     3.88e+05         2.04          3.2          151         11.1\n",
            "Wall time: 72.58317942099984\n",
            "! Best model       46 388209.692\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10     5.98e+04     1.22e+04     4.76e+04         1.63         2.26         35.7         4.46\n",
            "     47    20     5.32e+05     2.81e+04     5.04e+05         2.27         3.43          145         14.5\n",
            "     47    30     7.64e+04     3.62e+04     4.02e+04         2.78         3.89         16.4          4.1\n",
            "     47    40     2.54e+05     6.49e+04     1.89e+05         3.34         5.21          196          8.9\n",
            "     47    50      4.1e+05     1.93e+04     3.91e+05         1.89         2.84          205         12.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47     2     4.14e+05     2.33e+04      3.9e+05         1.99         3.12          178         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47   73.952    0.002     1.01e+05     2.41e+06     2.51e+06         3.26         5.89          211         19.3\n",
            "! Validation         47   73.952    0.002      2.5e+04     3.63e+05     3.88e+05         2.09         3.23          151         11.1\n",
            "Wall time: 73.95248904699929\n",
            "! Best model       47 387541.782\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10        8e+05     2.35e+04     7.76e+05         2.13         3.14          270           18\n",
            "     48    20      6.7e+04     1.56e+04     5.14e+04         1.89         2.56         37.1         4.64\n",
            "     48    30     2.94e+05     1.06e+04     2.83e+05         1.29         2.11          152         10.9\n",
            "     48    40     2.59e+05     1.13e+05     1.45e+05         5.53         6.89           78          7.8\n",
            "     48    50     6.49e+04     3.61e+04     2.88e+04         2.79         3.88         13.9         3.47\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48     2     4.19e+05     2.35e+04     3.96e+05         2.04         3.13          180           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48   75.322    0.002     1.17e+05     2.37e+06     2.49e+06         3.25         6.25          212         19.3\n",
            "! Validation         48   75.322    0.002     2.49e+04     3.65e+05      3.9e+05         2.11         3.22          152         11.2\n",
            "Wall time: 75.32268286599992\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10     7.64e+05     2.39e+04      7.4e+05         2.12         3.16          264         17.6\n",
            "     49    20     9.09e+05     1.14e+04     8.98e+05         1.55         2.18          271         19.4\n",
            "     49    30     3.98e+05     3.83e+04      3.6e+05         2.61            4          172         12.3\n",
            "     49    40     2.47e+05     4.85e+04     1.98e+05         2.97         4.51          200          9.1\n",
            "     49    50      9.5e+04     2.68e+04     6.81e+04          2.3         3.35         64.1         5.34\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49     2     4.18e+05     2.43e+04     3.93e+05         2.15         3.19          179         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49   76.671    0.002     1.13e+05     2.35e+06     2.46e+06         3.29         6.07          208         19.1\n",
            "! Validation         49   76.671    0.002     2.54e+04     3.63e+05     3.89e+05         2.18         3.26          151         11.2\n",
            "Wall time: 76.6711977319992\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10     1.35e+06     4.21e+05     9.27e+05         6.96         13.3          256         19.7\n",
            "     50    20     9.95e+04     9.02e+04     9.33e+03         4.42         6.14         9.88         1.98\n",
            "     50    30     9.21e+04     3.08e+04     6.12e+04         2.47         3.59         60.7         5.06\n",
            "     50    40     2.36e+05     5.11e+04     1.85e+05         3.15         4.62          194         8.81\n",
            "     50    50     5.83e+04     3.42e+04     2.41e+04         2.81         3.78         12.7         3.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50     2     4.08e+05     2.65e+04     3.81e+05          2.3         3.33          176         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50   78.034    0.002     1.33e+05     2.31e+06     2.44e+06         3.56          6.5          203         18.7\n",
            "! Validation         50   78.034    0.002     2.73e+04     3.55e+05     3.83e+05         2.31         3.38          149           11\n",
            "Wall time: 78.03455561299961\n",
            "! Best model       50 382789.627\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10      4.3e+05     4.03e+04     3.89e+05          3.3         4.11          268         12.8\n",
            "     51    20     9.94e+04     9.01e+04     9.22e+03         4.47         6.14         9.82         1.96\n",
            "     51    30     2.26e+05     6.24e+04     1.63e+05         3.47         5.11          182         8.26\n",
            "     51    40     3.51e+05     2.66e+04     3.25e+05         2.54         3.34          117         11.7\n",
            "     51    50     7.17e+05     1.66e+04     7.01e+05         1.85         2.64          257         17.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51     2     4.03e+05     2.86e+04     3.74e+05         2.41         3.46          174         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51   79.441    0.002     1.68e+05     2.25e+06     2.42e+06         3.71         7.16          201         18.5\n",
            "! Validation         51   79.441    0.002     2.91e+04     3.51e+05      3.8e+05         2.41         3.49          148         10.9\n",
            "Wall time: 79.4418302969998\n",
            "! Best model       51 379828.813\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10     5.37e+04     2.47e+04      2.9e+04         2.29         3.22         27.9         3.48\n",
            "     52    20     9.19e+04     7.39e+04      1.8e+04         4.04         5.56         13.7         2.75\n",
            "     52    30     4.22e+04     2.65e+04     1.57e+04         2.62         3.33         10.3         2.56\n",
            "     52    40     1.86e+05     7.45e+04     1.11e+05         4.32         5.58          102         6.82\n",
            "     52    50     1.35e+05     3.67e+04     9.82e+04         2.61         3.92          109         6.41\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52     2     3.94e+05     2.95e+04     3.64e+05         2.47         3.51          171         11.4\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52   80.799    0.002     1.43e+05     2.25e+06      2.4e+06         3.54         6.52          200         18.4\n",
            "! Validation         52   80.799    0.002     3.01e+04     3.45e+05     3.75e+05         2.47         3.55          146         10.8\n",
            "Wall time: 80.79952445099934\n",
            "! Best model       52 374883.023\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10     5.81e+04     4.01e+04      1.8e+04         3.37          4.1         46.6         2.74\n",
            "     53    20     4.31e+05     4.24e+04     3.89e+05         3.44         4.21          268         12.8\n",
            "     53    30     3.42e+05     3.53e+04     3.07e+05         2.98         3.84          113         11.3\n",
            "     53    40     2.42e+06      1.9e+05     2.23e+06         6.19         8.93          275         30.5\n",
            "     53    50     6.96e+05     2.46e+04     6.71e+05         2.31         3.21          302         16.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53     2      3.9e+05     3.07e+04     3.59e+05         2.54         3.58          170         11.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53   82.201    0.002     1.59e+05     2.22e+06     2.38e+06         3.64         6.87          199         18.3\n",
            "! Validation         53   82.201    0.002     3.12e+04     3.42e+05     3.73e+05         2.54         3.61          145         10.7\n",
            "Wall time: 82.20171683199987\n",
            "! Best model       53 372791.737\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10     1.73e+05     3.73e+04     1.35e+05         2.88         3.95          143         7.53\n",
            "     54    20     5.12e+05     6.06e+04     4.52e+05         3.47         5.03          220         13.7\n",
            "     54    30     5.46e+07     4.74e+06     4.99e+07         36.5         44.5          578          144\n",
            "     54    40     3.09e+05      4.1e+04     2.68e+05         3.27         4.14          106         10.6\n",
            "     54    50      2.2e+05     5.77e+04     1.62e+05         3.49         4.91          181         8.23\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54     2     3.81e+05     3.44e+04     3.46e+05         2.73          3.8          166         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54   83.560    0.002     2.05e+05     2.15e+06     2.35e+06         3.97         7.65          194         17.9\n",
            "! Validation         54   83.560    0.002     3.43e+04     3.34e+05     3.68e+05          2.7         3.79          144         10.6\n",
            "Wall time: 83.56047867799953\n",
            "! Best model       54 368257.204\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10     6.32e+05     3.22e+04        6e+05         2.09         3.67          127         15.8\n",
            "     55    20     6.21e+04     2.96e+04     3.26e+04         2.45         3.52         44.3         3.69\n",
            "     55    30     8.25e+04     6.03e+04     2.22e+04         3.66         5.02         15.3         3.05\n",
            "     55    40     7.23e+05     2.19e+04     7.01e+05         2.33         3.03          154         17.1\n",
            "     55    50     5.21e+04     3.18e+04     2.03e+04         2.65         3.65         23.3         2.91\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55     2     3.88e+05     3.43e+04     3.54e+05         2.74         3.79          168         11.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55   84.902    0.002     2.22e+05     2.12e+06     2.34e+06         3.64         7.52          199         18.2\n",
            "! Validation         55   84.902    0.002     3.41e+04     3.39e+05     3.73e+05         2.69         3.78          144         10.6\n",
            "Wall time: 84.90277568199963\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10     4.54e+06     5.58e+05     3.98e+06           11         15.3          612         40.8\n",
            "     56    20     2.96e+05     3.79e+04     2.58e+05         2.89         3.98          146         10.4\n",
            "     56    30     3.64e+05     3.89e+04     3.25e+05         3.05         4.03          117         11.7\n",
            "     56    40     2.89e+05     3.35e+04     2.55e+05         2.69         3.75          145         10.3\n",
            "     56    50     7.62e+06     3.57e+04     7.59e+06         2.85         3.87          563         56.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56     2     3.89e+05     3.53e+04     3.53e+05         2.78         3.84          168         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56   86.240    0.002     2.16e+05     2.11e+06     2.33e+06         3.75         7.68          197         18.1\n",
            "! Validation         56   86.240    0.002     3.52e+04     3.39e+05     3.74e+05         2.73         3.84          144         10.7\n",
            "Wall time: 86.241103716\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10     5.31e+07     5.89e+06     4.72e+07         39.8         49.6          562          141\n",
            "     57    20     4.71e+06     4.14e+05      4.3e+06         7.54         13.2          424         42.4\n",
            "     57    30     7.72e+05     3.92e+04     7.33e+05         2.65         4.05          263         17.5\n",
            "     57    40     4.64e+05     5.82e+04     4.06e+05         3.72         4.93          274           13\n",
            "     57    50     3.84e+05     2.07e+04     3.63e+05         2.07         2.94          259         12.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57     2     3.95e+05     3.77e+04     3.57e+05         2.84         3.97          169         11.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57   87.578    0.002     2.35e+05     2.07e+06     2.31e+06         3.83         8.06          195         17.9\n",
            "! Validation         57   87.578    0.002     3.68e+04     3.42e+05     3.79e+05         2.78         3.93          145         10.7\n",
            "Wall time: 87.57827382300002\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10     3.67e+05     1.81e+04     3.49e+05         2.03         2.76          254         12.1\n",
            "     58    20     1.71e+05     3.83e+04     1.33e+05         2.81         4.01          142         7.46\n",
            "     58    30     3.77e+04     2.81e+04      9.6e+03         2.69         3.43         34.1            2\n",
            "     58    40     7.57e+04     2.52e+04     5.06e+04         2.45         3.25         78.2          4.6\n",
            "     58    50      2.1e+06     1.54e+05     1.95e+06         6.07         8.02          257         28.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58     2     3.84e+05     3.67e+04     3.47e+05         2.82         3.92          166           11\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58   88.967    0.002     1.97e+05     2.08e+06     2.28e+06         3.57         7.17          192         17.7\n",
            "! Validation         58   88.967    0.002     3.66e+04     3.36e+05     3.72e+05         2.77         3.91          144         10.6\n",
            "Wall time: 88.9678240889998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10     7.99e+05     2.52e+04     7.74e+05         2.48         3.25          360           18\n",
            "     59    20     5.34e+05     5.08e+04     4.83e+05         3.34         4.61          185         14.2\n",
            "     59    30     4.97e+04      3.6e+04     1.37e+04         2.83         3.88         19.2         2.39\n",
            "     59    40     4.23e+05     8.02e+04     3.43e+05          4.3         5.79          192           12\n",
            "     59    50     3.81e+05      1.9e+04     3.62e+05         2.16         2.82          222         12.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59     2     3.83e+05     3.91e+04     3.44e+05          2.9         4.04          165         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59   90.314    0.002     2.25e+05     2.03e+06     2.25e+06         3.84         7.81          190         17.5\n",
            "! Validation         59   90.314    0.002     3.85e+04     3.34e+05     3.73e+05         2.84         4.01          143         10.6\n",
            "Wall time: 90.31493024799965\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10     2.48e+05      3.9e+04     2.09e+05         3.02         4.04          131         9.34\n",
            "     60    20     8.19e+05     2.42e+04     7.95e+05         2.53         3.18          365         18.2\n",
            "     60    30     3.38e+04     1.79e+04     1.58e+04         1.92         2.74         12.9         2.57\n",
            "     60    40     1.39e+05     2.35e+04     1.16e+05         2.51         3.14         76.5         6.96\n",
            "     60    50     3.27e+04     2.67e+04     6.01e+03         2.54         3.34           27         1.59\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60     2     3.81e+05     4.01e+04     3.41e+05         2.95          4.1          164         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60   91.659    0.002     3.83e+05     1.87e+06     2.25e+06          3.9         8.83          188         17.2\n",
            "! Validation         60   91.659    0.002     3.91e+04     3.32e+05     3.71e+05         2.87         4.05          143         10.6\n",
            "Wall time: 91.65963425499922\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10     3.32e+05     1.42e+04     3.18e+05          1.8         2.44          242         11.5\n",
            "     61    20     3.13e+05     3.34e+04      2.8e+05         2.87         3.74          108         10.8\n",
            "     61    30      1.9e+05     1.16e+05     7.43e+04         5.09         6.95         83.7         5.58\n",
            "     61    40     2.46e+04     9.45e+03     1.52e+04         1.82         1.99         10.1         2.52\n",
            "     61    50     4.31e+04     2.87e+04     1.44e+04         2.57         3.47         19.6         2.45\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61     2     3.78e+05     3.78e+04     3.41e+05         2.89         3.98          163         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61   93.011    0.002     1.64e+05     2.08e+06     2.25e+06         3.51         6.97          192         17.7\n",
            "! Validation         61   93.011    0.002     3.74e+04     3.32e+05      3.7e+05         2.83         3.96          143         10.6\n",
            "Wall time: 93.01237392199982\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10      3.5e+05     1.41e+04     3.36e+05         1.82         2.43          119         11.9\n",
            "     62    20     4.57e+04     3.62e+04     9.51e+03         3.03         3.89           16         1.99\n",
            "     62    30     2.32e+05     3.39e+04     1.98e+05         2.79         3.77          127          9.1\n",
            "     62    40     2.23e+04     1.13e+04     1.11e+04         1.97         2.17          8.6         2.15\n",
            "     62    50     3.76e+06     2.06e+06     1.69e+06         12.2         29.4          346         26.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62     2     3.77e+05     4.05e+04     3.36e+05            3         4.12          162         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62   94.572    0.002      2.4e+05     1.96e+06      2.2e+06         3.88         7.99          186         17.1\n",
            "! Validation         62   94.572    0.002     3.96e+04      3.3e+05     3.69e+05         2.92         4.08          143         10.5\n",
            "Wall time: 94.57274565599982\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10      5.2e+04     2.42e+04     2.78e+04          2.6         3.18         40.9         3.41\n",
            "     63    20     2.52e+05     2.35e+04     2.29e+05          2.3         3.13          137         9.79\n",
            "     63    30      6.5e+05     2.76e+04     6.22e+05         2.56          3.4          290         16.1\n",
            "     63    40     2.44e+04     1.42e+04     1.02e+04         1.68         2.44         10.3         2.06\n",
            "     63    50     1.97e+06     1.48e+05     1.83e+06         5.61         7.87          359         27.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63     2     3.79e+05     4.14e+04     3.38e+05         3.05         4.16          163         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63   95.932    0.002     2.83e+05     1.89e+06     2.17e+06         3.98         8.18          187         17.1\n",
            "! Validation         63   95.932    0.002     4.06e+04     3.31e+05     3.71e+05         2.96         4.12          143         10.6\n",
            "Wall time: 95.9322300949998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10     3.76e+05     2.01e+04     3.56e+05         2.24          2.9          220         12.2\n",
            "     64    20     1.28e+06        1e+05     1.18e+06         4.15         6.48          355         22.2\n",
            "     64    30     2.22e+05     7.32e+04     1.49e+05         4.02         5.53          174         7.89\n",
            "     64    40     8.95e+04     3.52e+04     5.43e+04          2.6         3.84         57.2         4.77\n",
            "     64    50     1.77e+05     6.82e+04     1.09e+05         3.64         5.34          128         6.75\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64     2     3.87e+05     4.47e+04     3.43e+05         3.19         4.32          164         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              64   97.261    0.002     2.45e+05      1.9e+06     2.15e+06         4.24         8.11          186         17.1\n",
            "! Validation         64   97.261    0.002     4.36e+04     3.33e+05     3.77e+05         3.08         4.27          144         10.6\n",
            "Wall time: 97.26123257999916\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65    10     1.82e+04     1.67e+04     1.49e+03         1.86         2.64         3.94        0.789\n",
            "     65    20      3.8e+06     2.27e+06     1.53e+06           13         30.8          328         25.3\n",
            "     65    30      7.8e+04      3.6e+04      4.2e+04         2.77         3.88         50.3         4.19\n",
            "     65    40     4.53e+04        4e+04     5.38e+03         3.07         4.09           12          1.5\n",
            "     65    50     7.19e+05        5e+04     6.69e+05         2.98         4.57          134         16.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65     2     3.77e+05      4.8e+04     3.29e+05         3.33         4.48          160         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              65   98.717    0.002     3.07e+05     1.81e+06     2.11e+06         4.34         8.89          177         16.3\n",
            "! Validation         65   98.717    0.002     4.68e+04     3.25e+05     3.72e+05         3.21         4.43          143         10.5\n",
            "Wall time: 98.71778690699921\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66    10     2.22e+05     4.61e+04     1.76e+05         3.28         4.39          120         8.59\n",
            "     66    20     4.59e+04     3.56e+04     1.03e+04          2.9         3.86         35.3         2.08\n",
            "     66    30     3.48e+05     3.03e+04     3.17e+05          2.9         3.56          242         11.5\n",
            "     66    40     5.59e+04     4.73e+04     8.62e+03         3.11         4.45         22.8          1.9\n",
            "     66    50     6.37e+05     3.48e+04     6.02e+05         2.87         3.82          286         15.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66     2     3.82e+05     4.86e+04     3.33e+05         3.38         4.51          161         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              66  100.069    0.002     2.73e+05     1.81e+06     2.08e+06         4.29         8.33          183         16.8\n",
            "! Validation         66  100.069    0.002     4.72e+04     3.27e+05     3.74e+05         3.24         4.45          143         10.6\n",
            "Wall time: 100.0697963119992\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67    10     4.01e+05     4.23e+04     3.58e+05         3.47         4.21          257         12.2\n",
            "     67    20      4.2e+05     2.61e+04     3.93e+05         2.49         3.31          231         12.8\n",
            "     67    30     7.54e+05     7.26e+04     6.82e+05         3.84         5.51          253         16.9\n",
            "     67    40     8.11e+05     3.86e+04     7.72e+05         2.88         4.02          252           18\n",
            "     67    50     3.61e+05     1.13e+05     2.48e+05         5.14         6.87          122         10.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67     2        4e+05     5.39e+04     3.46e+05         3.55         4.75          165         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              67  101.438    0.002     3.12e+05     1.74e+06     2.05e+06         4.77         9.18          182         16.6\n",
            "! Validation         67  101.438    0.002     5.17e+04     3.34e+05     3.86e+05         3.38         4.65          144         10.7\n",
            "Wall time: 101.43876442199962\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68    10     3.15e+06     9.41e+04     3.06e+06         4.85         6.28          358         35.8\n",
            "     68    20     1.78e+05      1.5e+05     2.82e+04         6.04         7.91         27.5         3.44\n",
            "     68    30     6.68e+05     8.23e+04     5.85e+05         4.27         5.87          203         15.7\n",
            "     68    40      2.1e+07     3.83e+04      2.1e+07         2.71            4          468         93.7\n",
            "     68    50     7.66e+06     1.34e+05     7.53e+06         5.59          7.5          561         56.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68     2     3.82e+05     5.63e+04     3.26e+05         3.66         4.86          159         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              68  102.881    0.002     3.27e+05     1.69e+06     2.01e+06         4.69         9.29          170         15.8\n",
            "! Validation         68  102.881    0.002     5.45e+04     3.22e+05     3.77e+05         3.49         4.78          142         10.5\n",
            "Wall time: 102.88186051699995\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69    10     7.57e+06     1.37e+05     7.43e+06         5.64         7.57          558         55.8\n",
            "     69    20     1.92e+05     7.64e+04     1.16e+05         3.74         5.65          132         6.96\n",
            "     69    30     1.45e+06      7.5e+05     6.98e+05         10.5         17.7          222         17.1\n",
            "     69    40     4.02e+05     9.01e+04     3.12e+05         4.49         6.14          240         11.4\n",
            "     69    50     1.24e+06     1.27e+05     1.12e+06         4.88          7.3          346         21.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69     2     3.85e+05     5.75e+04     3.28e+05          3.7          4.9          160         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              69  104.222    0.002     3.01e+05     1.66e+06     1.97e+06         4.68         8.82          175         16.1\n",
            "! Validation         69  104.222    0.002     5.55e+04     3.23e+05     3.79e+05         3.52         4.82          143         10.6\n",
            "Wall time: 104.22262659199987\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70    10     3.89e+05     3.76e+04     3.51e+05         3.28         3.97          121         12.1\n",
            "     70    20     5.96e+04     5.27e+04     6.91e+03          3.5         4.69         13.6          1.7\n",
            "     70    30     2.26e+06     1.18e+06     1.08e+06         11.7         22.3          276         21.2\n",
            "     70    40      3.6e+05     1.14e+05     2.46e+05         5.28          6.9          122         10.1\n",
            "     70    50     3.84e+06     1.23e+06     2.61e+06         16.8         22.7          495           33\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70     2     3.88e+05     5.97e+04     3.28e+05         3.77            5          160         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              70  105.583    0.002     4.09e+05     1.53e+06     1.94e+06         5.06         9.81          172         15.7\n",
            "! Validation         70  105.583    0.002     5.79e+04     3.23e+05     3.81e+05          3.6         4.93          143         10.6\n",
            "Wall time: 105.58374907500001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71    10     6.52e+05     5.85e+04     5.94e+05         3.53         4.95          236         15.8\n",
            "     71    20     8.82e+04     4.62e+04      4.2e+04         3.48         4.39         62.9         4.19\n",
            "     71    30     2.48e+05     3.96e+04     2.09e+05          3.1         4.07          131         9.34\n",
            "     71    40      1.7e+05     6.41e+04     1.06e+05          3.8         5.18          146         6.66\n",
            "     71    50     6.19e+04      5.6e+04      5.9e+03         3.72         4.84         26.7         1.57\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71     2     3.86e+05     5.85e+04     3.28e+05         3.71         4.95          160         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              71  106.967    0.002     3.11e+05      1.6e+06     1.92e+06          4.8         9.14          173         15.9\n",
            "! Validation         71  106.967    0.002     5.72e+04     3.22e+05     3.79e+05         3.56          4.9          143         10.6\n",
            "Wall time: 106.96780513399972\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72    10     3.71e+07     8.43e+06     2.86e+07         41.3         59.4          438          109\n",
            "     72    20        1e+06     1.51e+05     8.52e+05         6.48         7.94          245         18.9\n",
            "     72    30     1.23e+05     6.98e+04     5.35e+04         4.24         5.41           71         4.73\n",
            "     72    40      3.8e+06     1.92e+06     1.88e+06         13.1         28.4          280           28\n",
            "     72    50     7.03e+05     5.05e+04     6.52e+05         3.43          4.6          231         16.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72     2     3.93e+05     6.37e+04     3.29e+05         3.89         5.16          161         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              72  108.316    0.002      3.7e+05      1.5e+06     1.87e+06         5.53         10.3          168         15.4\n",
            "! Validation         72  108.316    0.002     6.29e+04     3.22e+05     3.85e+05         3.73         5.13          143         10.6\n",
            "Wall time: 108.31704860599984\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73    10      3.1e+05     9.27e+04     2.17e+05         4.89         6.23         95.4         9.54\n",
            "     73    20     1.98e+05     6.17e+04     1.36e+05         3.75         5.08         83.1         7.56\n",
            "     73    30     6.24e+05     7.53e+04     5.49e+05         4.94         5.61          136         15.2\n",
            "     73    40     8.82e+05     1.11e+05     7.71e+05         5.33         6.82          234           18\n",
            "     73    50     1.14e+06     1.59e+05     9.82e+05         5.58         8.16          324         20.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73     2     3.82e+05     6.58e+04     3.17e+05         3.97         5.25          157         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              73  109.660    0.002     3.68e+05     1.47e+06     1.84e+06         5.42         9.96          163           15\n",
            "! Validation         73  109.660    0.002     6.58e+04     3.15e+05     3.81e+05         3.83         5.25          141         10.5\n",
            "Wall time: 109.66020022699922\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74    10     7.69e+06      1.9e+05      7.5e+06         6.66         8.92          560           56\n",
            "     74    20     1.55e+06     6.62e+05     8.89e+05         11.7         16.6          251         19.3\n",
            "     74    30     1.83e+06     1.17e+06     6.67e+05         13.1         22.1          217         16.7\n",
            "     74    40     3.78e+06     1.53e+06     2.25e+06         17.8         25.3          460         30.7\n",
            "     74    50     1.16e+06     1.25e+05     1.03e+06         4.98         7.23          332         20.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74     2     3.77e+05     6.47e+04     3.12e+05         3.95          5.2          156         10.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              74  110.992    0.002     4.24e+05     1.42e+06     1.84e+06         5.48         10.6          165         15.1\n",
            "! Validation         74  110.992    0.002      6.5e+04     3.12e+05     3.77e+05         3.81         5.21          140         10.4\n",
            "Wall time: 110.99275974099965\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75    10     2.64e+05     5.64e+04     2.07e+05          3.8         4.86         93.1         9.31\n",
            "     75    20     2.58e+05     4.83e+04     2.09e+05         3.05          4.5          131         9.36\n",
            "     75    30     1.84e+05     1.13e+05     7.05e+04          5.3         6.88         43.5         5.43\n",
            "     75    40     1.11e+06     1.26e+05      9.8e+05         5.42         7.26          263         20.2\n",
            "     75    50     7.66e+04     4.13e+04     3.53e+04         3.18         4.16         65.3         3.84\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75     2     3.57e+05     5.83e+04     2.99e+05         3.75         4.94          152         10.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              75  112.340    0.002     3.43e+05     1.44e+06     1.78e+06         4.87         9.57          161         14.8\n",
            "! Validation         75  112.340    0.002     5.97e+04     3.05e+05     3.65e+05         3.65         4.99          139         10.3\n",
            "Wall time: 112.34086191999995\n",
            "! Best model       75 365141.050\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76    10     5.84e+05     5.61e+04     5.28e+05         3.71         4.85          267         14.9\n",
            "     76    20     1.68e+05     3.98e+04     1.28e+05          3.1         4.08          102         7.32\n",
            "     76    30     3.57e+05     3.61e+04     3.21e+05         3.11         3.89          116         11.6\n",
            "     76    40     5.73e+05     4.57e+04     5.27e+05          3.3         4.37          223         14.8\n",
            "     76    50     2.99e+05     9.19e+04     2.07e+05         4.87          6.2         93.1         9.31\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76     2     3.55e+05     5.73e+04     2.98e+05         3.74          4.9          152         10.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              76  113.687    0.002     4.13e+05     1.34e+06     1.76e+06          5.4         10.3          160         14.6\n",
            "! Validation         76  113.687    0.002     5.95e+04     3.05e+05     3.64e+05         3.65         4.98          139         10.3\n",
            "Wall time: 113.68728088199987\n",
            "! Best model       76 364051.733\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77    10     2.03e+05     1.52e+05     5.12e+04         5.18         7.97         55.5         4.63\n",
            "     77    20      1.7e+06     1.23e+06     4.67e+05         13.9         22.7          182           14\n",
            "     77    30     7.68e+05      2.3e+05     5.38e+05         7.46          9.8          135           15\n",
            "     77    40     5.14e+04     4.81e+04     3.28e+03         3.78         4.49         5.86         1.17\n",
            "     77    50      1.2e+05     5.03e+04     7.01e+04         3.38         4.59          119         5.42\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77     2     3.41e+05     5.85e+04     2.82e+05          3.8         4.95          147         9.99\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              77  115.075    0.002     4.36e+05     1.32e+06     1.75e+06         5.53         11.2          152           14\n",
            "! Validation         77  115.075    0.002      6.1e+04     2.96e+05     3.57e+05          3.7         5.05          137         10.2\n",
            "Wall time: 115.07567248399937\n",
            "! Best model       77 357010.479\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78    10      2.1e+05     5.31e+04     1.57e+05         3.39         4.71         89.2         8.11\n",
            "     78    20     1.15e+05     4.91e+04     6.64e+04         3.22         4.53          116         5.27\n",
            "     78    30     6.87e+05     5.33e+04     6.34e+05         3.54         4.72          228         16.3\n",
            "     78    40     7.25e+04     4.58e+04     2.67e+04         3.48         4.38         50.1         3.34\n",
            "     78    50     1.31e+06     6.46e+05     6.63e+05         10.7         16.4          216         16.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78     2     3.29e+05     5.65e+04     2.72e+05          3.7         4.86          144         9.76\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              78  116.428    0.002     3.94e+05     1.37e+06     1.76e+06         5.29         10.5          153         14.1\n",
            "! Validation         78  116.428    0.002     5.92e+04     2.91e+05      3.5e+05         3.62         4.97          135           10\n",
            "Wall time: 116.4286015939997\n",
            "! Best model       78 349740.977\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79    10     3.17e+05      2.8e+05     3.73e+04         6.43         10.8         51.4         3.95\n",
            "     79    20     7.99e+05     4.57e+04     7.53e+05         2.87         4.37          142         17.7\n",
            "     79    30      6.7e+05     8.77e+04     5.82e+05         4.42         6.06          203         15.6\n",
            "     79    40     1.85e+05     5.88e+04     1.26e+05         3.75         4.96         58.1         7.26\n",
            "     79    50     3.31e+05     2.72e+04     3.04e+05         2.81         3.37          113         11.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79     2     3.23e+05     5.28e+04      2.7e+05         3.56          4.7          144         9.71\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              79  117.823    0.002     4.29e+05     1.26e+06     1.69e+06         4.88         9.97          156         14.2\n",
            "! Validation         79  117.823    0.002     5.55e+04      2.9e+05     3.45e+05         3.49         4.81          135           10\n",
            "Wall time: 117.82390053499967\n",
            "! Best model       79 345376.554\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80    10     5.69e+04     5.47e+04     2.21e+03         3.69         4.78         7.69        0.961\n",
            "     80    20     4.96e+04     4.93e+04          298         3.52         4.54         6.01        0.353\n",
            "     80    30     5.47e+05     3.95e+04     5.08e+05         3.17         4.06          262         14.6\n",
            "     80    40      1.1e+06     4.72e+05     6.28e+05         9.49           14          211         16.2\n",
            "     80    50     5.48e+05     6.87e+04     4.79e+05         3.92         5.36          212         14.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80     2     3.19e+05     5.03e+04     2.69e+05         3.46         4.59          143         9.67\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              80  119.320    0.002     3.88e+05     1.27e+06     1.66e+06          4.8         9.78          154         14.1\n",
            "! Validation         80  119.320    0.002     5.31e+04     2.89e+05     3.42e+05          3.4         4.71          135         9.98\n",
            "Wall time: 119.3210654149998\n",
            "! Best model       80 342410.441\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81    10     1.27e+05      1.1e+05     1.74e+04         4.79         6.79         43.2          2.7\n",
            "     81    20     5.58e+05     3.69e+04     5.21e+05         2.97         3.93          295         14.8\n",
            "     81    30     1.14e+06      4.3e+05      7.1e+05         9.01         13.4          224         17.2\n",
            "     81    40     1.67e+06     8.65e+05     8.02e+05         11.9           19          165         18.3\n",
            "     81    50     2.65e+05     8.59e+04     1.79e+05         4.62            6          104         8.66\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81     2     3.03e+05     4.95e+04     2.53e+05         3.42         4.55          138         9.33\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              81  120.697    0.002      3.7e+05     1.28e+06     1.65e+06         4.81         9.81          149         13.8\n",
            "! Validation         81  120.697    0.002     5.22e+04     2.81e+05     3.34e+05         3.36         4.67          133         9.84\n",
            "Wall time: 120.69727218599928\n",
            "! Best model       81 333675.813\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82    10     1.16e+05     3.06e+04     8.58e+04         2.59         3.58         83.9         5.99\n",
            "     82    20     1.84e+06     1.02e+06     8.12e+05         12.8         20.7          166         18.4\n",
            "     82    30     1.17e+06     4.65e+05     7.04e+05         8.81         13.9          223         17.2\n",
            "     82    40     5.85e+04     2.96e+04      2.9e+04         2.63         3.52         59.2         3.48\n",
            "     82    50     5.77e+05     2.91e+04     5.48e+05         2.59         3.49          303         15.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82     2     2.95e+05     4.73e+04     2.48e+05         3.35         4.45          136         9.19\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              82  122.051    0.002     5.57e+05     1.13e+06     1.69e+06         4.77         11.7          147         13.4\n",
            "! Validation         82  122.051    0.002        5e+04     2.79e+05     3.29e+05         3.29         4.57          132         9.77\n",
            "Wall time: 122.05131208199964\n",
            "! Best model       82 328600.393\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83    10     5.11e+04     5.04e+04          782         3.57         4.59         8.01        0.572\n",
            "     83    20     6.46e+05     4.91e+04     5.97e+05         3.52         4.53          221         15.8\n",
            "     83    30     1.13e+05     2.76e+04     8.49e+04         2.48          3.4         83.5         5.96\n",
            "     83    40     1.22e+05     1.06e+05     1.62e+04         4.71         6.65         41.7          2.6\n",
            "     83    50     5.95e+05      3.3e+04     5.62e+05         2.88         3.72          307         15.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83     2     2.93e+05     4.76e+04     2.45e+05         3.34         4.46          135         9.13\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              83  123.409    0.002     2.93e+05     1.41e+06      1.7e+06         4.68         9.16          151         14.1\n",
            "! Validation         83  123.409    0.002     4.97e+04     2.77e+05     3.27e+05         3.26         4.56          132         9.74\n",
            "Wall time: 123.409997707\n",
            "! Best model       83 326946.645\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84    10     4.97e+04     4.97e+04         9.68         3.54         4.56         1.08       0.0637\n",
            "     84    20     1.39e+04     9.14e+03     4.72e+03          1.5         1.96         16.9         1.41\n",
            "     84    30     4.56e+05      7.3e+04     3.83e+05         4.41         5.53          127         12.7\n",
            "     84    40     9.05e+04     4.26e+04     4.79e+04         2.95         4.22         98.5         4.48\n",
            "     84    50     7.04e+04     4.66e+04     2.38e+04          3.3         4.42         37.9         3.16\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84     2      2.9e+05     4.89e+04     2.41e+05         3.37         4.52          134         9.06\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              84  124.783    0.002     4.57e+05     1.15e+06     1.61e+06         4.88         10.8          144         13.2\n",
            "! Validation         84  124.783    0.002     5.02e+04     2.75e+05     3.25e+05         3.27         4.58          131         9.71\n",
            "Wall time: 124.78414408999924\n",
            "! Best model       84 325378.571\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85    10     1.87e+05     1.47e+05     4.01e+04         6.18         7.84         61.4          4.1\n",
            "     85    20     3.89e+04     3.83e+04          589         3.12         4.01         3.97        0.496\n",
            "     85    30     1.03e+06     3.43e+05     6.83e+05         8.52           12          220         16.9\n",
            "     85    40     6.96e+05     8.11e+04     6.15e+05         4.16         5.82          209           16\n",
            "     85    50     3.68e+05     3.33e+05     3.55e+04         6.32         11.8         50.1         3.85\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85     2     2.84e+05     4.66e+04     2.37e+05         3.31         4.41          133         8.94\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              85  126.183    0.002     3.74e+05     1.24e+06     1.61e+06          4.8           10          150         13.8\n",
            "! Validation         85  126.183    0.002      4.8e+04     2.73e+05     3.21e+05         3.21         4.48          130         9.64\n",
            "Wall time: 126.18343733999973\n",
            "! Best model       85 321353.333\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86    10     6.15e+05     1.44e+04        6e+05         2.04         2.45          174         15.8\n",
            "     86    20     2.12e+05     3.76e+04     1.74e+05         3.05         3.96          119         8.53\n",
            "     86    30     3.73e+04     3.32e+04     4.11e+03          3.3         3.73         5.25         1.31\n",
            "     86    40     7.25e+05     1.03e+05     6.22e+05         4.73         6.56          210         16.1\n",
            "     86    50     1.31e+05     7.45e+04     5.61e+04         4.22         5.58           92         4.84\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86     2     2.83e+05      4.9e+04     2.34e+05         3.41         4.53          132         8.89\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              86  127.594    0.002     3.92e+05     1.17e+06     1.56e+06            5         10.1          144         13.2\n",
            "! Validation         86  127.594    0.002     5.02e+04     2.72e+05     3.22e+05         3.28         4.58          130          9.6\n",
            "Wall time: 127.5947487429994\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87    10     2.18e+05     2.27e+04     1.95e+05         2.46         3.08          190         9.04\n",
            "     87    20     2.29e+05     5.87e+04      1.7e+05         3.98         4.96         84.4         8.44\n",
            "     87    30     5.69e+04     3.75e+04     1.94e+04         3.23         3.96         42.7         2.85\n",
            "     87    40     4.16e+05     8.32e+04     3.33e+05         4.62          5.9          118         11.8\n",
            "     87    50     2.39e+05      8.7e+04     1.52e+05          4.5         6.03         95.7         7.97\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87     2     2.79e+05     4.86e+04      2.3e+05         3.38         4.51          131         8.78\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              87  128.939    0.002     4.56e+05      1.1e+06     1.56e+06         4.83         10.6          141         12.9\n",
            "! Validation         87  128.939    0.002     4.88e+04      2.7e+05     3.19e+05         3.22         4.52          129         9.56\n",
            "Wall time: 128.93915531899984\n",
            "! Best model       87 318690.045\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88    10     2.08e+04     2.07e+04         39.2         2.17         2.94         1.54        0.128\n",
            "     88    20     2.61e+05     3.81e+04     2.23e+05         2.71         3.99          135         9.66\n",
            "     88    30     7.98e+06     1.75e+05     7.81e+06         6.99         8.57          572         57.2\n",
            "     88    40     2.52e+05     5.97e+04     1.93e+05         3.82            5         98.8         8.98\n",
            "     88    50     2.12e+07     7.19e+04     2.11e+07         3.38         5.48          470           94\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88     2     2.73e+05     5.02e+04     2.23e+05         3.45         4.58          128         8.62\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              88  130.332    0.002     4.25e+05     1.15e+06     1.58e+06         4.86         11.1          138         12.8\n",
            "! Validation         88  130.332    0.002     4.96e+04     2.66e+05     3.16e+05         3.25         4.56          128         9.49\n",
            "Wall time: 130.3329422959996\n",
            "! Best model       88 315760.758\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89    10     7.11e+05     7.76e+04     6.33e+05         4.21          5.7          212         16.3\n",
            "     89    20     4.75e+05     3.61e+04     4.39e+05         3.31         3.89          122         13.6\n",
            "     89    30     1.22e+05     2.47e+04     9.78e+04         2.37         3.21         51.2          6.4\n",
            "     89    40      2.5e+04     2.28e+04     2.28e+03         2.74         3.09          3.9        0.976\n",
            "     89    50     2.38e+05     6.63e+04     1.72e+05         4.18         5.27         84.8         8.48\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89     2     2.66e+05     4.81e+04     2.18e+05         3.39         4.48          126         8.47\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              89  131.730    0.002     4.23e+05     1.18e+06      1.6e+06         4.94         10.6          143         13.2\n",
            "! Validation         89  131.730    0.002     4.78e+04     2.64e+05     3.12e+05          3.2         4.47          127         9.41\n",
            "Wall time: 131.7303583139992\n",
            "! Best model       89 311555.954\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90    10     2.39e+07     1.32e+07     1.07e+07         53.5         74.4          268         66.9\n",
            "     90    20     2.29e+05      3.2e+04     1.97e+05         2.59         3.66          163         9.07\n",
            "     90    30     2.57e+05     5.04e+04     2.07e+05         3.53         4.59          195          9.3\n",
            "     90    40      4.6e+04     2.17e+04     2.44e+04          2.3         3.01         54.3         3.19\n",
            "     90    50     1.01e+05     4.73e+04     5.41e+04         3.36         4.45         90.4         4.76\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90     2     2.63e+05     4.53e+04     2.17e+05         3.32         4.35          126         8.41\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              90  133.069    0.002     4.62e+05     1.08e+06     1.54e+06         4.72         10.5          144         13.1\n",
            "! Validation         90  133.069    0.002     4.54e+04     2.63e+05     3.09e+05         3.12         4.36          127         9.37\n",
            "Wall time: 133.0697941869994\n",
            "! Best model       90 308850.491\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91    10     7.87e+06     1.37e+05     7.74e+06         6.29         7.57          569         56.9\n",
            "     91    20     2.18e+05     2.76e+04      1.9e+05         2.44          3.4          161         8.92\n",
            "     91    30     6.03e+05        2e+05     4.04e+05         6.83         9.14          117           13\n",
            "     91    40     8.41e+05        5e+04     7.91e+05         3.13         4.57          146         18.2\n",
            "     91    50     2.25e+05     5.46e+04     1.71e+05         3.83         4.78         84.5         8.45\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91     2     2.66e+05     4.56e+04      2.2e+05         3.32         4.37          127         8.48\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              91  134.410    0.002     3.26e+05     1.16e+06     1.49e+06         4.48         9.73          141           13\n",
            "! Validation         91  134.410    0.002     4.51e+04     2.65e+05      3.1e+05          3.1         4.35          127          9.4\n",
            "Wall time: 134.41099981499974\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92    10     2.01e+05     3.08e+04      1.7e+05         2.52         3.59          152         8.43\n",
            "     92    20     1.49e+05     1.19e+05        3e+04          4.8         7.06         56.7         3.54\n",
            "     92    30     2.22e+07      1.1e+07     1.12e+07         54.5         67.8          273         68.3\n",
            "     92    40     7.33e+05     7.74e+04     6.55e+05         4.09         5.69          215         16.6\n",
            "     92    50     4.95e+05     4.86e+05     8.28e+03         6.95         14.3         24.2         1.86\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92     2     2.57e+05     4.65e+04     2.11e+05         3.35         4.41          123         8.25\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              92  135.745    0.002     3.92e+05     1.07e+06     1.46e+06         4.68         9.92          137         12.6\n",
            "! Validation         92  135.745    0.002     4.54e+04      2.6e+05     3.06e+05          3.1         4.36          126          9.3\n",
            "Wall time: 135.7452684639993\n",
            "! Best model       92 305670.962\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93    10     8.35e+05     4.81e+04     7.87e+05            3         4.49          145         18.1\n",
            "     93    20     6.21e+05     1.01e+04     6.11e+05          1.7         2.06          176           16\n",
            "     93    30     2.14e+07     1.07e+07     1.07e+07         54.6         66.8          268           67\n",
            "     93    40     8.15e+04     3.39e+04     4.76e+04         2.58         3.77         98.2         4.46\n",
            "     93    50     1.09e+05     5.45e+04     5.44e+04         3.58         4.78         90.6         4.77\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93     2      2.6e+05      4.6e+04     2.14e+05         3.36         4.39          125         8.32\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              93  137.086    0.002     3.79e+05     1.05e+06     1.43e+06         4.61         9.66          139         12.7\n",
            "! Validation         93  137.086    0.002     4.53e+04     2.61e+05     3.07e+05         3.11         4.35          126         9.31\n",
            "Wall time: 137.08636635899984\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94    10     4.18e+05     3.96e+04     3.79e+05         3.21         4.07          189         12.6\n",
            "     94    20     7.35e+04     2.14e+04      5.2e+04         2.28         2.99         65.3         4.67\n",
            "     94    30     5.12e+04     4.78e+04     3.39e+03         3.47         4.47         20.3         1.19\n",
            "     94    40     3.68e+05     7.42e+04     2.94e+05          4.1         5.57          111         11.1\n",
            "     94    50      8.7e+05     4.63e+04     8.24e+05         2.92          4.4          149         18.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94     2     2.53e+05      4.7e+04     2.06e+05         3.41         4.43          122         8.12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              94  138.437    0.002        4e+05     1.02e+06     1.42e+06          4.7         10.3          134         12.2\n",
            "! Validation         94  138.437    0.002     4.62e+04     2.57e+05     3.03e+05         3.15          4.4          125         9.22\n",
            "Wall time: 138.43779754699972\n",
            "! Best model       94 303296.543\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95    10     7.39e+05     8.28e+04     6.56e+05         4.22         5.88          215         16.6\n",
            "     95    20     3.19e+05     4.61e+04     2.73e+05         3.52         4.39          171         10.7\n",
            "     95    30      5.8e+06      5.6e+06        2e+05         19.4         48.4         91.5         9.15\n",
            "     95    40     1.82e+04     1.82e+04         53.5         2.11         2.76          1.8         0.15\n",
            "     95    50     5.85e+04     2.12e+04     3.73e+04         2.39         2.98         19.7         3.95\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95     2     2.51e+05     4.69e+04     2.04e+05         3.41         4.43          121         8.04\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              95  139.776    0.002     4.32e+05     1.04e+06     1.47e+06         4.88           11          133         12.2\n",
            "! Validation         95  139.776    0.002     4.62e+04     2.56e+05     3.02e+05         3.14          4.4          124         9.18\n",
            "Wall time: 139.7770363209993\n",
            "! Best model       95 301815.682\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96    10     7.93e+06     1.42e+05     7.79e+06         6.43         7.72          571         57.1\n",
            "     96    20     2.38e+05     4.46e+04     1.94e+05         3.13         4.32          189            9\n",
            "     96    30     4.25e+05     1.56e+04      4.1e+05         2.12         2.55          118         13.1\n",
            "     96    40     1.98e+05     4.36e+04     1.54e+05         3.39         4.27         80.3         8.03\n",
            "     96    50     3.98e+05     3.95e+05     3.61e+03         6.33         12.9           16         1.23\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96     2     2.46e+05     4.47e+04     2.01e+05          3.3         4.32          120         7.94\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              96  141.128    0.002     3.57e+05     1.03e+06     1.39e+06         4.34         9.41          136         12.4\n",
            "! Validation         96  141.128    0.002     4.34e+04     2.55e+05     2.98e+05         3.02         4.27          124         9.13\n",
            "Wall time: 141.12843407899982\n",
            "! Best model       96 297971.789\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97    10     1.96e+04     1.96e+04       0.0726         2.14         2.86       0.0661      0.00551\n",
            "     97    20     1.23e+05     1.09e+05     1.35e+04         4.65         6.75         38.1         2.38\n",
            "     97    30      2.4e+04     2.13e+04     2.71e+03         2.25         2.99         12.8         1.06\n",
            "     97    40      1.9e+07     9.33e+06     9.67e+06         53.2         62.5          254         63.6\n",
            "     97    50     1.05e+06     4.57e+05     5.89e+05         10.3         13.8          204         15.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97     2     2.42e+05     4.38e+04     1.98e+05         3.29         4.28          118         7.85\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              97  142.493    0.002     3.38e+05     1.02e+06     1.36e+06         4.56         9.21          134         12.3\n",
            "! Validation         97  142.493    0.002     4.33e+04     2.53e+05     2.96e+05         3.03         4.26          123         9.07\n",
            "Wall time: 142.49381856999935\n",
            "! Best model       97 295899.361\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98    10     4.06e+04     1.98e+04     2.08e+04         2.19         2.88         50.1         2.95\n",
            "     98    20     1.98e+05     7.54e+04     1.22e+05         4.11         5.62         85.9         7.16\n",
            "     98    30     3.58e+06     2.48e+06      1.1e+06         17.2         32.2          215         21.5\n",
            "     98    40      4.7e+05     2.37e+04     4.46e+05         2.53         3.15          246         13.7\n",
            "     98    50     1.01e+05     3.88e+04     6.19e+04         2.94         4.03         96.7         5.09\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98     2     2.41e+05     4.16e+04        2e+05         3.21         4.17          119         7.87\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              98  143.908    0.002     4.48e+05     9.68e+05     1.42e+06         4.66         10.6          139         12.4\n",
            "! Validation         98  143.908    0.002     4.14e+04     2.53e+05     2.94e+05         2.97         4.16          123         9.07\n",
            "Wall time: 143.90815919899978\n",
            "! Best model       98 294118.818\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99    10     1.71e+06     8.43e+05     8.68e+05         11.6         18.8          172         19.1\n",
            "     99    20     1.77e+05     1.49e+05     2.83e+04         6.28         7.89         51.6         3.44\n",
            "     99    30     1.56e+05     4.25e+04     1.14e+05         3.09         4.22         55.2          6.9\n",
            "     99    40     1.05e+06     5.12e+05     5.36e+05         10.7         14.6          195           15\n",
            "     99    50     4.41e+05     4.86e+04     3.92e+05         3.58         4.51          192         12.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99     2     2.43e+05     4.16e+04     2.01e+05         3.21         4.17          119          7.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              99  145.243    0.002     2.86e+05     1.12e+06     1.41e+06         4.54         9.54          137         12.8\n",
            "! Validation         99  145.243    0.002     4.13e+04     2.53e+05     2.94e+05         2.96         4.16          123         9.07\n",
            "Wall time: 145.2441239869995\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100    10     2.05e+05      3.1e+04     1.75e+05         2.87          3.6          179         8.55\n",
            "    100    20     1.78e+05     4.86e+04     1.29e+05         3.47         4.51         73.6         7.36\n",
            "    100    30     1.89e+04     1.85e+04          471         2.29         2.78         3.55        0.444\n",
            "    100    40     1.47e+05     1.19e+05     2.87e+04         4.69         7.05         55.4         3.46\n",
            "    100    50     4.19e+05     1.06e+05     3.13e+05         4.76         6.67          114         11.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100     2     2.35e+05     4.39e+04     1.91e+05         3.28         4.29          115         7.63\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train             100  146.578    0.002     3.84e+05     9.43e+05     1.33e+06         4.81           10          126         11.6\n",
            "! Validation        100  146.578    0.002     4.36e+04     2.48e+05     2.91e+05         3.02         4.27          122         8.95\n",
            "Wall time: 146.57890644099916\n",
            "! Best model      100 291294.458\n",
            "! Stop training: max epochs\n",
            "Wall time: 146.59246650299974\n",
            "Cumulative wall time: 146.59246650299974\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mDES-coarsened\u001b[0m at: \u001b[34mhttps://wandb.ai/anony-moose-739946801467655344/allegro-tutorial/runs/49wm4JT042cPI_nvXwXk2LL0rA84Kqm9YW5YrUfWoZg\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241213_002954-49wm4JT042cPI_nvXwXk2LL0rA84Kqm9YW5YrUfWoZg/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create New Potential file for coarsened model"
      ],
      "metadata": {
        "id": "PCLhPBZOAUAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=`pwd`:$PYTHONPATH nequip-deploy build --train-dir results/DES-tutorial/DES-coarsened as-coa-deployed.pth\n",
        "!ls *pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKPypunAAZcp",
        "outputId": "b1c09433-93d6-44cf-de45-e71c9fdb7659"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "INFO:root:Loading best_model.pth from training session...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/deploy.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\n",
            "INFO:root:Compiled & optimized model.\n",
            "as-coa-deployed.pth  as-deployed.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Just re-run dataset validation, confirming that model is not too far off from original."
      ],
      "metadata": {
        "id": "2XlBy9_RBDnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=`pwd`:$PYTHONPATH nequip-evaluate --train-dir results/DES-tutorial/DES-coarsened --batch-size 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-yLIxPoA8rs",
        "outputId": "4e2f92cc-8338-4e77-b6e3-d59ff059f4ac"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/evaluate.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  trainer = torch.load(\n",
            "Using device: cuda\n",
            "Please note that _all_ machine learning models running on CUDA hardware are generally somewhat nondeterministic and that this can manifest in small, generally unimportant variation in the final test errors.\n",
            "Loading model... \n",
            "/usr/local/lib/python3.10/dist-packages/nequip/train/trainer.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_state_dict = torch.load(traindir + \"/\" + model_name, map_location=device)\n",
            "    loaded model\n",
            "Loading original dataset...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (18700 frames) minus training (50 frames) and validation frames (10 frames), yielding a test set size of 18640 frames.\n",
            "Starting...\n",
            "  0% 0/18640 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  0% 5/18640 [00:01<1:16:36,  4.05it/s]\n",
            "  0% 10/18640 [00:01<48:29,  6.40it/s] \n",
            "  0% 15/18640 [00:03<1:26:05,  3.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  0% 50/18640 [00:03<16:10, 19.15it/s]  \n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  0% 90/18640 [00:04<07:24, 41.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 125/18640 [00:04<04:40, 65.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 160/18640 [00:04<03:15, 94.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 195/18640 [00:04<02:25, 126.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 230/18640 [00:04<01:55, 159.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 265/18640 [00:04<01:35, 192.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 300/18640 [00:04<01:21, 223.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 335/18640 [00:04<01:12, 251.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 370/18640 [00:04<01:06, 274.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 405/18640 [00:04<01:02, 292.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 445/18640 [00:05<00:59, 306.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 480/18640 [00:05<01:00, 302.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 515/18640 [00:05<00:57, 313.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 550/18640 [00:05<00:56, 321.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 585/18640 [00:05<00:54, 328.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 620/18640 [00:05<00:54, 333.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 655/18640 [00:05<00:53, 335.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 690/18640 [00:05<00:54, 331.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 725/18640 [00:05<00:54, 327.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 760/18640 [00:06<00:53, 332.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 795/18640 [00:06<00:53, 336.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 830/18640 [00:06<00:53, 332.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 865/18640 [00:06<00:53, 335.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 900/18640 [00:06<00:52, 338.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 935/18640 [00:06<00:54, 327.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 970/18640 [00:06<00:53, 332.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 1005/18640 [00:06<00:52, 336.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1040/18640 [00:06<00:51, 340.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1075/18640 [00:06<00:52, 337.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1110/18640 [00:07<00:51, 340.34it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1145/18640 [00:07<00:51, 341.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1180/18640 [00:07<00:50, 342.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1215/18640 [00:07<00:50, 342.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1250/18640 [00:07<00:51, 340.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1285/18640 [00:07<00:51, 339.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1320/18640 [00:07<00:50, 340.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1355/18640 [00:07<00:50, 341.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1395/18640 [00:07<00:50, 344.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1430/18640 [00:07<00:49, 345.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1470/18640 [00:08<00:49, 347.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1505/18640 [00:08<00:49, 346.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1540/18640 [00:08<00:49, 346.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1575/18640 [00:08<00:49, 346.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1610/18640 [00:08<00:49, 344.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1645/18640 [00:08<00:49, 344.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1680/18640 [00:08<00:49, 343.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1715/18640 [00:08<00:49, 342.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1750/18640 [00:08<00:49, 342.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1790/18640 [00:09<00:48, 345.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1830/18640 [00:09<00:48, 347.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1870/18640 [00:09<00:48, 347.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1910/18640 [00:09<00:48, 348.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1945/18640 [00:09<00:48, 346.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 1980/18640 [00:09<00:48, 346.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2015/18640 [00:09<00:47, 346.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2055/18640 [00:09<00:47, 347.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2090/18640 [00:09<00:47, 347.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2130/18640 [00:09<00:47, 349.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2165/18640 [00:10<00:47, 346.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2200/18640 [00:10<00:47, 345.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2235/18640 [00:10<00:47, 344.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2275/18640 [00:10<00:47, 347.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2310/18640 [00:10<00:46, 347.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2350/18640 [00:10<00:46, 348.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2385/18640 [00:10<00:46, 347.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2425/18640 [00:10<00:46, 348.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2460/18640 [00:10<00:46, 347.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2495/18640 [00:11<00:46, 345.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2530/18640 [00:11<00:47, 338.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2565/18640 [00:11<00:47, 337.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2600/18640 [00:11<00:47, 338.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2635/18640 [00:11<00:47, 339.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2670/18640 [00:11<00:46, 340.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2705/18640 [00:11<00:46, 343.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2740/18640 [00:11<00:46, 343.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2780/18640 [00:11<00:45, 346.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2815/18640 [00:11<00:45, 345.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2850/18640 [00:12<00:45, 343.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2885/18640 [00:12<00:46, 341.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2920/18640 [00:12<00:46, 339.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2955/18640 [00:12<00:46, 337.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2990/18640 [00:12<00:46, 338.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 3025/18640 [00:12<00:45, 341.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 3060/18640 [00:12<00:45, 341.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3095/18640 [00:12<00:45, 339.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3130/18640 [00:12<00:46, 336.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3165/18640 [00:13<00:45, 339.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3200/18640 [00:13<00:45, 341.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3235/18640 [00:13<00:44, 342.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3270/18640 [00:13<00:44, 344.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3305/18640 [00:13<00:44, 345.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3345/18640 [00:13<00:43, 348.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3380/18640 [00:13<00:43, 347.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3420/18640 [00:13<00:43, 348.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3460/18640 [00:13<00:43, 350.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3500/18640 [00:13<00:43, 350.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3540/18640 [00:14<00:42, 351.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3580/18640 [00:14<00:42, 352.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3620/18640 [00:14<00:43, 349.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3655/18640 [00:14<00:43, 348.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3690/18640 [00:14<00:42, 348.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3725/18640 [00:14<00:42, 347.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3760/18640 [00:14<00:43, 345.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3795/18640 [00:14<00:43, 342.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3830/18640 [00:14<00:43, 343.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3870/18640 [00:15<00:42, 346.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3910/18640 [00:15<00:42, 348.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3950/18640 [00:15<00:42, 349.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3985/18640 [00:15<00:41, 349.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4020/18640 [00:15<00:42, 347.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4055/18640 [00:15<00:42, 347.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4090/18640 [00:15<00:42, 344.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4125/18640 [00:15<00:42, 343.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4160/18640 [00:15<00:42, 344.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4195/18640 [00:15<00:41, 344.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4235/18640 [00:16<00:41, 349.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4275/18640 [00:16<00:40, 351.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4315/18640 [00:16<00:40, 351.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4355/18640 [00:16<00:40, 349.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4390/18640 [00:16<00:40, 349.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4425/18640 [00:16<00:40, 348.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4465/18640 [00:16<00:40, 350.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4505/18640 [00:16<00:40, 352.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4545/18640 [00:16<00:39, 353.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4585/18640 [00:17<00:39, 354.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4625/18640 [00:17<00:39, 355.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4665/18640 [00:17<00:39, 349.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4700/18640 [00:17<00:40, 348.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4735/18640 [00:17<00:39, 348.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4770/18640 [00:17<00:39, 348.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4810/18640 [00:17<00:39, 349.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4850/18640 [00:17<00:39, 350.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4890/18640 [00:17<00:39, 350.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4930/18640 [00:18<00:39, 350.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 4970/18640 [00:18<00:38, 352.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5010/18640 [00:18<00:38, 350.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5050/18640 [00:18<00:38, 351.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5090/18640 [00:18<00:38, 350.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5130/18640 [00:18<00:38, 351.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5170/18640 [00:18<00:38, 351.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5210/18640 [00:18<00:38, 352.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5250/18640 [00:18<00:37, 352.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5290/18640 [00:19<00:37, 354.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5330/18640 [00:19<00:37, 353.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5370/18640 [00:19<00:37, 355.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5410/18640 [00:19<00:37, 351.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5450/18640 [00:19<00:37, 351.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5490/18640 [00:19<00:37, 352.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5530/18640 [00:19<00:37, 353.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5570/18640 [00:19<00:36, 355.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5610/18640 [00:19<00:36, 353.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5650/18640 [00:20<00:36, 353.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5690/18640 [00:20<00:36, 350.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5730/18640 [00:20<00:36, 350.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5770/18640 [00:20<00:36, 349.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5810/18640 [00:20<00:36, 350.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5850/18640 [00:20<00:36, 349.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5885/18640 [00:20<00:36, 347.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5925/18640 [00:20<00:36, 347.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5960/18640 [00:21<00:36, 346.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5995/18640 [00:21<00:36, 345.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 6030/18640 [00:21<00:36, 346.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6065/18640 [00:21<00:36, 344.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6105/18640 [00:21<00:36, 347.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6140/18640 [00:21<00:36, 345.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6175/18640 [00:21<00:36, 345.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6210/18640 [00:21<00:35, 346.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6245/18640 [00:21<00:36, 343.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6285/18640 [00:21<00:35, 346.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6320/18640 [00:22<00:35, 345.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6360/18640 [00:22<00:35, 347.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6395/18640 [00:22<00:35, 344.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6430/18640 [00:22<00:35, 345.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6465/18640 [00:22<00:35, 339.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6500/18640 [00:22<00:35, 341.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6535/18640 [00:22<00:35, 343.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6575/18640 [00:22<00:34, 345.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6615/18640 [00:22<00:34, 347.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6650/18640 [00:23<00:34, 344.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6685/18640 [00:23<00:34, 345.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6720/18640 [00:23<00:34, 345.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6755/18640 [00:23<00:34, 344.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6790/18640 [00:23<00:34, 344.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6825/18640 [00:23<00:34, 343.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6860/18640 [00:23<00:34, 344.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6900/18640 [00:23<00:33, 349.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6935/18640 [00:23<00:33, 348.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6970/18640 [00:23<00:33, 348.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7010/18640 [00:24<00:33, 349.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7045/18640 [00:24<00:33, 347.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7080/18640 [00:24<00:33, 347.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7115/18640 [00:24<00:33, 344.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7150/18640 [00:24<00:33, 339.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7185/18640 [00:24<00:34, 335.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7220/18640 [00:24<00:33, 338.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7255/18640 [00:24<00:33, 340.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7295/18640 [00:24<00:32, 344.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7335/18640 [00:24<00:32, 347.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7375/18640 [00:25<00:32, 348.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7415/18640 [00:25<00:32, 350.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7455/18640 [00:25<00:31, 350.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7495/18640 [00:25<00:31, 351.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7535/18640 [00:25<00:31, 350.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7575/18640 [00:25<00:31, 352.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7615/18640 [00:25<00:31, 354.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7655/18640 [00:25<00:31, 353.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7695/18640 [00:26<00:30, 353.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7735/18640 [00:26<00:30, 355.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7775/18640 [00:26<00:30, 354.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7815/18640 [00:26<00:30, 354.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7855/18640 [00:26<00:30, 352.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7895/18640 [00:26<00:30, 352.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 7935/18640 [00:26<00:30, 353.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 7975/18640 [00:26<00:30, 355.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8015/18640 [00:26<00:29, 354.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8055/18640 [00:27<00:29, 355.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8095/18640 [00:27<00:29, 356.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8135/18640 [00:27<00:29, 356.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8175/18640 [00:27<00:29, 355.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8215/18640 [00:27<00:29, 351.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8255/18640 [00:27<00:29, 352.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8295/18640 [00:27<00:29, 354.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8335/18640 [00:27<00:28, 355.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8375/18640 [00:27<00:28, 354.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8415/18640 [00:28<00:28, 356.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8455/18640 [00:28<00:28, 356.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8495/18640 [00:28<00:28, 358.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8535/18640 [00:28<00:28, 357.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8575/18640 [00:28<00:28, 358.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8615/18640 [00:28<00:28, 357.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8655/18640 [00:28<00:28, 355.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8695/18640 [00:28<00:28, 354.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8735/18640 [00:28<00:27, 355.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8775/18640 [00:29<00:27, 355.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8815/18640 [00:29<00:27, 354.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8855/18640 [00:29<00:27, 354.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8895/18640 [00:29<00:27, 354.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8935/18640 [00:29<00:27, 353.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8975/18640 [00:29<00:27, 352.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 9015/18640 [00:29<00:27, 350.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9055/18640 [00:29<00:27, 349.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9090/18640 [00:29<00:27, 349.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9130/18640 [00:30<00:27, 350.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9170/18640 [00:30<00:26, 353.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9210/18640 [00:30<00:26, 354.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9250/18640 [00:30<00:26, 353.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9290/18640 [00:30<00:26, 352.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9330/18640 [00:30<00:26, 351.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9370/18640 [00:30<00:26, 351.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9410/18640 [00:30<00:26, 352.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9450/18640 [00:30<00:26, 352.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9490/18640 [00:31<00:25, 353.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9530/18640 [00:31<00:25, 354.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9570/18640 [00:31<00:25, 354.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9610/18640 [00:31<00:25, 353.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9650/18640 [00:31<00:25, 352.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9690/18640 [00:31<00:25, 353.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9730/18640 [00:31<00:25, 353.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9770/18640 [00:31<00:25, 352.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9810/18640 [00:31<00:25, 350.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9850/18640 [00:32<00:25, 351.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9890/18640 [00:32<00:24, 350.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9930/18640 [00:32<00:24, 350.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9970/18640 [00:32<00:24, 347.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10005/18640 [00:32<00:24, 347.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10040/18640 [00:32<00:25, 343.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10075/18640 [00:32<00:25, 342.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10110/18640 [00:32<00:25, 340.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10150/18640 [00:32<00:24, 344.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10185/18640 [00:33<00:24, 344.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10220/18640 [00:33<00:24, 343.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10255/18640 [00:33<00:24, 344.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10290/18640 [00:33<00:24, 343.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10325/18640 [00:33<00:24, 344.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10360/18640 [00:33<00:23, 345.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10395/18640 [00:33<00:23, 345.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10430/18640 [00:33<00:23, 344.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10470/18640 [00:33<00:23, 347.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10510/18640 [00:34<00:23, 349.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10550/18640 [00:34<00:23, 350.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10590/18640 [00:34<00:22, 350.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10630/18640 [00:34<00:22, 348.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10670/18640 [00:34<00:22, 350.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10710/18640 [00:34<00:22, 350.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10750/18640 [00:34<00:22, 352.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10790/18640 [00:34<00:22, 350.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10830/18640 [00:34<00:22, 345.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10865/18640 [00:35<00:22, 345.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10900/18640 [00:35<00:22, 345.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 10940/18640 [00:35<00:22, 346.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 10980/18640 [00:35<00:21, 348.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 11020/18640 [00:35<00:21, 349.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 11060/18640 [00:35<00:21, 350.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11100/18640 [00:35<00:21, 349.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11135/18640 [00:35<00:21, 345.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11170/18640 [00:35<00:21, 342.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11205/18640 [00:36<00:21, 340.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11240/18640 [00:36<00:21, 342.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11280/18640 [00:36<00:21, 344.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11315/18640 [00:36<00:21, 344.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11350/18640 [00:36<00:21, 345.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11385/18640 [00:36<00:20, 345.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11420/18640 [00:36<00:20, 345.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11460/18640 [00:36<00:20, 349.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11495/18640 [00:36<00:20, 349.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11535/18640 [00:36<00:20, 350.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11575/18640 [00:37<00:20, 347.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11610/18640 [00:37<00:20, 347.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11645/18640 [00:37<00:20, 347.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11680/18640 [00:37<00:20, 345.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11715/18640 [00:37<00:20, 345.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11750/18640 [00:37<00:19, 346.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11785/18640 [00:37<00:19, 344.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11820/18640 [00:37<00:19, 345.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11855/18640 [00:37<00:19, 345.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11895/18640 [00:38<00:19, 347.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11935/18640 [00:38<00:19, 348.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11970/18640 [00:38<00:19, 346.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 12005/18640 [00:38<00:19, 345.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12040/18640 [00:38<00:19, 345.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12080/18640 [00:38<00:18, 346.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12115/18640 [00:38<00:18, 345.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12155/18640 [00:38<00:18, 348.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12190/18640 [00:38<00:18, 347.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12225/18640 [00:38<00:18, 346.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12265/18640 [00:39<00:18, 348.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12305/18640 [00:39<00:18, 348.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12340/18640 [00:39<00:18, 347.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12380/18640 [00:39<00:17, 348.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12415/18640 [00:39<00:17, 347.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12455/18640 [00:39<00:17, 348.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12490/18640 [00:39<00:17, 349.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12530/18640 [00:39<00:17, 350.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12570/18640 [00:39<00:17, 351.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12610/18640 [00:40<00:17, 353.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12650/18640 [00:40<00:16, 353.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12690/18640 [00:40<00:16, 353.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12730/18640 [00:40<00:16, 353.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12770/18640 [00:40<00:16, 351.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12810/18640 [00:40<00:16, 350.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12850/18640 [00:40<00:16, 350.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12890/18640 [00:40<00:16, 348.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12930/18640 [00:40<00:16, 349.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 12965/18640 [00:41<00:16, 348.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13005/18640 [00:41<00:16, 349.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13040/18640 [00:41<00:16, 349.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13075/18640 [00:41<00:15, 348.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13115/18640 [00:41<00:15, 350.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13155/18640 [00:41<00:15, 348.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13195/18640 [00:41<00:15, 350.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13235/18640 [00:41<00:15, 350.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13275/18640 [00:41<00:15, 352.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13315/18640 [00:42<00:15, 353.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13355/18640 [00:42<00:15, 350.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13395/18640 [00:42<00:14, 350.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13435/18640 [00:42<00:14, 348.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13470/18640 [00:42<00:14, 348.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13510/18640 [00:42<00:14, 349.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13545/18640 [00:42<00:14, 348.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13580/18640 [00:42<00:14, 348.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13620/18640 [00:42<00:14, 349.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13655/18640 [00:43<00:14, 349.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13695/18640 [00:43<00:14, 351.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13735/18640 [00:43<00:14, 347.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13775/18640 [00:43<00:13, 349.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13815/18640 [00:43<00:13, 349.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13855/18640 [00:43<00:13, 350.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13895/18640 [00:43<00:13, 352.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13935/18640 [00:43<00:13, 352.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13975/18640 [00:43<00:13, 353.34it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 14015/18640 [00:44<00:13, 354.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 14055/18640 [00:44<00:12, 352.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14095/18640 [00:44<00:12, 352.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14135/18640 [00:44<00:12, 350.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14175/18640 [00:44<00:12, 350.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14215/18640 [00:44<00:12, 349.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14250/18640 [00:44<00:12, 347.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14290/18640 [00:44<00:12, 348.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14325/18640 [00:44<00:12, 348.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14365/18640 [00:45<00:12, 351.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14405/18640 [00:45<00:12, 350.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14445/18640 [00:45<00:11, 350.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14485/18640 [00:45<00:11, 351.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14525/18640 [00:45<00:11, 351.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14565/18640 [00:45<00:11, 350.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14605/18640 [00:45<00:11, 352.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14645/18640 [00:45<00:11, 349.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14680/18640 [00:45<00:11, 348.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14720/18640 [00:46<00:11, 349.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14755/18640 [00:46<00:11, 348.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14790/18640 [00:46<00:11, 348.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14825/18640 [00:46<00:11, 345.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14860/18640 [00:46<00:10, 345.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14900/18640 [00:46<00:10, 349.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14935/18640 [00:46<00:10, 345.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14970/18640 [00:46<00:10, 344.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 15005/18640 [00:46<00:10, 342.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15040/18640 [00:47<00:10, 344.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15080/18640 [00:47<00:10, 347.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15115/18640 [00:47<00:10, 346.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15150/18640 [00:47<00:10, 344.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15185/18640 [00:47<00:10, 341.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15220/18640 [00:47<00:10, 341.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15255/18640 [00:47<00:09, 341.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15290/18640 [00:47<00:09, 342.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15325/18640 [00:47<00:09, 342.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15360/18640 [00:47<00:09, 337.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15395/18640 [00:48<00:09, 335.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15430/18640 [00:48<00:09, 336.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15465/18640 [00:48<00:09, 324.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15500/18640 [00:48<00:09, 329.34it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15535/18640 [00:48<00:09, 328.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15570/18640 [00:48<00:09, 330.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15610/18640 [00:48<00:08, 337.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15650/18640 [00:48<00:08, 343.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15685/18640 [00:48<00:08, 343.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15725/18640 [00:49<00:08, 346.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15760/18640 [00:49<00:08, 346.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15800/18640 [00:49<00:08, 345.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15835/18640 [00:49<00:08, 342.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15870/18640 [00:49<00:10, 256.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15910/18640 [00:49<00:09, 282.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 15950/18640 [00:49<00:08, 301.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 15990/18640 [00:49<00:08, 315.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16030/18640 [00:50<00:07, 326.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16070/18640 [00:50<00:07, 335.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16105/18640 [00:50<00:07, 338.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16140/18640 [00:50<00:07, 339.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16180/18640 [00:50<00:07, 345.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16220/18640 [00:50<00:06, 347.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16260/18640 [00:50<00:06, 349.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16300/18640 [00:50<00:06, 349.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16340/18640 [00:50<00:06, 349.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16380/18640 [00:51<00:06, 350.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16420/18640 [00:51<00:06, 351.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16460/18640 [00:51<00:06, 349.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16495/18640 [00:51<00:06, 347.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16535/18640 [00:51<00:06, 349.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16575/18640 [00:51<00:05, 350.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16615/18640 [00:51<00:05, 351.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16655/18640 [00:51<00:05, 352.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16695/18640 [00:51<00:05, 353.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16735/18640 [00:52<00:05, 350.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16775/18640 [00:52<00:05, 351.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16815/18640 [00:52<00:05, 350.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16855/18640 [00:52<00:05, 348.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16895/18640 [00:52<00:04, 349.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16930/18640 [00:52<00:04, 346.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16970/18640 [00:52<00:04, 348.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 17010/18640 [00:52<00:04, 351.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 17050/18640 [00:52<00:04, 351.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17090/18640 [00:53<00:04, 353.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17130/18640 [00:53<00:04, 353.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17170/18640 [00:53<00:04, 347.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17210/18640 [00:53<00:04, 348.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17250/18640 [00:53<00:03, 349.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17290/18640 [00:53<00:03, 351.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17330/18640 [00:53<00:03, 353.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17370/18640 [00:53<00:03, 352.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17410/18640 [00:53<00:03, 353.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17450/18640 [00:54<00:03, 353.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17490/18640 [00:54<00:03, 355.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17530/18640 [00:54<00:03, 353.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17570/18640 [00:54<00:03, 352.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17610/18640 [00:54<00:02, 352.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17650/18640 [00:54<00:02, 353.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17690/18640 [00:54<00:02, 354.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17730/18640 [00:54<00:02, 353.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17770/18640 [00:54<00:02, 352.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17810/18640 [00:55<00:02, 353.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17850/18640 [00:55<00:02, 353.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17890/18640 [00:55<00:02, 349.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17930/18640 [00:55<00:02, 349.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17970/18640 [00:55<00:01, 350.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18010/18640 [00:55<00:01, 351.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18050/18640 [00:55<00:01, 352.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18090/18640 [00:55<00:01, 352.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18130/18640 [00:55<00:01, 352.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18170/18640 [00:56<00:01, 353.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18210/18640 [00:56<00:01, 351.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18250/18640 [00:56<00:01, 346.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18290/18640 [00:56<00:01, 348.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18325/18640 [00:56<00:00, 348.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18360/18640 [00:56<00:00, 348.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18400/18640 [00:56<00:00, 350.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18440/18640 [00:56<00:00, 349.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18480/18640 [00:56<00:00, 350.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18520/18640 [00:57<00:00, 353.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18560/18640 [00:57<00:00, 352.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18600/18640 [00:57<00:00, 349.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18640/18640 [00:57<00:00, 324.47it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  4.031485           \n",
            "              f_rmse =  7.504312           \n",
            "               e_mae =  284.500523         \n",
            "             e/N_mae =  31.601975          \n",
            "               f_mae =  4.031485           \n",
            "              f_rmse =  7.504312           \n",
            "               e_mae =  284.500523         \n",
            "             e/N_mae =  31.601975          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create RDF distribution using LAMMPS"
      ],
      "metadata": {
        "id": "I9Tpda8ADF9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lammps_input = \"\"\"\n",
        "units\tmetal\n",
        "atom_style atomic\n",
        "dimension 3\n",
        "\n",
        "# set newton on for pair_allegro (off for pair_nequip)\n",
        "newton on\n",
        "boundary p p p\n",
        "read_data ../des.data\n",
        "\n",
        "# if you want to run a larger system, simply replicate the system in space\n",
        "# replicate 3 3 3\n",
        "\n",
        "# allegro pair style\n",
        "pair_style\tallegro\n",
        "pair_coeff\t* * ../as-coa-deployed.pth N H\n",
        "\n",
        "# N, H mass.\n",
        "mass 1 14.0067\n",
        "mass 2 1.00784\n",
        "\n",
        "velocity all create 300.0 1234567 loop geom\n",
        "\n",
        "neighbor 1.0 bin\n",
        "neigh_modify delay 5 every 1\n",
        "\n",
        "timestep 0.0001\n",
        "thermo 1\n",
        "\n",
        "# nose-hoover thermostat, 300K\n",
        "fix  1 all nvt temp 300 300 $(100*dt)\n",
        "\n",
        "# compute rdf and average after some equilibration\n",
        "comm_modify cutoff 7.0\n",
        "compute rdfall all rdf 1000 cutoff 5.0\n",
        "fix 2 all ave/time 1 2500 5000 c_rdfall[*] file des_coa.rdf mode vector\n",
        "\n",
        "# run 5ps\n",
        "run 5000\n",
        "\"\"\"\n",
        "!rm -rf ./lammps_run/des_coa_rdf.in\n",
        "\n",
        "with open(\"lammps_run/des_coa_rdf.in\", \"w\") as f:\n",
        "    f.write(lammps_input)"
      ],
      "metadata": {
        "id": "SIA-h2j9CkO7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAMMPS will just check pairs..."
      ],
      "metadata": {
        "id": "1zrEdEavD7wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create coarsened RDF distribution\n",
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "!cd lammps_run/ && ../lammps/build/lmp -in des_coa_rdf.in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEi_z6wJD453",
        "outputId": "657cfefc-7f07-46e4-d907-9bdbb5ca191e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        30   19595.977     -122.86115      0             -112.72924      35315.131    \n",
            "        31   16145.923     -121.93422      0             -113.58612      43577.968    \n",
            "        32   13155.473     -121.07585      0             -114.27393      52595.906    \n",
            "        33   10809.183     -120.41287      0             -114.82408      61626.777    \n",
            "        34   9208.1205     -119.99414      0             -115.23316      70191.296    \n",
            "        35   8387.4555     -119.96776      0             -115.63111      77412.225    \n",
            "        36   8337.1726     -120.26017      0             -115.94951      82869.026    \n",
            "        37   9015.2712     -121.00068      0             -116.33942      86535.003    \n",
            "        38   10359.04      -122.09938      0             -116.74333      89158.963    \n",
            "        39   12285.527     -123.57455      0             -117.22244      89710.319    \n",
            "        40   14684.408     -125.43728      0             -117.84485      88861.754    \n",
            "        41   17404.519     -127.6031       0             -118.60426      87274.053    \n",
            "        42   20234.467     -130.03075      0             -119.56871      85482.186    \n",
            "        43   22896.962     -132.59032      0             -120.75166      83256.926    \n",
            "        44   25066.026     -135.07087      0             -122.11072      82115.834    \n",
            "        45   26439.264     -137.40247      0             -123.7323       80726.629    \n",
            "        46   26851.894     -139.34757      0             -125.46405      79092.366    \n",
            "        47   26346.764     -140.88806      0             -127.26572      75245.317    \n",
            "        48   25150.743     -142.08499      0             -129.08103      68640.513    \n",
            "        49   23565.069     -143.00411      0             -130.82001      58934.668    \n",
            "        50   21848.241     -143.69293      0             -132.39651      46939.674    \n",
            "        51   20156.222     -144.2943       0             -133.87271      33364.951    \n",
            "        52   18536.907     -144.78358      0             -135.19924      19340.074    \n",
            "        53   16955.298     -145.1669       0             -136.40033      5747.905     \n",
            "        54   15341.122     -145.40214      0             -137.47016     -6511.9315    \n",
            "        55   13640.659     -145.42198      0             -138.36921     -16722.73     \n",
            "        56   11854.484     -145.33973      0             -139.21048     -24547.061    \n",
            "        57   10041.166     -145.05535      0             -139.86366     -29569.116    \n",
            "        58   8292.4775     -144.70637      0             -140.41882     -31982.95     \n",
            "        59   6704.6128     -144.31849      0             -140.85193     -32442.445    \n",
            "        60   5356.1734     -143.91911      0             -141.14975     -31811.675    \n",
            "        61   4296.374      -143.63501      0             -141.41361     -30270.206    \n",
            "        62   3545.4662     -143.44692      0             -141.61377     -28655.247    \n",
            "        63   3097.4965     -143.3888       0             -141.78727     -27315.213    \n",
            "        64   2922.982      -143.45661      0             -141.9453      -26339.18     \n",
            "        65   2975.6715     -143.6172       0             -142.07866     -25635.519    \n",
            "        66   3198.1756     -143.8611       0             -142.20751     -25061.623    \n",
            "        67   3527.3615     -144.21484      0             -142.39105     -24661.635    \n",
            "        68   3901.4665     -144.54721      0             -142.52999     -24063.546    \n",
            "        69   4265.1398     -144.94237      0             -142.73712     -22848.326    \n",
            "        70   4570.7863     -145.2645       0             -142.90122     -21008.923    \n",
            "        71   4783.6115     -145.59071      0             -143.11739     -18268.926    \n",
            "        72   4883.5514     -145.81944      0             -143.29444     -14621.482    \n",
            "        73   4867.8484     -146.05088      0             -143.534       -10131.071    \n",
            "        74   4751.4417     -146.20907      0             -143.75238     -4896.8326    \n",
            "        75   4563.4527     -146.31534      0             -143.95585      318.42202    \n",
            "        76   4342.6046     -146.37864      0             -144.13334      5506.3208    \n",
            "        77   4128.0999     -146.43762      0             -144.30322      9931.0188    \n",
            "        78   3952.0348     -146.50059      0             -144.45723      13802.43     \n",
            "        79   3831.3645     -146.63768      0             -144.65671      16440.284    \n",
            "        80   3765.1846     -146.77207      0             -144.82531      18147.502    \n",
            "        81   3735.8439     -146.8953       0             -144.96372      19067.451    \n",
            "        82   3715.322      -147.05101      0             -145.13004      19122.306    \n",
            "        83   3671.682      -147.1637       0             -145.26529      18714.73     \n",
            "        84   3579.1179     -147.27133      0             -145.42078      17941.101    \n",
            "        85   3423.0406     -147.35906      0             -145.58921      16947.971    \n",
            "        86   3204.5456     -147.38498      0             -145.7281       15815.399    \n",
            "        87   2941.1972     -147.36757      0             -145.84685      14649.593    \n",
            "        88   2662.1089     -147.32256      0             -145.94615      13270.23     \n",
            "        89   2399.895      -147.27753      0             -146.03669      11777.75     \n",
            "        90   2181.9948     -147.25624      0             -146.12806      9776.294     \n",
            "        91   2025.2306     -147.26336      0             -146.21623      7488.8283    \n",
            "        92   1931.0577     -147.29797      0             -146.29954      4986.6447    \n",
            "        93   1885.4584     -147.35583      0             -146.38097      2021.8961    \n",
            "        94   1863.1441     -147.41392      0             -146.45059     -862.47667    \n",
            "        95   1834.9204     -147.44989      0             -146.50117     -3723.3333    \n",
            "        96   1775.4191     -147.49702      0             -146.57906     -6272.8768    \n",
            "        97   1668.9659     -147.5037       0             -146.64078     -8637.1518    \n",
            "        98   1514.163      -147.47379      0             -146.6909      -10650.406    \n",
            "        99   1324.332      -147.43088      0             -146.74614     -12234.614    \n",
            "       100   1124.9323     -147.3502       0             -146.76857     -13453.499    \n",
            "       101   945.42539     -147.29897      0             -146.81014     -14565.715    \n",
            "       102   813.29054     -147.26146      0             -146.84096     -15428.882    \n",
            "       103   748.49262     -147.27528      0             -146.88828     -16288.288    \n",
            "       104   758.11251     -147.31464      0             -146.92267     -17206.457    \n",
            "       105   835.74014     -147.35574      0             -146.92363     -17932.226    \n",
            "       106   963.85162     -147.46715      0             -146.9688      -18798.56     \n",
            "       107   1117.5086     -147.57983      0             -147.00204     -19325.704    \n",
            "       108   1270.038      -147.67623      0             -147.01957     -19457.277    \n",
            "       109   1399.02       -147.77033      0             -147.04698     -19339.668    \n",
            "       110   1490.7585     -147.88325      0             -147.11247     -18811.677    \n",
            "       111   1541.0058     -147.96163      0             -147.16487     -17816.367    \n",
            "       112   1554.7941     -148.01096      0             -147.20707     -16389.47     \n",
            "       113   1543.066      -148.06926      0             -147.27143     -14723.464    \n",
            "       114   1517.9712     -148.08742      0             -147.30257     -13043.931    \n",
            "       115   1488.1653     -148.09458      0             -147.32514     -10915.665    \n",
            "       116   1456.37       -148.17127      0             -147.41826     -9107.2731    \n",
            "       117   1419.0131     -148.15185      0             -147.41816     -7260.1993    \n",
            "       118   1367.9845     -148.2013       0             -147.494       -5312.5046    \n",
            "       119   1293.5053     -148.19557      0             -147.52678     -3482.7775    \n",
            "       120   1187.8662     -148.18023      0             -147.56606     -1633.5187    \n",
            "       121   1049.0267     -148.15785      0             -147.61546      253.0419     \n",
            "       122   882.05203     -148.08087      0             -147.62481      1853.9948    \n",
            "       123   699.45722     -148.00357      0             -147.64192      3566.8643    \n",
            "       124   518.3693      -147.95039      0             -147.68237      5060.4208    \n",
            "       125   357.19123     -147.87354      0             -147.68885      6290.9624    \n",
            "       126   233.35001     -147.79241      0             -147.67176      7497.6012    \n",
            "       127   159.55757     -147.75827      0             -147.67577      8197.4109    \n",
            "       128   141.61347     -147.75909      0             -147.68587      8434.529     \n",
            "       129   178.06715     -147.8          0             -147.70793      8433.4885    \n",
            "       130   260.53258     -147.84464      0             -147.70993      8060.9032    \n",
            "       131   375.49361     -147.88102      0             -147.68688      7404.5187    \n",
            "       132   506.44495     -147.98332      0             -147.72147      6442.2141    \n",
            "       133   636.61657     -148.07548      0             -147.74632      5265.9279    \n",
            "       134   751.42336     -148.1365       0             -147.74798      3914.8478    \n",
            "       135   840.8928      -148.20376      0             -147.76898      2504.3549    \n",
            "       136   900.53358     -148.24903      0             -147.78341      835.67322    \n",
            "       137   930.88701     -148.29768      0             -147.81638     -679.29467    \n",
            "       138   936.39047     -148.35775      0             -147.8736      -2441.3768    \n",
            "       139   922.64143     -148.33533      0             -147.85828     -4265.9155    \n",
            "       140   895.38123     -148.33753      0             -147.87458     -6098.9098    \n",
            "       141   858.87835     -148.3488       0             -147.90472     -7911.6658    \n",
            "       142   814.4781      -148.34004      0             -147.91892     -9776.9024    \n",
            "       143   761.09309     -148.31432      0             -147.9208      -11461.158    \n",
            "       144   696.70881     -148.33685      0             -147.97662     -13046.718    \n",
            "       145   620.38096     -148.29962      0             -147.97886     -14358.666    \n",
            "       146   532.36989     -148.28283      0             -148.00758     -15526.36     \n",
            "       147   436.10974     -148.20363      0             -147.97815     -16288.872    \n",
            "       148   338.20912     -148.1761       0             -148.00123     -16955.631    \n",
            "       149   247.39318     -148.14343      0             -148.01551     -17344.511    \n",
            "       150   172.58209     -148.10436      0             -148.01513     -17491.15     \n",
            "       151   121.44294     -148.12466      0             -148.06187     -17340.408    \n",
            "       152   99.743574     -148.0765       0             -148.02492     -17054.86     \n",
            "       153   108.96847     -148.09073      0             -148.03439     -16622.313    \n",
            "       154   146.34455     -148.14707      0             -148.07141     -15990.203    \n",
            "       155   205.51881     -148.14228      0             -148.03602     -15175.294    \n",
            "       156   277.45165     -148.19447      0             -148.05102     -14377.459    \n",
            "       157   352.20455     -148.24743      0             -148.06533     -13257.212    \n",
            "       158   420.43202     -148.27261      0             -148.05523     -11856.281    \n",
            "       159   475.2396      -148.34309      0             -148.09737     -10436.869    \n",
            "       160   512.66024     -148.32572      0             -148.06065     -8593.9311    \n",
            "       161   531.82319     -148.36066      0             -148.08569     -6974.7726    \n",
            "       162   535.01936     -148.35802      0             -148.08139     -4980.2751    \n",
            "       163   526.1688      -148.37376      0             -148.10171     -3008.2513    \n",
            "       164   509.57145     -148.42961      0             -148.16614     -1164.7412    \n",
            "       165   488.75097     -148.3905       0             -148.1378       631.41597    \n",
            "       166   465.67823     -148.39706      0             -148.15629      2349.5084    \n",
            "       167   440.64685     -148.41468      0             -148.18685      3998.6399    \n",
            "       168   412.91425     -148.37471      0             -148.16122      5322.524     \n",
            "       169   381.1353      -148.36154      0             -148.16448      6542.3821    \n",
            "       170   344.38013     -148.34899      0             -148.17093      7695.8918    \n",
            "       171   303.38976     -148.34335      0             -148.18649      8528.467     \n",
            "       172   260.29469     -148.30209      0             -148.16751      9233.1501    \n",
            "       173   219.01285     -148.31076      0             -148.19752      9704.4479    \n",
            "       174   183.80545     -148.29375      0             -148.19871      10042.462    \n",
            "       175   158.92148     -148.30873      0             -148.22656      9992.562     \n",
            "       176   147.9796      -148.28545      0             -148.20894      9732.6989    \n",
            "       177   152.32472     -148.33502      0             -148.25626      9229.1805    \n",
            "       178   170.84081     -148.28508      0             -148.19675      8570.1629    \n",
            "       179   200.01956     -148.33692      0             -148.2335       7732.9247    \n",
            "       180   234.61711     -148.32282      0             -148.20152      6659.4462    \n",
            "       181   268.44605     -148.34473      0             -148.20594      5500.6138    \n",
            "       182   295.73765     -148.39313      0             -148.24022      4244.8679    \n",
            "       183   312.5568      -148.4024       0             -148.2408       2886.7603    \n",
            "       184   316.49512     -148.4302       0             -148.26656      1552.7947    \n",
            "       185   308.1074      -148.37908      0             -148.21978      434.26944    \n",
            "       186   290.31235     -148.39919      0             -148.24909     -854.30832    \n",
            "       187   266.74634     -148.37199      0             -148.23407     -2132.8602    \n",
            "       188   241.89678     -148.3693       0             -148.24423     -3139.209     \n",
            "       189   219.8163      -148.36643      0             -148.25278     -4067.6836    \n",
            "       190   203.06499     -148.36545      0             -148.26046     -5092.8714    \n",
            "       191   192.44261     -148.37324      0             -148.27374     -5824.6623    \n",
            "       192   187.17526     -148.37907      0             -148.28229     -6488.8009    \n",
            "       193   185.55134     -148.36152      0             -148.26558     -6765.4375    \n",
            "       194   185.27966     -148.3638       0             -148.268       -7026.0925    \n",
            "       195   184.56143     -148.36645      0             -148.27102     -7217.2105    \n",
            "       196   182.68709     -148.39608      0             -148.30162     -6987.442     \n",
            "       197   179.80986     -148.38575      0             -148.29279     -6638.5016    \n",
            "       198   176.9679      -148.36355      0             -148.27205     -6092.6671    \n",
            "       199   175.69587     -148.39247      0             -148.30163     -5437.0371    \n",
            "       200   177.3513      -148.38136      0             -148.28966     -4430.659     \n",
            "       201   182.53296     -148.39118      0             -148.2968      -3581.4544    \n",
            "       202   190.74289     -148.39287      0             -148.29425     -2608.8269    \n",
            "       203   200.27686     -148.42353      0             -148.31997     -1470.2931    \n",
            "       204   208.47673     -148.40099      0             -148.2932      -276.99811    \n",
            "       205   212.64048     -148.43551      0             -148.32557      1043.7768    \n",
            "       206   210.37285     -148.4266       0             -148.31783      2311.9991    \n",
            "       207   200.25676     -148.43012      0             -148.32658      3413.2966    \n",
            "       208   182.45008     -148.3929       0             -148.29857      4715.4446    \n",
            "       209   158.47518     -148.40395      0             -148.32201      5941.277     \n",
            "       210   131.3408      -148.37644      0             -148.30853      7038.7385    \n",
            "       211   104.80769     -148.3966       0             -148.34241      7904.8652    \n",
            "       212   82.493182     -148.37664      0             -148.33399      8724.6414    \n",
            "       213   67.543386     -148.37425      0             -148.33933      9340.375     \n",
            "       214   61.832799     -148.39168      0             -148.35971      9732.0121    \n",
            "       215   65.535515     -148.38142      0             -148.34754      10036.65     \n",
            "       216   77.314114     -148.39349      0             -148.35352      10132.887    \n",
            "       217   94.805843     -148.36926      0             -148.32024      10074.411    \n",
            "       218   115.12418     -148.40734      0             -148.34782      9821.5814    \n",
            "       219   135.25429     -148.40825      0             -148.33831      9371.5371    \n",
            "       220   152.83973     -148.43422      0             -148.3552       8675.923     \n",
            "       221   166.34098     -148.42415      0             -148.33815      8021.5558    \n",
            "       222   175.28659     -148.43994      0             -148.34931      7188.0767    \n",
            "       223   179.97132     -148.42712      0             -148.33406      6176.61      \n",
            "       224   181.12917     -148.43464      0             -148.34099      5122.921     \n",
            "       225   179.53044     -148.43877      0             -148.34594      4015.9125    \n",
            "       226   175.72129     -148.44158      0             -148.35072      2900.5337    \n",
            "       227   169.83338     -148.45208      0             -148.36427      1609.1021    \n",
            "       228   161.57079     -148.44823      0             -148.3647       615.29175    \n",
            "       229   150.31326     -148.43536      0             -148.35764     -448.93497    \n",
            "       230   135.47526     -148.43775      0             -148.3677      -1550.0541    \n",
            "       231   117.03145     -148.4434       0             -148.38289     -2456.047     \n",
            "       232   95.764492     -148.44293      0             -148.39341     -3408.6422    \n",
            "       233   73.136594     -148.42515      0             -148.38733     -3999.7226    \n",
            "       234   51.174904     -148.4031       0             -148.37664     -4508.5423    \n",
            "       235   32.585658     -148.39882      0             -148.38197     -4828.1979    \n",
            "       236   19.857435     -148.40292      0             -148.39265     -5133.482     \n",
            "       237   14.847098     -148.36421      0             -148.35654     -5128.848     \n",
            "       238   18.44957      -148.39356      0             -148.38403     -5184.098     \n",
            "       239   30.335542     -148.38587      0             -148.37019     -5010.4826    \n",
            "       240   49.025913     -148.40795      0             -148.3826      -4643.8208    \n",
            "       241   72.074988     -148.41665      0             -148.37938     -4273.2094    \n",
            "       242   96.563978     -148.43868      0             -148.38876     -3620.2461    \n",
            "       243   119.57656     -148.4463       0             -148.38447     -3033.823     \n",
            "       244   138.74801     -148.45378      0             -148.38204     -2167.2013    \n",
            "       245   152.44467     -148.45577      0             -148.37695     -1212.2118    \n",
            "       246   159.98491     -148.47951      0             -148.39679     -269.57123    \n",
            "       247   161.57919     -148.47808      0             -148.39454      724.60407    \n",
            "       248   158.16383     -148.46675      0             -148.38497      1725.4564    \n",
            "       249   151.0984      -148.46431      0             -148.38619      2777.7317    \n",
            "       250   141.52998     -148.43401      0             -148.36084      3819.2541    \n",
            "       251   130.48742     -148.47223      0             -148.40476      4721.8578    \n",
            "       252   118.51506     -148.47132      0             -148.41005      5517.6579    \n",
            "       253   105.76194     -148.44976      0             -148.39507      6313.0821    \n",
            "       254   92.388478     -148.45628      0             -148.40852      6882.1884    \n",
            "       255   78.513303     -148.45123      0             -148.41064      7427.7375    \n",
            "       256   64.308686     -148.43265      0             -148.3994       7812.688     \n",
            "       257   50.493347     -148.40369      0             -148.37758      8171.1513    \n",
            "       258   38.40205      -148.41291      0             -148.39306      8284.624     \n",
            "       259   29.426336     -148.42032      0             -148.4051       8217.4638    \n",
            "       260   25.000565     -148.42598      0             -148.41305      8069.4278    \n",
            "       261   26.059647     -148.39956      0             -148.38609      7829.5902    \n",
            "       262   32.892329     -148.41745      0             -148.40044      7359.7657    \n",
            "       263   45.022102     -148.43322      0             -148.40994      6744.3931    \n",
            "       264   61.10417      -148.42283      0             -148.39124      6129.7773    \n",
            "       265   79.190404     -148.45482      0             -148.41388      5401.745     \n",
            "       266   97.054249     -148.43792      0             -148.38774      4439.9221    \n",
            "       267   112.46362     -148.45925      0             -148.4011       3523.961     \n",
            "       268   123.53368     -148.45888      0             -148.395        2588.0317    \n",
            "       269   129.20602     -148.47281      0             -148.406        1539.5045    \n",
            "       270   129.24263     -148.48311      0             -148.41629      577.43299    \n",
            "       271   124.39109     -148.45404      0             -148.38972     -260.4025     \n",
            "       272   116.06328     -148.44821      0             -148.3882      -1218.2205    \n",
            "       273   105.88292     -148.4437       0             -148.38896     -2039.3724    \n",
            "       274   95.403196     -148.45563      0             -148.4063      -2900.1538    \n",
            "       275   85.784028     -148.43449      0             -148.39014     -3639.057     \n",
            "       276   77.741513     -148.4644       0             -148.4242      -4280.1116    \n",
            "       277   71.41268      -148.44757      0             -148.41065     -4744.7133    \n",
            "       278   66.482046     -148.45351      0             -148.41914     -5210.6207    \n",
            "       279   62.505353     -148.43098      0             -148.39866     -5520.7964    \n",
            "       280   59.059632     -148.43989      0             -148.40935     -5663.3242    \n",
            "       281   55.877709     -148.43863      0             -148.40974     -5730.923     \n",
            "       282   53.172936     -148.45406      0             -148.42657     -5633.8925    \n",
            "       283   51.418952     -148.45229      0             -148.4257      -5410.2588    \n",
            "       284   51.188278     -148.42223      0             -148.39577     -5001.281     \n",
            "       285   52.970518     -148.44865      0             -148.42127     -4709.2699    \n",
            "       286   57.04132      -148.45756      0             -148.42807     -4283.6504    \n",
            "       287   63.189883     -148.43574      0             -148.40307     -3646.0883    \n",
            "       288   70.699207     -148.44878      0             -148.41223     -3105.5754    \n",
            "       289   78.499992     -148.46734      0             -148.42675     -2505.8743    \n",
            "       290   85.255989     -148.48204      0             -148.43796     -1739.7453    \n",
            "       291   89.84867      -148.46078      0             -148.41433     -1138.3187    \n",
            "       292   91.31183      -148.48227      0             -148.43506     -517.5068     \n",
            "       293   89.174743     -148.45284      0             -148.40673      248.48009    \n",
            "       294   83.762426     -148.47742      0             -148.43412      798.60456    \n",
            "       295   75.902064     -148.46388      0             -148.42464      1409.9439    \n",
            "       296   66.899543     -148.45698      0             -148.42239      1897.0386    \n",
            "       297   58.266068     -148.42487      0             -148.39475      2349.7234    \n",
            "       298   51.34856      -148.43928      0             -148.41273      2718.4625    \n",
            "       299   47.093545     -148.4528       0             -148.42845      3033.6749    \n",
            "       300   45.861606     -148.45423      0             -148.43052      3137.7748    \n",
            "       301   47.485076     -148.43844      0             -148.41389      3320.4947    \n",
            "       302   51.379376     -148.47744      0             -148.45088      3163.9773    \n",
            "       303   56.481824     -148.45224      0             -148.42304      2969.6266    \n",
            "       304   61.6761       -148.45941      0             -148.42752      2660.2863    \n",
            "       305   66.105925     -148.47086      0             -148.43668      2358.8947    \n",
            "       306   69.277234     -148.45152      0             -148.4157       1907.3329    \n",
            "       307   71.040213     -148.47205      0             -148.43532      1347.6565    \n",
            "       308   71.505463     -148.46292      0             -148.42595      808.23882    \n",
            "       309   71.115152     -148.47646      0             -148.43969      216.69       \n",
            "       310   70.314263     -148.47789      0             -148.44153     -464.52123    \n",
            "       311   69.394017     -148.46949      0             -148.43361     -1136.2605    \n",
            "       312   68.382124     -148.43646      0             -148.4011      -1867.1326    \n",
            "       313   67.075725     -148.47272      0             -148.43804     -2592.4702    \n",
            "       314   65.038527     -148.46072      0             -148.42709     -3295.4745    \n",
            "       315   61.795436     -148.46041      0             -148.42846     -4049.0644    \n",
            "       316   57.016554     -148.46606      0             -148.43658     -4643.5601    \n",
            "       317   50.605794     -148.45476      0             -148.42859     -5140.1478    \n",
            "       318   42.972507     -148.45207      0             -148.42985     -5644.7446    \n",
            "       319   34.970459     -148.44249      0             -148.4244      -5936.511     \n",
            "       320   27.688845     -148.44333      0             -148.42901     -6216.2074    \n",
            "       321   22.310635     -148.45284      0             -148.44131     -6443.8346    \n",
            "       322   19.841801     -148.43251      0             -148.42225     -6538.0442    \n",
            "       323   20.92988      -148.41685      0             -148.40603     -6534.243     \n",
            "       324   25.703257     -148.44337      0             -148.43008     -6434.0372    \n",
            "       325   33.693327     -148.43853      0             -148.4211      -6213.4168    \n",
            "       326   43.910591     -148.43483      0             -148.41212     -6122.8609    \n",
            "       327   55.022491     -148.43985      0             -148.4114      -5690.9047    \n",
            "       328   65.610359     -148.48018      0             -148.44626     -5326.9925    \n",
            "       329   74.386352     -148.46722      0             -148.42876     -4663.266     \n",
            "       330   80.486336     -148.46999      0             -148.42838     -4124.4942    \n",
            "       331   83.462812     -148.48091      0             -148.43776     -3438.816     \n",
            "       332   83.322651     -148.46225      0             -148.41917     -2642.3736    \n",
            "       333   80.616191     -148.45714      0             -148.41546     -1865.7634    \n",
            "       334   76.070076     -148.46888      0             -148.42955     -1194.5067    \n",
            "       335   70.371377     -148.46315      0             -148.42676     -512.60824    \n",
            "       336   64.125425     -148.47537      0             -148.44222      186.84869    \n",
            "       337   57.673709     -148.46392      0             -148.4341       868.91787    \n",
            "       338   51.092522     -148.4708       0             -148.44438      1602.1319    \n",
            "       339   44.358583     -148.46256      0             -148.43962      2059.7538    \n",
            "       340   37.409988     -148.45482      0             -148.43548      2640.4466    \n",
            "       341   30.318119     -148.45967      0             -148.44399      3002.4495    \n",
            "       342   23.507912     -148.44862      0             -148.43646      3303.875     \n",
            "       343   17.573069     -148.44447      0             -148.43539      3595.7874    \n",
            "       344   13.310941     -148.4115       0             -148.40462      3745.8197    \n",
            "       345   11.609599     -148.41098      0             -148.40498      3829.1707    \n",
            "       346   13.120892     -148.43758      0             -148.4308       3660.7063    \n",
            "       347   18.137817     -148.47545      0             -148.46608      3489.3662    \n",
            "       348   26.488648     -148.43421      0             -148.42051      3269.9505    \n",
            "       349   37.529924     -148.46161      0             -148.44221      2868.8967    \n",
            "       350   50.169456     -148.44328      0             -148.41734      2453.5522    \n",
            "       351   62.985999     -148.46077      0             -148.42821      1866.9726    \n",
            "       352   74.484761     -148.48756      0             -148.44905      1167.9951    \n",
            "       353   83.357897     -148.4651       0             -148.422        587.82507    \n",
            "       354   88.582        -148.48871      0             -148.44291     -50.953861    \n",
            "       355   89.796665     -148.46682      0             -148.42039     -710.48369    \n",
            "       356   87.366299     -148.45997      0             -148.41479     -1326.9016    \n",
            "       357   81.993311     -148.46934      0             -148.42694     -2073.2402    \n",
            "       358   74.631441     -148.49281      0             -148.45422     -2746.4073    \n",
            "       359   66.344094     -148.47012      0             -148.43581     -3237.2803    \n",
            "       360   58.056235     -148.4639       0             -148.43389     -3782.7404    \n",
            "       361   50.40625      -148.48306      0             -148.45699     -4286.4475    \n",
            "       362   43.767839     -148.44346      0             -148.42083     -4633.8288    \n",
            "       363   38.25703      -148.4897       0             -148.46992     -5079.3777    \n",
            "       364   33.719796     -148.47543      0             -148.458       -5412.8312    \n",
            "       365   29.980497     -148.48234      0             -148.46684     -5462.8924    \n",
            "       366   27.029427     -148.47099      0             -148.45702     -5447.7888    \n",
            "       367   24.968299     -148.42298      0             -148.41007     -5340.0897    \n",
            "       368   24.155367     -148.4318       0             -148.41931     -5155.4105    \n",
            "       369   25.107288     -148.46509      0             -148.45211     -4813.8173    \n",
            "       370   28.206655     -148.44714      0             -148.43256     -4452.9909    \n",
            "       371   33.575894     -148.45486      0             -148.4375      -3956.7244    \n",
            "       372   41.05102      -148.46582      0             -148.44459     -3393.1904    \n",
            "       373   50.091037     -148.446        0             -148.4201      -2793.0526    \n",
            "       374   59.811668     -148.45409      0             -148.42316     -2184.6118    \n",
            "       375   69.097597     -148.48217      0             -148.44645     -1506.5485    \n",
            "       376   76.702918     -148.46644      0             -148.42678     -872.33662    \n",
            "       377   81.617333     -148.48719      0             -148.44499     -120.49711    \n",
            "       378   83.228256     -148.47564      0             -148.43261      721.2167     \n",
            "       379   81.358485     -148.48335      0             -148.44128      1433.5125    \n",
            "       380   76.406122     -148.46252      0             -148.42301      2189.4092    \n",
            "       381   69.277899     -148.48821      0             -148.45239      2918.4393    \n",
            "       382   61.232679     -148.47523      0             -148.44357      3513.109     \n",
            "       383   53.461844     -148.46206      0             -148.43442      4102.0353    \n",
            "       384   46.912566     -148.44373      0             -148.41948      4552.7075    \n",
            "       385   42.280172     -148.45059      0             -148.42873      4895.7845    \n",
            "       386   39.808531     -148.46076      0             -148.44018      5139.3155    \n",
            "       387   39.310287     -148.45088      0             -148.43055      5293.2367    \n",
            "       388   40.345394     -148.46383      0             -148.44297      5239.0267    \n",
            "       389   42.207553     -148.45832      0             -148.4365       5163.9135    \n",
            "       390   44.286246     -148.45138      0             -148.42848      5087.5242    \n",
            "       391   46.250235     -148.48559      0             -148.46168      4674.3288    \n",
            "       392   47.896273     -148.4695       0             -148.44474      4399.3803    \n",
            "       393   49.365039     -148.46732      0             -148.4418       4004.4363    \n",
            "       394   50.949166     -148.45519      0             -148.42885      3518.7654    \n",
            "       395   52.861753     -148.47177      0             -148.44444      2958.0875    \n",
            "       396   55.214104     -148.47616      0             -148.44761      2352.1468    \n",
            "       397   57.866175     -148.46708      0             -148.43716      1672.3709    \n",
            "       398   60.429846     -148.47454      0             -148.4433       1075.6839    \n",
            "       399   62.346943     -148.46951      0             -148.43728      348.14401    \n",
            "       400   62.957593     -148.46987      0             -148.43731     -265.06958    \n",
            "       401   61.784735     -148.4697       0             -148.43775     -862.4306     \n",
            "       402   58.615406     -148.46617      0             -148.43586     -1390.8446    \n",
            "       403   53.592537     -148.49526      0             -148.46755     -1954.4404    \n",
            "       404   47.335924     -148.43956      0             -148.41508     -2231.0193    \n",
            "       405   40.843535     -148.47439      0             -148.45328     -2551.5078    \n",
            "       406   35.236063     -148.44064      0             -148.42243     -2740.0998    \n",
            "       407   31.575047     -148.42857      0             -148.41224     -2902.6227    \n",
            "       408   30.666258     -148.43736      0             -148.42151     -2892.9067    \n",
            "       409   32.84546      -148.43315      0             -148.41617     -2775.1294    \n",
            "       410   37.857232     -148.45538      0             -148.43581     -2653.6401    \n",
            "       411   44.933155     -148.43476      0             -148.41153     -2324.105     \n",
            "       412   53.047214     -148.466        0             -148.43857     -1893.9617    \n",
            "       413   60.962872     -148.44986      0             -148.41834     -1555.0571    \n",
            "       414   67.401762     -148.47389      0             -148.43904     -970.12096    \n",
            "       415   71.541083     -148.45583      0             -148.41884     -422.10252    \n",
            "       416   73.0203       -148.43739      0             -148.39964      447.1234     \n",
            "       417   71.915235     -148.47906      0             -148.44188      1084.652     \n",
            "       418   68.678735     -148.47279      0             -148.43728      1942.6186    \n",
            "       419   63.922831     -148.44837      0             -148.41532      2791.2694    \n",
            "       420   58.421254     -148.45606      0             -148.42585      3505.7381    \n",
            "       421   52.737861     -148.46528      0             -148.43801      4270.8121    \n",
            "       422   47.202817     -148.44903      0             -148.42462      4966.9454    \n",
            "       423   42.001416     -148.46407      0             -148.44235      5576.9182    \n",
            "       424   37.127355     -148.43882      0             -148.41962      6158.1317    \n",
            "       425   32.506168     -148.43667      0             -148.41986      6645.0626    \n",
            "       426   28.022774     -148.42532      0             -148.41083      7114.139     \n",
            "       427   23.849316     -148.44918      0             -148.43685      7378.7496    \n",
            "       428   20.516782     -148.43794      0             -148.42733      7611.6501    \n",
            "       429   18.759132     -148.43717      0             -148.42747      7683.69      \n",
            "       430   19.373663     -148.44417      0             -148.43416      7716.5751    \n",
            "       431   23.028074     -148.44476      0             -148.43285      7415.2031    \n",
            "       432   30.003918     -148.45164      0             -148.43613      7052.1183    \n",
            "       433   40.040108     -148.44533      0             -148.42463      6682.7413    \n",
            "       434   52.439868     -148.45366      0             -148.42655      6110.2119    \n",
            "       435   65.950989     -148.47256      0             -148.43846      5463.3383    \n",
            "       436   78.955533     -148.46538      0             -148.42455      4854.6148    \n",
            "       437   89.799326     -148.45205      0             -148.40562      3924.075     \n",
            "       438   97.01252      -148.47984      0             -148.42969      3139.5004    \n",
            "       439   99.752843     -148.48594      0             -148.43437      2289.3918    \n",
            "       440   97.69743      -148.47396      0             -148.42344      1387.3208    \n",
            "       441   91.261551     -148.4646       0             -148.41742      559.25384    \n",
            "       442   81.493279     -148.46768      0             -148.42554     -228.8569     \n",
            "       443   69.68337      -148.49287      0             -148.45684     -1140.3156    \n",
            "       444   57.334253     -148.44498      0             -148.41533     -1784.6205    \n",
            "       445   45.811178     -148.44559      0             -148.4219      -2517.8849    \n",
            "       446   36.12715      -148.43739      0             -148.41871     -3138.3957    \n",
            "       447   28.799672     -148.43502      0             -148.42012     -3585.3079    \n",
            "       448   23.837261     -148.40341      0             -148.39108     -4020.1476    \n",
            "       449   21.064736     -148.45784      0             -148.44694     -4435.3675    \n",
            "       450   20.205899     -148.41573      0             -148.40528     -4598.9517    \n",
            "       451   20.993177     -148.43454      0             -148.42369     -4657.8931    \n",
            "       452   23.440118     -148.45244      0             -148.44032     -4574.8702    \n",
            "       453   27.662751     -148.43664      0             -148.42233     -4293.8229    \n",
            "       454   33.904888     -148.44245      0             -148.42492     -3980.8452    \n",
            "       455   42.433454     -148.42381      0             -148.40187     -3580.9039    \n",
            "       456   53.348864     -148.44556      0             -148.41797     -3072.5423    \n",
            "       457   66.299971     -148.46558      0             -148.4313      -2462.347     \n",
            "       458   80.431429     -148.46738      0             -148.42579     -1862.4402    \n",
            "       459   94.530854     -148.45155      0             -148.40267     -1036.4733    \n",
            "       460   106.97368     -148.47706      0             -148.42175     -290.07799    \n",
            "       461   116.17023     -148.46297      0             -148.4029       419.96313    \n",
            "       462   120.88311     -148.48356      0             -148.42106      1173.2162    \n",
            "       463   120.33851     -148.48751      0             -148.42529      2013.7941    \n",
            "       464   114.45667     -148.47903      0             -148.41985      2698.0612    \n",
            "       465   103.96631     -148.44789      0             -148.39413      3526.9971    \n",
            "       466   90.410884     -148.47823      0             -148.43149      4100.131     \n",
            "       467   75.701362     -148.45836      0             -148.41921      4640.9124    \n",
            "       468   61.74307      -148.43752      0             -148.4056       5193.6701    \n",
            "       469   50.233866     -148.46195      0             -148.43598      5496.5506    \n",
            "       470   42.287537     -148.42736      0             -148.40549      5696.8695    \n",
            "       471   38.405923     -148.45143      0             -148.43157      5779.48      \n",
            "       472   38.447436     -148.43295      0             -148.41307      5795.8379    \n",
            "       473   41.735135     -148.42349      0             -148.40191      5538.6909    \n",
            "       474   47.228713     -148.43681      0             -148.41239      5303.4395    \n",
            "       475   53.834826     -148.4319       0             -148.40406      4874.0403    \n",
            "       476   60.677928     -148.4375       0             -148.40612      4312.9453    \n",
            "       477   67.344687     -148.44107      0             -148.40625      3757.8516    \n",
            "       478   73.830726     -148.44798      0             -148.40981      3088.7911    \n",
            "       479   80.356524     -148.46578      0             -148.42423      2342.3396    \n",
            "       480   87.228657     -148.44434      0             -148.39924      1469.1776    \n",
            "       481   94.448557     -148.46533      0             -148.4165       555.6672     \n",
            "       482   101.77993     -148.45546      0             -148.40284     -366.29171    \n",
            "       483   108.58026     -148.45114      0             -148.395       -1228.8415    \n",
            "       484   113.9992      -148.45036      0             -148.39141     -2110.9493    \n",
            "       485   117.09441     -148.44281      0             -148.38227     -2990.6707    \n",
            "       486   116.92167     -148.41975      0             -148.35929     -3843.2924    \n",
            "       487   112.90325     -148.46212      0             -148.40374     -4603.682     \n",
            "       488   105.17391     -148.45094      0             -148.39657     -5255.2968    \n",
            "       489   94.645903     -148.43535      0             -148.38641     -5736.4545    \n",
            "       490   82.866055     -148.46257      0             -148.41973     -6190.9912    \n",
            "       491   71.747826     -148.42977      0             -148.39267     -6635.9945    \n",
            "       492   63.176256     -148.43795      0             -148.40528     -6785.4758    \n",
            "       493   58.663016     -148.43356      0             -148.40323     -6858.9412    \n",
            "       494   59.086847     -148.4334       0             -148.40285     -6882.4401    \n",
            "       495   64.359683     -148.43709      0             -148.40382     -6702.162     \n",
            "       496   73.470127     -148.43004      0             -148.39206     -6414.273     \n",
            "       497   84.742987     -148.44808      0             -148.40426     -6076.9759    \n",
            "       498   96.215152     -148.42916      0             -148.37941     -5626.4296    \n",
            "       499   105.94847     -148.45277      0             -148.39799     -4918.339     \n",
            "       500   112.46654     -148.46537      0             -148.40722     -4286.7987    \n",
            "       501   115.03772     -148.44613      0             -148.38665     -3467.9115    \n",
            "       502   113.91051     -148.43851      0             -148.37961     -2645.449     \n",
            "       503   110.00716     -148.44263      0             -148.38575     -1868.824     \n",
            "       504   104.36546     -148.43749      0             -148.38353     -1036.1609    \n",
            "       505   98.285982     -148.44595      0             -148.39513     -176.54272    \n",
            "       506   92.798253     -148.43138      0             -148.3834       746.97011    \n",
            "       507   88.399473     -148.44258      0             -148.39687      1349.4231    \n",
            "       508   84.975807     -148.43173      0             -148.3878       2124.3063    \n",
            "       509   82.192102     -148.4528       0             -148.4103       2711.4565    \n",
            "       510   79.559214     -148.41573      0             -148.37459      3268.0259    \n",
            "       511   76.437313     -148.41856      0             -148.37903      3707.7394    \n",
            "       512   72.774228     -148.43671      0             -148.39908      3985.851     \n",
            "       513   69.094501     -148.40414      0             -148.36842      4231.1794    \n",
            "       514   66.441678     -148.38444      0             -148.35008      4282.3073    \n",
            "       515   66.302503     -148.39115      0             -148.35687      4162.1789    \n",
            "       516   69.954281     -148.40475      0             -148.36858      3894.3524    \n",
            "       517   78.254859     -148.41171      0             -148.37125      3559.2223    \n",
            "       518   91.453383     -148.42633      0             -148.37904      3021.4222    \n",
            "       519   108.67466     -148.44982      0             -148.39363      2348.3054    \n",
            "       520   128.07781     -148.44596      0             -148.37973      1522.0617    \n",
            "       521   147.22721     -148.41166      0             -148.33554      671.87737    \n",
            "       522   163.32863     -148.45838      0             -148.37394     -272.79878    \n",
            "       523   173.82202     -148.4749       0             -148.38503     -1395.7208    \n",
            "       524   176.85297     -148.45118      0             -148.35974     -2425.4897    \n",
            "       525   171.50827     -148.43212      0             -148.34344     -3414.3663    \n",
            "       526   158.38929     -148.45373      0             -148.37184     -4404.4756    \n",
            "       527   139.47395     -148.42733      0             -148.35521     -5425.7141    \n",
            "       528   117.24152     -148.46108      0             -148.40047     -6513.0687    \n",
            "       529   94.55823      -148.4152       0             -148.36631     -7284.6799    \n",
            "       530   74.201192     -148.39182      0             -148.35345     -8058.3281    \n",
            "       531   58.123392     -148.3657       0             -148.33565     -8857.9828    \n",
            "       532   47.422221     -148.37126      0             -148.34674     -9407.797     \n",
            "       533   42.179807     -148.36537      0             -148.34356     -9942.314     \n",
            "       534   41.732269     -148.38669      0             -148.36511     -10197.765    \n",
            "       535   45.077247     -148.39264      0             -148.36933     -10487.102    \n",
            "       536   51.322647     -148.38297      0             -148.35643     -10550.002    \n",
            "       537   59.985211     -148.40074      0             -148.36973     -10425.293    \n",
            "       538   71.138266     -148.38356      0             -148.34678     -10060.924    \n",
            "       539   85.121422     -148.36951      0             -148.3255      -9585.7126    \n",
            "       540   102.72848     -148.40817      0             -148.35505     -9070.5882    \n",
            "       541   124.53995     -148.42348      0             -148.35908     -8256.5201    \n",
            "       542   150.26849     -148.43945      0             -148.36176     -7424.4315    \n",
            "       543   178.81234     -148.45842      0             -148.36597     -6411.8875    \n",
            "       544   207.92004     -148.44612      0             -148.33861     -5326.6004    \n",
            "       545   234.46585     -148.46116      0             -148.33993     -4173.1849    \n",
            "       546   254.963       -148.47594      0             -148.34411     -2979.8905    \n",
            "       547   266.24382     -148.46721      0             -148.32955     -1798.5449    \n",
            "       548   265.97407     -148.48007      0             -148.34255     -445.28718    \n",
            "       549   253.26309     -148.45417      0             -148.32322      849.76972    \n",
            "       550   229.20607     -148.43739      0             -148.31889      2160.1938    \n",
            "       551   196.78168     -148.392        0             -148.29026      3413.1861    \n",
            "       552   160.08985     -148.4132       0             -148.33043      4450.8015    \n",
            "       553   123.76536     -148.38634      0             -148.32235      5478.9843    \n",
            "       554   92.337963     -148.37999      0             -148.33225      6217.7482    \n",
            "       555   69.44581      -148.37649      0             -148.34058      6859.7325    \n",
            "       556   57.04981      -148.36316      0             -148.33366      7389.8684    \n",
            "       557   55.38628      -148.33202      0             -148.30338      7787.954     \n",
            "       558   63.412495     -148.33461      0             -148.30183      7800.0158    \n",
            "       559   79.072861     -148.3761       0             -148.33521      7742.9555    \n",
            "       560   99.82878      -148.35195      0             -148.30034      7445.8652    \n",
            "       561   123.50592     -148.36484      0             -148.30099      7076.6899    \n",
            "       562   148.52608     -148.37084      0             -148.29405      6519.8516    \n",
            "       563   174.19984     -148.36031      0             -148.27024      5905.5515    \n",
            "       564   200.40426     -148.38466      0             -148.28104      4949.6333    \n",
            "       565   227.2159      -148.38598      0             -148.2685       3849.4739    \n",
            "       566   254.7738      -148.4396       0             -148.30787      2857.427     \n",
            "       567   282.37707     -148.4557       0             -148.3097       1646.2114    \n",
            "       568   308.31097     -148.46195      0             -148.30254      383.251      \n",
            "       569   329.85851     -148.45012      0             -148.27957     -928.41643    \n",
            "       570   343.73816     -148.46315      0             -148.28542     -2138.5119    \n",
            "       571   347.2737      -148.44773      0             -148.26817     -3326.4449    \n",
            "       572   338.60905     -148.43029      0             -148.25522     -4411.5983    \n",
            "       573   317.46126     -148.43134      0             -148.2672      -5432.1991    \n",
            "       574   285.68648     -148.40528      0             -148.25757     -6242.3022    \n",
            "       575   247.08808     -148.36025      0             -148.2325      -6878.867     \n",
            "       576   206.82315     -148.38971      0             -148.28277     -7446.0426    \n",
            "       577   170.61171     -148.34117      0             -148.25296     -7650.0284    \n",
            "       578   143.66843     -148.3233       0             -148.24901     -7737.0488    \n",
            "       579   129.43009     -148.30815      0             -148.24123     -7783.5834    \n",
            "       580   129.29543     -148.31494      0             -148.24809     -7739.9573    \n",
            "       581   142.30928     -148.31087      0             -148.23729     -7387.4411    \n",
            "       582   165.45966     -148.34527      0             -148.25972     -6817.2104    \n",
            "       583   194.47286     -148.34944      0             -148.24889     -5958.8879    \n",
            "       584   224.64422     -148.33573      0             -148.21958     -5115.3161    \n",
            "       585   252.06311     -148.36525      0             -148.23493     -4037.2022    \n",
            "       586   274.11884     -148.38897      0             -148.24724     -2742.8986    \n",
            "       587   289.94654     -148.3568       0             -148.20689     -1239.9843    \n",
            "       588   300.24874     -148.38706      0             -148.23182      277.80087    \n",
            "       589   306.85614     -148.38814      0             -148.22948      1961.1786    \n",
            "       590   312.01287     -148.38921      0             -148.22788      3610.3971    \n",
            "       591   317.2123      -148.39148      0             -148.22747      5268.9585    \n",
            "       592   322.94841     -148.39837      0             -148.23139      6835.5019    \n",
            "       593   328.37731     -148.38279      0             -148.213        8313.0942    \n",
            "       594   331.46788     -148.37198      0             -148.2006       9646.5528    \n",
            "       595   329.91918     -148.38125      0             -148.21067      10871.992    \n",
            "       596   322.01264     -148.3654       0             -148.1989       11898.177    \n",
            "       597   307.19635     -148.37065      0             -148.21182      12755.723    \n",
            "       598   286.78648     -148.38067      0             -148.23239      13528.289    \n",
            "       599   263.92594     -148.33139      0             -148.19493      14023.643    \n",
            "       600   243.02934     -148.30709      0             -148.18143      14276.631    \n",
            "       601   228.89321     -148.30861      0             -148.19026      14276.336    \n",
            "       602   225.81218     -148.31248      0             -148.19572      13900.72     \n",
            "       603   236.08446     -148.30998      0             -148.18792      13270.234    \n",
            "       604   259.40161     -148.30032      0             -148.1662       12485.023    \n",
            "       605   292.60598     -148.31063      0             -148.15934      11434.708    \n",
            "       606   330.25415     -148.35122      0             -148.18046      9996.1553    \n",
            "       607   365.62653     -148.34377      0             -148.15473      8479.4459    \n",
            "       608   392.0367      -148.35917      0             -148.15647      6771.0989    \n",
            "       609   404.1677      -148.35121      0             -148.14223      5098.9609    \n",
            "       610   399.43786     -148.37743      0             -148.1709       3292.5386    \n",
            "       611   378.31322     -148.34173      0             -148.14612      1642.2612    \n",
            "       612   344.36018     -148.32654      0             -148.14849     -2.2673418    \n",
            "       613   303.21441     -148.30143      0             -148.14466     -1646.47      \n",
            "       614   261.33193     -148.26946      0             -148.13434     -3128.1595    \n",
            "       615   225.21606     -148.24901      0             -148.13256     -4551.7224    \n",
            "       616   199.69736     -148.25523      0             -148.15198     -5806.2515    \n",
            "       617   186.89479     -148.21023      0             -148.1136      -6931.3441    \n",
            "       618   186.42501     -148.24326      0             -148.14687     -7921.6843    \n",
            "       619   196.45007     -148.23713      0             -148.13556     -8557.4211    \n",
            "       620   214.07521     -148.23297      0             -148.12228     -8983.0979    \n",
            "       621   235.9475      -148.24592      0             -148.12392     -9013.5525    \n",
            "       622   259.75841     -148.26428      0             -148.12997     -8939.6206    \n",
            "       623   284.94242     -148.27904      0             -148.13171     -8349.1817    \n",
            "       624   312.41451     -148.27504      0             -148.11351     -7565.7574    \n",
            "       625   343.98596     -148.29111      0             -148.11325     -6587.0312    \n",
            "       626   381.35086     -148.33135      0             -148.13418     -5245.0141    \n",
            "       627   424.9879      -148.30805      0             -148.08832     -3791.0213    \n",
            "       628   473.06284     -148.32188      0             -148.07729     -1942.6171    \n",
            "       629   521.19939     -148.36938      0             -148.09989     -325.89302    \n",
            "       630   562.98599     -148.38219      0             -148.0911       1648.3254    \n",
            "       631   590.67344     -148.38398      0             -148.07858      3583.7621    \n",
            "       632   596.9244      -148.37202      0             -148.06338      5673.0996    \n",
            "       633   576.89797     -148.38785      0             -148.08957      7692.0383    \n",
            "       634   529.59406     -148.3156       0             -148.04178      9790.367     \n",
            "       635   458.84169     -148.29226      0             -148.05502      11659.739    \n",
            "       636   372.52233     -148.2426       0             -148.04999      13672.635    \n",
            "       637   281.6198      -148.18437      0             -148.03876      15343.743    \n",
            "       638   198.57183     -148.16198      0             -148.05931      16709.354    \n",
            "       639   134.86414     -148.13075      0             -148.06102      17835.277    \n",
            "       640   98.856272     -148.08886      0             -148.03775      18632.113    \n",
            "       641   94.740021     -148.0911       0             -148.04211      19088.44     \n",
            "       642   121.85673     -148.08004      0             -148.01704      19209.459    \n",
            "       643   175.4942      -148.10786      0             -148.01712      18959.25     \n",
            "       644   247.84707     -148.18002      0             -148.05188      18397.821    \n",
            "       645   330.13213     -148.20836      0             -148.03767      17560.149    \n",
            "       646   414.33637     -148.23437      0             -148.02014      16442.474    \n",
            "       647   494.52364     -148.28982      0             -148.03413      15078.705    \n",
            "       648   567.08124     -148.2981       0             -148.0049       13521.994    \n",
            "       649   631.10694     -148.35506      0             -148.02875      11643.366    \n",
            "       650   687.64748     -148.37008      0             -148.01454      9687.9409    \n",
            "       651   737.3496      -148.37518      0             -147.99394      7698.3316    \n",
            "       652   779.71466     -148.38562      0             -147.98248      5353.5945    \n",
            "       653   812.4481      -148.40241      0             -147.98234      3032.2478    \n",
            "       654   831.06783     -148.40647      0             -147.97677      485.51109    \n",
            "       655   830.18997     -148.39958      0             -147.97034     -1725.9701    \n",
            "       656   804.9232      -148.3897       0             -147.97352     -3958.6433    \n",
            "       657   752.62612     -148.35291      0             -147.96377     -5951.8085    \n",
            "       658   674.5171      -148.31363      0             -147.96488     -7727.1936    \n",
            "       659   576.08995     -148.24416      0             -147.9463      -9140.982     \n",
            "       660   467.52118     -148.18812      0             -147.9464      -10319.93     \n",
            "       661   361.36951     -148.11387      0             -147.92703     -11143.293    \n",
            "       662   271.04147     -148.10475      0             -147.96461     -11835.269    \n",
            "       663   208.28708     -148.07403      0             -147.96633     -12388.671    \n",
            "       664   180.80113     -148.02627      0             -147.93279     -12745.74     \n",
            "       665   191.29178     -148.03661      0             -147.9377      -12805.574    \n",
            "       666   237.30412     -148.04678      0             -147.92408     -12814.237    \n",
            "       667   311.39941     -148.07766      0             -147.91665     -12530.558    \n",
            "       668   402.85819     -148.13355      0             -147.92526     -11977.667    \n",
            "       669   500.25292     -148.19173      0             -147.93308     -11279.048    \n",
            "       670   593.09599     -148.21368      0             -147.90703     -10387.312    \n",
            "       671   673.8983      -148.26753      0             -147.91909     -9103.1066    \n",
            "       672   739.2424      -148.29445      0             -147.91223     -7588.202     \n",
            "       673   789.58966     -148.31297      0             -147.90472     -5981.261     \n",
            "       674   827.71614     -148.321        0             -147.89303     -4346.5945    \n",
            "       675   857.52702     -148.3465       0             -147.90312     -2538.0496    \n",
            "       676   882.00329     -148.33416      0             -147.87813     -862.16885    \n",
            "       677   901.55679     -148.36328      0             -147.89713      763.18893    \n",
            "       678   913.73146     -148.32025      0             -147.84781      2291.271     \n",
            "       679   913.4744      -148.32482      0             -147.85252      3731.4807    \n",
            "       680   895.26068     -148.33273      0             -147.86984      4908.7282    \n",
            "       681   854.59699     -148.29444      0             -147.85258      6008.8911    \n",
            "       682   790.46971     -148.26311      0             -147.85441      6794.4834    \n",
            "       683   706.10474     -148.21374      0             -147.84866      7639.6462    \n",
            "       684   608.90992     -148.13961      0             -147.82478      8034.3244    \n",
            "       685   509.77912     -148.10425      0             -147.84067      8465.77      \n",
            "       686   421.19116     -148.02543      0             -147.80765      8459.431     \n",
            "       687   355.08043     -148.0166       0             -147.83301      8340.9611    \n",
            "       688   320.22918     -148.01231      0             -147.84674      7681.6209    \n",
            "       689   320.63162     -147.98836      0             -147.82258      6871.063     \n",
            "       690   354.61027     -148.02002      0             -147.83668      5613.0005    \n",
            "       691   415.14297     -148.06701      0             -147.85237      4128.7197    \n",
            "       692   491.43551     -148.10086      0             -147.84677      2477.9862    \n",
            "       693   570.40213     -148.13647      0             -147.84154      738.46425    \n",
            "       694   639.58186     -148.13063      0             -147.79994     -947.7349     \n",
            "       695   690.1198      -148.14298      0             -147.78616     -2773.0011    \n",
            "       696   717.43602     -148.17603      0             -147.80509     -4543.658     \n",
            "       697   721.86688     -148.19665      0             -147.82341     -6312.0808    \n",
            "       698   708.97142     -148.17116      0             -147.8046      -7905.644     \n",
            "       699   687.30033     -148.17107      0             -147.81571     -9341.6305    \n",
            "       700   665.62257     -148.15389      0             -147.80973     -10860.98     \n",
            "       701   650.63819     -148.13576      0             -147.79935     -12294.572    \n",
            "       702   645.527       -148.14201      0             -147.80824     -13787.843    \n",
            "       703   649.33843     -148.14559      0             -147.80986     -15073.225    \n",
            "       704   657.48615     -148.14847      0             -147.80852     -16023.428    \n",
            "       705   662.61486     -148.11992      0             -147.77732     -16820.902    \n",
            "       706   657.79713     -148.11846      0             -147.77836     -17320.303    \n",
            "       707   639.34701     -148.10997      0             -147.7794      -17658.103    \n",
            "       708   607.10121     -148.06356      0             -147.74967     -17642.372    \n",
            "       709   565.46623     -148.06326      0             -147.7709      -17267.356    \n",
            "       710   522.85678     -148.05232      0             -147.78198     -16758.601    \n",
            "       711   488.70907     -148.01522      0             -147.76254     -16051.539    \n",
            "       712   471.54555     -148.01082      0             -147.76701     -15126.604    \n",
            "       713   476.68191     -148.03491      0             -147.78845     -14137.157    \n",
            "       714   504.00426     -148.01811      0             -147.75752     -13033.345    \n",
            "       715   547.68787     -148.04912      0             -147.76595     -11973.805    \n",
            "       716   597.20188     -148.09192      0             -147.78314     -10823.88     \n",
            "       717   640.31447     -148.11345      0             -147.78238     -9483.1579    \n",
            "       718   665.31975     -148.10594      0             -147.76194     -8089.8801    \n",
            "       719   663.81729     -148.09327      0             -147.75005     -6561.7957    \n",
            "       720   633.88245     -148.10252      0             -147.77477     -4884.4849    \n",
            "       721   579.32023     -148.05756      0             -147.75803     -3102.4873    \n",
            "       722   509.72886     -148.02191      0             -147.75836     -1468.9319    \n",
            "       723   438.27076     -147.96109      0             -147.73448      220.09764    \n",
            "       724   377.73653     -147.94971      0             -147.75441      1808.2129    \n",
            "       725   338.88641     -147.94786      0             -147.77264      3072.0448    \n",
            "       726   327.94677     -147.91731      0             -147.74775      4074.0912    \n",
            "       727   345.44523     -147.94469      0             -147.76608      4923.3491    \n",
            "       728   386.51455     -147.95546      0             -147.75562      5443.1784    \n",
            "       729   442.75793     -147.99087      0             -147.76195      5968.7619    \n",
            "       730   504.30148     -148.01182      0             -147.75107      6083.2109    \n",
            "       731   562.15829     -148.07449      0             -147.78384      6178.6134    \n",
            "       732   610.80742     -148.06229      0             -147.74648      6090.3528    \n",
            "       733   648.4483      -148.08738      0             -147.75211      5762.5276    \n",
            "       734   677.37127     -148.11349      0             -147.76326      5407.3883    \n",
            "       735   702.38538     -148.10054      0             -147.73738      4707.3017    \n",
            "       736   729.29272     -148.12185      0             -147.74478      3740.6107    \n",
            "       737   762.42259     -148.1639       0             -147.7697       2552.1776    \n",
            "       738   802.47212     -148.17455      0             -147.75964      1156.7958    \n",
            "       739   846.12893     -148.2104       0             -147.77292     -492.54317    \n",
            "       740   886.11494     -148.2346       0             -147.77644     -2392.0328    \n",
            "       741   912.72409     -148.25136      0             -147.77945     -4285.3635    \n",
            "       742   915.71785     -148.24074      0             -147.76728     -6163.518     \n",
            "       743   887.62148     -148.21796      0             -147.75902     -7897.5055    \n",
            "       744   826.27626     -148.17335      0             -147.74613     -9539.4298    \n",
            "       745   735.06865     -148.16004      0             -147.77998     -11086.413    \n",
            "       746   623.03496     -148.08445      0             -147.76232     -12293.453    \n",
            "       747   503.36821     -148.0322       0             -147.77194     -13467.751    \n",
            "       748   391.6692      -147.94848      0             -147.74597     -14415.318    \n",
            "       749   302.21413     -147.89039      0             -147.73414     -15275.481    \n",
            "       750   245.5157      -147.88442      0             -147.75748     -16066.377    \n",
            "       751   226.95851     -147.8586       0             -147.74126     -16825.251    \n",
            "       752   245.774       -147.898        0             -147.77093     -17457.708    \n",
            "       753   296.03116     -147.89966      0             -147.7466      -17873.279    \n",
            "       754   368.30006     -147.9594       0             -147.76897     -18212.167    \n",
            "       755   451.55802     -147.99536      0             -147.76189     -18208.94     \n",
            "       756   536.50982     -148.03366      0             -147.75627     -17867.976    \n",
            "       757   616.95296     -148.05633      0             -147.73734     -17503.174    \n",
            "       758   690.63467     -148.08887      0             -147.73178     -16526.481    \n",
            "       759   758.83151     -148.12939      0             -147.73704     -15462.937    \n",
            "       760   825.19454     -148.17469      0             -147.74803     -14160.192    \n",
            "       761   893.78639     -148.23644      0             -147.77432     -12657.724    \n",
            "       762   966.14283     -148.271        0             -147.77146     -11058.36     \n",
            "       763   1039.7879     -148.28964      0             -147.75202     -9395.4155    \n",
            "       764   1108.3363     -148.34346      0             -147.77041     -7641.1968    \n",
            "       765   1161.8335     -148.37397      0             -147.77326     -5944.936     \n",
            "       766   1188.4738     -148.39524      0             -147.78075     -4009.9       \n",
            "       767   1178.2189     -148.39328      0             -147.78409     -2194.3686    \n",
            "       768   1125.1675     -148.36316      0             -147.7814      -163.27186    \n",
            "       769   1029.9767     -148.3274       0             -147.79486      1868.0005    \n",
            "       770   899.96687     -148.26373      0             -147.79841      3664.0357    \n",
            "       771   748.50051     -148.16559      0             -147.77859      5671.7497    \n",
            "       772   593.00546     -148.11761      0             -147.811        7348.1491    \n",
            "       773   451.23282     -148.02397      0             -147.79066      8877.7892    \n",
            "       774   338.28682     -147.96629      0             -147.79138      10115.028    \n",
            "       775   264.55395     -147.94968      0             -147.81289      10916.671    \n",
            "       776   234.35402     -147.94934      0             -147.82817      11640.625    \n",
            "       777   245.66203     -147.93274      0             -147.80572      11797.827    \n",
            "       778   291.31881     -147.97247      0             -147.82185      11940.039    \n",
            "       779   360.50706     -148.00076      0             -147.81437      11607.515    \n",
            "       780   441.61176     -148.01832      0             -147.78999      11422.41     \n",
            "       781   524.79932     -148.07226      0             -147.80092      10693.61     \n",
            "       782   603.07821     -148.14661      0             -147.83479      10063.01     \n",
            "       783   673.33218     -148.17118      0             -147.82304      9259.7748    \n",
            "       784   736.22129     -148.20321      0             -147.82256      8300.6185    \n",
            "       785   794.03662     -148.23695      0             -147.8264       7072.2321    \n",
            "       786   849.09193     -148.2806       0             -147.84159      5853.5466    \n",
            "       787   902.04954     -148.28998      0             -147.82358      4431.0757    \n",
            "       788   950.26327     -148.35264      0             -147.86131      2757.7486    \n",
            "       789   987.93946     -148.3294       0             -147.81859      1258.309     \n",
            "       790   1006.8666     -148.36946      0             -147.84887     -451.70503    \n",
            "       791   999.19256     -148.39794      0             -147.88131     -2029.4231    \n",
            "       792   959.25667     -148.3498       0             -147.85383     -3200.8184    \n",
            "       793   885.44629     -148.32342      0             -147.86561     -4374.1678    \n",
            "       794   782.22588     -148.28349      0             -147.87905     -5270.0308    \n",
            "       795   658.90728     -148.22013      0             -147.87945     -5895.2938    \n",
            "       796   529.11623     -148.15447      0             -147.8809      -6176.9007    \n",
            "       797   407.45251     -148.09494      0             -147.88427     -6498.3662    \n",
            "       798   306.79985     -148.05255      0             -147.89392     -6379.9481    \n",
            "       799   236.4272      -148.04036      0             -147.91812     -6150.3939    \n",
            "       800   200.00365     -148.01278      0             -147.90937     -5996.3301    \n",
            "       801   195.58905     -147.97561      0             -147.87448     -5642.0928    \n",
            "       802   216.58118     -148.00003      0             -147.88805     -5073.0243    \n",
            "       803   253.19814     -148.0344       0             -147.90349     -4379.1548    \n",
            "       804   295.59627     -148.05015      0             -147.89732     -3442.8255    \n",
            "       805   335.84146     -148.07239      0             -147.89874     -2275.1361    \n",
            "       806   368.977       -148.10722      0             -147.91645     -970.67441    \n",
            "       807   394.59848     -148.11744      0             -147.91342      412.31218    \n",
            "       808   415.59254     -148.13002      0             -147.91514      1999.4289    \n",
            "       809   436.86505     -148.15233      0             -147.92646      3704.2797    \n",
            "       810   463.23756     -148.17255      0             -147.93304      5288.3135    \n",
            "       811   497.4894      -148.17305      0             -147.91582      6896.4329    \n",
            "       812   539.16901     -148.2092       0             -147.93043      8389.4387    \n",
            "       813   584.2048      -148.23268      0             -147.93062      9758.0128    \n",
            "       814   625.75467     -148.26965      0             -147.94611      11008.663    \n",
            "       815   655.76304     -148.29499      0             -147.95593      12106.196    \n",
            "       816   667.09022     -148.31219      0             -147.96728      13150.464    \n",
            "       817   655.70113     -148.30599      0             -147.96697      13840.708    \n",
            "       818   621.75896     -148.27957      0             -147.9581       14587.195    \n",
            "       819   569.4         -148.25258      0             -147.95818      15115.259    \n",
            "       820   506.20836     -148.24192      0             -147.98019      15480.993    \n",
            "       821   441.91029     -148.19961      0             -147.97112      15382.266    \n",
            "       822   385.67622     -148.18321      0             -147.9838       15079.712    \n",
            "       823   344.51363     -148.16589      0             -147.98776      14585.135    \n",
            "       824   321.77578     -148.15107      0             -147.9847       13810.76     \n",
            "       825   316.5613      -148.15439      0             -147.99071      12738.807    \n",
            "       826   324.45676     -148.1362       0             -147.96844      11478.749    \n",
            "       827   338.77142     -148.16897      0             -147.99381      10083.704    \n",
            "       828   352.33255     -148.17447      0             -147.9923       8514.7685    \n",
            "       829   359.51681     -148.18856      0             -148.00268      6982.3645    \n",
            "       830   357.24654     -148.16424      0             -147.97953      5322.7823    \n",
            "       831   345.86255     -148.18284      0             -148.00401      3834.8577    \n",
            "       832   328.65401     -148.16114      0             -147.99121      2327.4223    \n",
            "       833   311.31326     -148.17267      0             -148.01171      780.49594    \n",
            "       834   300.17348     -148.16775      0             -148.01255     -506.95662    \n",
            "       835   300.37257     -148.1667       0             -148.0114      -1765.8709    \n",
            "       836   314.70149     -148.17535      0             -148.01264     -2963.679     \n",
            "       837   342.8326      -148.18076      0             -148.0035      -4126.0447    \n",
            "       838   381.28889     -148.23829      0             -148.04114     -5334.02      \n",
            "       839   424.59776     -148.24441      0             -148.02488     -6075.4505    \n",
            "       840   466.37176     -148.23889      0             -147.99775     -6660.5881    \n",
            "       841   500.35689     -148.29589      0             -148.03718     -7164.2517    \n",
            "       842   523.0117      -148.29529      0             -148.02487     -7282.0483    \n",
            "       843   533.32122     -148.30953      0             -148.03378     -7288.6835    \n",
            "       844   532.72276     -148.29413      0             -148.01869     -6839.4378    \n",
            "       845   525.44141     -148.31459      0             -148.04292     -6310.966     \n",
            "       846   516.22421     -148.3161       0             -148.04919     -5493.0224    \n",
            "       847   509.19842     -148.32799      0             -148.06472     -4783.1219    \n",
            "       848   506.81717     -148.3357       0             -148.07365     -3715.0603    \n",
            "       849   508.68186     -148.33926      0             -148.07625     -2679.059     \n",
            "       850   511.91763     -148.34956      0             -148.08488     -1497.0773    \n",
            "       851   511.78619     -148.34536      0             -148.08075     -275.06441    \n",
            "       852   503.19503     -148.33783      0             -148.07766      814.34021    \n",
            "       853   482.09442     -148.35228      0             -148.10301      2180.2513    \n",
            "       854   446.66913     -148.32352      0             -148.09258      3448.3875    \n",
            "       855   398.99275     -148.29992      0             -148.09362      4737.7953    \n",
            "       856   344.02834     -148.27883      0             -148.10096      5976.3516    \n",
            "       857   288.57413     -148.25399      0             -148.10478      7207.499     \n",
            "       858   240.61921     -148.20508      0             -148.08067      8370.8161    \n",
            "       859   207.24617     -148.20254      0             -148.09538      9190.2696    \n",
            "       860   193.27595     -148.19622      0             -148.09629      9933.5968    \n",
            "       861   200.35511     -148.19534      0             -148.09175      10443.384    \n",
            "       862   226.62447     -148.21985      0             -148.10267      10868.245    \n",
            "       863   267.55245     -148.28655      0             -148.14822      10862.205    \n",
            "       864   316.49029     -148.26852      0             -148.10488      10925.246    \n",
            "       865   366.49299     -148.3007       0             -148.11121      10736.141    \n",
            "       866   411.73075     -148.35572      0             -148.14284      10320.85     \n",
            "       867   448.10905     -148.34887      0             -148.11718      9822.9633    \n",
            "       868   474.4978      -148.38523      0             -148.13989      9222.8851    \n",
            "       869   491.64013     -148.38528      0             -148.13108      8427.9996    \n",
            "       870   501.98171     -148.41205      0             -148.15251      7541.3498    \n",
            "       871   508.32622     -148.43511      0             -148.17229      6419.8982    \n",
            "       872   512.33293     -148.40991      0             -148.14501      5403.4754    \n",
            "       873   514.20884     -148.41876      0             -148.15289      4135.2641    \n",
            "       874   512.54395     -148.43676      0             -148.17175      2833.543     \n",
            "       875   504.63187     -148.42552      0             -148.16461      1411.4665    \n",
            "       876   486.91968     -148.4279       0             -148.17614      244.65029    \n",
            "       877   456.72901     -148.39749      0             -148.16135     -1048.9541    \n",
            "       878   413.51092     -148.40746      0             -148.19365     -2269.0596    \n",
            "       879   358.54675     -148.36765      0             -148.18227     -3207.3612    \n",
            "       880   296.05312     -148.31963      0             -148.16656     -4184.6605    \n",
            "       881   232.1098      -148.30786      0             -148.18785     -4920.8125    \n",
            "       882   173.60122     -148.2654       0             -148.17564     -5626.1271    \n",
            "       883   126.97259     -148.2793       0             -148.21365     -6287.6407    \n",
            "       884   96.817349     -148.23175      0             -148.18169     -6863.2306    \n",
            "       885   85.375785     -148.2548       0             -148.21065     -7186.0862    \n",
            "       886   92.00322      -148.25115      0             -148.20358     -7537.7573    \n",
            "       887   113.59527     -148.2554       0             -148.19667     -7818.5396    \n",
            "       888   145.19678     -148.27877      0             -148.2037      -7843.1991    \n",
            "       889   181.39187     -148.29339      0             -148.1996      -7920.7599    \n",
            "       890   217.3923      -148.29402      0             -148.18162     -7736.9554    \n",
            "       891   249.88512     -148.34122      0             -148.21202     -7573.7343    \n",
            "       892   277.23489     -148.3341       0             -148.19076     -6961.8138    \n",
            "       893   299.5029      -148.34392      0             -148.18907     -6583.7898    \n",
            "       894   318.04508     -148.3568       0             -148.19236     -5888.389     \n",
            "       895   334.64058     -148.37316      0             -148.20014     -5183.3293    \n",
            "       896   350.47361     -148.39334      0             -148.21213     -4480.0852    \n",
            "       897   365.47142     -148.41051      0             -148.22154     -3850.3323    \n",
            "       898   378.18535     -148.40622      0             -148.21068     -3098.0703    \n",
            "       899   385.95139     -148.41323      0             -148.21367     -2551.5348    \n",
            "       900   385.54713     -148.40747      0             -148.20812     -1991.8738    \n",
            "       901   374.14911     -148.44122      0             -148.24777     -1221.2262    \n",
            "       902   350.38221     -148.43725      0             -148.25608     -758.02343    \n",
            "       903   314.57925     -148.41803      0             -148.25538     -194.81055    \n",
            "       904   269.34109     -148.38032      0             -148.24106      295.7725     \n",
            "       905   218.86478     -148.3432       0             -148.23004      730.84756    \n",
            "       906   168.40922     -148.3221       0             -148.23503      1023.245     \n",
            "       907   123.51106     -148.31723      0             -148.25337      1232.5095    \n",
            "       908   88.662621     -148.27872      0             -148.23288      1382.7149    \n",
            "       909   66.901746     -148.28586      0             -148.25126      1247.9012    \n",
            "       910   59.140859     -148.28696      0             -148.25638      1054.6048    \n",
            "       911   64.05865      -148.284        0             -148.25087      628.71063    \n",
            "       912   78.865389     -148.29188      0             -148.2511       272.83712    \n",
            "       913   99.855468     -148.29078      0             -148.23915     -247.97821    \n",
            "       914   123.03443     -148.34429      0             -148.28068     -879.73353    \n",
            "       915   145.24874     -148.34508      0             -148.26998     -1529.5702    \n",
            "       916   164.74997     -148.34768      0             -148.2625      -2169.3571    \n",
            "       917   180.70147     -148.34835      0             -148.25492     -2830.2725    \n",
            "       918   193.89794     -148.3544       0             -148.25415     -3539.5955    \n",
            "       919   206.0513      -148.37397      0             -148.26744     -4170.4602    \n",
            "       920   218.59138     -148.38877      0             -148.27575     -4778.0174    \n",
            "       921   232.54928     -148.359        0             -148.23877     -5468.4477    \n",
            "       922   247.93705     -148.38392      0             -148.25573     -6200.5185    \n",
            "       923   263.48626     -148.43463      0             -148.29839     -6837.2254    \n",
            "       924   276.90285     -148.3952       0             -148.25203     -7440.7229    \n",
            "       925   285.50728     -148.40355      0             -148.25593     -7791.705     \n",
            "       926   287.05742     -148.41793      0             -148.26951     -8127.7184    \n",
            "       927   279.90114     -148.40908      0             -148.26436     -8448.7905    \n",
            "       928   264.04552     -148.43069      0             -148.29417     -8591.8473    \n",
            "       929   241.47165     -148.41277      0             -148.28792     -8546.0526    \n",
            "       930   215.2275      -148.3997       0             -148.28842     -8488.2785    \n",
            "       931   188.84563     -148.3984       0             -148.30076     -8275.3445    \n",
            "       932   165.92824     -148.38388      0             -148.29808     -7953.1692    \n",
            "       933   149.31038     -148.38287      0             -148.30567     -7572.5736    \n",
            "       934   140.28318     -148.36929      0             -148.29676     -7145.5791    \n",
            "       935   138.44343     -148.3503       0             -148.27872     -6502.5664    \n",
            "       936   141.86526     -148.35603      0             -148.28268     -5946.8069    \n",
            "       937   147.7546      -148.33604      0             -148.25964     -5247.5407    \n",
            "       938   153.42368     -148.36804      0             -148.28871     -4665.4433    \n",
            "       939   156.6618      -148.38598      0             -148.30497     -3836.434     \n",
            "       940   156.42091     -148.38672      0             -148.30585     -2942.8123    \n",
            "       941   153.1647      -148.36314      0             -148.28395     -2107.5777    \n",
            "       942   148.51221     -148.37955      0             -148.30276     -1258.5523    \n",
            "       943   144.82426     -148.37159      0             -148.29671     -420.77341    \n",
            "       944   144.54949     -148.34427      0             -148.26954      496.57788    \n",
            "       945   149.53149     -148.36544      0             -148.28813      1229.7513    \n",
            "       946   160.50564     -148.39497      0             -148.31198      1899.7186    \n",
            "       947   176.76729     -148.38492      0             -148.29352      2477.628     \n",
            "       948   196.16637     -148.40189      0             -148.30047      3022.4466    \n",
            "       949   215.96077     -148.41298      0             -148.30131      3409.3683    \n",
            "       950   233.34336     -148.45672      0             -148.33607      3675.0097    \n",
            "       951   245.87837     -148.43287      0             -148.30574      4088.1814    \n",
            "       952   252.17899     -148.44028      0             -148.30989      4173.8522    \n",
            "       953   252.33978     -148.42023      0             -148.28976      4239.3469    \n",
            "       954   247.74565     -148.44641      0             -148.31831      4135.8238    \n",
            "       955   240.26743     -148.43521      0             -148.31098      3992.5682    \n",
            "       956   232.15031     -148.42649      0             -148.30645      3750.7132    \n",
            "       957   225.33173     -148.44892      0             -148.33241      3322.2511    \n",
            "       958   220.75149     -148.40824      0             -148.29411      2904.1321    \n",
            "       959   218.1576      -148.41146      0             -148.29867      2309.2195    \n",
            "       960   216.21209     -148.43107      0             -148.31928      1570.4072    \n",
            "       961   212.93749     -148.42949      0             -148.31939      795.53004    \n",
            "       962   206.46569     -148.43541      0             -148.32866      84.45954     \n",
            "       963   195.43217     -148.41478      0             -148.31373     -776.66636    \n",
            "       964   179.48488     -148.39944      0             -148.30664     -1471.3657    \n",
            "       965   159.56614     -148.42064      0             -148.33813     -2127.7501    \n",
            "       966   137.74519     -148.39431      0             -148.32309     -2819.6059    \n",
            "       967   116.9692      -148.37165      0             -148.31117     -3458.3835    \n",
            "       968   100.16352     -148.37596      0             -148.32417     -4019.4044    \n",
            "       969   89.791676     -148.37924      0             -148.33281     -4516.8226    \n",
            "       970   87.343948     -148.38101      0             -148.33585     -4894.4239    \n",
            "       971   93.050437     -148.38193      0             -148.33382     -5311.4479    \n",
            "       972   105.77398     -148.38352      0             -148.32883     -5615.4764    \n",
            "       973   123.33275     -148.42231      0             -148.35854     -5814.2588    \n",
            "       974   142.97965     -148.38255      0             -148.30863     -5861.3665    \n",
            "       975   162.02165     -148.39836      0             -148.31459     -5832.1412    \n",
            "       976   178.59908     -148.42465      0             -148.33231     -5693.6263    \n",
            "       977   191.72732     -148.41634      0             -148.31721     -5336.9146    \n",
            "       978   201.00119     -148.43208      0             -148.32815     -4988.1572    \n",
            "       979   207.14463     -148.44939      0             -148.34229     -4510.8107    \n",
            "       980   211.50004     -148.46256      0             -148.3532      -4092.0403    \n",
            "       981   214.8884      -148.46092      0             -148.34981     -3302.5061    \n",
            "       982   217.66527     -148.46272      0             -148.35018     -2594.6786    \n",
            "       983   219.58419     -148.47161      0             -148.35807     -1850.2851    \n",
            "       984   219.61892     -148.47116      0             -148.3576      -1057.6463    \n",
            "       985   216.22746     -148.47207      0             -148.36027     -286.60325    \n",
            "       986   207.73726     -148.46308      0             -148.35567      639.63606    \n",
            "       987   193.02732     -148.44887      0             -148.34906      1426.1686    \n",
            "       988   171.8839      -148.43923      0             -148.35036      2165.3343    \n",
            "       989   145.41701     -148.44207      0             -148.36689      3001.0469    \n",
            "       990   115.88799     -148.39962      0             -148.3397       3728.765     \n",
            "       991   86.093621     -148.38857      0             -148.34405      4424.291     \n",
            "       992   59.446003     -148.37745      0             -148.34671      5065.8793    \n",
            "       993   39.074013     -148.3675       0             -148.3473       5595.0876    \n",
            "       994   27.103375     -148.36757      0             -148.35356      5919.3743    \n",
            "       995   24.514268     -148.36844      0             -148.35577      6134.7165    \n",
            "       996   30.910067     -148.36525      0             -148.34927      6322.0524    \n",
            "       997   44.649191     -148.35663      0             -148.33355      6404.6139    \n",
            "       998   63.236367     -148.40318      0             -148.37049      6345.4426    \n",
            "       999   83.87036      -148.39008      0             -148.34672      6108.2392    \n",
            "      1000   104.13687     -148.39518      0             -148.34134      6027.8191    \n",
            "      1001   122.32077     -148.41169      0             -148.34844      5526.8845    \n",
            "      1002   137.46096     -148.41614      0             -148.34507      5234.9993    \n",
            "      1003   149.28204     -148.42393      0             -148.34675      4772.1496    \n",
            "      1004   158.42279     -148.41687      0             -148.33496      4231.187     \n",
            "      1005   165.75162     -148.41727      0             -148.33157      3758.3142    \n",
            "      1006   171.73155     -148.44869      0             -148.3599       3083.8457    \n",
            "      1007   176.52938     -148.4381       0             -148.34683      2587.0443    \n",
            "      1008   179.64851     -148.39897      0             -148.30608      2073.6188    \n",
            "      1009   180.00829     -148.44446      0             -148.35139      1417.0486    \n",
            "      1010   176.44656     -148.4433       0             -148.35207      956.001      \n",
            "      1011   167.80657     -148.43596      0             -148.3492       488.5605     \n",
            "      1012   153.48452     -148.43218      0             -148.35282      90.151416    \n",
            "      1013   133.97696     -148.43461      0             -148.36533     -344.82948    \n",
            "      1014   110.81747     -148.40123      0             -148.34394     -431.91745    \n",
            "      1015   86.355778     -148.40978      0             -148.36513     -476.04724    \n",
            "      1016   63.488416     -148.38104      0             -148.34822     -624.88814    \n",
            "      1017   44.869393     -148.37533      0             -148.35213     -550.4096     \n",
            "      1018   32.669517     -148.35895      0             -148.34205     -349.99791    \n",
            "      1019   27.999678     -148.3754       0             -148.36093     -175.54984    \n",
            "      1020   30.67696      -148.365        0             -148.34913      139.46363    \n",
            "      1021   39.456467     -148.38998      0             -148.36958      452.00468    \n",
            "      1022   52.413227     -148.392        0             -148.3649       979.73378    \n",
            "      1023   67.23608      -148.38863      0             -148.35386      1459.5323    \n",
            "      1024   81.630432     -148.42219      0             -148.37999      1961.9873    \n",
            "      1025   94.161888     -148.39436      0             -148.34568      2652.7337    \n",
            "      1026   104.19882     -148.41224      0             -148.35837      3311.1948    \n",
            "      1027   111.93414     -148.4191       0             -148.36122      3898.2904    \n",
            "      1028   118.40571     -148.42912      0             -148.36789      4544.4969    \n",
            "      1029   124.76194     -148.43813      0             -148.37362      5272.0699    \n",
            "      1030   131.97988     -148.41582      0             -148.34758      5855.8623    \n",
            "      1031   140.44041     -148.43448      0             -148.36187      6414.9488    \n",
            "      1032   149.70662     -148.43291      0             -148.3555       6887.0541    \n",
            "      1033   158.50221     -148.4408       0             -148.35885      7197.2009    \n",
            "      1034   165.40133     -148.43456      0             -148.34904      7593.0765    \n",
            "      1035   168.97953     -148.40902      0             -148.32165      7729.1154    \n",
            "      1036   167.89404     -148.45009      0             -148.36328      7841.633     \n",
            "      1037   161.8916      -148.44492      0             -148.36122      7781.1376    \n",
            "      1038   151.43804     -148.42773      0             -148.34943      7683.1103    \n",
            "      1039   137.93194     -148.42653      0             -148.35522      7575.5862    \n",
            "      1040   123.5657      -148.43486      0             -148.37097      7152.4069    \n",
            "      1041   110.35119     -148.41383      0             -148.35678      6826.4665    \n",
            "      1042   100.07053     -148.41829      0             -148.36655      6236.1222    \n",
            "      1043   93.835604     -148.4361       0             -148.38758      5778.3732    \n",
            "      1044   91.779575     -148.38451      0             -148.33706      5052.2409    \n",
            "      1045   93.114455     -148.38369      0             -148.33554      4242.8428    \n",
            "      1046   96.428102     -148.40283      0             -148.35298      3527.6917    \n",
            "      1047   100.0414      -148.40559      0             -148.35386      2614.2562    \n",
            "      1048   102.34514     -148.41636      0             -148.36344      1764.1024    \n",
            "      1049   102.49551     -148.40475      0             -148.35176      862.72905    \n",
            "      1050   100.52523     -148.42994      0             -148.37796     -2.0680639    \n",
            "      1051   97.12664      -148.40709      0             -148.35688     -634.60021    \n",
            "      1052   93.754692     -148.42452      0             -148.37605     -1410.4418    \n",
            "      1053   92.1398       -148.4223       0             -148.37466     -2029.2579    \n",
            "      1054   93.79291      -148.41675      0             -148.36825     -2730.2667    \n",
            "      1055   99.55333      -148.40621      0             -148.35473     -3204.7001    \n",
            "      1056   109.43955     -148.42383      0             -148.36725     -3784.0547    \n",
            "      1057   122.59572     -148.43321      0             -148.36982     -4091.8455    \n",
            "      1058   137.26625     -148.44191      0             -148.37094     -4362.1566    \n",
            "      1059   151.31191     -148.44521      0             -148.36697     -4473.3049    \n",
            "      1060   162.76418     -148.45116      0             -148.367       -4604.2668    \n",
            "      1061   170.28997     -148.44653      0             -148.35848     -4501.2918    \n",
            "      1062   173.29906     -148.43128      0             -148.34168     -4342.7438    \n",
            "      1063   171.97057     -148.47804      0             -148.38913     -4134.0898    \n",
            "      1064   167.23459     -148.45883      0             -148.37236     -3776.1191    \n",
            "      1065   160.48404     -148.44112      0             -148.35815     -3365.2257    \n",
            "      1066   153.10769     -148.45676      0             -148.37759     -2920.3905    \n",
            "      1067   146.13116     -148.43329      0             -148.35773     -2313.9945    \n",
            "      1068   139.91208     -148.45357      0             -148.38123     -1751.2721    \n",
            "      1069   134.17091     -148.44199      0             -148.37261     -1099.3531    \n",
            "      1070   128.17157     -148.44415      0             -148.37788     -553.46065    \n",
            "      1071   120.85615     -148.44934      0             -148.38685      35.875843    \n",
            "      1072   111.30629     -148.41802      0             -148.36047      727.51835    \n",
            "      1073   99.244566     -148.41658      0             -148.36527      1391.8335    \n",
            "      1074   85.046841     -148.40527      0             -148.36129      1860.3679    \n",
            "      1075   70.037182     -148.41516      0             -148.37895      2462.3225    \n",
            "      1076   56.196137     -148.37243      0             -148.34337      2901.0732    \n",
            "      1077   45.721223     -148.40329      0             -148.37965      3248.7211    \n",
            "      1078   40.589827     -148.38927      0             -148.36829      3598.4269    \n",
            "      1079   42.168831     -148.38787      0             -148.36607      3827.8696    \n",
            "      1080   50.885637     -148.3935       0             -148.36719      3983.9026    \n",
            "      1081   65.968266     -148.40967      0             -148.37557      4016.1213    \n",
            "      1082   85.672439     -148.38866      0             -148.34436      3980.4466    \n",
            "      1083   107.55055     -148.42538      0             -148.36977      3640.0472    \n",
            "      1084   128.96302     -148.43596      0             -148.36928      3415.0584    \n",
            "      1085   147.44643     -148.42569      0             -148.34945      3070.1418    \n",
            "      1086   161.2263      -148.44193      0             -148.35857      2696.9946    \n",
            "      1087   169.49908     -148.44184      0             -148.3542       2221.898     \n",
            "      1088   172.68218     -148.46956      0             -148.38028      1706.2928    \n",
            "      1089   171.73435     -148.42808      0             -148.33929      1114.3511    \n",
            "      1090   167.83088     -148.45791      0             -148.37114      510.73064    \n",
            "      1091   162.31526     -148.42506      0             -148.34114     -11.015879    \n",
            "      1092   156.02329     -148.42091      0             -148.34024     -655.95431    \n",
            "      1093   149.09392     -148.43115      0             -148.35407     -1307.4551    \n",
            "      1094   141.04216     -148.42189      0             -148.34896     -1955.2329    \n",
            "      1095   131.01673     -148.41105      0             -148.34331     -2599.0021    \n",
            "      1096   118.32834     -148.4203       0             -148.35912     -3159.6503    \n",
            "      1097   102.52686     -148.42877      0             -148.37576     -3714.1485    \n",
            "      1098   83.793587     -148.41305      0             -148.36972     -4061.4301    \n",
            "      1099   63.58463      -148.375        0             -148.34212     -4368.8679    \n",
            "      1100   43.922761     -148.37762      0             -148.35491     -4589.3726    \n",
            "      1101   27.25526      -148.3786       0             -148.36451     -4802.0578    \n",
            "      1102   15.985483     -148.38554      0             -148.37727     -4896.0907    \n",
            "      1103   11.989742     -148.38187      0             -148.37567     -4997.645     \n",
            "      1104   16.181921     -148.36457      0             -148.3562      -4886.4186    \n",
            "      1105   28.174927     -148.36669      0             -148.35212     -4767.8137    \n",
            "      1106   46.398265     -148.38438      0             -148.36039     -4689.3679    \n",
            "      1107   68.462693     -148.37581      0             -148.34042     -4528.1915    \n",
            "      1108   91.758867     -148.40436      0             -148.35692     -4181.6678    \n",
            "      1109   113.61283     -148.39898      0             -148.34024     -3766.4832    \n",
            "      1110   131.97643     -148.40984      0             -148.3416      -3486.8352    \n",
            "      1111   145.93314     -148.41451      0             -148.33906     -2907.6255    \n",
            "      1112   155.5141      -148.45038      0             -148.36997     -2437.866     \n",
            "      1113   161.48145     -148.42611      0             -148.34261     -1959.099     \n",
            "      1114   164.8919      -148.45457      0             -148.36932     -1463.4719    \n",
            "      1115   166.99091     -148.45992      0             -148.37358     -1029.326     \n",
            "      1116   168.48602     -148.44014      0             -148.35302     -484.13155    \n",
            "      1117   169.40538     -148.46026      0             -148.37267     -159.26034    \n",
            "      1118   169.16375     -148.44198      0             -148.35452      305.12933    \n",
            "      1119   166.65964     -148.45104      0             -148.36487      523.14474    \n",
            "      1120   160.70796     -148.4283       0             -148.34521      841.48516    \n",
            "      1121   150.45672     -148.42011      0             -148.34231      941.97373    \n",
            "      1122   135.92105     -148.45471      0             -148.38444      1020.1759    \n",
            "      1123   118.01796     -148.42216      0             -148.36114      1108.8048    \n",
            "      1124   98.535846     -148.43614      0             -148.3852       1119.6861    \n",
            "      1125   79.866803     -148.41267      0             -148.37137      890.38054    \n",
            "      1126   64.510618     -148.3768       0             -148.34345      703.32948    \n",
            "      1127   54.712141     -148.38136      0             -148.35307      380.22943    \n",
            "      1128   51.853845     -148.38796      0             -148.36115      21.736708    \n",
            "      1129   56.195276     -148.41051      0             -148.38145     -502.67795    \n",
            "      1130   66.977617     -148.37942      0             -148.34479     -1005.0209    \n",
            "      1131   82.253071     -148.39862      0             -148.35609     -1553.5762    \n",
            "      1132   99.547079     -148.42625      0             -148.37478     -2244.8158    \n",
            "      1133   116.43868     -148.42312      0             -148.36291     -2872.4       \n",
            "      1134   130.77124     -148.41402      0             -148.34641     -3610.919     \n",
            "      1135   141.59329     -148.4264       0             -148.35319     -4161.4399    \n",
            "      1136   148.94885     -148.40994      0             -148.33292     -4668.2508    \n",
            "      1137   153.54721     -148.40272      0             -148.32333     -5316.5813    \n",
            "      1138   156.91317     -148.43082      0             -148.34969     -5923.8122    \n",
            "      1139   160.5997      -148.43923      0             -148.35619     -6342.8861    \n",
            "      1140   165.87751     -148.42938      0             -148.34362     -6722.2534    \n",
            "      1141   173.25671     -148.42259      0             -148.33301     -7135.6621    \n",
            "      1142   182.32331     -148.43831      0             -148.34404     -7515.1161    \n",
            "      1143   191.83016     -148.43765      0             -148.33846     -7694.0554    \n",
            "      1144   199.91825     -148.41994      0             -148.31658     -7683.0061    \n",
            "      1145   204.58247     -148.41953      0             -148.31375     -7734.8454    \n",
            "      1146   204.55032     -148.43667      0             -148.33091     -7674.5506    \n",
            "      1147   199.36647     -148.45963      0             -148.35655     -7432.2665    \n",
            "      1148   189.43914     -148.45555      0             -148.3576      -7049.2728    \n",
            "      1149   176.33107     -148.44089      0             -148.34972     -6673.4176    \n",
            "      1150   162.36811     -148.43685      0             -148.3529      -6149.2698    \n",
            "      1151   149.83565     -148.42964      0             -148.35217     -5402.8997    \n",
            "      1152   140.51365     -148.41497      0             -148.34232     -4609.6361    \n",
            "      1153   135.40624     -148.41593      0             -148.34592     -3803.17      \n",
            "      1154   134.41839     -148.40862      0             -148.33912     -3098.9791    \n",
            "      1155   136.4996      -148.41354      0             -148.34296     -2221.495     \n",
            "      1156   139.87785     -148.40914      0             -148.33682     -1243.8703    \n",
            "      1157   142.61253     -148.39971      0             -148.32598     -349.14552    \n",
            "      1158   143.06914     -148.42476      0             -148.35079      447.2369     \n",
            "      1159   140.50463     -148.39867      0             -148.32602      1358.3789    \n",
            "      1160   135.45162     -148.38555      0             -148.31552      2367.2101    \n",
            "      1161   129.52501     -148.3819       0             -148.31493      3261.4826    \n",
            "      1162   124.93313     -148.40106      0             -148.33647      4094.0353    \n",
            "      1163   124.03944     -148.38813      0             -148.32399      4794.3752    \n",
            "      1164   128.84891     -148.38684      0             -148.32022      5458.9604    \n",
            "      1165   140.3314      -148.36904      0             -148.29648      5937.7251    \n",
            "      1166   158.13586     -148.38296      0             -148.3012       6274.7889    \n",
            "      1167   180.34406     -148.43882      0             -148.34558      6652.0043    \n",
            "      1168   204.08095     -148.44582      0             -148.3403       6644.4566    \n",
            "      1169   226.21856     -148.43718      0             -148.32022      6686.4577    \n",
            "      1170   243.79241     -148.47027      0             -148.34422      6611.6513    \n",
            "      1171   254.47453     -148.45838      0             -148.3268       6439.0487    \n",
            "      1172   257.51407     -148.44939      0             -148.31624      6091.3718    \n",
            "      1173   253.59282     -148.44794      0             -148.31682      5782.3721    \n",
            "      1174   244.23284     -148.43533      0             -148.30905      5351.2747    \n",
            "      1175   231.6913      -148.4373       0             -148.31751      4828.8882    \n",
            "      1176   218.33057     -148.44554      0             -148.33266      4174.3414    \n",
            "      1177   205.83215     -148.39736      0             -148.29093      3477.5577    \n",
            "      1178   194.739       -148.40908      0             -148.30839      2759.4754    \n",
            "      1179   184.55868     -148.39836      0             -148.30294      1974.3405    \n",
            "      1180   173.91291     -148.39488      0             -148.30496      1203.9476    \n",
            "      1181   161.34314     -148.39881      0             -148.31539      568.96713    \n",
            "      1182   145.66454     -148.37298      0             -148.29766     -186.30036    \n",
            "      1183   126.73991     -148.3745       0             -148.30897     -872.30144    \n",
            "      1184   105.8956      -148.37089      0             -148.31614     -1353.149     \n",
            "      1185   85.453796     -148.32372      0             -148.27953     -1851.4979    \n",
            "      1186   68.726741     -148.32439      0             -148.28886     -2202.4632    \n",
            "      1187   59.140198     -148.32456      0             -148.29398     -2518.8151    \n",
            "      1188   59.437643     -148.31909      0             -148.28836     -2712.7836    \n",
            "      1189   71.141131     -148.31968      0             -148.28289     -2763.1824    \n",
            "      1190   93.988338     -148.34193      0             -148.29333     -2783.5329    \n",
            "      1191   125.84166     -148.36111      0             -148.29605     -2711.1291    \n",
            "      1192   163.21813     -148.3603       0             -148.2759      -2592.308     \n",
            "      1193   201.67245     -148.41107      0             -148.3068      -2288.0517    \n",
            "      1194   236.77578     -148.41453      0             -148.2921      -1918.6386    \n",
            "      1195   265.18705     -148.41305      0             -148.27594     -1366.216     \n",
            "      1196   284.77367     -148.44372      0             -148.29648     -688.75126    \n",
            "      1197   294.96079     -148.43698      0             -148.28447      147.06589    \n",
            "      1198   296.85708     -148.44763      0             -148.29415      742.20168    \n",
            "      1199   292.41914     -148.42094      0             -148.26975      1669.4498    \n",
            "      1200   283.73425     -148.44298      0             -148.29628      2492.9475    \n",
            "      1201   272.6031      -148.40569      0             -148.26474      3398.605     \n",
            "      1202   259.92183     -148.41649      0             -148.2821       4165.2737    \n",
            "      1203   245.65693     -148.36611      0             -148.2391       4970.9173    \n",
            "      1204   228.95349     -148.38619      0             -148.26782      5554.8914    \n",
            "      1205   208.62176     -148.38202      0             -148.27416      6136.2452    \n",
            "      1206   183.82123     -148.34597      0             -148.25092      6728.8981    \n",
            "      1207   155.11381     -148.33565      0             -148.25545      7125.4987    \n",
            "      1208   124.12459     -148.3213       0             -148.25712      7523.7801    \n",
            "      1209   93.564433     -148.3058       0             -148.25743      7765.2924    \n",
            "      1210   67.54336      -148.28529      0             -148.25037      7918.4102    \n",
            "      1211   50.31353      -148.27579      0             -148.24977      7904.9484    \n",
            "      1212   45.380658     -148.28059      0             -148.25713      7769.4032    \n",
            "      1213   54.912836     -148.32456      0             -148.29617      7448.6284    \n",
            "      1214   79.121082     -148.31616      0             -148.27525      6980.1053    \n",
            "      1215   115.92029     -148.3355       0             -148.27557      6389.2425    \n",
            "      1216   161.36666     -148.32028      0             -148.23685      5687.743     \n",
            "      1217   210.16546     -148.36202      0             -148.25336      4852.3346    \n",
            "      1218   256.68684     -148.3832       0             -148.25048      4037.112     \n",
            "      1219   296.14115     -148.39684      0             -148.24372      3148.6302    \n",
            "      1220   325.36872     -148.42991      0             -148.26168      2224.7071    \n",
            "      1221   342.9402      -148.43062      0             -148.25331      1330.1433    \n",
            "      1222   349.72567     -148.4353       0             -148.25448      480.02812    \n",
            "      1223   348.26765     -148.42142      0             -148.24135     -256.58859    \n",
            "      1224   341.38689     -148.41849      0             -148.24198     -1133.0584    \n",
            "      1225   332.20666     -148.4095       0             -148.23774     -1875.1958    \n",
            "      1226   323.12007     -148.41529      0             -148.24822     -2551.129     \n",
            "      1227   314.73414     -148.41178      0             -148.24905     -3138.7157    \n",
            "      1228   306.27661     -148.39271      0             -148.23436     -3633.3273    \n",
            "      1229   296.13305     -148.37669      0             -148.22357     -3924.8465    \n",
            "      1230   282.05828     -148.38066      0             -148.23482     -4236.7597    \n",
            "      1231   262.62592     -148.36895      0             -148.23317     -4275.9257    \n",
            "      1232   238.25915     -148.35783      0             -148.23464     -4090.8305    \n",
            "      1233   210.62794     -148.32201      0             -148.2131      -3775.6465    \n",
            "      1234   182.87651     -148.34427      0             -148.24972     -3443.7507    \n",
            "      1235   159.34334     -148.31876      0             -148.23638     -2777.1289    \n",
            "      1236   144.26071     -148.26603      0             -148.19144     -1926.1404    \n",
            "      1237   141.00094     -148.29118      0             -148.21828     -1191.3868    \n",
            "      1238   151.19542     -148.27078      0             -148.19261     -322.73864    \n",
            "      1239   173.94842     -148.30757      0             -148.21763      545.77235    \n",
            "      1240   206.19661     -148.31148      0             -148.20487      1562.2793    \n",
            "      1241   243.16118     -148.34588      0             -148.22016      2461.8229    \n",
            "      1242   279.49388     -148.34931      0             -148.2048       3603.2906    \n",
            "      1243   310.39104     -148.3545       0             -148.19402      4625.9478    \n",
            "      1244   332.28582     -148.3902       0             -148.21839      5708.7435    \n",
            "      1245   344.54591     -148.36293      0             -148.18479      6861.3023    \n",
            "      1246   348.72357     -148.38229      0             -148.20199      7914.6798    \n",
            "      1247   347.62812     -148.40869      0             -148.22895      8889.4989    \n",
            "      1248   345.57078     -148.38113      0             -148.20246      9778.3906    \n",
            "      1249   346.32993     -148.34721      0             -148.16814      10490.073    \n",
            "      1250   352.01617     -148.36229      0             -148.18029      11007.666    \n",
            "      1251   362.81362     -148.38564      0             -148.19805      11290.315    \n",
            "      1252   376.75191     -148.38821      0             -148.19342      11542.508    \n",
            "      1253   390.2506      -148.38741      0             -148.18563      11495.457    \n",
            "      1254   399.05752     -148.36522      0             -148.15889      11217.807    \n",
            "      1255   399.79193     -148.37474      0             -148.16803      10913.941    \n",
            "      1256   390.34787     -148.3791       0             -148.17728      10370.501    \n",
            "      1257   370.88823     -148.35285      0             -148.16109      9739.8399    \n",
            "      1258   344.18259     -148.36307      0             -148.18512      8865.1289    \n",
            "      1259   314.5868      -148.33395      0             -148.17129      7941.6253    \n",
            "      1260   287.3121      -148.29556      0             -148.14701      6970.9321    \n",
            "      1261   266.79812     -148.33199      0             -148.19404      5804.7863    \n",
            "      1262   255.72167     -148.27959      0             -148.14737      4565.5596    \n",
            "      1263   254.39461     -148.30471      0             -148.17318      3104.7504    \n",
            "      1264   260.95675     -148.29956      0             -148.16463      1769.3668    \n",
            "      1265   271.47545     -148.25427      0             -148.11391      253.48801    \n",
            "      1266   281.20034     -148.25789      0             -148.1125      -1262.9864    \n",
            "      1267   286.21629     -148.3174       0             -148.16942     -2759.4357    \n",
            "      1268   284.38254     -148.2827       0             -148.13566     -4072.2226    \n",
            "      1269   276.02676     -148.26641      0             -148.12369     -5153.0597    \n",
            "      1270   263.55146     -148.31307      0             -148.17681     -6428.2997    \n",
            "      1271   251.66894     -148.25023      0             -148.1201      -7272.1123    \n",
            "      1272   245.73112     -148.28233      0             -148.15528     -8227.7515    \n",
            "      1273   250.49983     -148.25604      0             -148.12653     -9027.1313    \n",
            "      1274   268.70536     -148.28213      0             -148.1432      -9624.3947    \n",
            "      1275   300.25864     -148.28663      0             -148.13139     -10259.41     \n",
            "      1276   342.16975     -148.31432      0             -148.1374      -10764.087    \n",
            "      1277   388.99087     -148.30956      0             -148.10843     -11088.03     \n",
            "      1278   434.35869     -148.3595       0             -148.13492     -11122.558    \n",
            "      1279   471.68996     -148.36046      0             -148.11658     -11036.858    \n",
            "      1280   495.92388     -148.38253      0             -148.12611     -10968.129    \n",
            "      1281   504.64361     -148.39066      0             -148.12974     -10550.882    \n",
            "      1282   497.94525     -148.34598      0             -148.08852     -9804.9919    \n",
            "      1283   478.65948     -148.37235      0             -148.12486     -9223.7672    \n",
            "      1284   451.27301     -148.34317      0             -148.10985     -8314.4616    \n",
            "      1285   420.57794     -148.34251      0             -148.12505     -7491.8719    \n",
            "      1286   390.44151     -148.29578      0             -148.09391     -6510.1579    \n",
            "      1287   363.10984     -148.28572      0             -148.09798     -5617.0826    \n",
            "      1288   338.74603     -148.25868      0             -148.08353     -4685.4146    \n",
            "      1289   315.63525     -148.26526      0             -148.10206     -3883.1985    \n",
            "      1290   291.48382     -148.22828      0             -148.07757     -2980.9736    \n",
            "      1291   264.25301     -148.23841      0             -148.10178     -2330.5048    \n",
            "      1292   232.83154     -148.1983       0             -148.07791     -1409.1202    \n",
            "      1293   198.317       -148.1881       0             -148.08556     -683.41883    \n",
            "      1294   164.17574     -148.16261      0             -148.07772      10.004128    \n",
            "      1295   135.53067     -148.15175      0             -148.08168      604.40006    \n",
            "      1296   118.49593     -148.13412      0             -148.07285      1025.2797    \n",
            "      1297   118.69909     -148.15802      0             -148.09664      1342.3196    \n",
            "      1298   140.19306     -148.15344      0             -148.08095      1347.0955    \n",
            "      1299   184.25743     -148.19151      0             -148.09625      1330.996     \n",
            "      1300   248.54921     -148.20035      0             -148.07184      1145.8185    \n",
            "      1301   327.45114     -148.22694      0             -148.05763      655.00155    \n",
            "      1302   412.56609     -148.27015      0             -148.05683     -4.9326501    \n",
            "      1303   494.15582     -148.31919      0             -148.06369     -599.67159    \n",
            "      1304   563.02936     -148.35076      0             -148.05965     -1332.8757    \n",
            "      1305   612.03861     -148.35355      0             -148.0371      -2225.1095    \n",
            "      1306   637.64062     -148.41515      0             -148.08546     -3021.4407    \n",
            "      1307   639.95485     -148.37119      0             -148.04031     -3893.3333    \n",
            "      1308   622.61903     -148.36394      0             -148.04202     -4684.1839    \n",
            "      1309   591.29286     -148.3633       0             -148.05757     -5657.6309    \n",
            "      1310   552.44287     -148.31619      0             -148.03055     -6398.7768    \n",
            "      1311   511.73658     -148.28683      0             -148.02224     -7192.2413    \n",
            "      1312   472.59249     -148.27536      0             -148.03101     -8166.2472    \n",
            "      1313   435.57234     -148.2559       0             -148.03069     -8909.0024    \n",
            "      1314   399.02053     -148.25482      0             -148.04851     -9750.3353    \n",
            "      1315   359.89175     -148.23237      0             -148.04629     -10418.285    \n",
            "      1316   315.4149      -148.18001      0             -148.01692     -10873.632    \n",
            "      1317   264.94422     -148.14588      0             -148.0089      -11190.304    \n",
            "      1318   210.40089     -148.13521      0             -148.02642     -11336.242    \n",
            "      1319   156.76561     -148.09682      0             -148.01577     -11355.774    \n",
            "      1320   111.53363     -148.05975      0             -148.00208     -11280.783    \n",
            "      1321   83.12188      -148.06272      0             -148.01974     -10974.492    \n",
            "      1322   78.748356     -148.05179      0             -148.01107     -10590.756    \n",
            "      1323   102.75009     -148.05543      0             -148.00231     -10125.16     \n",
            "      1324   155.38346     -148.08946      0             -148.00912     -9705.4576    \n",
            "      1325   232.47934     -148.12791      0             -148.00771     -9133.0153    \n",
            "      1326   325.76653     -148.18972      0             -148.02129     -8486.4406    \n",
            "      1327   424.62314     -148.20896      0             -147.98941     -7557.7693    \n",
            "      1328   517.77526     -148.24686      0             -147.97914     -6737.8963    \n",
            "      1329   595.69913     -148.27358      0             -147.96558     -5568.5466    \n",
            "      1330   652.71377     -148.35402      0             -148.01654     -4477.2468    \n",
            "      1331   686.99817     -148.33639      0             -147.98118     -3114.3959    \n",
            "      1332   700.57442     -148.38046      0             -148.01823     -1837.2886    \n",
            "      1333   698.86762     -148.38387      0             -148.02253     -282.81576    \n",
            "      1334   687.76436     -148.32118      0             -147.96558      1099.4549    \n",
            "      1335   672.2966      -148.35067      0             -148.00307      2287.0838    \n",
            "      1336   655.26491     -148.3045       0             -147.96571      3432.1389    \n",
            "      1337   636.31126     -148.28315      0             -147.95415      4398.7878    \n",
            "      1338   612.5971      -148.30563      0             -147.98889      5073.8597    \n",
            "      1339   580.47592     -148.25814      0             -147.95801      5844.636     \n",
            "      1340   536.96618     -148.22183      0             -147.94419      6395.9297    \n",
            "      1341   480.96747     -148.1983       0             -147.94962      6686.6007    \n",
            "      1342   414.95888     -148.17935      0             -147.9648       7017.2816    \n",
            "      1343   344.95125     -148.12708      0             -147.94873      7093.5163    \n",
            "      1344   279.59227     -148.09605      0             -147.95149      7036.7858    \n",
            "      1345   228.53453     -148.06665      0             -147.94849      6590.1143    \n",
            "      1346   200.41982     -148.04646      0             -147.94284      6052.7727    \n",
            "      1347   201.32035     -148.06059      0             -147.9565       5304.5658    \n",
            "      1348   233.00661     -148.10917      0             -147.9887       4242.7042    \n",
            "      1349   291.91314     -148.08292      0             -147.93199      3070.5721    \n",
            "      1350   370.37672     -148.17796      0             -147.98646      1679.0561    \n",
            "      1351   457.7949      -148.16999      0             -147.93329      174.93508    \n",
            "      1352   541.98151     -148.22453      0             -147.94431     -1457.4064    \n",
            "      1353   612.56725     -148.23594      0             -147.91921     -3105.324     \n",
            "      1354   662.45315     -148.297        0             -147.95449     -4815.3667    \n",
            "      1355   688.50834     -148.27896      0             -147.92298     -6432.31      \n",
            "      1356   692.67032     -148.29738      0             -147.93924     -7905.22      \n",
            "      1357   681.08401     -148.2794       0             -147.92725     -9435.1221    \n",
            "      1358   661.59868     -148.26661      0             -147.92453     -10751.168    \n",
            "      1359   641.75214     -148.25195      0             -147.92013     -12057.863    \n",
            "      1360   626.9806      -148.23073      0             -147.90655     -13304.44     \n",
            "      1361   619.57311     -148.22547      0             -147.90512     -14389.642    \n",
            "      1362   618.15885     -148.22349      0             -147.90388     -15470.732    \n",
            "      1363   618.36864     -148.23021      0             -147.91049     -16318.344    \n",
            "      1364   614.78184     -148.22586      0             -147.90799     -16729.154    \n",
            "      1365   602.94808     -148.21015      0             -147.8984      -17151.554    \n",
            "      1366   580.0601      -148.21199      0             -147.91207     -17080.77     \n",
            "      1367   547.02316     -148.17824      0             -147.8954      -16683.043    \n",
            "      1368   507.7762      -148.14028      0             -147.87774     -15932.065    \n",
            "      1369   469.02238     -148.1388       0             -147.89629     -15067.197    \n",
            "      1370   438.22282     -148.13102      0             -147.90444     -13877.071    \n",
            "      1371   421.78351     -148.10272      0             -147.88464     -12470.825    \n",
            "      1372   423.66199     -148.11711      0             -147.89806     -10958.041    \n",
            "      1373   443.74291     -148.12548      0             -147.89605     -9332.431     \n",
            "      1374   477.81103     -148.14598      0             -147.89893     -7535.5746    \n",
            "      1375   518.33908     -148.13507      0             -147.86707     -5646.5913    \n",
            "      1376   556.43548     -148.18351      0             -147.89581     -3562.8632    \n",
            "      1377   583.06582     -148.19605      0             -147.89459     -1473.2258    \n",
            "      1378   592.03537     -148.21056      0             -147.90445      834.07583    \n",
            "      1379   581.36007     -148.17431      0             -147.87373      3039.3383    \n",
            "      1380   553.34999     -148.15177      0             -147.86566      5465.1757    \n",
            "      1381   514.81717     -148.1484       0             -147.88222      7778.4304    \n",
            "      1382   475.64725     -148.13395      0             -147.88802      9869.3134    \n",
            "      1383   446.34047     -148.1232       0             -147.89242      11906.584    \n",
            "      1384   435.61087     -148.13285      0             -147.90762      13507.206    \n",
            "      1385   448.25787     -148.13823      0             -147.90646      15009.854    \n",
            "      1386   484.44444     -148.15168      0             -147.9012       15912.59     \n",
            "      1387   538.95719     -148.15867      0             -147.88001      16634.77     \n",
            "      1388   602.92349     -148.20815      0             -147.89641      17075.901    \n",
            "      1389   665.38956     -148.20154      0             -147.8575       17249.135    \n",
            "      1390   716.162       -148.26708      0             -147.8968       17029.794    \n",
            "      1391   748.2771      -148.2837       0             -147.89681      16595.243    \n",
            "      1392   758.95989     -148.24152      0             -147.84911      16016.323    \n",
            "      1393   750.27968     -148.23958      0             -147.85165      15136.422    \n",
            "      1394   727.74936     -148.25944      0             -147.88317      14035.019    \n",
            "      1395   698.93363     -148.24286      0             -147.88148      12804.547    \n",
            "      1396   670.56485     -148.22714      0             -147.88043      11243.015    \n",
            "      1397   646.67163     -148.19503      0             -147.86067      9665.036     \n",
            "      1398   627.6066      -148.19349      0             -147.86899      7820.2259    \n",
            "      1399   609.73895     -148.20412      0             -147.88886      5938.3914    \n",
            "      1400   587.10546     -148.17492      0             -147.87136      4140.1353    \n",
            "      1401   553.35199     -148.17747      0             -147.89136      2312.6786    \n",
            "      1402   504.44163     -148.14651      0             -147.88569      666.97827    \n",
            "      1403   440.90755     -148.07746      0             -147.84949     -628.56041    \n",
            "      1404   367.97223     -148.06409      0             -147.87384     -1958.5662    \n",
            "      1405   295.49887     -148.03015      0             -147.87737     -2805.9122    \n",
            "      1406   235.97711     -147.98919      0             -147.86718     -3632.9423    \n",
            "      1407   201.62044     -147.96954      0             -147.86529     -4244.7993    \n",
            "      1408   202.12461     -147.99665      0             -147.89215     -4619.6887    \n",
            "      1409   241.95862     -148.02537      0             -147.90027     -4741.0423    \n",
            "      1410   319.19247     -148.02624      0             -147.8612      -4838.5085    \n",
            "      1411   425.79974     -148.09304      0             -147.87288     -4636.4772    \n",
            "      1412   548.67016     -148.16578      0             -147.88209     -4173.5789    \n",
            "      1413   672.50258     -148.21551      0             -147.8678      -3482.6423    \n",
            "      1414   782.84727     -148.25981      0             -147.85504     -2528.0512    \n",
            "      1415   868.22632     -148.3103       0             -147.8614      -1063.6281    \n",
            "      1416   922.25625     -148.33567      0             -147.85883      481.8825     \n",
            "      1417   944.1201      -148.36823      0             -147.88008      2391.8724    \n",
            "      1418   937.74324     -148.3785       0             -147.89365      4381.8703    \n",
            "      1419   909.55832     -148.35876      0             -147.88848      6463.4547    \n",
            "      1420   866.77219     -148.34463      0             -147.89648      8504.7835    \n",
            "      1421   815.07053     -148.3007       0             -147.87928      10505.06     \n",
            "      1422   757.39975     -148.2881       0             -147.8965       12332.758    \n",
            "      1423   693.99927     -148.24348      0             -147.88466      13977.65     \n",
            "      1424   623.26776     -148.20695      0             -147.8847       15466.495    \n",
            "      1425   544.25129     -148.17698      0             -147.89558      16702.128    \n",
            "      1426   457.82094     -148.12252      0             -147.88581      17645.531    \n",
            "      1427   368.07896     -148.06923      0             -147.87892      18496.205    \n",
            "      1428   282.32707     -148.03582      0             -147.88984      18979.341    \n",
            "      1429   210.62628     -147.97832      0             -147.86942      19333.82     \n",
            "      1430   163.70585     -147.98185      0             -147.89721      19251.913    \n",
            "      1431   150.91736     -147.97359      0             -147.89556      18850.88     \n",
            "      1432   178.46688     -147.97113      0             -147.87886      18088.886    \n",
            "      1433   247.28394     -148.02574      0             -147.89788      17054.551    \n",
            "      1434   352.01975     -148.09626      0             -147.91425      15409.209    \n",
            "      1435   481.68012     -148.14393      0             -147.89488      13551.881    \n",
            "      1436   621.32964     -148.20644      0             -147.88519      11405.458    \n",
            "      1437   754.07237     -148.32251      0             -147.93262      9052.6608    \n",
            "      1438   864.07816     -148.33906      0             -147.8923       6613.4834    \n",
            "      1439   939.40962     -148.38623      0             -147.90051      4127.9323    \n",
            "      1440   973.30816     -148.41908      0             -147.91584      1540.8013    \n",
            "      1441   965.52002     -148.42926      0             -147.93004     -847.92792    \n",
            "      1442   922.31013     -148.38642      0             -147.90954     -3057.5355    \n",
            "      1443   853.8516      -148.37215      0             -147.93067     -5229.0103    \n",
            "      1444   771.78911     -148.32776      0             -147.92871     -7342.7375    \n",
            "      1445   686.65866     -148.26745      0             -147.91242     -9221.3874    \n",
            "      1446   606.50879     -148.24971      0             -147.93612     -10912.334    \n",
            "      1447   535.82629     -148.22519      0             -147.94814     -12461.341    \n",
            "      1448   475.57628     -148.20974      0             -147.96385     -13707.784    \n",
            "      1449   424.48956     -148.14423      0             -147.92475     -14575.91     \n",
            "      1450   380.34748     -148.12489      0             -147.92824     -15186.104    \n",
            "      1451   341.59743     -148.13552      0             -147.9589      -15525.096    \n",
            "      1452   308.31277     -148.08435      0             -147.92494     -15323.638    \n",
            "      1453   282.81125     -148.08699      0             -147.94076     -14893.463    \n",
            "      1454   269.17743     -148.08631      0             -147.94714     -13920.03     \n",
            "      1455   272.46474     -148.09843      0             -147.95755     -12827.389    \n",
            "      1456   296.72958     -148.09412      0             -147.9407      -11507.353    \n",
            "      1457   343.66885     -148.13297      0             -147.95528     -10086.967    \n",
            "      1458   411.47971     -148.16792      0             -147.95517     -8325.3513    \n",
            "      1459   494.34882     -148.24296      0             -147.98736     -6596.3417    \n",
            "      1460   583.05643     -148.25249      0             -147.95103     -4656.7674    \n",
            "      1461   665.91461     -148.28412      0             -147.93982     -2745.1303    \n",
            "      1462   731.50932     -148.33717      0             -147.95895     -588.0211     \n",
            "      1463   770.47584     -148.37289      0             -147.97453      1556.3028    \n",
            "      1464   777.94513     -148.33178      0             -147.92955      3769.5955    \n",
            "      1465   754.30174     -148.34051      0             -147.95051      5979.1368    \n",
            "      1466   704.91312     -148.35813      0             -147.99366      8093.4598    \n",
            "      1467   639.6174      -148.3038       0             -147.9731       10136.88     \n",
            "      1468   569.67594     -148.28899      0             -147.99444      11898.93     \n",
            "      1469   506.12196     -148.2178       0             -147.95611      13413.249    \n",
            "      1470   457.19263     -148.22784      0             -147.99145      14504.06     \n",
            "      1471   426.8973      -148.21642      0             -147.9957       15199.702    \n",
            "      1472   414.83387     -148.22124      0             -148.00675      15460.374    \n",
            "      1473   416.86793     -148.20946      0             -147.99392      15589.6      \n",
            "      1474   426.56589     -148.21031      0             -147.98976      15401.201    \n",
            "      1475   437.25127     -148.22457      0             -147.9985       14752.481    \n",
            "      1476   443.87483     -148.25242      0             -148.02292      14070.745    \n",
            "      1477   444.26361     -148.24369      0             -148.01399      13055.544    \n",
            "      1478   439.35607     -148.23281      0             -148.00565      11982.362    \n",
            "      1479   432.41571     -148.2416       0             -148.01803      10492.079    \n",
            "      1480   428.27656     -148.2483       0             -148.02687      9002.4682    \n",
            "      1481   430.873       -148.2256       0             -148.00282      7296.9076    \n",
            "      1482   441.82696     -148.22921      0             -148.00077      5508.4771    \n",
            "      1483   459.79513     -148.26337      0             -148.02564      3444.8803    \n",
            "      1484   480.37373     -148.25868      0             -148.01031      1315.8686    \n",
            "      1485   497.52947     -148.29383      0             -148.03659     -675.90664    \n",
            "      1486   504.53533     -148.26703      0             -148.00617     -2603.3064    \n",
            "      1487   496.01751     -148.30153      0             -148.04507     -4581.9888    \n",
            "      1488   470.13731     -148.29205      0             -148.04897     -6254.9296    \n",
            "      1489   429.51951     -148.25049      0             -148.02841     -7788.5212    \n",
            "      1490   380.13635     -148.26362      0             -148.06707     -9019.5976    \n",
            "      1491   330.6405      -148.21557      0             -148.04461     -10127.215    \n",
            "      1492   290.78058     -148.19754      0             -148.04719     -11038.753    \n",
            "      1493   268.39003     -148.19071      0             -148.05194     -11866.658    \n",
            "      1494   268.1824      -148.188        0             -148.04934     -12364.475    \n",
            "      1495   290.63803     -148.20141      0             -148.05113     -12955.602    \n",
            "      1496   331.89116     -148.23196      0             -148.06035     -13092.13     \n",
            "      1497   384.75983     -148.26604      0             -148.0671      -13185.702    \n",
            "      1498   440.16776     -148.2866       0             -148.05902     -13011.004    \n",
            "      1499   489.16509     -148.29265      0             -148.03973     -12504.524    \n",
            "      1500   525.07701     -148.33402      0             -148.06253     -11953.568    \n",
            "      1501   544.03358     -148.34132      0             -148.06003     -11027.8      \n",
            "      1502   545.78043     -148.3757       0             -148.09351     -10075.511    \n",
            "      1503   532.98157     -148.37181      0             -148.09623     -8827.5459    \n",
            "      1504   510.12898     -148.38025      0             -148.1165      -7560.5699    \n",
            "      1505   481.95931     -148.34133      0             -148.09214     -6137.6952    \n",
            "      1506   452.11147     -148.34526      0             -148.1115      -4836.4249    \n",
            "      1507   422.43275     -148.32409      0             -148.10568     -3505.8956    \n",
            "      1508   392.9797      -148.28989      0             -148.0867      -2232.3308    \n",
            "      1509   362.39301     -148.29924      0             -148.11187     -1044.7139    \n",
            "      1510   328.83913     -148.29197      0             -148.12195     -26.665457    \n",
            "      1511   290.98404     -148.24485      0             -148.0944       1020.0808    \n",
            "      1512   249.39186     -148.23502      0             -148.10608      1827.506     \n",
            "      1513   207.02065     -148.21875      0             -148.11171      2619.7431    \n",
            "      1514   168.22893     -148.23616      0             -148.14918      3147.5957    \n",
            "      1515   138.72141     -148.16211      0             -148.09039      3679.8626    \n",
            "      1516   124.30238     -148.15532      0             -148.09105      3894.6182    \n",
            "      1517   129.61668     -148.15049      0             -148.08347      3803.0628    \n",
            "      1518   156.8181      -148.20897      0             -148.12789      3350.1176    \n",
            "      1519   204.96526     -148.23844      0             -148.13246      2932.653     \n",
            "      1520   269.75613     -148.23606      0             -148.09659      2151.9241    \n",
            "      1521   343.77865     -148.31051      0             -148.13276      1058.2701    \n",
            "      1522   418.07646     -148.32145      0             -148.10529     -108.0015     \n",
            "      1523   483.2841      -148.38052      0             -148.13064     -1421.2016    \n",
            "      1524   531.3283      -148.41142      0             -148.1367      -2776.5907    \n",
            "      1525   556.96296     -148.42391      0             -148.13594     -4270.0433    \n",
            "      1526   558.10771     -148.44536      0             -148.1568      -5752.9197    \n",
            "      1527   536.41288     -148.43707      0             -148.15972     -7047.0112    \n",
            "      1528   496.6379      -148.40626      0             -148.14948     -8416.7592    \n",
            "      1529   445.26294     -148.40647      0             -148.17625     -9644.1472    \n",
            "      1530   389.2857      -148.38685      0             -148.18557     -10773.024    \n",
            "      1531   334.6486      -148.36475      0             -148.19172     -11946.076    \n",
            "      1532   285.29353     -148.29657      0             -148.14906     -12869.102    \n",
            "      1533   243.05131     -148.30114      0             -148.17547     -13749.764    \n",
            "      1534   207.66708     -148.29823      0             -148.19086     -14506.309    \n",
            "      1535   178.06315     -148.24663      0             -148.15456     -14926.848    \n",
            "      1536   153.03303     -148.25207      0             -148.17295     -15205.315    \n",
            "      1537   131.83999     -148.22552      0             -148.15735     -15235.189    \n",
            "      1538   115.4622      -148.26048      0             -148.20078     -15116.67     \n",
            "      1539   105.8791      -148.23021      0             -148.17546     -14565.319    \n",
            "      1540   106.47018     -148.24009      0             -148.18504     -13878.587    \n",
            "      1541   120.42489     -148.22482      0             -148.16256     -12889.57     \n",
            "      1542   149.64973     -148.25332      0             -148.17595     -11937.961    \n",
            "      1543   194.21481     -148.26813      0             -148.16771     -10774.734    \n",
            "      1544   251.25453     -148.31223      0             -148.18232     -9354.6617    \n",
            "      1545   315.36041     -148.31385      0             -148.15079     -7944.3319    \n",
            "      1546   379.36138     -148.3492       0             -148.15306     -6328.6511    \n",
            "      1547   434.97351     -148.41392      0             -148.18902     -4528.7266    \n",
            "      1548   474.92831     -148.4224       0             -148.17685     -2808.5613    \n",
            "      1549   494.17813     -148.44399      0             -148.18848     -902.10307    \n",
            "      1550   490.70702     -148.44405      0             -148.19034      1020.7679    \n",
            "      1551   466.19668     -148.45335      0             -148.21231      2928.5191    \n",
            "      1552   425.15109     -148.44485      0             -148.22503      4754.2791    \n",
            "      1553   374.05852     -148.42029      0             -148.22689      6566.5909    \n",
            "      1554   320.15369     -148.39209      0             -148.22655      8184.8207    \n",
            "      1555   269.95749     -148.34203      0             -148.20245      9691.9726    \n",
            "      1556   228.01236     -148.35004      0             -148.23215      10703.849    \n",
            "      1557   196.46176     -148.30962      0             -148.20804      11656.022    \n",
            "      1558   175.10159     -148.30358      0             -148.21305      12394.499    \n",
            "      1559   162.36515     -148.28896      0             -148.20501      12819.386    \n",
            "      1560   155.90181     -148.30115      0             -148.22054      13066.811    \n",
            "      1561   153.64204     -148.30016      0             -148.22072      12948.69     \n",
            "      1562   154.47524     -148.30239      0             -148.22252      12589.159    \n",
            "      1563   158.56999     -148.28731      0             -148.20532      12196.747    \n",
            "      1564   167.19799     -148.30904      0             -148.22259      11448.377    \n",
            "      1565   181.96691     -148.32138      0             -148.2273       10530.274    \n",
            "      1566   203.91785     -148.35335      0             -148.24791      9320.4685    \n",
            "      1567   232.91383     -148.35029      0             -148.22986      8093.5123    \n",
            "      1568   267.04276     -148.37322      0             -148.23515      6448.0588    \n",
            "      1569   302.74849     -148.39987      0             -148.24334      4988.7808    \n",
            "      1570   335.15204     -148.3995       0             -148.22622      3354.0015    \n",
            "      1571   358.89666     -148.40733      0             -148.22176      1633.441     \n",
            "      1572   369.72012     -148.45205      0             -148.26089     -55.06235     \n",
            "      1573   365.27829     -148.43848      0             -148.24962     -1678.2178    \n",
            "      1574   345.69868     -148.40225      0             -148.22351     -3016.6344    \n",
            "      1575   314.00541     -148.41813      0             -148.25577     -4481.2206    \n",
            "      1576   275.21689     -148.39654      0             -148.25424     -5601.5082    \n",
            "      1577   235.37447     -148.35758      0             -148.23589     -6657.8835    \n",
            "      1578   200.6172      -148.35488      0             -148.25115     -7382.0164    \n",
            "      1579   175.77009     -148.32639      0             -148.23551     -7989.6245    \n",
            "      1580   163.49506     -148.29615      0             -148.21161     -8560.2727    \n",
            "      1581   163.98105     -148.32261      0             -148.23782     -8804.7199    \n",
            "      1582   174.96768     -148.33724      0             -148.24677     -8923.6326    \n",
            "      1583   192.74573     -148.35925      0             -148.2596      -8825.0311    \n",
            "      1584   212.97535     -148.35217      0             -148.24205     -8330.6635    \n",
            "      1585   231.55779     -148.38167      0             -148.26195     -7658.8549    \n",
            "      1586   245.60285     -148.37347      0             -148.24648     -6894.771     \n",
            "      1587   253.74497     -148.37724      0             -148.24605     -5820.1498    \n",
            "      1588   256.21348     -148.41517      0             -148.2827      -4723.5513    \n",
            "      1589   254.4037      -148.37819      0             -148.24665     -3358.9195    \n",
            "      1590   250.11961     -148.40229      0             -148.27297     -1950.2759    \n",
            "      1591   244.98834     -148.42645      0             -148.29978     -442.96145    \n",
            "      1592   239.9536      -148.38955      0             -148.26548      1213.4007    \n",
            "      1593   234.71281     -148.37099      0             -148.24963      2643.5288    \n",
            "      1594   228.36319     -148.39746      0             -148.27939      4069.1058    \n",
            "      1595   219.64907     -148.38824      0             -148.27467      5450.7926    \n",
            "      1596   207.1012      -148.37171      0             -148.26463      6694.1614    \n",
            "      1597   190.17002     -148.33473      0             -148.2364       7944.6315    \n",
            "      1598   169.73604     -148.37243      0             -148.28467      8994.6402    \n",
            "      1599   148.02579     -148.35983      0             -148.28329      9890.1598    \n",
            "      1600   128.29927     -148.36284      0             -148.2965       10477.786    \n",
            "      1601   114.45294     -148.35133      0             -148.29216      10905.892    \n",
            "      1602   109.82434     -148.34625      0             -148.28947      11240.343    \n",
            "      1603   116.59218     -148.33187      0             -148.27159      11155.065    \n",
            "      1604   135.28402     -148.32817      0             -148.25823      10987.932    \n",
            "      1605   164.24005     -148.35238      0             -148.26746      10424.117    \n",
            "      1606   199.82576     -148.37644      0             -148.27313      9537.3896    \n",
            "      1607   236.95762     -148.40557      0             -148.28305      8704.9826    \n",
            "      1608   270.1508      -148.43225      0             -148.29257      7656.2269    \n",
            "      1609   294.38389     -148.43167      0             -148.27947      6504.8901    \n",
            "      1610   305.96529     -148.42996      0             -148.27177      5226.2093    \n",
            "      1611   303.53188     -148.44802      0             -148.29108      3885.3109    \n",
            "      1612   287.7249      -148.43587      0             -148.28711      2617.3526    \n",
            "      1613   261.08943     -148.41157      0             -148.27657      1350.509     \n",
            "      1614   227.8259      -148.43858      0             -148.32079      64.636285    \n",
            "      1615   192.45278     -148.38198      0             -148.28247     -977.90508    \n",
            "      1616   158.87737     -148.37113      0             -148.28898     -2132.4992    \n",
            "      1617   130.01393     -148.35723      0             -148.29001     -3107.4355    \n",
            "      1618   107.307       -148.34773      0             -148.29225     -3977.0798    \n",
            "      1619   90.964266     -148.35525      0             -148.30822     -4642.7648    \n",
            "      1620   80.379531     -148.3411       0             -148.29954     -5156.6209    \n",
            "      1621   74.352997     -148.33399      0             -148.29555     -5325.4065    \n",
            "      1622   72.032166     -148.32717      0             -148.28992     -5559.6969    \n",
            "      1623   73.397434     -148.36813      0             -148.33018     -5389.6741    \n",
            "      1624   78.83036      -148.33499      0             -148.29423     -4959.5078    \n",
            "      1625   89.438803     -148.33607      0             -148.28983     -4429.0951    \n",
            "      1626   106.58356     -148.33171      0             -148.2766      -3757.4846    \n",
            "      1627   130.83707     -148.35573      0             -148.28808     -2787.7719    \n",
            "      1628   161.62068     -148.36292      0             -148.27935     -1636.24      \n",
            "      1629   197.02197     -148.42196      0             -148.32009     -452.42589    \n",
            "      1630   233.52823     -148.40199      0             -148.28124      709.6589     \n",
            "      1631   266.58958     -148.44838      0             -148.31054      2141.0697    \n",
            "      1632   291.45061     -148.4404       0             -148.28971      3482.5829    \n",
            "      1633   304.1316      -148.45063      0             -148.29338      4946.3974    \n",
            "      1634   302.22687     -148.46789      0             -148.31163      6403.2781    \n",
            "      1635   285.26855     -148.45796      0             -148.31047      7890.0387    \n",
            "      1636   255.20796     -148.42497      0             -148.29302      9238.2701    \n",
            "      1637   216.08742     -148.41206      0             -148.30033      10564.547    \n",
            "      1638   173.13722     -148.43771      0             -148.34819      11657.3      \n",
            "      1639   131.74102     -148.38626      0             -148.31815      12647.047    \n",
            "      1640   96.585042     -148.38005      0             -148.33011      13354.645    \n",
            "      1641   70.885177     -148.38354      0             -148.34689      13897.93     \n",
            "      1642   56.140626     -148.34864      0             -148.31961      14129.929    \n",
            "      1643   52.065358     -148.33399      0             -148.30707      14220.328    \n",
            "      1644   56.836333     -148.33201      0             -148.30263      14025.136    \n",
            "      1645   68.005174     -148.34133      0             -148.30617      13764.713    \n",
            "      1646   83.270231     -148.34985      0             -148.3068       13079.788    \n",
            "      1647   100.793       -148.36576      0             -148.31364      12195.918    \n",
            "      1648   119.53108     -148.37375      0             -148.31195      11186.214    \n",
            "      1649   139.01824     -148.39356      0             -148.32168      10114.255    \n",
            "      1650   159.39533     -148.38418      0             -148.30177      8873.9724    \n",
            "      1651   180.93056     -148.41321      0             -148.31966      7290.1877    \n",
            "      1652   203.32999     -148.416        0             -148.31087      5918.4412    \n",
            "      1653   225.47757     -148.41702      0             -148.30044      4183.0859    \n",
            "      1654   245.2513      -148.42198      0             -148.29518      2570.8248    \n",
            "      1655   259.96075     -148.46219      0             -148.32778      829.19535    \n",
            "      1656   266.98979     -148.46378      0             -148.32574     -710.89323    \n",
            "      1657   264.44955     -148.43934      0             -148.30261     -2248.4251    \n",
            "      1658   251.51362     -148.42972      0             -148.29968     -3689.0987    \n",
            "      1659   229.03091     -148.43727      0             -148.31885     -5094.202     \n",
            "      1660   199.47655     -148.40521      0             -148.30207     -6211.5862    \n",
            "      1661   166.49717     -148.41669      0             -148.3306      -7316.6917    \n",
            "      1662   134.65412     -148.40718      0             -148.33756     -8099.7397    \n",
            "      1663   108.20661     -148.37077      0             -148.31482     -8700.5649    \n",
            "      1664   90.240952     -148.38248      0             -148.33582     -9267.4639    \n",
            "      1665   82.522815     -148.37127      0             -148.3286      -9490.6305    \n",
            "      1666   85.053293     -148.37345      0             -148.32947     -9684.416     \n",
            "      1667   96.216012     -148.38633      0             -148.33658     -9711.4054    \n",
            "      1668   113.30547     -148.40531      0             -148.34673     -9493.0049    \n",
            "      1669   132.73883     -148.39483      0             -148.3262      -9065.3585    \n",
            "      1670   151.23984     -148.41001      0             -148.33182     -8518.1743    \n",
            "      1671   166.61057     -148.4127       0             -148.32656     -7759.8298    \n",
            "      1672   177.51411     -148.42456      0             -148.33277     -6990.3362    \n",
            "      1673   183.75909     -148.41963      0             -148.32462     -6030.2875    \n",
            "      1674   186.29514     -148.41103      0             -148.31471     -5026.0592    \n",
            "      1675   186.52592     -148.4468       0             -148.35036     -3963.1015    \n",
            "      1676   185.70938     -148.44304      0             -148.34703     -2881.8627    \n",
            "      1677   184.63923     -148.45712      0             -148.36166     -1725.8784    \n",
            "      1678   183.54964     -148.42348      0             -148.32857     -634.08371    \n",
            "      1679   182.00123     -148.4461       0             -148.352        295.26108    \n",
            "      1680   178.90593     -148.40634      0             -148.31384      1404.2693    \n",
            "      1681   173.19485     -148.40598      0             -148.31643      2159.7037    \n",
            "      1682   164.21917     -148.40056      0             -148.31566      2908.0514    \n",
            "      1683   152.20433     -148.40806      0             -148.32936      3487.1462    \n",
            "      1684   138.31457     -148.4076       0             -148.33608      4016.1982    \n",
            "      1685   124.36322     -148.40609      0             -148.34179      4375.2807    \n",
            "      1686   112.87147     -148.39458      0             -148.33622      4471.7702    \n",
            "      1687   106.48641     -148.40239      0             -148.34733      4494.3604    \n",
            "      1688   106.99213     -148.4139       0             -148.35858      4360.4045    \n",
            "      1689   115.1109      -148.39244      0             -148.33292      4030.2564    \n",
            "      1690   130.10795     -148.43386      0             -148.36659      3495.0474    \n",
            "      1691   149.91186     -148.42222      0             -148.34471      2869.8942    \n",
            "      1692   171.39403     -148.41598      0             -148.32736      2083.8102    \n",
            "      1693   190.81929     -148.41481      0             -148.31615      1294.97      \n",
            "      1694   204.86328     -148.46174      0             -148.35581      302.55744    \n",
            "      1695   210.98088     -148.44946      0             -148.34038     -680.8902     \n",
            "      1696   208.08449     -148.46371      0             -148.35612     -1636.5761    \n",
            "      1697   196.74386     -148.44917      0             -148.34745     -2662.3548    \n",
            "      1698   178.89441     -148.43269      0             -148.3402      -3663.662     \n",
            "      1699   157.44071     -148.39025      0             -148.30885     -4492.1553    \n",
            "      1700   135.56228     -148.4163       0             -148.3462      -5248.3614    \n",
            "      1701   116.0399      -148.39349      0             -148.3335      -6104.9646    \n",
            "      1702   100.82002     -148.38935      0             -148.33722     -6865.2811    \n",
            "      1703   90.662607     -148.3697       0             -148.32283     -7436.5511    \n",
            "      1704   85.217906     -148.384        0             -148.33994     -7941.6526    \n",
            "      1705   83.381633     -148.38629      0             -148.34318     -8274.5531    \n",
            "      1706   83.991535     -148.39138      0             -148.34795     -8683.9626    \n",
            "      1707   85.816364     -148.40188      0             -148.35751     -8668.4492    \n",
            "      1708   88.239236     -148.37889      0             -148.33327     -8600.2476    \n",
            "      1709   91.975537     -148.37311      0             -148.32555     -8292.0599    \n",
            "      1710   98.090181     -148.40547      0             -148.35475     -8065.1342    \n",
            "      1711   107.51233     -148.39274      0             -148.33715     -7371.2926    \n",
            "      1712   121.2601      -148.40681      0             -148.34411     -6760.7853    \n",
            "      1713   139.63358     -148.38834      0             -148.31615     -5967.7594    \n",
            "      1714   161.53888     -148.41913      0             -148.33561     -5141.2946    \n",
            "      1715   184.64948     -148.43546      0             -148.33999     -4267.263     \n",
            "      1716   205.78348     -148.45843      0             -148.35203     -3291.2682    \n",
            "      1717   221.47171     -148.45866      0             -148.34415     -2380.6151    \n",
            "      1718   228.63758     -148.48313      0             -148.36491     -1219.7151    \n",
            "      1719   225.28387     -148.45197      0             -148.33549      15.563199    \n",
            "      1720   210.90388     -148.44128      0             -148.33223      1103.3763    \n",
            "      1721   187.12489     -148.43307      0             -148.33632      2133.5603    \n",
            "      1722   156.87085     -148.41945      0             -148.33834      3102.9221    \n",
            "      1723   123.89044     -148.38717      0             -148.32311      4150.7028    \n",
            "      1724   92.401489     -148.41749      0             -148.36971      4920.4195    \n",
            "      1725   66.090059     -148.3744       0             -148.34023      5531.8591    \n",
            "      1726   47.572709     -148.3592       0             -148.3346       6134.0057    \n",
            "      1727   38.057227     -148.34979      0             -148.33011      6536.0856    \n",
            "      1728   37.287767     -148.32871      0             -148.30943      6659.9612    \n",
            "      1729   43.854994     -148.34596      0             -148.32328      6577.4514    \n",
            "      1730   55.847027     -148.37015      0             -148.34128      6419.4151    \n",
            "      1731   71.199779     -148.36022      0             -148.32341      6163.9702    \n",
            "      1732   88.397144     -148.37288      0             -148.32717      5568.3554    \n",
            "      1733   106.50727     -148.38775      0             -148.33268      4937.6778    \n",
            "      1734   125.25546     -148.42534      0             -148.36058      4154.0081    \n",
            "      1735   144.74749     -148.40036      0             -148.32552      3213.3049    \n",
            "      1736   165.12614     -148.40622      0             -148.32085      2170.9859    \n",
            "      1737   186.16936     -148.45419      0             -148.35793      1034.3951    \n",
            "      1738   206.94557     -148.45519      0             -148.34819     -251.28412    \n",
            "      1739   225.73528     -148.44025      0             -148.32353     -1494.7245    \n",
            "      1740   240.0244      -148.4559       0             -148.3318      -2839.8126    \n",
            "      1741   247.25899     -148.44539      0             -148.31754     -4165.2496    \n",
            "      1742   245.55322     -148.44941      0             -148.32245     -5329.2925    \n",
            "      1743   233.79517     -148.4401       0             -148.31922     -6677.8406    \n",
            "      1744   212.38615     -148.4594       0             -148.34959     -7722.6       \n",
            "      1745   183.14941     -148.43226      0             -148.33757     -8698.6965    \n",
            "      1746   149.06826     -148.41915      0             -148.34208     -9575.1281    \n",
            "      1747   114.26003     -148.40259      0             -148.34352     -10245.333    \n",
            "      1748   83.137994     -148.39764      0             -148.35465     -10683.001    \n",
            "      1749   59.364057     -148.36212      0             -148.33143     -11164.854    \n",
            "      1750   45.403905     -148.34217      0             -148.3187      -11360.989    \n",
            "      1751   42.256311     -148.35394      0             -148.3321      -11444.393    \n",
            "      1752   49.386095     -148.35623      0             -148.3307      -11357.432    \n",
            "      1753   64.962139     -148.39139      0             -148.35781     -11176.403    \n",
            "      1754   86.039489     -148.38296      0             -148.33847     -10719.941    \n",
            "      1755   109.37686     -148.38321      0             -148.32666     -10023.902    \n",
            "      1756   132.46192     -148.42149      0             -148.353       -9289.2653    \n",
            "      1757   153.55244     -148.42807      0             -148.34867     -8295.5261    \n",
            "      1758   171.602       -148.44762      0             -148.3589      -7246.085     \n",
            "      1759   186.65616     -148.44833      0             -148.35182     -5993.4533    \n",
            "      1760   199.35143     -148.42262      0             -148.31955     -4770.3179    \n",
            "      1761   210.24016     -148.43927      0             -148.33057     -3200.3146    \n",
            "      1762   219.54548     -148.45593      0             -148.34241     -1879.8759    \n",
            "      1763   227.02986     -148.42769      0             -148.31031     -486.83423    \n",
            "      1764   231.74932     -148.45506      0             -148.33524      857.62827    \n",
            "      1765   232.25344     -148.44998      0             -148.3299       2330.2289    \n",
            "      1766   227.22185     -148.4534       0             -148.33592      3588.1913    \n",
            "      1767   215.73434     -148.42342      0             -148.31187      4685.1072    \n",
            "      1768   197.96541     -148.39016      0             -148.2878       5932.9845    \n",
            "      1769   175.14603     -148.39889      0             -148.30834      6863.1871    \n",
            "      1770   149.69251     -148.40712      0             -148.32973      7795.3362    \n",
            "      1771   125.14721     -148.41916      0             -148.35446      8282.9232    \n",
            "      1772   104.97514     -148.36642      0             -148.31215      8834.7305    \n",
            "      1773   92.328835     -148.39124      0             -148.3435       9136.6098    \n",
            "      1774   89.452271     -148.39963      0             -148.35338      9245.7647    \n",
            "      1775   96.895535     -148.37969      0             -148.32959      9072.0697    \n",
            "      1776   113.38643     -148.38436      0             -148.32574      8735.4157    \n",
            "      1777   136.09532     -148.39401      0             -148.32365      8336.3786    \n",
            "      1778   161.23775     -148.39493      0             -148.31156      7710.605     \n",
            "      1779   184.7618      -148.41204      0             -148.31652      6901.5999    \n",
            "      1780   203.21697     -148.4518       0             -148.34673      6046.9737    \n",
            "      1781   214.21055     -148.4578       0             -148.34705      5041.931     \n",
            "      1782   217.14718     -148.42061      0             -148.30833      4127.5455    \n",
            "      1783   212.9069      -148.39878      0             -148.28869      3103.3633    \n",
            "      1784   203.47561     -148.42729      0             -148.32209      2084.4905    \n",
            "      1785   191.3909      -148.40031      0             -148.30135      1064.2819    \n",
            "      1786   179.16767     -148.41722      0             -148.32458      69.18223     \n",
            "      1787   168.64518     -148.40989      0             -148.32269     -870.3954     \n",
            "      1788   160.54694     -148.39907      0             -148.31606     -1748.9822    \n",
            "      1789   154.52729     -148.38815      0             -148.30826     -2477.7739    \n",
            "      1790   149.55312     -148.38273      0             -148.30541     -3151.8196    \n",
            "      1791   144.53878     -148.41293      0             -148.3382      -3611.3735    \n",
            "      1792   138.76104     -148.38361      0             -148.31187     -3909.6721    \n",
            "      1793   132.11782     -148.37207      0             -148.30376     -4010.0308    \n",
            "      1794   125.63214     -148.38761      0             -148.32265     -3889.7115    \n",
            "      1795   121.24811     -148.37161      0             -148.30892     -3657.7663    \n",
            "      1796   121.03897     -148.37767      0             -148.31509     -3244.2896    \n",
            "      1797   126.87448     -148.38022      0             -148.31462     -2625.0539    \n",
            "      1798   139.72933     -148.40145      0             -148.32921     -1954.4675    \n",
            "      1799   158.82683     -148.38682      0             -148.3047      -1084.1601    \n",
            "      1800   181.91139     -148.41527      0             -148.32122     -201.52915    \n",
            "      1801   205.31789     -148.40428      0             -148.29813      922.25419    \n",
            "      1802   224.79294     -148.42192      0             -148.3057       2112.4085    \n",
            "      1803   236.70669     -148.44403      0             -148.32164      3199.3335    \n",
            "      1804   238.29581     -148.47626      0             -148.35305      4386.2886    \n",
            "      1805   228.15602     -148.41703      0             -148.29906      5741.2135    \n",
            "      1806   207.49917     -148.42532      0             -148.31803      6982.6847    \n",
            "      1807   179.36898     -148.40649      0             -148.31375      8195.453     \n",
            "      1808   147.63019     -148.36318      0             -148.28685      9360.437     \n",
            "      1809   116.87391     -148.39467      0             -148.33424      10305.44     \n",
            "      1810   91.519946     -148.36775      0             -148.32043      11206.764    \n",
            "      1811   74.599152     -148.36361      0             -148.32504      11738.59     \n",
            "      1812   67.397853     -148.32743      0             -148.29258      12272.966    \n",
            "      1813   69.561953     -148.34314      0             -148.30717      12474.384    \n",
            "      1814   79.529292     -148.34706      0             -148.30594      12470.124    \n",
            "      1815   94.972103     -148.35813      0             -148.30902      12305.053    \n",
            "      1816   113.51252     -148.36139      0             -148.3027       11927.197    \n",
            "      1817   133.15718     -148.36975      0             -148.3009       11297.328    \n",
            "      1818   152.8225      -148.37182      0             -148.2928       10620.623    \n",
            "      1819   172.29351     -148.37876      0             -148.28968      9617.6879    \n",
            "      1820   191.79754     -148.39381      0             -148.29464      8423.1652    \n",
            "      1821   211.54656     -148.41397      0             -148.30459      7224.1169    \n",
            "      1822   231.48449     -148.39841      0             -148.27872      5847.6412    \n",
            "      1823   250.57413     -148.41302      0             -148.28346      4183.0433    \n",
            "      1824   266.85893     -148.44106      0             -148.30308      2541.8156    \n",
            "      1825   277.83953     -148.41075      0             -148.26709      901.3514     \n",
            "      1826   280.64727     -148.45488      0             -148.30978     -869.58481    \n",
            "      1827   272.82761     -148.43944      0             -148.29838     -2390.0332    \n",
            "      1828   253.13299     -148.41418      0             -148.2833      -3922.1324    \n",
            "      1829   222.25711     -148.38958      0             -148.27466     -5283.4952    \n",
            "      1830   182.75856     -148.39238      0             -148.29789     -6624.5614    \n",
            "      1831   138.70865     -148.34662      0             -148.27491     -7612.3686    \n",
            "      1832   95.403175     -148.33741      0             -148.28808     -8522.3169    \n",
            "      1833   58.434483     -148.32866      0             -148.29845     -9276.5435    \n",
            "      1834   32.634528     -148.29708      0             -148.28021     -9899.5673    \n",
            "      1835   21.375743     -148.30186      0             -148.29081     -10187.627    \n",
            "      1836   25.968134     -148.27326      0             -148.25984     -10336.573    \n",
            "      1837   45.577154     -148.3015       0             -148.27793     -10301.532    \n",
            "      1838   77.37169      -148.30342      0             -148.26341     -10012.446    \n",
            "      1839   117.23767     -148.33236      0             -148.27174     -9591.9599    \n",
            "      1840   160.50471     -148.36094      0             -148.27795     -9030.0322    \n",
            "      1841   202.77226     -148.37143      0             -148.26659     -8059.9738    \n",
            "      1842   240.72887     -148.40651      0             -148.28205     -6990.7123    \n",
            "      1843   272.43792     -148.43028      0             -148.28942     -5795.8702    \n",
            "      1844   297.38988     -148.40349      0             -148.24973     -4315.5095    \n",
            "      1845   316.05137     -148.44637      0             -148.28296     -2816.0801    \n",
            "      1846   329.15956     -148.44528      0             -148.2751      -1318.5651    \n",
            "      1847   337.47714     -148.4372       0             -148.26271      410.29307    \n",
            "      1848   341.08237     -148.42723      0             -148.25087      2015.9781    \n",
            "      1849   339.15601     -148.46599      0             -148.29064      3383.5198    \n",
            "      1850   330.33322     -148.42909      0             -148.2583       5014.5231    \n",
            "      1851   313.22056     -148.4332       0             -148.27125      6160.4913    \n",
            "      1852   287.09568     -148.40131      0             -148.25287      7509.8481    \n",
            "      1853   252.40364     -148.39722      0             -148.26672      8583.3475    \n",
            "      1854   211.31211     -148.38005      0             -148.2708       9594.5368    \n",
            "      1855   168.00275     -148.33623      0             -148.24937      10445.462    \n",
            "      1856   127.53435     -148.30822      0             -148.24228      11073.346    \n",
            "      1857   95.463581     -148.30441      0             -148.25505      11320.891    \n",
            "      1858   77.02913      -148.27475      0             -148.23492      11542.481    \n",
            "      1859   75.54841      -148.29411      0             -148.25505      11473.244    \n",
            "      1860   91.905787     -148.3109       0             -148.26338      11040.285    \n",
            "      1861   124.28109     -148.31376      0             -148.2495       10360.693    \n",
            "      1862   168.41643     -148.33104      0             -148.24397      9616.75      \n",
            "      1863   218.32443     -148.34894      0             -148.23606      8641.5435    \n",
            "      1864   267.26239     -148.36817      0             -148.22999      7484.3622    \n",
            "      1865   309.29342     -148.37956      0             -148.21964      6263.8022    \n",
            "      1866   340.33972     -148.41473      0             -148.23876      4787.1115    \n",
            "      1867   358.53164     -148.42057      0             -148.23519      3419.2797    \n",
            "      1868   364.47038     -148.43311      0             -148.24466      2044.2253    \n",
            "      1869   360.67003     -148.44285      0             -148.25637      462.26869    \n",
            "      1870   350.45517     -148.42329      0             -148.24209     -981.44687    \n",
            "      1871   337.4228      -148.43828      0             -148.26382     -2369.2082    \n",
            "      1872   324.15884     -148.41829      0             -148.25069     -3672.1837    \n",
            "      1873   311.48784     -148.36686      0             -148.20581     -4995.9652    \n",
            "      1874   298.91402     -148.36977      0             -148.21522     -6068.1275    \n",
            "      1875   284.85235     -148.37723      0             -148.22995     -7157.4827    \n",
            "      1876   267.65073     -148.37725      0             -148.23886     -8048.9461    \n",
            "      1877   246.46532     -148.37335      0             -148.24591     -8772.8259    \n",
            "      1878   221.65168     -148.35344      0             -148.23884     -9234.0101    \n",
            "      1879   195.93188     -148.30111      0             -148.1998      -9523.7446    \n",
            "      1880   173.05758     -148.30853      0             -148.21906     -9654.4472    \n",
            "      1881   157.45841     -148.30886      0             -148.22744     -9579.3891    \n",
            "      1882   153.05671     -148.26947      0             -148.19033     -9335.3062    \n",
            "      1883   162.19007     -148.2943       0             -148.21044     -8875.4855    \n",
            "      1884   184.8651      -148.31425      0             -148.21866     -8464.8038    \n",
            "      1885   218.39878     -148.33826      0             -148.22534     -7813.6843    \n",
            "      1886   257.99576     -148.31719      0             -148.1838      -7113.3095    \n",
            "      1887   297.28561     -148.36386      0             -148.21015     -6260.2679    \n",
            "      1888   329.92659     -148.39377      0             -148.22318     -5258.0382    \n",
            "      1889   350.82541     -148.402        0             -148.22061     -4332.6758    \n",
            "      1890   357.03188     -148.37099      0             -148.18639     -3311.0341    \n",
            "      1891   348.77852     -148.38974      0             -148.20941     -2153.2325    \n",
            "      1892   328.57031     -148.37288      0             -148.20299     -981.55975    \n",
            "      1893   300.8656      -148.35634      0             -148.20078      183.99762    \n",
            "      1894   271.52893     -148.31725      0             -148.17686      1144.1134    \n",
            "      1895   245.56246     -148.31508      0             -148.18812      2048.1707    \n",
            "      1896   226.56097     -148.30535      0             -148.18821      2792.6888    \n",
            "      1897   216.2896      -148.30858      0             -148.19675      3395.5105    \n",
            "      1898   214.28605     -148.29303      0             -148.18224      3587.8313    \n",
            "      1899   218.59028     -148.30265      0             -148.18963      3815.9446    \n",
            "      1900   226.35744     -148.31094      0             -148.1939       3820.4494    \n",
            "      1901   234.66702     -148.30468      0             -148.18335      3582.1297    \n",
            "      1902   241.74235     -148.33495      0             -148.20996      3227.2033    \n",
            "      1903   246.91585     -148.31811      0             -148.19044      2728.5304    \n",
            "      1904   251.16475     -148.29158      0             -148.16172      1974.6654    \n",
            "      1905   256.56343     -148.32798      0             -148.19533      1074.7841    \n",
            "      1906   265.34606     -148.28649      0             -148.1493       191.19155    \n",
            "      1907   279.21039     -148.3115       0             -148.16714     -1158.3907    \n",
            "      1908   298.33053     -148.31734      0             -148.1631      -2468.8478    \n",
            "      1909   321.16967     -148.3153       0             -148.14924     -3891.4283    \n",
            "      1910   344.19845     -148.34014      0             -148.16217     -5488.537     \n",
            "      1911   362.5382      -148.34894      0             -148.16149     -7175.3643    \n",
            "      1912   371.39455     -148.34016      0             -148.14814     -8763.2237    \n",
            "      1913   366.82854     -148.32443      0             -148.13477     -10276.418    \n",
            "      1914   346.68316     -148.32728      0             -148.14803     -11663.176    \n",
            "      1915   311.35955     -148.32424      0             -148.16325     -12953.534    \n",
            "      1916   264.26338     -148.28278      0             -148.14615     -13938.498    \n",
            "      1917   211.58536     -148.25283      0             -148.14344     -15007.043    \n",
            "      1918   161.10003     -148.23314      0             -148.14985     -15708.689    \n",
            "      1919   120.20324     -148.211        0             -148.14885     -16362.759    \n",
            "      1920   95.324399     -148.20027      0             -148.15098     -16759.379    \n",
            "      1921   90.463556     -148.19478      0             -148.148       -17167.874    \n",
            "      1922   106.28559     -148.21966      0             -148.1647      -17245.587    \n",
            "      1923   140.20356     -148.20419      0             -148.1317      -17186.784    \n",
            "      1924   187.34506     -148.25708      0             -148.16021     -16897.661    \n",
            "      1925   241.42776     -148.26432      0             -148.13949     -16306.159    \n",
            "      1926   296.01371     -148.3041       0             -148.15104     -15596.456    \n",
            "      1927   346.42121     -148.3324       0             -148.15329     -14506.667    \n",
            "      1928   389.7018      -148.31253      0             -148.11103     -13207.469    \n",
            "      1929   424.98225     -148.35037      0             -148.13064     -11703.205    \n",
            "      1930   453.29349     -148.36714      0             -148.13277     -9950.9447    \n",
            "      1931   476.48619     -148.3963       0             -148.14994     -8210.5305    \n",
            "      1932   496.30025     -148.40109      0             -148.14448     -6198.4241    \n",
            "      1933   512.88071     -148.41347      0             -148.14829     -4145.8197    \n",
            "      1934   524.30214     -148.3627       0             -148.09161     -2131.0417    \n",
            "      1935   527.20811     -148.40053      0             -148.12795     -192.45429    \n",
            "      1936   517.55607     -148.36282      0             -148.09523      1827.0913    \n",
            "      1937   491.25261     -148.35332      0             -148.09932      3670.8846    \n",
            "      1938   446.4518      -148.3404       0             -148.10957      5604.3322    \n",
            "      1939   384.50406     -148.30268      0             -148.10387      7342.3036    \n",
            "      1940   310.57632     -148.28424      0             -148.12366      9019.0672    \n",
            "      1941   233.07902     -148.21059      0             -148.09008      10581.76     \n",
            "      1942   161.89066     -148.18145      0             -148.09775      11704.012    \n",
            "      1943   107.55854     -148.16995      0             -148.11434      12922.229    \n",
            "      1944   79.057843     -148.12465      0             -148.08377      13765.952    \n",
            "      1945   81.624985     -148.12822      0             -148.08602      14140.454    \n",
            "      1946   115.94305     -148.15826      0             -148.09831      14084.661    \n",
            "      1947   178.15999     -148.17579      0             -148.08367      13813.524    \n",
            "      1948   260.37923     -148.22495      0             -148.09032      13317.798    \n",
            "      1949   352.29472     -148.27648      0             -148.09433      12508.263    \n",
            "      1950   443.20151     -148.30696      0             -148.07781      11514.972    \n",
            "      1951   524.00638     -148.34078      0             -148.06985      10232.784    \n",
            "      1952   588.79286     -148.40484      0             -148.10041      8930.6176    \n",
            "      1953   635.2067      -148.406        0             -148.07757      7507.6128    \n",
            "      1954   664.27689     -148.40897      0             -148.06551      5849.9054    \n",
            "      1955   679.12354     -148.4233       0             -148.07217      4050.0362    \n",
            "      1956   683.78475     -148.41599      0             -148.06245      2397.8005    \n",
            "      1957   681.08786     -148.4153       0             -148.06315      426.6738     \n",
            "      1958   671.59309     -148.41473      0             -148.06749     -1459.6922    \n",
            "      1959   653.98853     -148.37641      0             -148.03827     -3312.945     \n",
            "      1960   625.69233     -148.40095      0             -148.07744     -5106.2973    \n",
            "      1961   583.99216     -148.34542      0             -148.04348     -6682.8492    \n",
            "      1962   527.95788     -148.30948      0             -148.0365      -8106.9982    \n",
            "      1963   459.17349     -148.29344      0             -148.05603     -9209.499     \n",
            "      1964   382.55802     -148.22362      0             -148.02582     -10007.287    \n",
            "      1965   306.43969     -148.19859      0             -148.04015     -10568.813    \n",
            "      1966   240.79258     -148.17525      0             -148.05075     -10815.429    \n",
            "      1967   195.68441     -148.15393      0             -148.05275     -11004.507    \n",
            "      1968   178.58628     -148.10269      0             -148.01036     -10859.456    \n",
            "      1969   193.18314     -148.131        0             -148.03112     -10507.621    \n",
            "      1970   238.36716     -148.15267      0             -148.02942     -10040.818    \n",
            "      1971   308.39694     -148.20299      0             -148.04354     -9338.6491    \n",
            "      1972   393.80849     -148.23936      0             -148.03575     -8576.2821    \n",
            "      1973   482.92461     -148.29786      0             -148.04817     -7311.8117    \n",
            "      1974   564.47148     -148.31987      0             -148.02801     -5917.7648    \n",
            "      1975   629.57423     -148.34484      0             -148.01932     -4298.2116    \n",
            "      1976   672.86253     -148.36342      0             -148.01552     -2457.2277    \n",
            "      1977   693.24588     -148.39093      0             -148.03249     -597.23581    \n",
            "      1978   693.30849     -148.36517      0             -148.0067       1558.4003    \n",
            "      1979   678.42007     -148.36792      0             -148.01715      3638.4697    \n",
            "      1980   654.83813     -148.35861      0             -148.02003      5686.1263    \n",
            "      1981   627.99001     -148.35308      0             -148.02839      7590.1409    \n",
            "      1982   601.48823     -148.31853      0             -148.00753      9273.6093    \n",
            "      1983   576.49107     -148.29676      0             -147.99869      10737.323    \n",
            "      1984   551.66493     -148.28581      0             -148.00058      11962.149    \n",
            "      1985   524.4769      -148.28501      0             -148.01383      12821.009    \n",
            "      1986   492.92832     -148.26016      0             -148.0053       13456.966    \n",
            "      1987   456.16319     -148.21901      0             -147.98315      13996.397    \n",
            "      1988   415.77869     -148.20843      0             -147.99346      14059.644    \n",
            "      1989   375.67894     -148.18994      0             -147.9957       14009.993    \n",
            "      1990   341.67756     -148.16104      0             -147.98437      13559.312    \n",
            "      1991   319.78861     -148.15434      0             -147.989        13079.461    \n",
            "      1992   315.17257     -148.14414      0             -147.98118      11972.644    \n",
            "      1993   330.56428     -148.14032      0             -147.9694       10769.399    \n",
            "      1994   364.8543      -148.21396      0             -148.02531      9218.3124    \n",
            "      1995   413.3212      -148.18811      0             -147.9744       7473.4508    \n",
            "      1996   467.9366      -148.20308      0             -147.96114      5441.4729    \n",
            "      1997   518.65734     -148.23494      0             -147.96677      3470.8093    \n",
            "      1998   555.74964     -148.24981      0             -147.96246      1385.0972    \n",
            "      1999   571.74842     -148.27094      0             -147.97533     -629.80915    \n",
            "      2000   563.04938     -148.26065      0             -147.96953     -2484.4379    \n",
            "      2001   531.11231     -148.23517      0             -147.96056     -4072.1639    \n",
            "      2002   481.94734     -148.19699      0             -147.9478      -5401.3398    \n",
            "      2003   425.19213     -148.18054      0             -147.96069     -6542.4776    \n",
            "      2004   372.03776     -148.18192      0             -147.98956     -7601.0899    \n",
            "      2005   332.93479     -148.12131      0             -147.94917     -8324.4222    \n",
            "      2006   315.50149     -148.12003      0             -147.9569      -8805.6029    \n",
            "      2007   322.91722     -148.16117      0             -147.99421     -9192.9232    \n",
            "      2008   353.51155     -148.12969      0             -147.94691     -9094.0905    \n",
            "      2009   401.02393     -148.18238      0             -147.97504     -8850.365     \n",
            "      2010   456.53842     -148.19265      0             -147.9566      -8072.2442    \n",
            "      2011   510.59794     -148.23291      0             -147.96891     -7071.8893    \n",
            "      2012   555.15929     -148.23154      0             -147.9445      -5781.1236    \n",
            "      2013   585.44258     -148.26984      0             -147.96715     -4073.4527    \n",
            "      2014   600.23227     -148.29688      0             -147.98654     -2076.0044    \n",
            "      2015   601.98584     -148.29348      0             -147.98223      147.40115    \n",
            "      2016   595.3517      -148.25039      0             -147.94257      2624.5158    \n",
            "      2017   585.16701     -148.24654      0             -147.94398      5091.3333    \n",
            "      2018   574.66331     -148.22868      0             -147.93155      7710.1844    \n",
            "      2019   564.5357      -148.23658      0             -147.94469      10217.33     \n",
            "      2020   552.45573     -148.24276      0             -147.95712      12619.433    \n",
            "      2021   533.75431     -148.20235      0             -147.92638      14950.917    \n",
            "      2022   503.40669     -148.21813      0             -147.95784      17140.563    \n",
            "      2023   458.19312     -148.18148      0             -147.94457      19134.254    \n",
            "      2024   398.75242     -148.14536      0             -147.93919      21028.615    \n",
            "      2025   330.22193     -148.10606      0             -147.93532      22779.849    \n",
            "      2026   262.26036     -148.07658      0             -147.94098      24094.736    \n",
            "      2027   207.02257     -148.05011      0             -147.94307      25221.741    \n",
            "      2028   176.7345      -148.01717      0             -147.92579      25815.622    \n",
            "      2029   181.14759     -148.02444      0             -147.93078      26048.822    \n",
            "      2030   225.03116     -148.05319      0             -147.93684      25807.388    \n",
            "      2031   306.55981     -148.10752      0             -147.94902      25099.993    \n",
            "      2032   417.40212     -148.1615       0             -147.94569      24075.078    \n",
            "      2033   544.13425     -148.22318      0             -147.94184      22452.869    \n",
            "      2034   670.66821     -148.28854      0             -147.94177      20590.176    \n",
            "      2035   781.37537     -148.34485      0             -147.94085      18496.378    \n",
            "      2036   863.91677     -148.37108      0             -147.9244       16248.213    \n",
            "      2037   911.68829     -148.40393      0             -147.93255      13888.437    \n",
            "      2038   924.37216     -148.41616      0             -147.93822      11313.783    \n",
            "      2039   906.60832     -148.41499      0             -147.94624      8771.7585    \n",
            "      2040   866.69861     -148.40533      0             -147.95721      5999.2549    \n",
            "      2041   813.40489     -148.36845      0             -147.94788      3295.1941    \n",
            "      2042   754.61899     -148.32306      0             -147.93289      827.39216    \n",
            "      2043   695.39763     -148.31512      0             -147.95557     -1702.4075    \n",
            "      2044   637.05882     -148.27295      0             -147.94356     -4209.9575    \n",
            "      2045   578.57326     -148.21941      0             -147.92027     -6321.6528    \n",
            "      2046   518.00129     -148.21617      0             -147.94835     -8288.0501    \n",
            "      2047   453.95835     -148.20344      0             -147.96873     -9988.6683    \n",
            "      2048   387.907       -148.14792      0             -147.94735     -11284.192    \n",
            "      2049   324.20838     -148.10645      0             -147.93882     -12207.35     \n",
            "      2050   269.80562     -148.078        0             -147.9385      -12951.691    \n",
            "      2051   233.22949     -148.04973      0             -147.92914     -13294.283    \n",
            "      2052   222.72459     -148.05178      0             -147.93662     -13386.797    \n",
            "      2053   244.31043     -148.04995      0             -147.92363     -13286.471    \n",
            "      2054   299.64073     -148.09409      0             -147.93916     -12990.243    \n",
            "      2055   385.49745     -148.12086      0             -147.92155     -12525.917    \n",
            "      2056   493.70271     -148.19933      0             -147.94407     -11890.048    \n",
            "      2057   612.04542     -148.25141      0             -147.93495     -11122.422    \n",
            "      2058   726.26136     -148.30676      0             -147.93125     -10015.104    \n",
            "      2059   822.75503     -148.37064      0             -147.94524     -8875.392     \n",
            "      2060   890.78277     -148.40176      0             -147.94119     -7425.6715    \n",
            "      2061   923.77522     -148.41993      0             -147.9423      -5895.3083    \n",
            "      2062   921.12828     -148.41716      0             -147.9409      -4157.4003    \n",
            "      2063   887.52691     -148.40572      0             -147.94683     -2412.8991    \n",
            "      2064   830.82217     -148.3771       0             -147.94754     -845.23999    \n",
            "      2065   761.32924     -148.32711      0             -147.93347      844.6088     \n",
            "      2066   688.33378     -148.30627      0             -147.95038      2199.3809    \n",
            "      2067   618.83154     -148.27626      0             -147.9563       3377.2096    \n",
            "      2068   556.93963     -148.26772      0             -147.97976      4300.0597    \n",
            "      2069   503.81885     -148.21701      0             -147.95651      4979.9011    \n",
            "      2070   458.00012     -148.20199      0             -147.96519      5408.952     \n",
            "      2071   416.66577     -148.15237      0             -147.93694      5588.0429    \n",
            "      2072   377.81197     -148.16007      0             -147.96472      5566.9346    \n",
            "      2073   340.94636     -148.12035      0             -147.94407      5409.0988    \n",
            "      2074   307.57266     -148.09826      0             -147.93923      5000.4896    \n",
            "      2075   281.59607     -148.11821      0             -147.97262      4468.2585    \n",
            "      2076   267.52407     -148.11146      0             -147.97314      3744.5443    \n",
            "      2077   269.95252     -148.11252      0             -147.97294      2830.1182    \n",
            "      2078   292.10579     -148.13989      0             -147.98886      1618.5801    \n",
            "      2079   333.95926     -148.16004      0             -147.98737      335.4241     \n",
            "      2080   392.26065     -148.19056      0             -147.98775     -1073.9737    \n",
            "      2081   460.21959     -148.19491      0             -147.95695     -2572.974     \n",
            "      2082   528.6108      -148.26231      0             -147.98899     -4107.567     \n",
            "      2083   586.87375     -148.26648      0             -147.96304     -5624.8762    \n",
            "      2084   625.7284      -148.30031      0             -147.97679     -7029.2637    \n",
            "      2085   639.3881      -148.30627      0             -147.97568     -8321.5491    \n",
            "      2086   626.70703     -148.30863      0             -147.9846      -9350.0527    \n",
            "      2087   591.2187      -148.30515      0             -147.99946     -10415.729    \n",
            "      2088   540.79916     -148.25248      0             -147.97287     -10901.624    \n",
            "      2089   485.77217     -148.23401      0             -147.98285     -11467.396    \n",
            "      2090   436.33566     -148.20225      0             -147.97665     -11834.989    \n",
            "      2091   400.83512     -148.19115      0             -147.9839      -12088.242    \n",
            "      2092   383.32699     -148.1645       0             -147.9663      -12214.34     \n",
            "      2093   383.06049     -148.16646      0             -147.96841     -12029.266    \n",
            "      2094   395.267       -148.21459      0             -148.01022     -11914.955    \n",
            "      2095   412.28299     -148.21111      0             -147.99794     -11464.207    \n",
            "      2096   425.80597     -148.22966      0             -148.0095      -10892.118    \n",
            "      2097   429.34387     -148.22332      0             -148.00133     -9935.7656    \n",
            "      2098   419.90529     -148.18945      0             -147.97234     -8884.0324    \n",
            "      2099   398.22908     -148.22919      0             -148.02329     -7637.7684    \n",
            "      2100   368.90537     -148.19357      0             -148.00283     -6251.5254    \n",
            "      2101   338.85624     -148.18457      0             -148.00937     -4795.9209    \n",
            "      2102   314.80161     -148.17176      0             -148.00899     -3210.8322    \n",
            "      2103   301.45229     -148.15448      0             -147.99862     -1729.9132    \n",
            "      2104   300.51787     -148.15318      0             -147.9978      -265.43675    \n",
            "      2105   309.88599     -148.16767      0             -148.00744      1002.7898    \n",
            "      2106   324.2507      -148.19117      0             -148.02351      2366.0527    \n",
            "      2107   337.11379     -148.18728      0             -148.01298      3614.857     \n",
            "      2108   342.55961     -148.19168      0             -148.01456      4716.9413    \n",
            "      2109   337.22659     -148.19372      0             -148.01936      5862.0718    \n",
            "      2110   321.22924     -148.19026      0             -148.02417      6688.1014    \n",
            "      2111   298.74408     -148.18126      0             -148.02679      7610.2958    \n",
            "      2112   276.4688      -148.14016      0             -147.99722      8291.1685    \n",
            "      2113   262.47191     -148.14907      0             -148.01336      8795.2414    \n",
            "      2114   264.26565     -148.14295      0             -148.00632      9148.5203    \n",
            "      2115   286.19643     -148.15241      0             -148.00443      8930.0475    \n",
            "      2116   328.70985     -148.21467      0             -148.04471      8661.4722    \n",
            "      2117   387.88791     -148.2336       0             -148.03305      8066.7335    \n",
            "      2118   455.91665     -148.25418      0             -148.01846      7127.0376    \n",
            "      2119   523.13977     -148.27737      0             -148.00689      6140.417     \n",
            "      2120   579.56772     -148.29817      0             -147.99851      4844.5233    \n",
            "      2121   617.1236      -148.34666      0             -148.02758      3453.9917    \n",
            "      2122   631.6455      -148.35236      0             -148.02578      1984.504     \n",
            "      2123   622.90029     -148.34655      0             -148.02449      411.12212    \n",
            "      2124   594.52233     -148.34661      0             -148.03922     -1204.8592    \n",
            "      2125   552.91254     -148.35439      0             -148.06852     -2858.1376    \n",
            "      2126   505.69126     -148.31167      0             -148.0502      -4473.544     \n",
            "      2127   459.25181     -148.30273      0             -148.06528     -6188.6175    \n",
            "      2128   418.03413     -148.2855       0             -148.06936     -7831.7153    \n",
            "      2129   383.78403     -148.27206      0             -148.07363     -9417.3962    \n",
            "      2130   355.52074     -148.21948      0             -148.03566     -10787.128    \n",
            "      2131   330.59626     -148.21385      0             -148.04292     -12203.588    \n",
            "      2132   306.14576     -148.22197      0             -148.06368     -13487.837    \n",
            "      2133   280.00649     -148.18439      0             -148.03962     -14487.589    \n",
            "      2134   252.20622     -148.19011      0             -148.05971     -15230.427    \n",
            "      2135   225.53004     -148.15037      0             -148.03376     -15790.926    \n",
            "      2136   204.44644     -148.16125      0             -148.05554     -16170.211    \n",
            "      2137   194.91077     -148.14771      0             -148.04693     -16271.433    \n",
            "      2138   202.41593     -148.15075      0             -148.04609     -16046.678    \n",
            "      2139   230.59767     -148.18211      0             -148.06288     -15856.534    \n",
            "      2140   280.39419     -148.20341      0             -148.05844     -15403.562    \n",
            "      2141   348.99382     -148.24297      0             -148.06253     -14959.946    \n",
            "      2142   430.16005     -148.32456      0             -148.10215     -14400.634    \n",
            "      2143   514.92306     -148.34663      0             -148.0804      -13481.984    \n",
            "      2144   593.14736     -148.35466      0             -148.04798     -12419.249    \n",
            "      2145   655.60749     -148.40758      0             -148.06861     -11238.715    \n",
            "      2146   695.37834     -148.41701      0             -148.05747     -9788.6535    \n",
            "      2147   709.30684     -148.42459      0             -148.05785     -8244.6075    \n",
            "      2148   697.61333     -148.45672      0             -148.09602     -6517.8329    \n",
            "      2149   664.36361     -148.45429      0             -148.11079     -4776.9758    \n",
            "      2150   616.31634     -148.42258      0             -148.10392     -2938.597     \n",
            "      2151   560.61027     -148.42418      0             -148.13432     -1268.7015    \n",
            "      2152   503.86154     -148.36672      0             -148.1062       342.85018    \n",
            "      2153   450.7777      -148.33901      0             -148.10594      1816.6383    \n",
            "      2154   403.65905     -148.32511      0             -148.1164       3248.109     \n",
            "      2155   362.62332     -148.29963      0             -148.11214      4389.8022    \n",
            "      2156   326.28477     -148.30384      0             -148.13514      5422.6606    \n",
            "      2157   293.09219     -148.27727      0             -148.12573      6228.6127    \n",
            "      2158   261.89622     -148.24037      0             -148.10496      6901.2508    \n",
            "      2159   233.27998     -148.23506      0             -148.11444      7631.4374    \n",
            "      2160   209.69309     -148.22988      0             -148.12146      7965.2174    \n",
            "      2161   194.59725     -148.21362      0             -148.11301      8294.097     \n",
            "      2162   192.40332     -148.227        0             -148.12752      8459.5256    \n",
            "      2163   206.64412     -148.23545      0             -148.1286       8252.7254    \n",
            "      2164   238.76892     -148.24937      0             -148.12592      7900.8315    \n",
            "      2165   287.28659     -148.28298      0             -148.13444      7456.0559    \n",
            "      2166   347.86411     -148.30521      0             -148.12535      6824.5259    \n",
            "      2167   413.34729     -148.3864       0             -148.17268      5895.948     \n",
            "      2168   475.01322     -148.38899      0             -148.14339      5004.7752    \n",
            "      2169   524.29134     -148.39092      0             -148.11984      4040.8546    \n",
            "      2170   554.82611     -148.45953      0             -148.17266      3172.9174    \n",
            "      2171   562.85407     -148.43371      0             -148.14269      2139.938     \n",
            "      2172   548.51219     -148.42794      0             -148.14434      1406.7143    \n",
            "      2173   515.55464     -148.42343      0             -148.15686      763.88142    \n",
            "      2174   470.42301     -148.40077      0             -148.15754      74.296836    \n",
            "      2175   421.09247     -148.37175      0             -148.15402     -492.963      \n",
            "      2176   374.37558     -148.35162      0             -148.15805     -958.01011    \n",
            "      2177   335.02005     -148.32947      0             -148.15625     -1261.777     \n",
            "      2178   304.93069     -148.31068      0             -148.15302     -1607.9101    \n",
            "      2179   283.20941     -148.3279       0             -148.18147     -1774.4485    \n",
            "      2180   266.63979     -148.29702      0             -148.15915     -1719.2876    \n",
            "      2181   251.16308     -148.30729      0             -148.17743     -1451.7215    \n",
            "      2182   233.72325     -148.3047       0             -148.18386     -1058.777     \n",
            "      2183   212.72755     -148.28162      0             -148.17163     -523.15371    \n",
            "      2184   189.11151     -148.25996      0             -148.16219      247.36599    \n",
            "      2185   165.96483     -148.25891      0             -148.1731       1008.4767    \n",
            "      2186   147.68202     -148.27499      0             -148.19864      1981.8781    \n",
            "      2187   138.73574     -148.25994      0             -148.18821      3081.027     \n",
            "      2188   142.01684     -148.24122      0             -148.16779      4207.2429    \n",
            "      2189   158.31326     -148.27943      0             -148.19758      5217.9653    \n",
            "      2190   185.71459     -148.26068      0             -148.16466      6256.2946    \n",
            "      2191   219.94838     -148.27462      0             -148.1609       7208.9016    \n",
            "      2192   255.21695     -148.30819      0             -148.17624      8167.8392    \n",
            "      2193   285.62038     -148.35816      0             -148.21049      9005.9658    \n",
            "      2194   306.72387     -148.33898      0             -148.18039      10017.595    \n",
            "      2195   316.26577     -148.35208      0             -148.18856      10833.799    \n",
            "      2196   314.83492     -148.36544      0             -148.20266      11550.09     \n",
            "      2197   305.30599     -148.3438       0             -148.18595      12087.278    \n",
            "      2198   292.28212     -148.3605       0             -148.20938      12614.488    \n",
            "      2199   280.98227     -148.31697      0             -148.17169      12977.069    \n",
            "      2200   275.25337     -148.31371      0             -148.17139      12980.66     \n",
            "      2201   276.96956     -148.32737      0             -148.18416      12785.223    \n",
            "      2202   285.65736     -148.35423      0             -148.20653      12403.586    \n",
            "      2203   298.53269     -148.34743      0             -148.19307      11760.463    \n",
            "      2204   311.15399     -148.37333      0             -148.21245      10868.99     \n",
            "      2205   318.86746     -148.37611      0             -148.21124      9997.4351    \n",
            "      2206   318.09115     -148.36512      0             -148.20066      8931.3988    \n",
            "      2207   306.84119     -148.35777      0             -148.19912      7748.2032    \n",
            "      2208   285.51767     -148.32777      0             -148.18014      6610.3056    \n",
            "      2209   256.77645     -148.347        0             -148.21423      5312.1468    \n",
            "      2210   224.9516      -148.3444       0             -148.2281       3786.7293    \n",
            "      2211   194.84477     -148.3162       0             -148.21546      2568.7268    \n",
            "      2212   170.49325     -148.30261      0             -148.21446      1211.0969    \n",
            "      2213   154.72597     -148.3065       0             -148.2265      -138.77144    \n",
            "      2214   148.64362     -148.30771      0             -148.23085     -1429.7238    \n",
            "      2215   151.22692     -148.28716      0             -148.20897     -2600.0928    \n",
            "      2216   159.96281     -148.30858      0             -148.22587     -3837.2134    \n",
            "      2217   171.60637     -148.30687      0             -148.21814     -4941.7278    \n",
            "      2218   183.17372     -148.289        0             -148.19429     -5732.4973    \n",
            "      2219   192.63662     -148.31539      0             -148.21578     -6475.0772    \n",
            "      2220   199.70117     -148.32867      0             -148.22541     -7111.9471    \n",
            "      2221   205.68339     -148.34597      0             -148.23962     -7319.5342    \n",
            "      2222   212.79716     -148.32176      0             -148.21174     -7492.3075    \n",
            "      2223   224.01793     -148.33329      0             -148.21746     -7417.421     \n",
            "      2224   241.59463     -148.35993      0             -148.23501     -7371.5386    \n",
            "      2225   266.50015     -148.37742      0             -148.23963     -6992.8253    \n",
            "      2226   297.73921     -148.35846      0             -148.20451     -6408.6982    \n",
            "      2227   332.30197     -148.39181      0             -148.21999     -5781.3501    \n",
            "      2228   365.73998     -148.41801      0             -148.2289      -5091.5227    \n",
            "      2229   393.06174     -148.45634      0             -148.25311     -4306.2226    \n",
            "      2230   409.35632     -148.43842      0             -148.22677     -3185.6061    \n",
            "      2231   411.16392     -148.43418      0             -148.22159     -2236.0899    \n",
            "      2232   397.63405     -148.45084      0             -148.24525     -1088.4873    \n",
            "      2233   369.95799     -148.44455      0             -148.25327      185.3707     \n",
            "      2234   331.75653     -148.40566      0             -148.23413      1402.6515    \n",
            "      2235   288.03752     -148.38744      0             -148.23851      2707.7387    \n",
            "      2236   244.38542     -148.3699       0             -148.24355      3862.1206    \n",
            "      2237   205.84208     -148.36558      0             -148.25915      4902.2773    \n",
            "      2238   175.88511     -148.32875      0             -148.23781      5792.8175    \n",
            "      2239   155.95068     -148.35868      0             -148.27805      6687.7235    \n",
            "      2240   145.68328     -148.33652      0             -148.2612       7340.4832    \n",
            "      2241   143.18284     -148.34044      0             -148.26641      7920.54      \n",
            "      2242   145.78719     -148.35917      0             -148.28379      8441.6531    \n",
            "      2243   151.14877     -148.3428       0             -148.26465      8721.6685    \n",
            "      2244   157.69569     -148.35822      0             -148.27669      8910.1014    \n",
            "      2245   165.49226     -148.33868      0             -148.25311      9025.5392    \n",
            "      2246   175.61687     -148.36728      0             -148.27648      8953.7399    \n",
            "      2247   189.59255     -148.3604       0             -148.26237      8745.3928    \n",
            "      2248   209.12703     -148.3812       0             -148.27307      8269.1432    \n",
            "      2249   235.03535     -148.40818      0             -148.28665      7948.0983    \n",
            "      2250   266.49741     -148.39943      0             -148.26164      7304.5503    \n",
            "      2251   300.98782     -148.42         0             -148.26438      6575.4907    \n",
            "      2252   334.54333     -148.45526      0             -148.28229      5722.4676    \n",
            "      2253   362.39095     -148.45716      0             -148.26978      4922.7424    \n",
            "      2254   379.7301      -148.47978      0             -148.28344      3967.6871    \n",
            "      2255   383.37633     -148.44585      0             -148.24763      3052.4612    \n",
            "      2256   371.88585     -148.47915      0             -148.28687      2095.3055    \n",
            "      2257   346.25628     -148.41996      0             -148.24093      1351.416     \n",
            "      2258   309.86612     -148.43756      0             -148.27734      607.18644    \n",
            "      2259   267.45525     -148.42741      0             -148.28912     -216.84271    \n",
            "      2260   224.50549     -148.382        0             -148.26592     -829.03446    \n",
            "      2261   185.72501     -148.3662       0             -148.27017     -1453.3922    \n",
            "      2262   154.44616     -148.35358      0             -148.27372     -2043.8821    \n",
            "      2263   131.97754     -148.3603       0             -148.29206     -2583.1844    \n",
            "      2264   117.76311     -148.34623      0             -148.28534     -2908.3828    \n",
            "      2265   109.81111     -148.33409      0             -148.27731     -3273.496     \n",
            "      2266   105.40221     -148.3295       0             -148.27501     -3620.2627    \n",
            "      2267   102.52073     -148.35103      0             -148.29802     -3769.4758    \n",
            "      2268   99.790617     -148.31364      0             -148.26204     -3643.2131    \n",
            "      2269   97.32859      -148.32509      0             -148.27477     -3635.5827    \n",
            "      2270   96.684156     -148.32773      0             -148.27774     -3494.6463    \n",
            "      2271   99.898027     -148.28913      0             -148.23748     -3234.595     \n",
            "      2272   109.04214     -148.36295      0             -148.30657     -3052.5237    \n",
            "      2273   125.55595     -148.36037      0             -148.29545     -2670.4762    \n",
            "      2274   149.37547     -148.36469      0             -148.28746     -2276.0279    \n",
            "      2275   178.82594     -148.37977      0             -148.28731     -1985.0901    \n",
            "      2276   210.64769     -148.39423      0             -148.28531     -1699.2501    \n",
            "      2277   240.45162     -148.40255      0             -148.27822     -1414.9147    \n",
            "      2278   264.13792     -148.42139      0             -148.28482     -1039.897     \n",
            "      2279   278.29798     -148.43927      0             -148.29538     -859.14242    \n",
            "      2280   281.17439     -148.47136      0             -148.32598     -589.76932    \n",
            "      2281   273.09736     -148.45273      0             -148.31153     -204.40346    \n",
            "      2282   255.93083     -148.42449      0             -148.29216      93.243529    \n",
            "      2283   232.83362     -148.41001      0             -148.28962      160.80456    \n",
            "      2284   207.59718     -148.40446      0             -148.29713      402.3631     \n",
            "      2285   183.76922     -148.40683      0             -148.31182      369.14848    \n",
            "      2286   163.86239     -148.37667      0             -148.29195      294.97661    \n",
            "      2287   148.9175      -148.39033      0             -148.31334      97.876045    \n",
            "      2288   138.48049     -148.3636       0             -148.292       -293.99728    \n",
            "      2289   131.22395     -148.39128      0             -148.32343     -688.53153    \n",
            "      2290   125.24632     -148.36127      0             -148.29652     -1140.4836    \n",
            "      2291   118.43769     -148.34312      0             -148.28188     -1757.414     \n",
            "      2292   109.6132      -148.3638       0             -148.30712     -2335.9916    \n",
            "      2293   98.876225     -148.34272      0             -148.29159     -2933.2096    \n",
            "      2294   87.282503     -148.35369      0             -148.30856     -3620.5511    \n",
            "      2295   76.926965     -148.34769      0             -148.30791     -4242.273     \n",
            "      2296   70.416201     -148.34898      0             -148.31258     -4964.8098    \n",
            "      2297   69.736021     -148.34418      0             -148.30812     -5675.7117    \n",
            "      2298   76.247444     -148.32435      0             -148.28493     -6319.8528    \n",
            "      2299   89.935946     -148.35029      0             -148.30379     -7041.2233    \n",
            "      2300   109.47768     -148.33587      0             -148.27927     -7673.3326    \n",
            "      2301   132.44604     -148.36299      0             -148.29451     -8337.9943    \n",
            "      2302   155.7286      -148.38252      0             -148.302       -8925.9534    \n",
            "      2303   176.06264     -148.40366      0             -148.31262     -9467.6767    \n",
            "      2304   191.05818     -148.40054      0             -148.30176     -9811.9602    \n",
            "      2305   199.65948     -148.40198      0             -148.29875     -10072.077    \n",
            "      2306   201.93017     -148.40446      0             -148.30005     -10314.381    \n",
            "      2307   199.52682     -148.44332      0             -148.34015     -10351.903    \n",
            "      2308   195.12715     -148.41528      0             -148.31439     -10193.213    \n",
            "      2309   191.18392     -148.40816      0             -148.30931     -10015.677    \n",
            "      2310   189.57878     -148.4089       0             -148.31088     -9744.9977    \n",
            "      2311   191.40401     -148.41191      0             -148.31294     -9374.7964    \n",
            "      2312   196.38542     -148.40111      0             -148.29957     -8931.7591    \n",
            "      2313   202.92391     -148.41184      0             -148.30692     -8470.364     \n",
            "      2314   208.50729     -148.38987      0             -148.28206     -7765.126     \n",
            "      2315   210.38135     -148.41659      0             -148.30782     -7124.5312    \n",
            "      2316   206.43037     -148.4343       0             -148.32757     -6357.0503    \n",
            "      2317   195.6935      -148.43359      0             -148.33241     -5578.2637    \n",
            "      2318   178.7328      -148.41293      0             -148.32052     -4527.3177    \n",
            "      2319   157.75986     -148.37202      0             -148.29045     -3558.8263    \n",
            "      2320   135.74989     -148.37306      0             -148.30287     -2554.4276    \n",
            "      2321   115.92546     -148.36222      0             -148.30228     -1590.4251    \n",
            "      2322   101.42103     -148.38734      0             -148.3349      -727.49827    \n",
            "      2323   94.303752     -148.37512      0             -148.32636      251.26379    \n",
            "      2324   95.119441     -148.32388      0             -148.2747       1109.0047    \n",
            "      2325   102.79036     -148.3853       0             -148.33216      1912.2679    \n",
            "      2326   115.08442     -148.38646      0             -148.32696      2555.6154    \n",
            "      2327   129.3682      -148.39192      0             -148.32503      3173.402     \n",
            "      2328   143.30381     -148.40055      0             -148.32645      3750.067     \n",
            "      2329   155.15692     -148.37783      0             -148.29761      4294.42      \n",
            "      2330   164.43655     -148.39777      0             -148.31275      4557.979     \n",
            "      2331   171.85027     -148.40733      0             -148.31847      4931.9971    \n",
            "      2332   178.92974     -148.41641      0             -148.32389      5266.4168    \n",
            "      2333   187.74465     -148.38972      0             -148.29264      5262.0824    \n",
            "      2334   199.69225     -148.41812      0             -148.31487      5220.7811    \n",
            "      2335   215.13062     -148.43709      0             -148.32586      5129.8481    \n",
            "      2336   233.18502     -148.43164      0             -148.31107      4769.172     \n",
            "      2337   251.67698     -148.4424       0             -148.31228      4559.5484    \n",
            "      2338   267.55497     -148.45628      0             -148.31794      3914.942     \n",
            "      2339   277.30556     -148.47804      0             -148.33466      3305.5727    \n",
            "      2340   278.41453     -148.46085      0             -148.3169       2803.3562    \n",
            "      2341   269.46739     -148.4882       0             -148.34888      2151.4587    \n",
            "      2342   250.81776     -148.44768      0             -148.318        1642.2013    \n",
            "      2343   224.75859     -148.44924      0             -148.33303      930.92652    \n",
            "      2344   194.46446     -148.38993      0             -148.28938      359.97696    \n",
            "      2345   163.88463     -148.42171      0             -148.33697     -247.29301    \n",
            "      2346   136.7273      -148.3949       0             -148.32421     -961.03774    \n",
            "      2347   115.69358     -148.36443      0             -148.30461     -1535.454     \n",
            "      2348   102.00808     -148.38955      0             -148.33681     -2179.9904    \n",
            "      2349   95.45175      -148.36129      0             -148.31194     -2682.9637    \n",
            "      2350   94.427073     -148.36148      0             -148.31265     -3151.67      \n",
            "      2351   96.984815     -148.36038      0             -148.31024     -3486.6053    \n",
            "      2352   101.28061     -148.37738      0             -148.32502     -3960.5436    \n",
            "      2353   105.89257     -148.36168      0             -148.30693     -4171.7447    \n",
            "      2354   110.59333     -148.38674      0             -148.32956     -4201.4773    \n",
            "      2355   116.03134     -148.3729       0             -148.31291     -4323.7185    \n",
            "      2356   123.6926      -148.38437      0             -148.32041     -4361.2772    \n",
            "      2357   135.25193     -148.40163      0             -148.3317      -4308.7377    \n",
            "      2358   151.82968     -148.37421      0             -148.29571     -3892.2706    \n",
            "      2359   173.57893     -148.40991      0             -148.32016     -3743.9733    \n",
            "      2360   199.28125     -148.42796      0             -148.32492     -3392.3032    \n",
            "      2361   226.30894     -148.46238      0             -148.34537     -3125.5984    \n",
            "      2362   251.27821     -148.45009      0             -148.32017     -2613.953     \n",
            "      2363   270.5985      -148.46729      0             -148.32738     -2245.7656    \n",
            "      2364   280.98537     -148.46122      0             -148.31594     -1615.9166    \n",
            "      2365   280.59025     -148.43764      0             -148.29256     -1145.1194    \n",
            "      2366   268.99531     -148.47605      0             -148.33697     -480.78238    \n",
            "      2367   247.38269     -148.43513      0             -148.30722      56.194884    \n",
            "      2368   218.54616     -148.45233      0             -148.33934      766.72729    \n",
            "      2369   186.13881     -148.41143      0             -148.31519      1371.8819    \n",
            "      2370   153.92816     -148.37146      0             -148.29187      1869.1603    \n",
            "      2371   125.0165      -148.37102      0             -148.30638      2227.4709    \n",
            "      2372   101.5531      -148.35872      0             -148.30621      2585.1385    \n",
            "      2373   84.393119     -148.36984      0             -148.3262       2842.6856    \n",
            "      2374   73.275814     -148.35433      0             -148.31644      3078.8469    \n",
            "      2375   66.95734      -148.36306      0             -148.32844      3152.9341    \n",
            "      2376   63.939633     -148.3322       0             -148.29914      3162.6622    \n",
            "      2377   62.834321     -148.35781      0             -148.32532      3122.9693    \n",
            "      2378   62.952166     -148.34618      0             -148.31363      2974.6064    \n",
            "      2379   64.400109     -148.34334      0             -148.31005      2876.5968    \n",
            "      2380   68.268652     -148.35907      0             -148.32378      2531.8926    \n",
            "      2381   76.047907     -148.38148      0             -148.34216      2323.8624    \n",
            "      2382   89.056209     -148.35106      0             -148.30501      2083.6742    \n",
            "      2383   107.89093     -148.37432      0             -148.31854      1661.0689    \n",
            "      2384   132.08501     -148.38391      0             -148.31561      1191.5282    \n",
            "      2385   159.99826     -148.41189      0             -148.32916      709.15491    \n",
            "      2386   189.02333     -148.41165      0             -148.31392      350.53942    \n",
            "      2387   215.75753     -148.41644      0             -148.30488     -179.16027    \n",
            "      2388   236.66011     -148.43578      0             -148.31342     -633.91133    \n",
            "      2389   248.92708     -148.46009      0             -148.33139     -1003.1359    \n",
            "      2390   251.15254     -148.45177      0             -148.32192     -1219.3054    \n",
            "      2391   243.49178     -148.43256      0             -148.30666     -1479.6196    \n",
            "      2392   227.67503     -148.44908      0             -148.33137     -1596.6465    \n",
            "      2393   206.67513     -148.41974      0             -148.31288     -1617.2917    \n",
            "      2394   184.00154     -148.42594      0             -148.3308      -1614.0124    \n",
            "      2395   162.91622     -148.39777      0             -148.31354     -1508.5172    \n",
            "      2396   145.90718     -148.36886      0             -148.29342     -1246.6941    \n",
            "      2397   134.07804     -148.38542      0             -148.3161      -934.11194    \n",
            "      2398   127.05977     -148.3752       0             -148.3095      -607.48288    \n",
            "      2399   123.25916     -148.37269      0             -148.30896     -102.06998    \n",
            "      2400   120.57753     -148.37313      0             -148.31078      484.49697    \n",
            "      2401   117.18111     -148.37958      0             -148.31899      981.75226    \n",
            "      2402   111.87303     -148.37159      0             -148.31374      1687.7424    \n",
            "      2403   104.56562     -148.35009      0             -148.29603      2497.0217    \n",
            "      2404   96.681086     -148.37069      0             -148.3207       3323.458     \n",
            "      2405   90.407173     -148.37824      0             -148.3315       4247.6577    \n",
            "      2406   88.280264     -148.38235      0             -148.3367       5181.2969    \n",
            "      2407   92.687066     -148.3701       0             -148.32218      6038.4176    \n",
            "      2408   104.8076      -148.35645      0             -148.30226      6887.0942    \n",
            "      2409   124.25188     -148.3608       0             -148.29656      7719.0077    \n",
            "      2410   149.09297     -148.37876      0             -148.30168      8442.2919    \n",
            "      2411   176.22617     -148.38572      0             -148.29461      9012.3091    \n",
            "      2412   202.0169      -148.43056      0             -148.32611      9463.0022    \n",
            "      2413   223.10023     -148.42426      0             -148.30891      10013.137    \n",
            "      2414   237.16905     -148.41802      0             -148.29539      10426.272    \n",
            "      2415   243.68422     -148.42713      0             -148.30113      10789.09     \n",
            "      2416   243.73755     -148.44559      0             -148.31957      11035.693    \n",
            "      2417   239.57925     -148.42288      0             -148.29901      11151.519    \n",
            "      2418   234.01244     -148.44693      0             -148.32594      11024.189    \n",
            "      2419   229.56954     -148.4394       0             -148.3207       10860.585    \n",
            "      2420   227.7487      -148.41793      0             -148.30018      10562.32     \n",
            "      2421   228.80055     -148.4319       0             -148.3136       9962.7623    \n",
            "      2422   231.6123      -148.44897      0             -148.32921      9438.4612    \n",
            "      2423   233.85123     -148.43363      0             -148.31272      8720.3597    \n",
            "      2424   232.81264     -148.42329      0             -148.30291      7868.9856    \n",
            "      2425   226.55618     -148.40369      0             -148.28655      7063.6701    \n",
            "      2426   213.92128     -148.40674      0             -148.29613      6060.2333    \n",
            "      2427   195.43774     -148.38915      0             -148.2881       5269.0134    \n",
            "      2428   173.45299     -148.36905      0             -148.27937      4271.5322    \n",
            "      2429   150.85011     -148.38012      0             -148.30212      3241.8829    \n",
            "      2430   131.23191     -148.36769      0             -148.29984      2304.3155    \n",
            "      2431   117.88901     -148.33762      0             -148.27667      1345.0965    \n",
            "      2432   113.03951     -148.37298      0             -148.31453      308.28662    \n",
            "      2433   117.35138     -148.35305      0             -148.29238     -734.92084    \n",
            "      2434   129.72981     -148.35659      0             -148.28952     -1684.8539    \n",
            "      2435   147.82867     -148.38872      0             -148.31228     -2487.8422    \n",
            "      2436   168.50426     -148.37915      0             -148.29203     -3400.8419    \n",
            "      2437   188.55772     -148.41053      0             -148.31304     -4026.4539    \n",
            "      2438   205.89201     -148.38324      0             -148.27679     -4600.314     \n",
            "      2439   219.54263     -148.40807      0             -148.29456     -5103.8257    \n",
            "      2440   229.52184     -148.40438      0             -148.28571     -5474.195     \n",
            "      2441   237.24199     -148.39595      0             -148.27329     -5680.8501    \n",
            "      2442   244.82025     -148.41548      0             -148.2889      -5716.5865    \n",
            "      2443   253.88085     -148.41303      0             -148.28176     -5864.302     \n",
            "      2444   265.24654     -148.42889      0             -148.29174     -5767.7494    \n",
            "      2445   278.54766     -148.41085      0             -148.26683     -5648.4138    \n",
            "      2446   291.97211     -148.41633      0             -148.26537     -5238.6873    \n",
            "      2447   302.79835     -148.42232      0             -148.26576     -5081.2076    \n",
            "      2448   307.8332      -148.43585      0             -148.27669     -4751.0453    \n",
            "      2449   304.07671     -148.41867      0             -148.26145     -4259.6833    \n",
            "      2450   289.89805     -148.4297       0             -148.27981     -3761.8977    \n",
            "      2451   265.32461     -148.41661      0             -148.27942     -3221.7182    \n",
            "      2452   232.06781     -148.40544      0             -148.28545     -2628.4405    \n",
            "      2453   193.69375     -148.38573      0             -148.28558     -1947.9636    \n",
            "      2454   154.78549     -148.35809      0             -148.27806     -1456.2666    \n",
            "      2455   119.8136      -148.33753      0             -148.27559     -906.12404    \n",
            "      2456   92.6716       -148.33219      0             -148.28428     -372.88946    \n",
            "      2457   75.905315     -148.31746      0             -148.27822      157.95575    \n",
            "      2458   70.36531      -148.30632      0             -148.26993      481.42522    \n",
            "      2459   75.194727     -148.31197      0             -148.27309      749.7398     \n",
            "      2460   88.210031     -148.33109      0             -148.28549      819.66686    \n",
            "      2461   106.33539     -148.32989      0             -148.27491      938.81235    \n",
            "      2462   126.65335     -148.35122      0             -148.28573      912.87169    \n",
            "      2463   146.99306     -148.34954      0             -148.27354      1029.8865    \n",
            "      2464   166.14475     -148.3541       0             -148.26819      901.52807    \n",
            "      2465   184.17532     -148.371        0             -148.27577      872.45947    \n",
            "      2466   202.32641     -148.38233      0             -148.27772      623.84222    \n",
            "      2467   221.85739     -148.37827      0             -148.26356      302.13437    \n",
            "      2468   243.64316     -148.37388      0             -148.24791      33.359362    \n",
            "      2469   267.78397     -148.42637      0             -148.28792     -413.70633    \n",
            "      2470   293.07455     -148.40815      0             -148.25661     -869.63507    \n",
            "      2471   316.86328     -148.44028      0             -148.27645     -1357.6583    \n",
            "      2472   335.35077     -148.44433      0             -148.27094     -1935.0157    \n",
            "      2473   344.62235     -148.42041      0             -148.24223     -2280.1769    \n",
            "      2474   341.77873     -148.44795      0             -148.27124     -2816.0403    \n",
            "      2475   325.3381      -148.40702      0             -148.23881     -3219.8227    \n",
            "      2476   296.2972      -148.38814      0             -148.23494     -3585.0641    \n",
            "      2477   257.62888     -148.42353      0             -148.29033     -3917.7978    \n",
            "      2478   213.64515     -148.38031      0             -148.26985     -4111.8138    \n",
            "      2479   169.86196     -148.3395       0             -148.25167     -4253.0635    \n",
            "      2480   131.45277     -148.31017      0             -148.24221     -4238.7146    \n",
            "      2481   102.20341     -148.32166      0             -148.26882     -4396.7056    \n",
            "      2482   84.076389     -148.30618      0             -148.26271     -4279.0783    \n",
            "      2483   76.831185     -148.29988      0             -148.26015     -4266.0503    \n",
            "      2484   78.384088     -148.28104      0             -148.24052     -4097.2854    \n",
            "      2485   85.577628     -148.28883      0             -148.24458     -4005.5125    \n",
            "      2486   95.081762     -148.28926      0             -148.2401      -3683.8909    \n",
            "      2487   104.55626     -148.30477      0             -148.25071     -3416.4992    \n",
            "      2488   113.31116     -148.30246      0             -148.24387     -2971.6509    \n",
            "      2489   122.10975     -148.30353      0             -148.2404      -2460.5905    \n",
            "      2490   133.00752     -148.3004       0             -148.23163     -1849.1995    \n",
            "      2491   148.67655     -148.31766      0             -148.24079     -1392.0011    \n",
            "      2492   171.16188     -148.34013      0             -148.25163     -785.49431    \n",
            "      2493   201.3048      -148.32451      0             -148.22042     -169.22898    \n",
            "      2494   237.89028     -148.36623      0             -148.24323      306.06499    \n",
            "      2495   277.65456     -148.40498      0             -148.26142      768.6345     \n",
            "      2496   316.06957     -148.40349      0             -148.24007      1276.8963    \n",
            "      2497   347.91796     -148.43323      0             -148.25335      1632.7093    \n",
            "      2498   368.65394     -148.42805      0             -148.23744      2110.3895    \n",
            "      2499   375.35211     -148.43925      0             -148.24518      2471.6454    \n",
            "      2500   367.45205     -148.42485      0             -148.23487      2776.6536    \n",
            "      2501   347.08992     -148.41617      0             -148.23671      3022.0655    \n",
            "      2502   318.38623     -148.37604      0             -148.21142      3243.8639    \n",
            "      2503   286.42428     -148.38392      0             -148.23583      3309.9313    \n",
            "      2504   256.27717     -148.34757      0             -148.21506      3215.0512    \n",
            "      2505   231.77366     -148.35488      0             -148.23504      3135.3065    \n",
            "      2506   214.70554     -148.34187      0             -148.23086      2722.6748    \n",
            "      2507   204.99705     -148.32276      0             -148.21677      2351.235     \n",
            "      2508   200.77691     -148.33381      0             -148.23         1648.4637    \n",
            "      2509   198.8862      -148.32301      0             -148.22018      977.1841     \n",
            "      2510   196.48377     -148.30653      0             -148.20494      307.43538    \n",
            "      2511   191.66715     -148.30719      0             -148.20809     -484.79311    \n",
            "      2512   183.93753     -148.33169      0             -148.23659     -1352.8333    \n",
            "      2513   174.67593     -148.29448      0             -148.20417     -2226.8992    \n",
            "      2514   166.71753     -148.31227      0             -148.22607     -3107.1678    \n",
            "      2515   163.56266     -148.2918       0             -148.20723     -3896.167     \n",
            "      2516   168.45233     -148.30423      0             -148.21713     -4937.1195    \n",
            "      2517   183.39401     -148.32206      0             -148.22723     -5954.072     \n",
            "      2518   208.5575      -148.32493      0             -148.2171      -7022.0505    \n",
            "      2519   241.9271      -148.35257      0             -148.22749     -7886.9955    \n",
            "      2520   279.68449     -148.36007      0             -148.21546     -8896.9317    \n",
            "      2521   317.03601     -148.38999      0             -148.22607     -9795.7129    \n",
            "      2522   349.11484     -148.39022      0             -148.20972     -10571.23     \n",
            "      2523   371.91812     -148.41502      0             -148.22272     -11291.13     \n",
            "      2524   383.8515      -148.43188      0             -148.23341     -11738.217    \n",
            "      2525   385.0552      -148.42874      0             -148.22965     -12126.499    \n",
            "      2526   377.91446     -148.38062      0             -148.18522     -12230.864    \n",
            "      2527   366.39426     -148.38364      0             -148.1942      -12468.628    \n",
            "      2528   354.63167     -148.39053      0             -148.20717     -12365.65     \n",
            "      2529   346.07784     -148.38327      0             -148.20433     -12171.497    \n",
            "      2530   342.62765     -148.36096      0             -148.18381     -11969.471    \n",
            "      2531   343.9636      -148.36005      0             -148.18221     -11396.759    \n",
            "      2532   347.77806     -148.35306      0             -148.17324     -11056.636    \n",
            "      2533   350.51521     -148.35372      0             -148.17249     -10345.783    \n",
            "      2534   348.19125     -148.38398      0             -148.20395     -9588.2989    \n",
            "      2535   337.57037     -148.36885      0             -148.19431     -8660.3728    \n",
            "      2536   317.23298     -148.33344      0             -148.16942     -7673.0423    \n",
            "      2537   287.94954     -148.35721      0             -148.20833     -6518.4411    \n",
            "      2538   252.83638     -148.33544      0             -148.20471     -5197.9754    \n",
            "      2539   216.81636     -148.29192      0             -148.17982     -4021.5331    \n",
            "      2540   185.54202     -148.25915      0             -148.16322     -2679.2457    \n",
            "      2541   164.17618     -148.25785      0             -148.17296     -1388.8536    \n",
            "      2542   156.50203     -148.25796      0             -148.17704     -110.17333    \n",
            "      2543   163.91239     -148.24401      0             -148.15926      1018.0623    \n",
            "      2544   185.13015     -148.27271      0             -148.17699      2115.8217    \n",
            "      2545   216.53816     -148.25206      0             -148.1401       3097.1121    \n",
            "      2546   253.14648     -148.29867      0             -148.16778      4044.1948    \n",
            "      2547   289.79757     -148.32564      0             -148.17581      4900.4842    \n",
            "      2548   322.27386     -148.33721      0             -148.17058      5610.768     \n",
            "      2549   348.39886     -148.35109      0             -148.17095      6426.7951    \n",
            "      2550   367.9485      -148.37399      0             -148.18375      6829.1925    \n",
            "      2551   382.3963      -148.36673      0             -148.16901      7371.1589    \n",
            "      2552   395.05879     -148.3659       0             -148.16164      7791.5156    \n",
            "      2553   409.0344      -148.35268      0             -148.14119      8058.5297    \n",
            "      2554   426.06809     -148.36336      0             -148.14306      7940.74      \n",
            "      2555   445.975       -148.40382      0             -148.17323      7757.429     \n",
            "      2556   465.98881     -148.4099       0             -148.16897      7349.887     \n",
            "      2557   481.48387     -148.39583      0             -148.14688      7014.8753    \n",
            "      2558   486.79558     -148.37131      0             -148.11961      6608.3011    \n",
            "      2559   476.87142     -148.38823      0             -148.14167      5966.1277    \n",
            "      2560   448.86031     -148.36312      0             -148.13105      5404.164     \n",
            "      2561   402.83812     -148.34319      0             -148.13491      4886.1469    \n",
            "      2562   342.41242     -148.29955      0             -148.12251      4469.5414    \n",
            "      2563   274.52545     -148.25259      0             -148.11064      3850.6469    \n",
            "      2564   207.5782      -148.25063      0             -148.14331      3476.3593    \n",
            "      2565   150.0047      -148.2022       0             -148.12464      3061.0622    \n",
            "      2566   108.48149     -148.18012      0             -148.12403      2638.0032    \n",
            "      2567   86.620858     -148.16868      0             -148.12389      2227.6376    \n",
            "      2568   84.418989     -148.18222      0             -148.13857      1713.6007    \n",
            "      2569   98.843387     -148.17136      0             -148.12026      1413.9868    \n",
            "      2570   124.5939      -148.2012       0             -148.13678      1259.1107    \n",
            "      2571   155.86913     -148.1682       0             -148.08761      1217.0756    \n",
            "      2572   188.36206     -148.23264      0             -148.13525      1203.7001    \n",
            "      2573   219.70145     -148.26302      0             -148.14943      1297.4689    \n",
            "      2574   249.86707     -148.25281      0             -148.12362      1703.9666    \n",
            "      2575   281.11859     -148.24763      0             -148.10228      2102.9394    \n",
            "      2576   316.88692     -148.2931       0             -148.12926      2762.5774    \n",
            "      2577   359.81655     -148.29988      0             -148.11384      3351.9       \n",
            "      2578   410.52026     -148.33349      0             -148.12123      4047.745     \n",
            "      2579   466.76069     -148.32455      0             -148.08322      4845.1212    \n",
            "      2580   523.28267     -148.38679      0             -148.11623      5394.7874    \n",
            "      2581   572.7601      -148.40469      0             -148.10855      6293.4845    \n",
            "      2582   606.77878     -148.40343      0             -148.0897       7060.5359    \n",
            "      2583   618.49077     -148.42786      0             -148.10807      7870.0757    \n",
            "      2584   604.40827     -148.41813      0             -148.10563      8735.6751    \n",
            "      2585   564.23795     -148.40521      0             -148.11348      9447.4683    \n",
            "      2586   502.49101     -148.36274      0             -148.10294      10175.054    \n",
            "      2587   427.26843     -148.29778      0             -148.07686      10955.99     \n",
            "      2588   348.54705     -148.26807      0             -148.08786      11473.506    \n",
            "      2589   276.02694     -148.20752      0             -148.0648       11795.455    \n",
            "      2590   217.71209     -148.2165       0             -148.10394      11842.472    \n",
            "      2591   178.75787     -148.16956      0             -148.07714      11761.739    \n",
            "      2592   160.49742     -148.16337      0             -148.08039      11498.547    \n",
            "      2593   160.92903     -148.15567      0             -148.07246      10864.319    \n",
            "      2594   175.76648     -148.18006      0             -148.08919      10237.79     \n",
            "      2595   199.75698     -148.19384      0             -148.09055      9476.2004    \n",
            "      2596   228.01527     -148.18374      0             -148.06585      8532.0376    \n",
            "      2597   257.35156     -148.18046      0             -148.0474       7410.2706    \n",
            "      2598   286.75102     -148.22342      0             -148.07516      6313.5478    \n",
            "      2599   317.10178     -148.24075      0             -148.07679      5119.8155    \n",
            "      2600   350.56471     -148.24703      0             -148.06578      3782.3017    \n",
            "      2601   389.63421     -148.25008      0             -148.04862      2626.0011    \n",
            "      2602   435.54853     -148.30137      0             -148.07618      1178.7488    \n",
            "      2603   487.50528     -148.31716      0             -148.0651      -381.87476    \n",
            "      2604   542.08972     -148.36418      0             -148.0839      -1957.0339    \n",
            "      2605   593.66884     -148.37821      0             -148.07126     -3388.673     \n",
            "      2606   635.65768     -148.37921      0             -148.05055     -4713.3727    \n",
            "      2607   661.54773     -148.4129       0             -148.07085     -5911.8417    \n",
            "      2608   666.56196     -148.37973      0             -148.03509     -6985.3897    \n",
            "      2609   648.76863     -148.38939      0             -148.05395     -7836.3769    \n",
            "      2610   610.37587     -148.35118      0             -148.03559     -8290.4525    \n",
            "      2611   556.84135     -148.30925      0             -148.02134     -8533.0488    \n",
            "      2612   495.85505     -148.2856       0             -148.02922     -8659.8919    \n",
            "      2613   436.47355     -148.2777       0             -148.05202     -8523.843     \n",
            "      2614   386.83881     -148.22335      0             -148.02334     -8189.9441    \n",
            "      2615   352.53559     -148.20258      0             -148.02031     -7753.4166    \n",
            "      2616   335.76622     -148.21111      0             -148.0375      -7185.0661    \n",
            "      2617   334.97882     -148.20959      0             -148.03639     -6402.6431    \n",
            "      2618   345.62358     -148.1957       0             -148.017       -5545.5038    \n",
            "      2619   361.3811      -148.21103      0             -148.02418     -4459.4484    \n",
            "      2620   376.06974     -148.23911      0             -148.04466     -3253.2126    \n",
            "      2621   384.96189     -148.23317      0             -148.03413     -1712.9333    \n",
            "      2622   386.07732     -148.25914      0             -148.05953     -291.80447    \n",
            "      2623   381.04614     -148.22944      0             -148.03242      1440.8209    \n",
            "      2624   374.17314     -148.22309      0             -148.02963      3255.4884    \n",
            "      2625   371.11865     -148.20328      0             -148.0114       5105.1679    \n",
            "      2626   377.76176     -148.24141      0             -148.0461       6703.2449    \n",
            "      2627   398.29473     -148.21557      0             -148.00964      8487.4316    \n",
            "      2628   433.69536     -148.22289      0             -147.99865      9964.5429    \n",
            "      2629   481.15962     -148.27312      0             -148.02434      11208.552    \n",
            "      2630   534.41027     -148.28308      0             -148.00677      12363.788    \n",
            "      2631   585.51489     -148.29445      0             -147.99171      13372.379    \n",
            "      2632   626.45978     -148.32852      0             -148.00462      14110.875    \n",
            "      2633   650.67258     -148.35262      0             -148.01619      14826.796    \n",
            "      2634   655.41713     -148.37615      0             -148.03727      15282.12     \n",
            "      2635   642.12712     -148.3471       0             -148.01509      15685.824    \n",
            "      2636   615.84726     -148.34472      0             -148.02631      15732.915    \n",
            "      2637   584.04284     -148.32252      0             -148.02055      15657.146    \n",
            "      2638   554.63667     -148.30139      0             -148.01462      15304.221    \n",
            "      2639   533.5474      -148.27125      0             -147.99538      14624.264    \n",
            "      2640   523.0594      -148.28424      0             -148.0138       13775.101    \n",
            "      2641   521.12374     -148.28849      0             -148.01904      12672.545    \n",
            "      2642   522.12347     -148.28362      0             -148.01366      11395.531    \n",
            "      2643   518.8081      -148.27439      0             -148.00615      10050.83     \n",
            "      2644   504.42165     -148.26349      0             -148.00268      8641.6019    \n",
            "      2645   474.41094     -148.24202      0             -147.99673      7260.3422    \n",
            "      2646   428.4933      -148.22025      0             -147.99871      5712.8749    \n",
            "      2647   370.98128     -148.1918       0             -147.99999      4468.591     \n",
            "      2648   310.06468     -148.17084      0             -148.01052      3026.9027    \n",
            "      2649   256.06169     -148.11678      0             -147.98439      1768.8128    \n",
            "      2650   218.47171     -148.08773      0             -147.97478      417.94687    \n",
            "      2651   204.24332     -148.10075      0             -147.99515     -825.41857    \n",
            "      2652   216.0715      -148.09009      0             -147.97837     -2071.2494    \n",
            "      2653   251.54322     -148.10929      0             -147.97923     -3256.5882    \n",
            "      2654   304.20417     -148.15961      0             -148.00233     -4475.0894    \n",
            "      2655   365.34162     -148.17908      0             -147.99018     -5422.4419    \n",
            "      2656   426.05648     -148.20932      0             -147.98904     -6321.0005    \n",
            "      2657   479.31972     -148.22042      0             -147.97259     -6972.0422    \n",
            "      2658   521.60128     -148.25213      0             -147.98244     -7507.1763    \n",
            "      2659   553.20388     -148.28439      0             -147.99836     -7726.5777    \n",
            "      2660   577.55148     -148.2837       0             -147.98508     -7724.7851    \n",
            "      2661   599.64477     -148.28501      0             -147.97497     -7598.0286    \n",
            "      2662   623.8741      -148.3          0             -147.97743     -7403.9091    \n",
            "      2663   652.19689     -148.32443      0             -147.98722     -7196.5243    \n",
            "      2664   683.34804     -148.35535      0             -148.00203     -6835.1216    \n",
            "      2665   712.66647     -148.3341       0             -147.96562     -6298.7319    \n",
            "      2666   732.67613     -148.35356      0             -147.97474     -5851.4116    \n",
            "      2667   735.38978     -148.35902      0             -147.97879     -5461.7897    \n",
            "      2668   714.11699     -148.35247      0             -147.98324     -4833.4763    \n",
            "      2669   665.95199     -148.30029      0             -147.95597     -4129.3836    \n",
            "      2670   592.58762     -148.2782       0             -147.97181     -3399.0453    \n",
            "      2671   500.19565     -148.24753      0             -147.98891     -2565.6445    \n",
            "      2672   398.62308     -148.18655      0             -147.98045     -1998.5259    \n",
            "      2673   299.47057     -148.14622      0             -147.99138     -1270.0994    \n",
            "      2674   214.42578     -148.098        0             -147.98714     -997.84891    \n",
            "      2675   152.9719      -148.07995      0             -148.00086     -675.00226    \n",
            "      2676   121.11839     -148.04026      0             -147.97764     -418.46992    \n",
            "      2677   120.42707     -148.02146      0             -147.9592      -490.50202    \n",
            "      2678   148.00432     -148.0743       0             -147.99778     -787.07251    \n",
            "      2679   197.96116     -148.07544      0             -147.97308     -1250.2076    \n",
            "      2680   262.27677     -148.14515      0             -148.00954     -1840.6778    \n",
            "      2681   332.44557     -148.13472      0             -147.96284     -2469.9041    \n",
            "      2682   401.27705     -148.20049      0             -147.99301     -3214.8944    \n",
            "      2683   463.74685     -148.23353      0             -147.99375     -3971.7863    \n",
            "      2684   518.10759     -148.25542      0             -147.98753     -4725.2386    \n",
            "      2685   564.79464     -148.26651      0             -147.97448     -5685.7706    \n",
            "      2686   605.84562     -148.29522      0             -147.98197     -6561.8585    \n",
            "      2687   643.93744     -148.32656      0             -147.99362     -7650.1207    \n",
            "      2688   680.35619     -148.31501      0             -147.96324     -8702.8053    \n",
            "      2689   714.29057     -148.33718      0             -147.96786     -9886.9471    \n",
            "      2690   742.5876      -148.35826      0             -147.97431     -11142.21     \n",
            "      2691   759.90008     -148.36114      0             -147.96824     -12309.267    \n",
            "      2692   760.13325     -148.34924      0             -147.95622     -13169.568    \n",
            "      2693   738.38619     -148.35413      0             -147.97236     -14201.102    \n",
            "      2694   692.32967     -148.33661      0             -147.97865     -14804.779    \n",
            "      2695   623.26557     -148.3201       0             -147.99785     -15389.246    \n",
            "      2696   536.48416     -148.24297      0             -147.96558     -15547.945    \n",
            "      2697   441.23184     -148.19515      0             -147.96701     -15804.581    \n",
            "      2698   348.71087     -148.16141      0             -147.98111     -15670.487    \n",
            "      2699   270.06621     -148.12356      0             -147.98392     -15431.305    \n",
            "      2700   214.41903     -148.11952      0             -148.00866     -15123.024    \n",
            "      2701   186.8792      -148.08698      0             -147.99036     -14666.523    \n",
            "      2702   188.034       -148.10345      0             -148.00623     -14150.409    \n",
            "      2703   213.67554     -148.09298      0             -147.9825      -13396.033    \n",
            "      2704   256.47075     -148.10318      0             -147.97057     -12717.905    \n",
            "      2705   307.69645     -148.12654      0             -147.96745     -11819.648    \n",
            "      2706   358.7136      -148.17114      0             -147.98567     -10622.562    \n",
            "      2707   403.52527     -148.224        0             -148.01536     -9414.5137    \n",
            "      2708   439.76487     -148.19692      0             -147.96954     -7896.2767    \n",
            "      2709   468.26791     -148.22241      0             -147.9803      -6389.5576    \n",
            "      2710   492.72735     -148.23425      0             -147.97949     -4536.399     \n",
            "      2711   518.19118     -148.249        0             -147.98108     -2815.3789    \n",
            "      2712   548.57351     -148.28257      0             -147.99893     -1166.9793    \n",
            "      2713   585.2442      -148.26091      0             -147.95831      594.72058    \n",
            "      2714   625.94172     -148.33767      0             -148.01403      2197.924     \n",
            "      2715   665.29289     -148.35416      0             -148.01018      3587.2878    \n",
            "      2716   695.48208     -148.34764      0             -147.98805      5092.1594    \n",
            "      2717   708.81993     -148.36687      0             -148.00038      6332.7955    \n",
            "      2718   699.74995     -148.35919      0             -147.99739      7609.9807    \n",
            "      2719   665.97662     -148.35246      0             -148.00812      8669.7588    \n",
            "      2720   610.25763     -148.31652      0             -148.00099      9688.739     \n",
            "      2721   540.09743     -148.27719      0             -147.99794      10514.809    \n",
            "      2722   465.90655     -148.22035      0             -147.97946      11094.877    \n",
            "      2723   398.61247     -148.20081      0             -147.99472      11555.255    \n",
            "      2724   347.65253     -148.19836      0             -148.01861      11671.221    \n",
            "      2725   318.67274     -148.1506       0             -147.98583      11402.603    \n",
            "      2726   312.55716     -148.14771      0             -147.98611      10830.333    \n",
            "      2727   325.41096     -148.16616      0             -147.99791      10023.894    \n",
            "      2728   349.93819     -148.17426      0             -147.99332      9192.9632    \n",
            "      2729   377.56485     -148.19485      0             -147.99964      7949.4946    \n",
            "      2730   400.49619     -148.19279      0             -147.98572      6802.1708    \n",
            "      2731   413.63956     -148.20025      0             -147.98639      5439.5244    \n",
            "      2732   415.35545     -148.2194       0             -148.00465      4121.9841    \n",
            "      2733   408.1965      -148.21585      0             -148.0048       2666.5909    \n",
            "      2734   397.65244     -148.19846      0             -147.99286      1142.2331    \n",
            "      2735   390.13078     -148.20219      0             -148.00048     -382.83185    \n",
            "      2736   391.2285      -148.18576      0             -147.98348     -1888.1403    \n",
            "      2737   403.79643     -148.25165      0             -148.04287     -3646.738     \n",
            "      2738   427.31665     -148.24238      0             -148.02144     -5213.029     \n",
            "      2739   457.90291     -148.25322      0             -148.01646     -6763.0383    \n",
            "      2740   489.50089     -148.25668      0             -148.00359     -8252.4763    \n",
            "      2741   515.23543     -148.28669      0             -148.0203      -9770.7376    \n",
            "      2742   529.5632      -148.31055      0             -148.03674     -10889.256    \n",
            "      2743   530.08351     -148.28314      0             -148.00907     -11698.754    \n",
            "      2744   517.31648     -148.31065      0             -148.04318     -12429.852    \n",
            "      2745   495.54185     -148.26655      0             -148.01033     -12863.329    \n",
            "      2746   471.28483     -148.27051      0             -148.02684     -13032.292    \n",
            "      2747   451.35307     -148.24912      0             -148.01575     -13104.129    \n",
            "      2748   441.48998     -148.24874      0             -148.02047     -12765.224    \n",
            "      2749   444.18362     -148.29884      0             -148.06918     -12496.595    \n",
            "      2750   458.53667     -148.27541      0             -148.03833     -11922.086    \n",
            "      2751   480.37287     -148.29577      0             -148.04739     -11377.928    \n",
            "      2752   502.65045     -148.27895      0             -148.01906     -10512.262    \n",
            "      2753   517.69214     -148.29091      0             -148.02324     -9450.692     \n",
            "      2754   518.93649     -148.32104      0             -148.05273     -8293.059     \n",
            "      2755   501.94302     -148.3032       0             -148.04368     -6822.5552    \n",
            "      2756   466.58291     -148.30599      0             -148.06475     -5406.6293    \n",
            "      2757   416.35087     -148.27995      0             -148.06468     -3797.2757    \n",
            "      2758   357.4523      -148.2432       0             -148.05839     -2078.1617    \n",
            "      2759   298.59326     -148.22411      0             -148.06973     -388.20614    \n",
            "      2760   248.08132     -148.20955      0             -148.08128      1164.4281    \n",
            "      2761   212.66123     -148.13967      0             -148.02972      2675.4126    \n",
            "      2762   196.71795     -148.13791      0             -148.0362       4007.0673    \n",
            "      2763   201.14658     -148.18027      0             -148.07627      5101.235     \n",
            "      2764   223.509       -148.18081      0             -148.06524      5986.5281    \n",
            "      2765   258.94653     -148.19633      0             -148.06244      6823.0416    \n",
            "      2766   301.41752     -148.20769      0             -148.05184      7488.0991    \n",
            "      2767   345.20494     -148.24182      0             -148.06334      7738.4677    \n",
            "      2768   385.72996     -148.26996      0             -148.07052      7931.708     \n",
            "      2769   420.38937     -148.29988      0             -148.08252      8115.3963    \n",
            "      2770   449.2971      -148.30827      0             -148.07597      8129.5576    \n",
            "      2771   474.13647     -148.31728      0             -148.07213      7743.432     \n",
            "      2772   497.3094      -148.32988      0             -148.07275      7383.946     \n",
            "      2773   521.03352     -148.34737      0             -148.07798      6575.5888    \n",
            "      2774   545.52001     -148.36828      0             -148.08622      5722.7344    \n",
            "      2775   568.77639     -148.34101      0             -148.04693      4861.2056    \n",
            "      2776   586.61653     -148.41292      0             -148.10961      3711.9845    \n",
            "      2777   593.53958     -148.40593      0             -148.09904      2471.4164    \n",
            "      2778   584.1323      -148.41036      0             -148.10834      1336.5873    \n",
            "      2779   554.73142     -148.39203      0             -148.10521      281.78105    \n",
            "      2780   504.40876     -148.33682      0             -148.07602     -694.43323    \n",
            "      2781   435.96892     -148.33344      0             -148.10802     -1651.6932    \n",
            "      2782   356.06968     -148.30592      0             -148.12182     -2355.8525    \n",
            "      2783   273.68104     -148.23566      0             -148.09416     -2834.1486    \n",
            "      2784   198.81393     -148.21205      0             -148.10926     -3267.4232    \n",
            "      2785   140.83333     -148.14748      0             -148.07467     -3403.5921    \n",
            "      2786   106.27956     -148.16096      0             -148.10601     -3661.2924    \n",
            "      2787   97.951461     -148.14883      0             -148.09819     -3690.5234    \n",
            "      2788   114.7874      -148.17985      0             -148.1205      -3666.6546    \n",
            "      2789   151.95747     -148.1778       0             -148.09923     -3316.3574    \n",
            "      2790   202.06257     -148.19853      0             -148.09406     -2897.7583    \n",
            "      2791   257.33632     -148.23544      0             -148.10238     -2172.1713    \n",
            "      2792   311.20421     -148.29894      0             -148.13803     -1292.8659    \n",
            "      2793   359.01638     -148.26669      0             -148.08107     -74.700799    \n",
            "      2794   399.21721     -148.32001      0             -148.1136       1196.7142    \n",
            "      2795   432.53985     -148.34467      0             -148.12103      2597.3238    \n",
            "      2796   461.14738     -148.31957      0             -148.08114      4283.5567    \n",
            "      2797   487.21338     -148.34588      0             -148.09397      5931.8987    \n",
            "      2798   511.57503     -148.39856      0             -148.13406      7533.6036    \n",
            "      2799   532.90931     -148.3995       0             -148.12396      9102.5189    \n",
            "      2800   547.64735     -148.40479      0             -148.12163      10609.555    \n",
            "      2801   550.74965     -148.41332      0             -148.12856      12278.378    \n",
            "      2802   537.57236     -148.40856      0             -148.13061      13659.542    \n",
            "      2803   504.9615      -148.36924      0             -148.10815      14926.013    \n",
            "      2804   452.80455     -148.37266      0             -148.13854      16098.815    \n",
            "      2805   384.95289     -148.33601      0             -148.13698      17121.226    \n",
            "      2806   308.29595     -148.2967       0             -148.1373       17963.083    \n",
            "      2807   232.0449      -148.26051      0             -148.14053      18556.391    \n",
            "      2808   165.86849     -148.23159      0             -148.14583      18886.582    \n",
            "      2809   118.33144     -148.211        0             -148.14982      18832.987    \n",
            "      2810   94.85293      -148.18806      0             -148.13901      18529.823    \n",
            "      2811   96.79073      -148.17819      0             -148.12815      17955.123    \n",
            "      2812   121.57949     -148.21116      0             -148.14829      17045.911    \n",
            "      2813   163.32349     -148.23362      0             -148.14917      15956.118    \n",
            "      2814   214.26767     -148.26339      0             -148.1526       14577.339    \n",
            "      2815   266.57689     -148.27491      0             -148.13708      13032.829    \n",
            "      2816   313.76893     -148.31649      0             -148.15426      11270.424    \n",
            "      2817   351.63348     -148.35074      0             -148.16893      9563.058     \n",
            "      2818   378.90518     -148.33437      0             -148.13846      7557.3833    \n",
            "      2819   396.56376     -148.34042      0             -148.13538      5586.6221    \n",
            "      2820   407.36763     -148.36825      0             -148.15762      3630.2693    \n",
            "      2821   414.20665     -148.37703      0             -148.16287      1536.7411    \n",
            "      2822   418.99088     -148.3888       0             -148.17217     -336.40451    \n",
            "      2823   422.27531     -148.38191      0             -148.16358     -2444.1192    \n",
            "      2824   422.88072     -148.38226      0             -148.16361     -4165.7887    \n",
            "      2825   418.48167     -148.37311      0             -148.15674     -6028.2565    \n",
            "      2826   406.40674     -148.38119      0             -148.17106     -7621.8287    \n",
            "      2827   384.6177      -148.36019      0             -148.16133     -9015.7239    \n",
            "      2828   352.95433     -148.35423      0             -148.17174     -10104.833    \n",
            "      2829   313.39803     -148.36984      0             -148.2078      -11093.25     \n",
            "      2830   269.94742     -148.31332      0             -148.17374     -11518.93     \n",
            "      2831   227.83811     -148.31757      0             -148.19977     -11942.441    \n",
            "      2832   192.72722     -148.27705      0             -148.1774      -12016.035    \n",
            "      2833   169.67978     -148.26843      0             -148.1807      -11959.538    \n",
            "      2834   161.64893     -148.25497      0             -148.17139     -11623.939    \n",
            "      2835   169.14746     -148.28507      0             -148.19762     -11178.916    \n",
            "      2836   190.00199     -148.26163      0             -148.16339     -10621.885    \n",
            "      2837   219.6448      -148.28478      0             -148.17122     -9846.3487    \n",
            "      2838   252.14609     -148.33936      0             -148.20899     -8938.7441    \n",
            "      2839   281.62288     -148.34315      0             -148.19754     -7901.7852    \n",
            "      2840   303.30561     -148.34332      0             -148.1865      -6657.9379    \n",
            "      2841   314.06167     -148.33871      0             -148.17633     -5358.0671    \n",
            "      2842   313.38892     -148.35609      0             -148.19406     -3950.3043    \n",
            "      2843   303.30756     -148.35435      0             -148.19753     -2461.4163    \n",
            "      2844   287.58931     -148.34614      0             -148.19745     -983.76515    \n",
            "      2845   270.81888     -148.34158      0             -148.20155      445.07217    \n",
            "      2846   257.34112     -148.32257      0             -148.18952      1813.666     \n",
            "      2847   250.22323     -148.2978       0             -148.16842      2999.7549    \n",
            "      2848   250.66492     -148.32079      0             -148.19119      4106.9602    \n",
            "      2849   257.90174     -148.34154      0             -148.2082       5011.4567    \n",
            "      2850   269.48401     -148.33295      0             -148.19362      5589.8206    \n",
            "      2851   282.18528     -148.35269      0             -148.20678      6230.7406    \n",
            "      2852   292.61661     -148.3612       0             -148.20991      6566.8902    \n",
            "      2853   298.51906     -148.36943      0             -148.21508      6798.4074    \n",
            "      2854   299.11175     -148.36149      0             -148.20684      6846.8388    \n",
            "      2855   295.46697     -148.37821      0             -148.22544      6658.4707    \n",
            "      2856   290.05652     -148.37847      0             -148.22849      6329.0815    \n",
            "      2857   285.63306     -148.33232      0             -148.18464      6041.4345    \n",
            "      2858   284.92349     -148.38855      0             -148.24124      5384.3572    \n",
            "      2859   289.30262     -148.35134      0             -148.20176      4565.1532    \n",
            "      2860   298.35788     -148.36446      0             -148.21019      3655.0099    \n",
            "      2861   309.76763     -148.41789      0             -148.25772      2560.0223    \n",
            "      2862   319.77045     -148.38844      0             -148.2231       1560.6071    \n",
            "      2863   323.92045     -148.37759      0             -148.21011      395.64744    \n",
            "      2864   318.64387     -148.41078      0             -148.24603     -697.76912    \n",
            "      2865   301.80293     -148.388        0             -148.23196     -1897.8576    \n",
            "      2866   273.37991     -148.35897      0             -148.21762     -2758.4291    \n",
            "      2867   236.85556     -148.36127      0             -148.23881     -3715.3587    \n",
            "      2868   197.08362     -148.32364      0             -148.22174     -4424.8081    \n",
            "      2869   159.92373     -148.30665      0             -148.22396     -5159.1857    \n",
            "      2870   131.19735     -148.30472      0             -148.23688     -5918.3193    \n",
            "      2871   115.1555      -148.30799      0             -148.24845     -6412.7039    \n",
            "      2872   113.84744     -148.29219      0             -148.23333     -6790.695     \n",
            "      2873   126.62287     -148.30347      0             -148.238       -7108.368     \n",
            "      2874   150.47986     -148.30751      0             -148.22971     -7277.7559    \n",
            "      2875   180.93071     -148.33532      0             -148.24177     -7317.3022    \n",
            "      2876   213.05274     -148.35888      0             -148.24873     -7308.1572    \n",
            "      2877   242.61256     -148.36855      0             -148.24311     -6995.3718    \n",
            "      2878   266.9146      -148.35227      0             -148.21426     -6566.3316    \n",
            "      2879   285.19948     -148.37108      0             -148.22362     -5958.4323    \n",
            "      2880   298.60318     -148.39302      0             -148.23863     -5253.6041    \n",
            "      2881   308.91454     -148.40917      0             -148.24945     -4394.9831    \n",
            "      2882   318.01175     -148.39541      0             -148.23098     -3556.0388    \n",
            "      2883   327.01853     -148.41374      0             -148.24466     -2664.0411    \n",
            "      2884   335.45486     -148.41612      0             -148.24267     -1681.44      \n",
            "      2885   341.14473     -148.40823      0             -148.23185     -616.59153    \n",
            "      2886   341.191       -148.43435      0             -148.25794      218.48269    \n",
            "      2887   332.34408     -148.39569      0             -148.22385      1136.2749    \n",
            "      2888   312.18673     -148.39634      0             -148.23493      1929.5287    \n",
            "      2889   280.26758     -148.37899      0             -148.23408      2857.0914    \n",
            "      2890   238.31359     -148.36622      0             -148.243        3560.7928    \n",
            "      2891   190.44444     -148.34756      0             -148.2491       4246.5873    \n",
            "      2892   142.12433     -148.32989      0             -148.2564       4818.5276    \n",
            "      2893   99.361872     -148.30622      0             -148.25485      5081.2506    \n",
            "      2894   68.074437     -148.29915      0             -148.26395      5346.5336    \n",
            "      2895   52.272516     -148.26352      0             -148.23649      5375.1488    \n",
            "      2896   53.553615     -148.30038      0             -148.27269      5093.2477    \n",
            "      2897   71.176725     -148.29653      0             -148.25973      4805.4334    \n",
            "      2898   101.93572     -148.3061       0             -148.25339      4180.5483    \n",
            "      2899   141.07049     -148.33428      0             -148.26134      3460.8166    \n",
            "      2900   183.17302     -148.35317      0             -148.25846      2607.4266    \n",
            "      2901   223.17088     -148.37836      0             -148.26297      1469.8838    \n",
            "      2902   257.42425     -148.40083      0             -148.26773      393.45927    \n",
            "      2903   283.90835     -148.3975       0             -148.25071     -668.40464    \n",
            "      2904   302.05796     -148.41924      0             -148.26306     -2037.1334    \n",
            "      2905   312.90964     -148.41842      0             -148.25664     -3408.5401    \n",
            "      2906   318.09241     -148.42781      0             -148.26334     -4672.8518    \n",
            "      2907   319.01408     -148.42816      0             -148.26321     -6057.9506    \n",
            "      2908   316.54663     -148.43289      0             -148.26922     -7459.6487    \n",
            "      2909   310.65486     -148.43388      0             -148.27326     -8751.9258    \n",
            "      2910   300.46964     -148.42491      0             -148.26955     -10070.555    \n",
            "      2911   284.65706     -148.40825      0             -148.26107     -11290.566    \n",
            "      2912   262.00721     -148.40134      0             -148.26587     -12251.163    \n",
            "      2913   232.19867     -148.39014      0             -148.27008     -13194.911    \n",
            "      2914   196.2843      -148.35199      0             -148.2505      -13835.971    \n",
            "      2915   156.79102     -148.36495      0             -148.28388     -14331.853    \n",
            "      2916   117.54279     -148.31793      0             -148.25715     -14661.999    \n",
            "      2917   83.169713     -148.31646      0             -148.27346     -14849.256    \n",
            "      2918   58.258781     -148.30103      0             -148.27091     -14830.606    \n",
            "      2919   46.266993     -148.29726      0             -148.27334     -14647.473    \n",
            "      2920   48.985666     -148.27828      0             -148.25295     -14205.331    \n",
            "      2921   66.143841     -148.28669      0             -148.25249     -13667.649    \n",
            "      2922   95.343283     -148.31441      0             -148.26512     -12987.6      \n",
            "      2923   132.70964     -148.36717      0             -148.29855     -12305.658    \n",
            "      2924   173.17801     -148.3749       0             -148.28536     -11259.286    \n",
            "      2925   211.47153     -148.38021      0             -148.27088     -10023.102    \n",
            "      2926   243.49877     -148.39681      0             -148.27091     -8785.2338    \n",
            "      2927   266.72585     -148.41321      0             -148.2753      -7349.2707    \n",
            "      2928   280.04466     -148.4171       0             -148.2723      -5689.0037    \n",
            "      2929   284.35572     -148.42473      0             -148.27771     -4187.7698    \n",
            "      2930   281.84604     -148.40883      0             -148.26311     -2502.1806    \n",
            "      2931   275.12751     -148.42718      0             -148.28493     -814.53242    \n",
            "      2932   266.66109     -148.44672      0             -148.30884      714.61389    \n",
            "      2933   258.11475     -148.39093      0             -148.25748      2216.2122    \n",
            "      2934   249.93432     -148.41999      0             -148.29077      3565.0038    \n",
            "      2935   241.3242      -148.40867      0             -148.2839       4852.6035    \n",
            "      2936   231.04896     -148.41839      0             -148.29893      5948.8787    \n",
            "      2937   218.10504     -148.37145      0             -148.25869      6946.1976    \n",
            "      2938   202.01099     -148.40199      0             -148.29754      7845.5744    \n",
            "      2939   183.20518     -148.35632      0             -148.26159      8471.7864    \n",
            "      2940   163.64219     -148.33716      0             -148.25255      8988.4481    \n",
            "      2941   146.07102     -148.36047      0             -148.28495      9300.117     \n",
            "      2942   133.46187     -148.35487      0             -148.28586      9423.7681    \n",
            "      2943   128.7999      -148.34304      0             -148.27644      9292.8153    \n",
            "      2944   133.79208     -148.35964      0             -148.29047      9003.05      \n",
            "      2945   148.31969     -148.35765      0             -148.28096      8514.0344    \n",
            "      2946   170.39814     -148.38004      0             -148.29193      7796.8892    \n",
            "      2947   196.402       -148.39555      0             -148.294        6990.4866    \n",
            "      2948   221.80459     -148.41647      0             -148.30179      5964.5284    \n",
            "      2949   241.95869     -148.36769      0             -148.24259      5036.2595    \n",
            "      2950   253.22644     -148.414        0             -148.28307      4000.0925    \n",
            "      2951   253.99588     -148.4146       0             -148.28328      2895.568     \n",
            "      2952   244.81911     -148.40813      0             -148.28155      1848.6131    \n",
            "      2953   227.95567     -148.40798      0             -148.29012      904.09222    \n",
            "      2954   207.11877     -148.39738      0             -148.29029     -79.394605    \n",
            "      2955   186.64869     -148.38823      0             -148.29173     -1073.8366    \n",
            "      2956   170.40114     -148.37028      0             -148.28217     -1886.4206    \n",
            "      2957   160.85687     -148.36917      0             -148.286       -2597.5722    \n",
            "      2958   158.69991     -148.39991      0             -148.31786     -3289.6528    \n",
            "      2959   163.02308     -148.35417      0             -148.26988     -3719.372     \n",
            "      2960   171.66674     -148.3873       0             -148.29854     -4041.6452    \n",
            "      2961   182.13982     -148.39078      0             -148.29661     -4252.8676    \n",
            "      2962   192.03414     -148.40016      0             -148.30087     -4295.0962    \n",
            "      2963   199.77745     -148.40186      0             -148.29857     -4145.1835    \n",
            "      2964   205.34205     -148.38435      0             -148.27818     -3788.9453    \n",
            "      2965   209.58265     -148.40633      0             -148.29796     -3234.6721    \n",
            "      2966   213.78794     -148.41054      0             -148.30001     -2606.5926    \n",
            "      2967   219.57901     -148.42827      0             -148.31474     -1659.7347    \n",
            "      2968   227.82305     -148.41825      0             -148.30046     -780.72769    \n",
            "      2969   238.04161     -148.45164      0             -148.32856      270.12079    \n",
            "      2970   248.6262      -148.42621      0             -148.29766      1293.7008    \n",
            "      2971   256.85061     -148.42921      0             -148.29641      2338.5641    \n",
            "      2972   259.41222     -148.41718      0             -148.28305      3547.0922    \n",
            "      2973   254.00666     -148.43607      0             -148.30474      4765.676     \n",
            "      2974   239.33946     -148.43444      0             -148.31069      5730.1313    \n",
            "      2975   215.6465      -148.39671      0             -148.28521      6818.9422    \n",
            "      2976   185.20312     -148.40425      0             -148.30849      7882.5077    \n",
            "      2977   151.80631     -148.39634      0             -148.31785      8681.8542    \n",
            "      2978   120.36089     -148.36853      0             -148.3063       9395.651     \n",
            "      2979   95.568541     -148.35023      0             -148.30081      10059.579    \n",
            "      2980   81.366495     -148.34971      0             -148.30764      10433.548    \n",
            "      2981   79.937375     -148.33835      0             -148.29702      10543.507    \n",
            "      2982   91.371963     -148.35832      0             -148.31108      10468.047    \n",
            "      2983   113.76888     -148.35556      0             -148.29674      10155.743    \n",
            "      2984   143.64321     -148.36938      0             -148.29511      9669.0153    \n",
            "      2985   176.68192     -148.40032      0             -148.30897      9017.4145    \n",
            "      2986   208.66383     -148.38784      0             -148.27995      8239.4438    \n",
            "      2987   235.93113     -148.42154      0             -148.29956      7188.0088    \n",
            "      2988   256.1269      -148.43268      0             -148.30025      6037.8031    \n",
            "      2989   268.23911     -148.41656      0             -148.27787      4870.5996    \n",
            "      2990   272.72367     -148.42828      0             -148.28727      3596.666     \n",
            "      2991   271.08826     -148.44577      0             -148.3056       2167.2956    \n",
            "      2992   264.61384     -148.448        0             -148.31119      703.50976    \n",
            "      2993   254.63329     -148.4384       0             -148.30674     -636.9167     \n",
            "      2994   241.78032     -148.44068      0             -148.31567     -2035.5233    \n",
            "      2995   226.00681     -148.42494      0             -148.30808     -3307.8602    \n",
            "      2996   206.89123     -148.39144      0             -148.28447     -4616.2025    \n",
            "      2997   183.98425     -148.41489      0             -148.31976     -5768.6211    \n",
            "      2998   157.00444     -148.3631       0             -148.28192     -6773.4693    \n",
            "      2999   126.92475     -148.38098      0             -148.31536     -7343.3954    \n",
            "      3000   96.108828     -148.3636       0             -148.31391     -7991.2446    \n",
            "      3001   67.496412     -148.34315      0             -148.30825     -8287.4587    \n",
            "      3002   44.721047     -148.32229      0             -148.29917     -8348.9133    \n",
            "      3003   31.486994     -148.32345      0             -148.30717     -8396.7486    \n",
            "      3004   30.751043     -148.30601      0             -148.29011     -8059.1992    \n",
            "      3005   43.883312     -148.3301       0             -148.30741     -7581.5494    \n",
            "      3006   70.317907     -148.33862      0             -148.30227     -6890.7201    \n",
            "      3007   107.57184     -148.33255      0             -148.27693     -6029.0481    \n",
            "      3008   151.44028     -148.39644      0             -148.31814     -5079.9978    \n",
            "      3009   196.67053     -148.40065      0             -148.29896     -3858.4603    \n",
            "      3010   237.80868     -148.45169      0             -148.32874     -2619.7954    \n",
            "      3011   270.14268     -148.45187      0             -148.3122      -1175.3661    \n",
            "      3012   290.58343     -148.44299      0             -148.29274      361.73808    \n",
            "      3013   298.17616     -148.43706      0             -148.28289      2104.3456    \n",
            "      3014   293.76835     -148.44332      0             -148.29143      3791.8149    \n",
            "      3015   279.5209      -148.44063      0             -148.29611      5415.4212    \n",
            "      3016   258.69808     -148.42119      0             -148.28743      7091.127     \n",
            "      3017   234.79089     -148.42849      0             -148.30709      8625.207     \n",
            "      3018   210.50283     -148.43887      0             -148.33003      10051.01     \n",
            "      3019   187.55438     -148.36703      0             -148.27005      11300.073    \n",
            "      3020   166.42433     -148.39708      0             -148.31103      12278.761    \n",
            "      3021   146.93694     -148.36716      0             -148.29119      13101.42     \n",
            "      3022   128.65775     -148.33534      0             -148.26882      13797.144    \n",
            "      3023   111.41439     -148.35382      0             -148.29621      14215.95     \n",
            "      3024   95.702375     -148.34657      0             -148.29709      14449.526    \n",
            "      3025   82.95608      -148.35374      0             -148.31085      14517.489    \n",
            "      3026   75.380496     -148.31449      0             -148.27552      14264.725    \n",
            "      3027   75.223506     -148.33521      0             -148.29631      13876.517    \n",
            "      3028   84.408541     -148.33806      0             -148.29442      13181.774    \n",
            "      3029   103.58285     -148.34202      0             -148.28846      12275.605    \n",
            "      3030   131.73182     -148.37408      0             -148.30597      11249.647    \n",
            "      3031   166.01605     -148.37561      0             -148.28978      9973.3814    \n",
            "      3032   202.12558     -148.41079      0             -148.30629      8626.9112    \n",
            "      3033   235.2835      -148.40059      0             -148.27894      7075.7468    \n",
            "      3034   260.79431     -148.43094      0             -148.29609      5590.1768    \n",
            "      3035   275.13414     -148.43877      0             -148.29651      4057.9986    \n",
            "      3036   276.74563     -148.43585      0             -148.29276      2610.4881    \n",
            "      3037   266.34276     -148.41184      0             -148.27413      1000.8663    \n",
            "      3038   246.8134      -148.40768      0             -148.28007     -303.18355    \n",
            "      3039   222.19461     -148.41228      0             -148.2974      -1511.8314    \n",
            "      3040   197.06715     -148.39103      0             -148.28914     -2765.8642    \n",
            "      3041   175.59298     -148.41084      0             -148.32005     -3980.0549    \n",
            "      3042   160.60838     -148.4093       0             -148.32626     -4924.1222    \n",
            "      3043   153.27983     -148.36942      0             -148.29017     -5633.8243    \n",
            "      3044   152.89587     -148.36685      0             -148.2878      -6374.8709    \n",
            "      3045   157.37503     -148.35379      0             -148.27242     -6841.972     \n",
            "      3046   164.41953     -148.38852      0             -148.30351     -7140.7464    \n",
            "      3047   171.82954     -148.39238      0             -148.30354     -7280.115     \n",
            "      3048   177.97235     -148.38103      0             -148.28901     -7121.7799    \n",
            "      3049   182.64732     -148.42717      0             -148.33274     -6905.2936    \n",
            "      3050   186.66231     -148.36609      0             -148.26958     -6380.8358    \n",
            "      3051   191.11273     -148.39439      0             -148.29557     -5815.7648    \n",
            "      3052   197.36578     -148.39851      0             -148.29647     -5098.8238    \n",
            "      3053   206.33358     -148.42429      0             -148.3176      -4292.0386    \n",
            "      3054   217.77371     -148.40581      0             -148.29321     -3530.3833    \n",
            "      3055   230.23319     -148.42117      0             -148.30213     -2666.9497    \n",
            "      3056   241.1846      -148.42488      0             -148.30018     -1873.9228    \n",
            "      3057   247.73717     -148.44502      0             -148.31693     -833.94437    \n",
            "      3058   247.32659     -148.39699      0             -148.26911      94.176497    \n",
            "      3059   238.44353     -148.41735      0             -148.29407      927.05992    \n",
            "      3060   221.22801     -148.39714      0             -148.28276      1781.1515    \n",
            "      3061   197.65934     -148.39954      0             -148.29734      2439.0377    \n",
            "      3062   170.99174     -148.36477      0             -148.27636      3064.0388    \n",
            "      3063   145.59363     -148.33084      0             -148.25556      3651.0536    \n",
            "      3064   126.05921     -148.37422      0             -148.30904      4011.9953    \n",
            "      3065   115.87688     -148.35305      0             -148.29313      4089.5287    \n",
            "      3066   117.09369     -148.34247      0             -148.28193      4031.6428    \n",
            "      3067   129.80415     -148.34291      0             -148.27579      3796.6865    \n",
            "      3068   152.07324     -148.34891      0             -148.27028      3302.76      \n",
            "      3069   180.65501     -148.39696      0             -148.30355      2697.1276    \n",
            "      3070   211.30562     -148.35082      0             -148.24157      1970.6873    \n",
            "      3071   239.36543     -148.36989      0             -148.24613      960.4809     \n",
            "      3072   261.06723     -148.42813      0             -148.29314     -58.840836    \n",
            "      3073   274.16808     -148.42242      0             -148.28066     -1172.3409    \n",
            "      3074   277.88913     -148.40193      0             -148.25825     -2315.4839    \n",
            "      3075   273.19391     -148.40933      0             -148.26807     -3577.5466    \n",
            "      3076   262.30419     -148.40881      0             -148.27319     -4720.0378    \n",
            "      3077   247.66102     -148.40052      0             -148.27247     -6015.9497    \n",
            "      3078   231.40439     -148.40503      0             -148.28538     -7308.6923    \n",
            "      3079   215.01255     -148.42049      0             -148.30932     -8484.1575    \n",
            "      3080   198.96467     -148.36304      0             -148.26017     -9493.8755    \n",
            "      3081   182.91484     -148.38162      0             -148.28704     -10497.89     \n",
            "      3082   165.88343     -148.36178      0             -148.27601     -11406.695    \n",
            "      3083   146.96229     -148.364        0             -148.28801     -12086.894    \n",
            "      3084   126.27928     -148.34503      0             -148.27974     -12812.117    \n",
            "      3085   104.99442     -148.33356      0             -148.27928     -12981.795    \n",
            "      3086   85.240868     -148.3035       0             -148.25943     -13163.292    \n",
            "      3087   70.291151     -148.32019      0             -148.28385     -13144.716    \n",
            "      3088   63.822494     -148.3197       0             -148.2867      -12915.826    \n",
            "      3089   68.80566      -148.30644      0             -148.27087     -12520.242    \n",
            "      3090   86.875653     -148.30586      0             -148.26094     -11807.533    \n",
            "      3091   117.84404     -148.35109      0             -148.29016     -11193.598    \n",
            "      3092   159.31825     -148.35357      0             -148.2712      -10297.27     \n",
            "      3093   206.75281     -148.37577      0             -148.26887     -9295.2283    \n",
            "      3094   254.27007     -148.40235      0             -148.27088     -8183.2294    \n",
            "      3095   296.00327     -148.3816       0             -148.22856     -6734.4775    \n",
            "      3096   326.73273     -148.44847      0             -148.27954     -5380.994     \n",
            "      3097   342.77723     -148.42887      0             -148.25164     -3817.1037    \n",
            "      3098   343.08317     -148.45688      0             -148.27949     -2204.875     \n",
            "      3099   328.81629     -148.41954      0             -148.24953     -417.82075    \n",
            "      3100   303.07736     -148.43059      0             -148.27389      1231.4274    \n",
            "      3101   270.21555     -148.36832      0             -148.2286       2791.7298    \n",
            "      3102   234.59598     -148.38286      0             -148.26156      4340.3178    \n",
            "      3103   199.95183     -148.36799      0             -148.2646       5699.7145    \n",
            "      3104   168.55188     -148.33774      0             -148.25059      7044.8486    \n",
            "      3105   141.2834      -148.33364      0             -148.26059      7990.934     \n",
            "      3106   117.98076     -148.32476      0             -148.26376      8984.6062    \n",
            "      3107   97.920227     -148.296        0             -148.24537      9610.871     \n",
            "      3108   80.616424     -148.31144      0             -148.26976      10154.556    \n",
            "      3109   66.426414     -148.33034      0             -148.29599      10446.073    \n",
            "      3110   56.708194     -148.30409      0             -148.27477      10568.384    \n",
            "      3111   53.981248     -148.28132      0             -148.25341      10423.659    \n",
            "      3112   61.127113     -148.28864      0             -148.25704      10114        \n",
            "      3113   80.238071     -148.3045       0             -148.26301      9491.9167    \n",
            "      3114   112.05456     -148.31576      0             -148.25782      8796.7154    \n",
            "      3115   155.41722     -148.33381      0             -148.25346      7760.0546    \n",
            "      3116   206.98635     -148.35743      0             -148.25041      6667.3135    \n",
            "      3117   261.49956     -148.37393      0             -148.23872      5250.6178    \n",
            "      3118   312.50405     -148.40044      0             -148.23887      3820.8172    \n",
            "      3119   353.41221     -148.42753      0             -148.2448       2080.0809    \n",
            "      3120   379.13133     -148.44606      0             -148.25004      540.45269    \n",
            "      3121   386.88657     -148.46111      0             -148.26107     -1045.1169    \n",
            "      3122   376.51442     -148.45029      0             -148.25562     -2679.1889    \n",
            "      3123   351.06972     -148.41543      0             -148.23392     -4046.4291    \n",
            "      3124   315.23912     -148.42171      0             -148.25872     -5521.3056    \n",
            "      3125   275.00618     -148.40206      0             -148.25987     -6819.3614    \n",
            "      3126   236.37229     -148.38967      0             -148.26745     -8017.436     \n",
            "      3127   203.7466      -148.36049      0             -148.25514     -9081.7573    \n",
            "      3128   179.7642      -148.33084      0             -148.2379      -9925.5733    \n",
            "      3129   164.90486     -148.31673      0             -148.23147     -10755.611    \n",
            "      3130   157.9958      -148.32438      0             -148.24269     -11361.674    \n",
            "      3131   156.94143     -148.31663      0             -148.23549     -11665.551    \n",
            "      3132   159.4274      -148.30588      0             -148.22345     -11804.378    \n",
            "      3133   163.91166     -148.31231      0             -148.22757     -11789.147    \n",
            "      3134   169.9764      -148.32797      0             -148.24009     -11423.655    \n",
            "      3135   178.56372     -148.33657      0             -148.24425     -10953.709    \n",
            "      3136   191.42761     -148.31512      0             -148.21614     -10063.094    \n",
            "      3137   210.00014     -148.3229       0             -148.21432     -9142.3082    \n",
            "      3138   235.04031     -148.32796      0             -148.20643     -8005.525     \n",
            "      3139   266.13798     -148.37618      0             -148.23857     -6873.3512    \n",
            "      3140   300.9656      -148.36994      0             -148.21433     -5596.5037    \n",
            "      3141   335.61435     -148.38033      0             -148.2068      -4252.5915    \n",
            "      3142   365.33762     -148.39741      0             -148.20852     -2704.9639    \n",
            "      3143   385.38989     -148.42918      0             -148.22992     -1306.6082    \n",
            "      3144   391.97484     -148.42235      0             -148.21968      237.01022    \n",
            "      3145   383.51682     -148.40625      0             -148.20796      1610.5617    \n",
            "      3146   360.91294     -148.40752      0             -148.22091      3066.1463    \n",
            "      3147   327.39491     -148.39519      0             -148.22591      4537.8003    \n",
            "      3148   288.64483     -148.35372      0             -148.20448      5702.7699    \n",
            "      3149   251.07427     -148.35093      0             -148.22111      6794.3822    \n",
            "      3150   220.82904     -148.31677      0             -148.20259      7675.111     \n",
            "      3151   202.85442     -148.31205      0             -148.20717      8261.5313    \n",
            "      3152   199.66545     -148.3104       0             -148.20716      8608.617     \n",
            "      3153   211.01653     -148.31529      0             -148.20619      8660.1475    \n",
            "      3154   234.26702     -148.34394      0             -148.22282      8463.2641    \n",
            "      3155   265.01858     -148.35792      0             -148.22089      8083.6551    \n",
            "      3156   297.66083     -148.33888      0             -148.18498      7487.7877    \n",
            "      3157   326.84659     -148.36883      0             -148.19984      6682.1141    \n",
            "      3158   349.07487     -148.38495      0             -148.20447      5794.0068    \n",
            "      3159   362.20779     -148.38664      0             -148.19937      4714.4256    \n",
            "      3160   366.56407     -148.39078      0             -148.20126      3655.2658    \n",
            "      3161   364.05653     -148.35999      0             -148.17175      2420.6165    \n",
            "      3162   357.22021     -148.36002      0             -148.17533      1111.6897    \n",
            "      3163   348.76523     -148.37477      0             -148.19444     -167.52315    \n",
            "      3164   340.54732     -148.37586      0             -148.19978     -1464.4523    \n",
            "      3165   332.88785     -148.38574      0             -148.21362     -2866.4015    \n",
            "      3166   324.67441     -148.32736      0             -148.15949     -4098.1476    \n",
            "      3167   314.07207     -148.35116      0             -148.18877     -5135.9439    \n",
            "      3168   299.20405     -148.34914      0             -148.19444     -6100.2999    \n",
            "      3169   278.96682     -148.32759      0             -148.18335     -6806.144     \n",
            "      3170   254.27335     -148.31438      0             -148.18291     -7256.6569    \n",
            "      3171   227.832       -148.30229      0             -148.18449     -7480.2542    \n",
            "      3172   204.06797     -148.3054       0             -148.19989     -7435.884     \n",
            "      3173   188.47279     -148.26885      0             -148.1714      -7161.1168    \n",
            "      3174   185.95245     -148.27188      0             -148.17573     -6721.2556    \n",
            "      3175   200.17671     -148.28445      0             -148.18095     -5959.1538    \n",
            "      3176   231.96703     -148.31359      0             -148.19366     -4993.9457    \n",
            "      3177   278.69127     -148.32119      0             -148.17709     -3767.3625    \n",
            "      3178   334.74142     -148.35724      0             -148.18416     -2523.6425    \n",
            "      3179   392.25419     -148.3897       0             -148.18689     -933.70532    \n",
            "      3180   442.09716     -148.39482      0             -148.16624      778.87268    \n",
            "      3181   476.52188     -148.42198      0             -148.1756       2654.8018    \n",
            "      3182   490.12516     -148.38204      0             -148.12862      4771.038     \n",
            "      3183   480.93602     -148.41817      0             -148.1695       6856.0572    \n",
            "      3184   450.96535     -148.41391      0             -148.18074      9076.5521    \n",
            "      3185   405.29224     -148.36581      0             -148.15625      11316.882    \n",
            "      3186   350.88666     -148.33649      0             -148.15507      13352.462    \n",
            "      3187   294.89948     -148.29141      0             -148.13894      15359.559    \n",
            "      3188   243.53001     -148.29522      0             -148.1693       17024.599    \n",
            "      3189   200.86338     -148.26849      0             -148.16464      18513.111    \n",
            "      3190   168.29098     -148.27083      0             -148.18382      19771.394    \n",
            "      3191   145.2169      -148.21835      0             -148.14327      20809.285    \n",
            "      3192   130.19998     -148.20997      0             -148.14265      21429.71     \n",
            "      3193   121.81749     -148.20839      0             -148.14541      21884.457    \n",
            "      3194   119.62467     -148.21792      0             -148.15607      21936.116    \n",
            "      3195   124.6775      -148.21461      0             -148.15015      21697.6      \n",
            "      3196   139.44888     -148.21427      0             -148.14217      21261.737    \n",
            "      3197   166.66631     -148.23729      0             -148.15112      20265.906    \n",
            "      3198   208.38585     -148.26478      0             -148.15704      19208.678    \n",
            "      3199   264.7906      -148.29129      0             -148.15438      17681.081    \n",
            "      3200   333.16244     -148.3367       0             -148.16444      15840.631    \n",
            "      3201   407.77105     -148.37281      0             -148.16198      13772.517    \n",
            "      3202   480.31759     -148.41416      0             -148.16582      11425.928    \n",
            "      3203   541.45211     -148.4317       0             -148.15174      9085.3469    \n",
            "      3204   582.41496     -148.43115      0             -148.13001      6700.6476    \n",
            "      3205   596.68049     -148.44901      0             -148.1405       4124.409     \n",
            "      3206   581.64617     -148.40971      0             -148.10897      1822.4523    \n",
            "      3207   539.03856     -148.43714      0             -148.15844     -500.12716    \n",
            "      3208   474.73475     -148.36141      0             -148.11596     -2630.1393    \n",
            "      3209   397.68717     -148.35307      0             -148.14744     -4652.3862    \n",
            "      3210   317.95654     -148.28675      0             -148.12236     -6499.7572    \n",
            "      3211   245.30409     -148.2534       0             -148.12657     -8082.6222    \n",
            "      3212   187.42221     -148.23207      0             -148.13516     -9408.04      \n",
            "      3213   148.63146     -148.18415      0             -148.1073      -10683.807    \n",
            "      3214   129.72336     -148.20765      0             -148.14058     -11794.532    \n",
            "      3215   128.89112     -148.17884      0             -148.1122      -12425.375    \n",
            "      3216   142.66058     -148.21586      0             -148.1421      -12931.728    \n",
            "      3217   166.98067     -148.19882      0             -148.11248     -13025.178    \n",
            "      3218   197.85547     -148.21061      0             -148.10831     -12804.012    \n",
            "      3219   232.86027     -148.26185      0             -148.14145     -12411.209    \n",
            "      3220   271.50566     -148.24508      0             -148.10471     -11446.629    \n",
            "      3221   313.78537     -148.3015       0             -148.13926     -10568.162    \n",
            "      3222   360.52136     -148.27305      0             -148.08664     -9250.1434    \n",
            "      3223   411.98376     -148.33394      0             -148.12093     -7837.2039    \n",
            "      3224   466.70697     -148.35838      0             -148.11707     -6313.6741    \n",
            "      3225   521.24356     -148.37193      0             -148.10242     -4750.4182    \n",
            "      3226   570.5085      -148.40085      0             -148.10587     -3099.5293    \n",
            "      3227   608.18399     -148.41662      0             -148.10217     -1446.1855    \n",
            "      3228   628.20418     -148.43531      0             -148.1105       301.2665     \n",
            "      3229   626.01231     -148.38764      0             -148.06396      1912.7703    \n",
            "      3230   599.56717     -148.40876      0             -148.09876      3560.1741    \n",
            "      3231   550.78775     -148.38421      0             -148.09943      5121.0548    \n",
            "      3232   485.12067     -148.35325      0             -148.10242      6494.9984    \n",
            "      3233   411.06916     -148.29669      0             -148.08415      7636.3879    \n",
            "      3234   338.94229     -148.21931      0             -148.04406      8756.913     \n",
            "      3235   278.56561     -148.23774      0             -148.09371      9297.2486    \n",
            "      3236   237.92592     -148.20966      0             -148.08665      9795.6081    \n",
            "      3237   221.69259     -148.20588      0             -148.09125      9946.7912    \n",
            "      3238   230.50236     -148.1914       0             -148.07222      9707.5951    \n",
            "      3239   261.096       -148.2053       0             -148.0703       9198.5185    \n",
            "      3240   306.87028     -148.24141      0             -148.08274      8353.4936    \n",
            "      3241   359.76082     -148.27664      0             -148.09063      7487.187     \n",
            "      3242   411.85959     -148.28027      0             -148.06732      6303.9174    \n",
            "      3243   456.55341     -148.31775      0             -148.08169      5138.9267    \n",
            "      3244   489.84278     -148.33432      0             -148.08105      3753.8297    \n",
            "      3245   511.13951     -148.35148      0             -148.0872       2326.611     \n",
            "      3246   522.34153     -148.3662       0             -148.09613      889.61793    \n",
            "      3247   526.78212     -148.35123      0             -148.07887     -719.38162    \n",
            "      3248   528.29023     -148.34338      0             -148.07023     -2071.3917    \n",
            "      3249   529.61108     -148.33804      0             -148.06421     -3607.8321    \n",
            "      3250   531.2682      -148.33399      0             -148.05931     -5170.355     \n",
            "      3251   531.5564      -148.35058      0             -148.07575     -6566.5715    \n",
            "      3252   527.27298     -148.34916      0             -148.07653     -7810.8082    \n",
            "      3253   514.43723     -148.3336       0             -148.06762     -8992.1024    \n",
            "      3254   490.36576     -148.30578      0             -148.05225     -9775.3897    \n",
            "      3255   455.16131     -148.29068      0             -148.05534     -10393.31     \n",
            "      3256   411.59966     -148.26362      0             -148.05081     -10711.077    \n",
            "      3257   365.23728     -148.19357      0             -148.00473     -10884.087    \n",
            "      3258   323.76032     -148.22579      0             -148.0584      -10819.269    \n",
            "      3259   295.35744     -148.21139      0             -148.05868     -10560.706    \n",
            "      3260   286.22777     -148.1998       0             -148.05181     -10066.283    \n",
            "      3261   299.20438     -148.19343      0             -148.03873     -9504.1535    \n",
            "      3262   332.8715      -148.22329      0             -148.05118     -8712.7704    \n",
            "      3263   381.54591     -148.26459      0             -148.06732     -7949.65      \n",
            "      3264   436.154       -148.2822       0             -148.05669     -6876.0496    \n",
            "      3265   486.3181      -148.28961      0             -148.03817     -5636.0348    \n",
            "      3266   522.72646     -148.31659      0             -148.04632     -4437.5027    \n",
            "      3267   538.99095     -148.32675      0             -148.04806     -2932.5304    \n",
            "      3268   533.06944     -148.32711      0             -148.05149     -1343.0474    \n",
            "      3269   507.46947     -148.31014      0             -148.04775      161.94989    \n",
            "      3270   468.02996     -148.27079      0             -148.0288       1779.6541    \n",
            "      3271   422.50623     -148.26192      0             -148.04347      3342.854     \n",
            "      3272   378.75655     -148.24595      0             -148.05012      4853.1207    \n",
            "      3273   342.79431     -148.19447      0             -148.01723      6176.8152    \n",
            "      3274   317.61554     -148.19934      0             -148.03512      7171.7246    \n",
            "      3275   303.07557     -148.20802      0             -148.05131      8056.6699    \n",
            "      3276   296.05636     -148.18936      0             -148.03628      8723.7993    \n",
            "      3277   292.49089     -148.17598      0             -148.02475      9189.6626    \n",
            "      3278   289.33827     -148.19797      0             -148.04837      9483.1902    \n",
            "      3279   284.87677     -148.18772      0             -148.04043      9579.2307    \n",
            "      3280   280.00612     -148.17799      0             -148.03322      9400.2756    \n",
            "      3281   278.04249     -148.1766       0             -148.03284      9076.1224    \n",
            "      3282   283.50699     -148.1799       0             -148.03332      8382.7726    \n",
            "      3283   300.97098     -148.19379      0             -148.03818      7552.4097    \n",
            "      3284   333.66362     -148.21869      0             -148.04618      6428.6885    \n",
            "      3285   381.82648     -148.22913      0             -148.03171      5005.4139    \n",
            "      3286   441.84018     -148.25324      0             -148.02479      3264.3163    \n",
            "      3287   506.76534     -148.28592      0             -148.0239       1425.1185    \n",
            "      3288   567.29778     -148.32248      0             -148.02916     -610.72453    \n",
            "      3289   613.34194     -148.34569      0             -148.02857     -2799.167     \n",
            "      3290   636.16741     -148.37868      0             -148.04976     -4990.8997    \n",
            "      3291   630.85563     -148.36293      0             -148.03675     -7071.5322    \n",
            "      3292   596.44433     -148.30756      0             -147.99918     -9068.2423    \n",
            "      3293   536.42083     -148.29729      0             -148.01994     -10976.529    \n",
            "      3294   458.74397     -148.24965      0             -148.01246     -12737.946    \n",
            "      3295   373.08222     -148.1741       0             -147.9812      -14393.689    \n",
            "      3296   290.1526      -148.15834      0             -148.00832     -15878.727    \n",
            "      3297   219.06874     -148.14605      0             -148.03279     -17307.718    \n",
            "      3298   165.91815     -148.10704      0             -148.02125     -18501.423    \n",
            "      3299   133.49518     -148.09236      0             -148.02334     -19563.232    \n",
            "      3300   120.93246     -148.06974      0             -148.00722     -20560.09     \n",
            "      3301   125.05107     -148.09747      0             -148.03282     -21296.584    \n",
            "      3302   141.35708     -148.08771      0             -148.01462     -21779.282    \n",
            "      3303   165.51632     -148.12275      0             -148.03717     -21953.678    \n",
            "      3304   194.5899      -148.12211      0             -148.02149     -21914.545    \n",
            "      3305   227.78119     -148.15446      0             -148.03668     -21523.598    \n",
            "      3306   266.03799     -148.13775      0             -148.0002      -20669.341    \n",
            "      3307   311.42429     -148.178        0             -148.01698     -19840.155    \n",
            "      3308   366.3585      -148.20468      0             -148.01526     -18606.348    \n",
            "      3309   431.50563     -148.26672      0             -148.04361     -17370.262    \n",
            "      3310   505.19032     -148.29489      0             -148.03369     -15835.188    \n",
            "      3311   582.56598     -148.3217       0             -148.02049     -14218.141    \n",
            "      3312   656.05609     -148.35237      0             -148.01316     -12595.457    \n",
            "      3313   716.27469     -148.37166      0             -148.00132     -10698.039    \n",
            "      3314   753.7713      -148.40544      0             -148.01571     -8737.9698    \n",
            "      3315   761.07755     -148.38646      0             -147.99296     -6661.1185    \n",
            "      3316   734.84141     -148.39465      0             -148.0147      -4632.2064    \n",
            "      3317   676.60913     -148.34373      0             -147.9939      -2450.8451    \n",
            "      3318   592.35217     -148.2982       0             -147.99193     -247.24754    \n",
            "      3319   492.33905     -148.28794      0             -148.03338      1671.1129    \n",
            "      3320   389.20999     -148.20191      0             -148.00067      3611.6829    \n",
            "      3321   295.72321     -148.1624       0             -148.0095       5228.6011    \n",
            "      3322   222.40865     -148.13948      0             -148.02449      6789.0941    \n",
            "      3323   176.18434     -148.11431      0             -148.02322      7782.6227    \n",
            "      3324   159.39951     -148.10956      0             -148.02714      8661.47      \n",
            "      3325   169.86321     -148.11395      0             -148.02613      9309.6406    \n",
            "      3326   202.40778     -148.12056      0             -148.01591      9609.1905    \n",
            "      3327   249.77313     -148.14101      0             -148.01187      9737.1221    \n",
            "      3328   304.73467     -148.16436      0             -148.0068       9582.3635    \n",
            "      3329   361.50791     -148.19256      0             -148.00564      9297.8782    \n",
            "      3330   416.86536     -148.22644      0             -148.01091      8925.823     \n",
            "      3331   470.01847     -148.28887      0             -148.04585      8143.2543    \n",
            "      3332   522.0767      -148.29974      0             -148.0298       7360.8106    \n",
            "      3333   574.79543     -148.30498      0             -148.00779      6214.202     \n",
            "      3334   628.98314     -148.31024      0             -147.98503      5138.6402    \n",
            "      3335   683.07452     -148.36825      0             -148.01507      3706.153     \n",
            "      3336   732.53675     -148.43076      0             -148.05201      2263.7332    \n",
            "      3337   770.81021     -148.40235      0             -148.00381      651.34178    \n",
            "      3338   790.33597     -148.40988      0             -148.00125     -754.33776    \n",
            "      3339   784.64999     -148.41002      0             -148.00433     -2157.1077    \n",
            "      3340   750.25027     -148.40856      0             -148.02065     -3370.3775    \n",
            "      3341   688.05703     -148.36939      0             -148.01364     -4502.0806    \n",
            "      3342   604.03947     -148.3243       0             -148.01199     -5333.7344    \n",
            "      3343   508.06978     -148.2812       0             -148.01851     -6048.4746    \n",
            "      3344   413.02838     -148.21527      0             -148.00172     -6399.7302    \n",
            "      3345   331.33649     -148.21543      0             -148.04412     -6824.3889    \n",
            "      3346   272.77193     -148.15145      0             -148.01042     -6919.9572    \n",
            "      3347   242.65551     -148.12638      0             -148.00092     -6970.5079    \n",
            "      3348   241.15041     -148.12734      0             -148.00265     -6810.0084    \n",
            "      3349   263.6193      -148.15763      0             -148.02133     -6585.3263    \n",
            "      3350   302.32437     -148.16169      0             -148.00538     -5981.1519    \n",
            "      3351   347.70447     -148.16813      0             -147.98835     -5271.6309    \n",
            "      3352   391.26418     -148.20681      0             -148.00452     -4478.249     \n",
            "      3353   427.4625      -148.22402      0             -148.003       -3240.1472    \n",
            "      3354   454.28269     -148.27074      0             -148.03586     -1920.7575    \n",
            "      3355   473.52827     -148.28837      0             -148.04354     -326.51876    \n",
            "      3356   488.82501     -148.26056      0             -148.00782      1385.7009    \n",
            "      3357   504.53316     -148.30713      0             -148.04626      2905.1702    \n",
            "      3358   523.9439      -148.28843      0             -148.01753      4688.8886    \n",
            "      3359   547.59126     -148.29642      0             -148.0133       6232.8728    \n",
            "      3360   572.95609     -148.2859       0             -147.98966      7846.664     \n",
            "      3361   595.08298     -148.32414      0             -148.01646      9193.6703    \n",
            "      3362   607.70152     -148.30626      0             -147.99205      10629.663    \n",
            "      3363   604.93651     -148.34975      0             -148.03697      11795.057    \n",
            "      3364   583.6618      -148.31386      0             -148.01209      12831.638    \n",
            "      3365   544.9422      -148.29108      0             -148.00933      13802.846    \n",
            "      3366   493.33639     -148.25935      0             -148.00427      14356.038    \n",
            "      3367   436.2415      -148.2659       0             -148.04035      14886.571    \n",
            "      3368   382.96464     -148.21596      0             -148.01795      15002.451    \n",
            "      3369   341.93721     -148.19122      0             -148.01442      14785.225    \n",
            "      3370   319.45774     -148.20317      0             -148.038        14372.907    \n",
            "      3371   318.13321     -148.17649      0             -148.012        13463.108    \n",
            "      3372   336.13902     -148.19733      0             -148.02353      12239.893    \n",
            "      3373   367.8293      -148.19792      0             -148.00774      10948.104    \n",
            "      3374   405.04413     -148.23074      0             -148.02132      9165.7413    \n",
            "      3375   438.50791     -148.23911      0             -148.01238      7398.3631    \n",
            "      3376   460.2946      -148.24827      0             -148.01028      5420.0192    \n",
            "      3377   465.59877     -148.28401      0             -148.04327      3516.5615    \n",
            "      3378   453.05582     -148.25518      0             -148.02093      1551.8535    \n",
            "      3379   424.77761     -148.26493      0             -148.0453      -410.08772    \n",
            "      3380   386.22683     -148.24856      0             -148.04887     -2388.1615    \n",
            "      3381   344.60046     -148.22401      0             -148.04584     -4186.6677    \n",
            "      3382   306.96165     -148.19036      0             -148.03164     -5842.884     \n",
            "      3383   279.10435     -148.15904      0             -148.01473     -7455.0039    \n",
            "      3384   264.07201     -148.144        0             -148.00746     -9030.5171    \n",
            "      3385   261.8668      -148.17916      0             -148.04377     -10244.113    \n",
            "      3386   270.26298     -148.20137      0             -148.06163     -11411.605    \n",
            "      3387   285.29872     -148.1675       0             -148.01999     -12230.95     \n",
            "      3388   302.69611     -148.19188      0             -148.03538     -12725.847    \n",
            "      3389   319.05466     -148.21162      0             -148.04666     -13050.469    \n",
            "      3390   333.06409     -148.2205       0             -148.04829     -12740.649    \n",
            "      3391   345.34633     -148.2181       0             -148.03955     -12326.77     \n",
            "      3392   358.24675     -148.22057      0             -148.03535     -11389.383    \n",
            "      3393   375.43606     -148.24111      0             -148.04699     -10312.822    \n",
            "      3394   399.70715     -148.26182      0             -148.05515     -8968.7514    \n",
            "      3395   432.23303     -148.27842      0             -148.05494     -7540.938     \n",
            "      3396   471.75743     -148.29831      0             -148.0544      -5742.9983    \n",
            "      3397   513.86648     -148.3094       0             -148.04371     -3927.4216    \n",
            "      3398   551.63715     -148.3713       0             -148.08608     -1990.7138    \n",
            "      3399   577.06484     -148.35063      0             -148.05227      88.756334    \n",
            "      3400   582.90385     -148.37913      0             -148.07774      2331.5868    \n",
            "      3401   564.54764     -148.3352       0             -148.0433       4470.1091    \n",
            "      3402   521.47852     -148.34673      0             -148.0771       6677.9569    \n",
            "      3403   457.35877     -148.28302      0             -148.04654      8942.1488    \n",
            "      3404   379.65881     -148.24596      0             -148.04966      11066.144    \n",
            "      3405   299.31307     -148.22628      0             -148.07153      13047.172    \n",
            "      3406   227.60266     -148.18325      0             -148.06557      14915.087    \n",
            "      3407   174.41389     -148.1438       0             -148.05363      16356.13     \n",
            "      3408   146.45313     -148.13636      0             -148.06064      17453.127    \n",
            "      3409   145.98941     -148.1288       0             -148.05332      18470.371    \n",
            "      3410   170.68093     -148.17467      0             -148.08643      18938.405    \n",
            "      3411   214.8049      -148.18707      0             -148.07601      19222.332    \n",
            "      3412   270.79224     -148.18668      0             -148.04667      19249.8      \n",
            "      3413   330.80808     -148.23361      0             -148.06257      18947.077    \n",
            "      3414   388.30883     -148.26441      0             -148.06364      18471.2      \n",
            "      3415   439.07052     -148.30353      0             -148.07652      17636.199    \n",
            "      3416   482.39473     -148.29322      0             -148.04381      16832.119    \n",
            "      3417   519.6808      -148.3253       0             -148.05661      15552.935    \n",
            "      3418   553.01082     -148.37607      0             -148.09014      14124.047    \n",
            "      3419   584.00065     -148.40532      0             -148.10337      12639.452    \n",
            "      3420   612.40066     -148.42595      0             -148.10931      10866.749    \n",
            "      3421   635.69112     -148.42376      0             -148.09508      8947.2502    \n",
            "      3422   649.54061     -148.42784      0             -148.092        7024.2003    \n",
            "      3423   648.50332     -148.42016      0             -148.08486      5107.5929    \n",
            "      3424   627.61695     -148.42134      0             -148.09684      3227.9895    \n",
            "      3425   584.87542     -148.37176      0             -148.06936      1602.8889    \n",
            "      3426   521.76311     -148.35789      0             -148.08811     -29.729621    \n",
            "      3427   443.42086     -148.34289      0             -148.11362     -1456.0993    \n",
            "      3428   358.16533     -148.29051      0             -148.10533     -2624.4607    \n",
            "      3429   276.58992     -148.25647      0             -148.11346     -3590.736     \n",
            "      3430   208.94452     -148.22094      0             -148.11291     -4399.5142    \n",
            "      3431   163.12671     -148.1888       0             -148.10446     -5230.1077    \n",
            "      3432   143.63937     -148.17824      0             -148.10397     -5861.4829    \n",
            "      3433   150.69753     -148.1602       0             -148.08228     -6196.75      \n",
            "      3434   180.54453     -148.1972       0             -148.10385     -6422.4885    \n",
            "      3435   226.48266     -148.2299       0             -148.1128      -6716.1777    \n",
            "      3436   280.49159     -148.25294      0             -148.10792     -6581.0013    \n",
            "      3437   335.04094     -148.29212      0             -148.11889     -6426.6133    \n",
            "      3438   384.50263     -148.27155      0             -148.07274     -5832.08      \n",
            "      3439   425.87478     -148.32283      0             -148.10263     -5277.2409    \n",
            "      3440   458.96617     -148.36735      0             -148.13005     -4513.2307    \n",
            "      3441   485.45359     -148.37022      0             -148.11922     -3498.1732    \n",
            "      3442   507.77468     -148.39613      0             -148.13359     -2643.8582    \n",
            "      3443   527.95265     -148.40929      0             -148.13632     -1692.9822    \n",
            "      3444   546.22988     -148.38815      0             -148.10572     -686.14546    \n",
            "      3445   560.91161     -148.42767      0             -148.13766      171.05058    \n",
            "      3446   568.89057     -148.43417      0             -148.14003      1115.6486    \n",
            "      3447   566.04933     -148.41181      0             -148.11914      1849.3284    \n",
            "      3448   548.68791     -148.42278      0             -148.13909      2487.0255    \n",
            "      3449   514.98511     -148.43485      0             -148.16859      3120.5428    \n",
            "      3450   465.32343     -148.36665      0             -148.12606      3623.4914    \n",
            "      3451   403.44135     -148.34305      0             -148.13446      4075.2168    \n",
            "      3452   335.47471     -148.31921      0             -148.14575      4239.3614    \n",
            "      3453   268.985       -148.2896       0             -148.15052      4224.7613    \n",
            "      3454   211.7789      -148.22462      0             -148.11513      4250.332     \n",
            "      3455   170.20285     -148.22799      0             -148.13999      3875.0855    \n",
            "      3456   148.1838      -148.19442      0             -148.11781      3418.338     \n",
            "      3457   146.54794     -148.22062      0             -148.14485      2619.967     \n",
            "      3458   162.72631     -148.21188      0             -148.12774      1670.7551    \n",
            "      3459   191.62598     -148.24901      0             -148.14993      565.90046    \n",
            "      3460   227.01611     -148.26465      0             -148.14728     -649.76193    \n",
            "      3461   262.28707     -148.27281      0             -148.1372      -1858.6797    \n",
            "      3462   291.89165     -148.32112      0             -148.1702      -3069.7553    \n",
            "      3463   312.30671     -148.30569      0             -148.14421     -4524.0451    \n",
            "      3464   322.70913     -148.29153      0             -148.12468     -5646.6081    \n",
            "      3465   324.79029     -148.32437      0             -148.15644     -6996.8545    \n",
            "      3466   322.0393      -148.32334      0             -148.15683     -8118.9723    \n",
            "      3467   318.09926     -148.32655      0             -148.16208     -9252.6313    \n",
            "      3468   316.27269     -148.33225      0             -148.16872     -10234.893    \n",
            "      3469   318.56595     -148.33989      0             -148.17518     -11242.984    \n",
            "      3470   324.83258     -148.33363      0             -148.16568     -12145.248    \n",
            "      3471   333.07055     -148.30694      0             -148.13473     -12791.074    \n",
            "      3472   340.10049     -148.34481      0             -148.16896     -13406.126    \n",
            "      3473   342.47183     -148.3537       0             -148.17663     -13800.749    \n",
            "      3474   337.53476     -148.32589      0             -148.15137     -13834.62     \n",
            "      3475   324.35289     -148.35632      0             -148.18862     -13821.022    \n",
            "      3476   303.92461     -148.32372      0             -148.16657     -13582.026    \n",
            "      3477   279.52716     -148.31945      0             -148.17493     -13111.22     \n",
            "      3478   255.6305      -148.30757      0             -148.1754      -12333.297    \n",
            "      3479   236.678       -148.2918       0             -148.16943     -11512.296    \n",
            "      3480   226.41095     -148.32433      0             -148.20727     -10731.963    \n",
            "      3481   226.28724     -148.31775      0             -148.20075     -9588.5604    \n",
            "      3482   235.17453     -148.29383      0             -148.17224     -8461.7714    \n",
            "      3483   249.88344     -148.33217      0             -148.20297     -7369.7073    \n",
            "      3484   265.60775     -148.34014      0             -148.20281     -6183.4791    \n",
            "      3485   276.93293     -148.31248      0             -148.1693      -4812.5821    \n",
            "      3486   279.58685     -148.31983      0             -148.17527     -3382.3966    \n",
            "      3487   271.32719     -148.33535      0             -148.19506     -1939.1502    \n",
            "      3488   252.65446     -148.28099      0             -148.15036     -528.08633    \n",
            "      3489   226.69457     -148.31298      0             -148.19576      878.33875    \n",
            "      3490   198.56811     -148.2974       0             -148.19473      2228.5945    \n",
            "      3491   174.03803     -148.27996      0             -148.18997      3485.8705    \n",
            "      3492   158.20684     -148.28075      0             -148.19895      4789.5102    \n",
            "      3493   154.52202     -148.29283      0             -148.21293      5733.0395    \n",
            "      3494   163.90569     -148.28503      0             -148.20028      6575.3822    \n",
            "      3495   184.56793     -148.28782      0             -148.19239      7214.7507    \n",
            "      3496   212.72886     -148.31953      0             -148.20954      7752.5399    \n",
            "      3497   243.61128     -148.34441      0             -148.21845      7909.8132    \n",
            "      3498   272.38231     -148.32524      0             -148.1844       7941.5375    \n",
            "      3499   295.45316     -148.35183      0             -148.19907      8024.5729    \n",
            "      3500   311.41983     -148.32913      0             -148.16811      7743.9516    \n",
            "      3501   320.84396     -148.36092      0             -148.19503      7310.1821    \n",
            "      3502   325.8829      -148.38057      0             -148.21208      6800.9262    \n",
            "      3503   329.65394     -148.369        0             -148.19856      6075.1028    \n",
            "      3504   334.66352     -148.38044      0             -148.20741      5259.809     \n",
            "      3505   342.21531     -148.3867       0             -148.20976      4148.2757    \n",
            "      3506   351.6008      -148.41096      0             -148.22917      3065.9687    \n",
            "      3507   360.17133     -148.39336      0             -148.20713      1896.731     \n",
            "      3508   364.25787     -148.40278      0             -148.21445      550.98235    \n",
            "      3509   359.90929     -148.40389      0             -148.2178      -821.04387    \n",
            "      3510   344.25262     -148.40221      0             -148.22422     -2184.4104    \n",
            "      3511   316.47755     -148.41399      0             -148.25036     -3307.6591    \n",
            "      3512   278.16452     -148.36495      0             -148.22113     -4451.753     \n",
            "      3513   233.70669     -148.34808      0             -148.22724     -5523.8417    \n",
            "      3514   188.60166     -148.31473      0             -148.21721     -6428.0928    \n",
            "      3515   149.19792     -148.26729      0             -148.19015     -7279.8763    \n",
            "      3516   121.23954     -148.26736      0             -148.20467     -8055.4994    \n",
            "      3517   108.36305     -148.28729      0             -148.23126     -8701.2183    \n",
            "      3518   111.80586     -148.29389      0             -148.23608     -9235.9563    \n",
            "      3519   130.26316     -148.29294      0             -148.22559     -9706.9352    \n",
            "      3520   160.2316      -148.27755      0             -148.19471     -10146.866    \n",
            "      3521   196.94769     -148.29027      0             -148.18844     -10130.697    \n",
            "      3522   235.32956     -148.34712      0             -148.22544     -10244.001    \n",
            "      3523   271.36046     -148.36565      0             -148.22534     -10027.442    \n",
            "      3524   302.62288     -148.39907      0             -148.2426      -9631.1122    \n",
            "      3525   328.15745     -148.40594      0             -148.23626     -9012.7768    \n",
            "      3526   348.60206     -148.40118      0             -148.22094     -8399.9452    \n",
            "      3527   365.40643     -148.4157       0             -148.22677     -7521.5328    \n",
            "      3528   379.90544     -148.43151      0             -148.23509     -6634.7476    \n",
            "      3529   392.96235     -148.44624      0             -148.24306     -5761.7254    \n",
            "      3530   404.00008     -148.43621      0             -148.22733     -4807.4519    \n",
            "      3531   410.96142     -148.41767      0             -148.20519     -3695.2946    \n",
            "      3532   411.01691     -148.43561      0             -148.2231      -2808.9283    \n",
            "      3533   401.44655     -148.43825      0             -148.23069     -1702.4455    \n",
            "      3534   379.82409     -148.4346       0             -148.23822     -749.47751    \n",
            "      3535   345.68959     -148.4022       0             -148.22346      265.31319    \n",
            "      3536   301.01086     -148.39416      0             -148.23852      1190.3221    \n",
            "      3537   249.26545     -148.37443      0             -148.24555      2037.3994    \n",
            "      3538   195.7282      -148.34003      0             -148.23884      2800.4212    \n",
            "      3539   146.64842     -148.31034      0             -148.23452      3461.2456    \n",
            "      3540   107.65832     -148.28676      0             -148.2311       3962.3799    \n",
            "      3541   83.082688     -148.29439      0             -148.25143      4389.8668    \n",
            "      3542   75.131486     -148.28203      0             -148.24318      4442.7981    \n",
            "      3543   83.658386     -148.30746      0             -148.26421      4397.6314    \n",
            "      3544   106.09431     -148.27751      0             -148.22265      4251.4735    \n",
            "      3545   138.19115     -148.33256      0             -148.26111      3901.8558    \n",
            "      3546   174.85607     -148.35229      0             -148.26188      3437.5225    \n",
            "      3547   211.02816     -148.35877      0             -148.24966      3009.9533    \n",
            "      3548   242.85191     -148.37469      0             -148.24913      2443.2118    \n",
            "      3549   267.95223     -148.38423      0             -148.24569      1772.4504    \n",
            "      3550   285.71178     -148.43317      0             -148.28545      1159.9396    \n",
            "      3551   297.21113     -148.38481      0             -148.23114      447.08855    \n",
            "      3552   304.35985     -148.41237      0             -148.255       -211.51601    \n",
            "      3553   308.96673     -148.41999      0             -148.26024     -1006.961     \n",
            "      3554   312.36665     -148.42039      0             -148.25889     -1673.419     \n",
            "      3555   314.67966     -148.42661      0             -148.2639      -2226.022     \n",
            "      3556   314.78401     -148.43956      0             -148.27681     -2866.6247    \n",
            "      3557   310.75172     -148.42804      0             -148.26737     -3402.0252    \n",
            "      3558   300.32325     -148.42301      0             -148.26773     -3652.8437    \n",
            "      3559   281.9245      -148.39124      0             -148.24548     -3965.1782    \n",
            "      3560   255.43084     -148.40294      0             -148.27088     -3966.1109    \n",
            "      3561   222.31144     -148.39885      0             -148.2839      -3917.2437    \n",
            "      3562   185.69706     -148.37484      0             -148.27882     -3645.8073    \n",
            "      3563   149.83612     -148.353        0             -148.27553     -3196.6719    \n",
            "      3564   119.22183     -148.3132       0             -148.25156     -2577.7739    \n",
            "      3565   97.732932     -148.33825      0             -148.28772     -1833.4428    \n",
            "      3566   87.597639     -148.29125      0             -148.24596     -975.35204    \n",
            "      3567   89.056907     -148.29193      0             -148.24588     -131.31218    \n",
            "      3568   100.26229     -148.29993      0             -148.24809      777.04586    \n",
            "      3569   117.57734     -148.32718      0             -148.26639      1780.1595    \n",
            "      3570   136.70473     -148.33852      0             -148.26784      2811.7143    \n",
            "      3571   153.60663     -148.32153      0             -148.24211      4130.9668    \n",
            "      3572   165.20625     -148.35615      0             -148.27073      5304.6122    \n",
            "      3573   170.47485     -148.34838      0             -148.26023      6528.2416    \n",
            "      3574   170.4824      -148.34453      0             -148.25638      7783.8822    \n",
            "      3575   167.51497     -148.34509      0             -148.25848      8994.8939    \n",
            "      3576   164.7936      -148.36427      0             -148.27906      10130.249    \n",
            "      3577   165.35576     -148.34311      0             -148.25762      11154.628    \n",
            "      3578   171.19367     -148.35969      0             -148.27118      12093.992    \n",
            "      3579   182.64897     -148.37717      0             -148.28274      12773.491    \n",
            "      3580   198.20168     -148.38657      0             -148.28409      13350.212    \n",
            "      3581   215.05866     -148.37475      0             -148.26356      13812.452    \n",
            "      3582   229.82094     -148.3807       0             -148.26188      13851.911    \n",
            "      3583   239.48539     -148.40762      0             -148.2838       13870.211    \n",
            "      3584   242.12342     -148.39869      0             -148.2735       13800.07     \n",
            "      3585   237.31592     -148.41523      0             -148.29252      13417.893    \n",
            "      3586   226.77311     -148.41097      0             -148.29372      13083.953    \n",
            "      3587   213.15028     -148.3764       0             -148.26619      12500.409    \n",
            "      3588   199.38865     -148.37894      0             -148.27585      11641.711    \n",
            "      3589   188.39341     -148.38644      0             -148.28903      10638.967    \n",
            "      3590   181.86755     -148.38051      0             -148.28648      9645.8236    \n",
            "      3591   180.03247     -148.36649      0             -148.27341      8445.8797    \n",
            "      3592   181.56904     -148.39224      0             -148.29836      7066.5438    \n",
            "      3593   183.83147     -148.38075      0             -148.2857       5751.0389    \n",
            "      3594   183.94193     -148.38216      0             -148.28705      4375.9408    \n",
            "      3595   179.74441     -148.36174      0             -148.26881      3101.4431    \n",
            "      3596   170.05717     -148.35414      0             -148.26621      1713.1525    \n",
            "      3597   155.51016     -148.37224      0             -148.29184      566.22338    \n",
            "      3598   138.17568     -148.36706      0             -148.29561     -627.76116    \n",
            "      3599   121.48634     -148.33121      0             -148.26839     -1634.7308    \n",
            "      3600   109.35611     -148.33614      0             -148.2796      -2507.4522    \n",
            "      3601   105.14972     -148.32765      0             -148.27328     -3396.8075    \n",
            "      3602   110.97897     -148.36149      0             -148.30411     -4128.0239    \n",
            "      3603   127.30083     -148.31429      0             -148.24847     -4707.5855    \n",
            "      3604   152.57458     -148.36926      0             -148.29038     -5095.6369    \n",
            "      3605   183.81112     -148.39828      0             -148.30324     -5475.488     \n",
            "      3606   217.02481     -148.35449      0             -148.24228     -5564.9366    \n",
            "      3607   248.03324     -148.38543      0             -148.25719     -5505.138     \n",
            "      3608   273.41829     -148.43597      0             -148.2946      -5373.4074    \n",
            "      3609   291.07962     -148.44317      0             -148.29267     -5075.8044    \n",
            "      3610   300.36061     -148.43286      0             -148.27757     -4625.2451    \n",
            "      3611   302.11432     -148.42903      0             -148.27283     -3937.7088    \n",
            "      3612   298.35476     -148.43047      0             -148.27621     -3195.6159    \n",
            "      3613   291.02919     -148.42672      0             -148.27625     -2394.4282    \n",
            "      3614   281.94019     -148.42527      0             -148.2795      -1615.9207    \n",
            "      3615   272.04154     -148.44335      0             -148.30269     -690.90243    \n",
            "      3616   261.23004     -148.40108      0             -148.26601      93.50198     \n",
            "      3617   248.6187      -148.41018      0             -148.28163      989.35176    \n",
            "      3618   232.99898     -148.42422      0             -148.30375      1718.8052    \n",
            "      3619   213.05531     -148.40702      0             -148.29686      2489.4506    \n",
            "      3620   188.51314     -148.392        0             -148.29453      3286.6243    \n",
            "      3621   160.46953     -148.36443      0             -148.28146      3923.74      \n",
            "      3622   131.11783     -148.34409      0             -148.2763       4552.0731    \n",
            "      3623   103.93949     -148.36371      0             -148.30997      4980.6703    \n",
            "      3624   83.047722     -148.31146      0             -148.26852      5477.7026    \n",
            "      3625   72.037024     -148.33162      0             -148.29438      5692.1144    \n",
            "      3626   73.35256      -148.33933      0             -148.30141      5843.9988    \n",
            "      3627   87.937546     -148.35904      0             -148.31357      5745.141     \n",
            "      3628   114.63557     -148.37174      0             -148.31247      5576.4769    \n",
            "      3629   150.46624     -148.35026      0             -148.27247      5211.5626    \n",
            "      3630   191.13713     -148.37715      0             -148.27832      4742.3699    \n",
            "      3631   231.64583     -148.42321      0             -148.30344      4257.7751    \n",
            "      3632   267.31316     -148.4403       0             -148.30209      3497.5317    \n",
            "      3633   294.49574     -148.43187      0             -148.27961      3009.8292    \n",
            "      3634   311.29431     -148.45383      0             -148.29288      2385.0003    \n",
            "      3635   317.77919     -148.45836      0             -148.29406      1631.2306    \n",
            "      3636   315.40345     -148.44669      0             -148.28361      901.44848    \n",
            "      3637   306.72076     -148.4372       0             -148.27862      186.09517    \n",
            "      3638   294.49194     -148.45034      0             -148.29807     -487.34333    \n",
            "      3639   280.8834      -148.43317      0             -148.28795     -1093.237     \n",
            "      3640   266.87598     -148.44492      0             -148.30694     -1836.3333    \n",
            "      3641   252.25705     -148.42032      0             -148.2899      -2373.2378    \n",
            "      3642   235.81726     -148.41071      0             -148.28879     -2967.9627    \n",
            "      3643   216.25944     -148.39164      0             -148.27983     -3343.9789    \n",
            "      3644   192.72528     -148.36474      0             -148.26509     -3646.0979    \n",
            "      3645   165.01471     -148.367        0             -148.28168     -3815.1098    \n",
            "      3646   134.55186     -148.34352      0             -148.27395     -3925.0286    \n",
            "      3647   104.30774     -148.34919      0             -148.29526     -3924.0911    \n",
            "      3648   77.689637     -148.30983      0             -148.26966     -3666.4889    \n",
            "      3649   58.429958     -148.32597      0             -148.29576     -3619.1988    \n",
            "      3650   49.604026     -148.31212      0             -148.28648     -3370.219     \n",
            "      3651   52.71705      -148.31262      0             -148.28536     -2870.0498    \n",
            "      3652   67.389319     -148.3179       0             -148.28305     -2435.4602    \n",
            "      3653   91.385575     -148.3355       0             -148.28825     -2013.8709    \n",
            "      3654   120.96816     -148.34839      0             -148.28584     -1442.596     \n",
            "      3655   151.69019     -148.36026      0             -148.28183     -1003.2898    \n",
            "      3656   179.59042     -148.38796      0             -148.29511     -351.23521    \n",
            "      3657   201.67713     -148.38208      0             -148.2778       282.77405    \n",
            "      3658   216.59081     -148.39575      0             -148.28376      916.3425     \n",
            "      3659   224.71613     -148.43067      0             -148.31448      1486.1554    \n",
            "      3660   227.58725     -148.41003      0             -148.29236      2171.2587    \n",
            "      3661   227.63357     -148.40506      0             -148.28737      2685.1825    \n",
            "      3662   227.29027     -148.39195      0             -148.27444      3206.5711    \n",
            "      3663   228.09912     -148.41985      0             -148.30191      3589.3899    \n",
            "      3664   230.33531     -148.41465      0             -148.29556      3853.6809    \n",
            "      3665   233.0694      -148.41518      0             -148.29467      4081.2874    \n",
            "      3666   234.51939     -148.39611      0             -148.27486      4171.8685    \n",
            "      3667   232.32045     -148.37075      0             -148.25063      4104.4212    \n",
            "      3668   224.5525      -148.41566      0             -148.29956      3914.8519    \n",
            "      3669   210.32319     -148.38681      0             -148.27807      3659.3536    \n",
            "      3670   190.21547     -148.36172      0             -148.26337      3345.6961    \n",
            "      3671   166.43267     -148.3819       0             -148.29584      2841.798     \n",
            "      3672   142.04459     -148.35865      0             -148.28521      2207.5316    \n",
            "      3673   120.36091     -148.36868      0             -148.30644      1569.158     \n",
            "      3674   104.51812     -148.33956      0             -148.28552      830.65456    \n",
            "      3675   96.621773     -148.32684      0             -148.27688      41.482371    \n",
            "      3676   97.155399     -148.31904      0             -148.26881     -890.92307    \n",
            "      3677   104.91962     -148.33533      0             -148.28109     -2029.5414    \n",
            "      3678   117.54159     -148.35412      0             -148.29335     -3034.7845    \n",
            "      3679   132.11608     -148.33536      0             -148.26705     -4055.1405    \n",
            "      3680   145.44015     -148.36453      0             -148.28933     -5005.804     \n",
            "      3681   155.31402     -148.34458      0             -148.26428     -6028.5813    \n",
            "      3682   160.85667     -148.40658      0             -148.32341     -7008.8324    \n",
            "      3683   162.43271     -148.37416      0             -148.29017     -7764.8923    \n",
            "      3684   161.87079     -148.37435      0             -148.29066     -8549.0273    \n",
            "      3685   161.81757     -148.35785      0             -148.27419     -9303.8201    \n",
            "      3686   164.77086     -148.36712      0             -148.28193     -9848.3437    \n",
            "      3687   172.50315     -148.3847       0             -148.29551     -10431.29     \n",
            "      3688   185.81097     -148.36726      0             -148.27118     -10824.468    \n",
            "      3689   203.93322     -148.38366      0             -148.27821     -11180.797    \n",
            "      3690   224.76035     -148.381        0             -148.26479     -11330.687    \n",
            "      3691   245.29659     -148.38394      0             -148.25711     -11425.472    \n",
            "      3692   262.11541     -148.41229      0             -148.27676     -11403.453    \n",
            "      3693   272.39493     -148.42377      0             -148.28293     -11245.525    \n",
            "      3694   274.9165      -148.42344      0             -148.28129     -10854.439    \n",
            "      3695   269.39499     -148.40861      0             -148.26932     -10487.532    \n",
            "      3696   257.06928     -148.43868      0             -148.30576     -9855.8918    \n",
            "      3697   240.72488     -148.42777      0             -148.30331     -9159.5576    \n",
            "      3698   223.34843     -148.38447      0             -148.26899     -8411.7167    \n",
            "      3699   207.6284      -148.38901      0             -148.28166     -7539.7261    \n",
            "      3700   195.32475     -148.40013      0             -148.29914     -6605.6159    \n",
            "      3701   186.92976     -148.38808      0             -148.29143     -5614.8622    \n",
            "      3702   181.77015     -148.372        0             -148.27802     -4896.1337    \n",
            "      3703   178.24193     -148.34256      0             -148.2504      -3816.3735    \n",
            "      3704   174.23055     -148.3887       0             -148.29861     -2994.0926    \n",
            "      3705   168.02869     -148.35913      0             -148.27225     -2149.5205    \n",
            "      3706   159.08176     -148.35277      0             -148.27052     -1184.7999    \n",
            "      3707   148.07257     -148.37129      0             -148.29473     -347.12373    \n",
            "      3708   136.99074     -148.34678      0             -148.27595      524.53747    \n",
            "      3709   128.92505     -148.34475      0             -148.27809      1210.5005    \n",
            "      3710   127.08276     -148.3149       0             -148.24919      1981.4642    \n",
            "      3711   134.16662     -148.34807      0             -148.2787       2556.0961    \n",
            "      3712   151.68141     -148.33701      0             -148.25859      2953.2905    \n",
            "      3713   179.28053     -148.37523      0             -148.28254      3284.2626    \n",
            "      3714   214.51753     -148.36576      0             -148.25485      3604.3625    \n",
            "      3715   253.41416     -148.40463      0             -148.2736       3581.7223    \n",
            "      3716   291.20259     -148.42251      0             -148.27195      3533.4961    \n",
            "      3717   323.2493      -148.41969      0             -148.25255      3452.5479    \n",
            "      3718   345.71497     -148.44967      0             -148.27092      3257.4045    \n",
            "      3719   356.71186     -148.4422       0             -148.25777      2998.7221    \n",
            "      3720   356.59592     -148.464        0             -148.27963      2711.4182    \n",
            "      3721   346.94103     -148.44934      0             -148.26996      2291.5643    \n",
            "      3722   330.7093      -148.4394       0             -148.26841      1848.2708    \n",
            "      3723   311.29035     -148.42616      0             -148.26521      1360.1462    \n",
            "      3724   291.49678     -148.4542       0             -148.30348      835.99074    \n",
            "      3725   272.91027     -148.44462      0             -148.30351      282.36674    \n",
            "      3726   255.56082     -148.40869      0             -148.27655     -240.50526    \n",
            "      3727   238.33241     -148.38047      0             -148.25724     -664.02084    \n",
            "      3728   219.44837     -148.38825      0             -148.27479     -1361.3532    \n",
            "      3729   197.46632     -148.37353      0             -148.27143     -1665.4538    \n",
            "      3730   171.78154     -148.34491      0             -148.25609     -1956.339     \n",
            "      3731   143.4146      -148.34211      0             -148.26795     -2180.3259    \n",
            "      3732   115.25358     -148.30429      0             -148.2447      -2376.4479    \n",
            "      3733   90.845118     -148.30629      0             -148.25932     -2405.0537    \n",
            "      3734   74.358491     -148.30851      0             -148.27006     -2476.1402    \n",
            "      3735   69.642031     -148.31538      0             -148.27937     -2490.6813    \n",
            "      3736   78.810858     -148.28427      0             -148.24353     -2318.9647    \n",
            "      3737   101.97754     -148.331        0             -148.27827     -2202.1775    \n",
            "      3738   137.08504     -148.32841      0             -148.25753     -1837.398     \n",
            "      3739   179.93599     -148.36421      0             -148.27118     -1648.1157    \n",
            "      3740   225.20457     -148.3484       0             -148.23196     -1197.9881    \n",
            "      3741   267.55691     -148.39848      0             -148.26014     -673.87648    \n",
            "      3742   302.55924     -148.41436      0             -148.25792     -216.36437    \n",
            "      3743   327.47358     -148.4224       0             -148.25308      435.28056    \n",
            "      3744   341.63922     -148.41766      0             -148.24102      1217.5706    \n",
            "      3745   346.14558     -148.42113      0             -148.24216      1940.0819    \n",
            "      3746   343.3059      -148.45978      0             -148.28228      2648.2154    \n",
            "      3747   336.01889     -148.42096      0             -148.24722      3321.2449    \n",
            "      3748   326.60259     -148.42435      0             -148.25548      4137.8815    \n",
            "      3749   316.09908     -148.4067       0             -148.24326      4572.2087    \n",
            "      3750   304.2826      -148.40039      0             -148.24306      5251.354     \n",
            "      3751   289.69883     -148.40193      0             -148.25215      5597.5229    \n",
            "      3752   270.57523     -148.38525      0             -148.24536      5966.1549    \n",
            "      3753   245.44884     -148.37012      0             -148.24321      6204.7071    \n",
            "      3754   213.82359     -148.3648       0             -148.25424      6335.0562    \n",
            "      3755   177.21177     -148.35823      0             -148.26661      6507.3765    \n",
            "      3756   138.67888     -148.31185      0             -148.24014      6605.8116    \n",
            "      3757   102.30853     -148.28956      0             -148.23666      6415.789     \n",
            "      3758   73.02793      -148.2688       0             -148.23104      6357.7154    \n",
            "      3759   54.975094     -148.28594      0             -148.25751      5849.7837    \n",
            "      3760   51.090511     -148.2935       0             -148.26708      5369.244     \n",
            "      3761   62.380987     -148.27947      0             -148.24721      4820.3653    \n",
            "      3762   87.517691     -148.2732       0             -148.22795      4106.9661    \n",
            "      3763   123.08775     -148.30185      0             -148.2382       3398.4184    \n",
            "      3764   164.1446      -148.32651      0             -148.24164      2428.537     \n",
            "      3765   205.33451     -148.31313      0             -148.20696      1591.8914    \n",
            "      3766   241.88198     -148.37851      0             -148.25344      637.81668    \n",
            "      3767   270.61001     -148.38257      0             -148.24265     -249.38845    \n",
            "      3768   289.94968     -148.39234      0             -148.24243     -984.11588    \n",
            "      3769   300.31319     -148.40812      0             -148.25285     -1762.7451    \n",
            "      3770   303.95965     -148.40689      0             -148.24973     -2568.8894    \n",
            "      3771   304.04213     -148.38224      0             -148.22504     -3248.4816    \n",
            "      3772   303.42883     -148.37639      0             -148.21951     -3810.9211    \n",
            "      3773   303.99461     -148.39201      0             -148.23484     -4332.6938    \n",
            "      3774   306.06712     -148.38137      0             -148.22312     -4674.2007    \n",
            "      3775   308.54538     -148.41276      0             -148.25323     -5063.1569    \n",
            "      3776   309.15897     -148.39581      0             -148.23596     -5104.3582    \n",
            "      3777   304.94955     -148.40442      0             -148.24674     -5251.5408    \n",
            "      3778   293.56252     -148.35829      0             -148.2065      -5097.58      \n",
            "      3779   274.17866     -148.35985      0             -148.21809     -4758.8666    \n",
            "      3780   247.63668     -148.3613       0             -148.23326     -4336.3583    \n",
            "      3781   216.36471     -148.31452      0             -148.20265     -3620.5063    \n",
            "      3782   184.68748     -148.30953      0             -148.21404     -2898.2936    \n",
            "      3783   157.29119     -148.28367      0             -148.20234     -2077.9226    \n",
            "      3784   138.38372     -148.29474      0             -148.22319     -1136.0157    \n",
            "      3785   130.79107     -148.24677      0             -148.17915     -55.127876    \n",
            "      3786   135.40086     -148.29239      0             -148.22238      802.80668    \n",
            "      3787   150.79767     -148.29896      0             -148.22099      1854.5263    \n",
            "      3788   173.65549     -148.30176      0             -148.21197      2832.0087    \n",
            "      3789   199.76383     -148.33594      0             -148.23265      3981.0553    \n",
            "      3790   224.69113     -148.3002       0             -148.18402      5046.9659    \n",
            "      3791   245.21068     -148.36123      0             -148.23445      6075.1988    \n",
            "      3792   259.9473      -148.33225      0             -148.19785      7267.1157    \n",
            "      3793   269.42764     -148.34404      0             -148.20473      8183.2423    \n",
            "      3794   276.0102      -148.35419      0             -148.21148      9127.7868    \n",
            "      3795   283.30982     -148.3734       0             -148.22691      10011.455    \n",
            "      3796   294.73047     -148.35309      0             -148.20071      10857.656    \n",
            "      3797   312.22872     -148.38432      0             -148.22289      11381.956    \n",
            "      3798   335.96001     -148.39131      0             -148.21761      11692.017    \n",
            "      3799   363.79469     -148.40024      0             -148.21215      11958.002    \n",
            "      3800   391.78102     -148.42216      0             -148.21959      12094.7      \n",
            "      3801   414.85949     -148.41509      0             -148.20059      12012.294    \n",
            "      3802   428.15117     -148.41325      0             -148.19188      11731.453    \n",
            "      3803   428.19933     -148.42347      0             -148.20207      11322.588    \n",
            "      3804   413.94455     -148.42208      0             -148.20805      10833.887    \n",
            "      3805   387.3379      -148.39923      0             -148.19896      10397.407    \n",
            "      3806   352.32475     -148.3865       0             -148.20433      9668.6226    \n",
            "      3807   314.25023     -148.35492      0             -148.19244      8919.7259    \n",
            "      3808   278.81587     -148.32756      0             -148.1834       8117.2091    \n",
            "      3809   250.2502      -148.29984      0             -148.17045      7153.6869    \n",
            "      3810   230.85971     -148.29559      0             -148.17623      6197.8981    \n",
            "      3811   220.35062     -148.32323      0             -148.2093       5079.9652    \n",
            "      3812   216.52665     -148.31427      0             -148.20232      3994.8901    \n",
            "      3813   215.97685     -148.27353      0             -148.16186      2944.8936    \n",
            "      3814   215.08005     -148.30856      0             -148.19736      1800.628     \n",
            "      3815   211.75029     -148.28114      0             -148.17166      921.44885    \n",
            "      3816   205.4994      -148.28645      0             -148.1802       37.828007    \n",
            "      3817   198.14577     -148.29473      0             -148.19228     -782.47658    \n",
            "      3818   193.29141     -148.26886      0             -148.16892     -1519.8125    \n",
            "      3819   195.31346     -148.31018      0             -148.2092      -1984.6038    \n",
            "      3820   208.282       -148.28699      0             -148.1793      -2675.8406    \n",
            "      3821   234.42578     -148.311        0             -148.18979     -3185.5744    \n",
            "      3822   273.50737     -148.28094      0             -148.13953     -3472.2008    \n",
            "      3823   322.48476     -148.36829      0             -148.20155     -3876.6845    \n",
            "      3824   376.06901     -148.39079      0             -148.19635     -4124.6695    \n",
            "      3825   427.51554     -148.39087      0             -148.16983     -4200.9801    \n",
            "      3826   470.04464     -148.44288      0             -148.19985     -4212.4341    \n",
            "      3827   498.21041     -148.41888      0             -148.16128     -4165.7719    \n",
            "      3828   509.02125     -148.44339      0             -148.18021     -3850.8764    \n",
            "      3829   502.54546     -148.43271      0             -148.17287     -3477.5289    \n",
            "      3830   481.20706     -148.39952      0             -148.15072     -3001.8568    \n",
            "      3831   449.49527     -148.37547      0             -148.14306     -2346.9141    \n",
            "      3832   412.67971     -148.40749      0             -148.19412     -1833.2489    \n",
            "      3833   375.1874      -148.36169      0             -148.1677      -1391.1443    \n",
            "      3834   340.04735     -148.32505      0             -148.14924     -817.97824    \n",
            "      3835   308.21388     -148.31652      0             -148.15716     -394.41323    \n",
            "      3836   278.88728     -148.30844      0             -148.16424     -44.795358    \n",
            "      3837   250.51813     -148.27644      0             -148.14691      295.29901    \n",
            "      3838   221.32502     -148.25064      0             -148.13621      647.31548    \n",
            "      3839   190.46026     -148.25195      0             -148.15348      758.51082    \n",
            "      3840   158.67456     -148.21128      0             -148.12923      1109.7348    \n",
            "      3841   128.51177     -148.2047       0             -148.13826      1220.9484    \n",
            "      3842   104.09336     -148.2243       0             -148.17048      1287.4549    \n",
            "      3843   90.240041     -148.18036      0             -148.13371      1156.0359    \n",
            "      3844   91.399331     -148.20471      0             -148.15746      1044.665     \n",
            "      3845   110.76257     -148.18676      0             -148.12949      844.74333    \n",
            "      3846   149.12155     -148.19974      0             -148.12264      505.13532    \n",
            "      3847   204.28705     -148.24108      0             -148.13546     -65.462469    \n",
            "      3848   271.16144     -148.27966      0             -148.13946     -656.93645    \n",
            "      3849   342.67907     -148.29471      0             -148.11753     -1454.4711    \n",
            "      3850   411.03599     -148.36143      0             -148.14891     -2352.1904    \n",
            "      3851   468.49969     -148.3865       0             -148.14426     -3111.2389    \n",
            "      3852   509.28425     -148.40596      0             -148.14264     -3991.2395    \n",
            "      3853   530.28409     -148.40282      0             -148.12864     -4760.7729    \n",
            "      3854   531.58869     -148.40949      0             -148.13464     -5481.1199    \n",
            "      3855   516.49065     -148.40009      0             -148.13305     -6161.9358    \n",
            "      3856   489.98382     -148.38674      0             -148.1334      -6789.9471    \n",
            "      3857   457.58671     -148.37622      0             -148.13963     -7532.6693    \n",
            "      3858   424.02284     -148.34031      0             -148.12107     -8045.8       \n",
            "      3859   392.28663     -148.3433       0             -148.14047     -8682.6352    \n",
            "      3860   363.3446      -148.30794      0             -148.12008     -9210.6532    \n",
            "      3861   335.93332     -148.29821      0             -148.12452     -9606.5385    \n",
            "      3862   307.33932     -148.2756       0             -148.11669     -10016.376    \n",
            "      3863   275.20537     -148.26382      0             -148.12153     -10095.12     \n",
            "      3864   238.07242     -148.24256      0             -148.11947     -10227.296    \n",
            "      3865   196.76865     -148.23172      0             -148.12998     -10140.845    \n",
            "      3866   154.8068      -148.19617      0             -148.11613     -10019.06     \n",
            "      3867   117.08896     -148.1419       0             -148.08136     -9522.4053    \n",
            "      3868   89.875397     -148.15404      0             -148.10757     -9046.1152    \n",
            "      3869   79.115326     -148.15473      0             -148.11382     -8668.7217    \n",
            "      3870   88.874184     -148.16481      0             -148.11886     -8042.9138    \n",
            "      3871   120.21627     -148.15964      0             -148.09749     -7441.4409    \n",
            "      3872   170.75838     -148.18727      0             -148.09898     -6706.7824    \n",
            "      3873   234.93098     -148.22447      0             -148.103       -6036.5854    \n",
            "      3874   305.03787     -148.2784       0             -148.12068     -5354.721     \n",
            "      3875   372.71667     -148.3034       0             -148.11069     -4590.7146    \n",
            "      3876   430.61476     -148.33661      0             -148.11397     -3562.1994    \n",
            "      3877   473.9511      -148.33417      0             -148.08912     -2658.0618    \n",
            "      3878   501.11837     -148.36351      0             -148.10442     -1559.3657    \n",
            "      3879   513.87675     -148.38622      0             -148.12052     -542.17337    \n",
            "      3880   516.29945     -148.3719       0             -148.10495      511.02833    \n",
            "      3881   513.33702     -148.37923      0             -148.11382      1439.549     \n",
            "      3882   509.38767     -148.37706      0             -148.11369      2327.4404    \n",
            "      3883   506.68877     -148.37821      0             -148.11623      2951.7932    \n",
            "      3884   504.9974      -148.36362      0             -148.10252      3536.4767    \n",
            "      3885   501.86722     -148.37461      0             -148.11513      4004.0297    \n",
            "      3886   493.04241     -148.34098      0             -148.08606      4397.4076    \n",
            "      3887   474.37133     -148.33329      0             -148.08802      4478.4906    \n",
            "      3888   443.29503     -148.28546      0             -148.05626      4659.6634    \n",
            "      3889   399.87574     -148.28838      0             -148.08163      4581.1509    \n",
            "      3890   347.25859     -148.267        0             -148.08745      4584.7904    \n",
            "      3891   291.52438     -148.22323      0             -148.0725       4367.4859    \n",
            "      3892   240.28706     -148.20959      0             -148.08535      3955.7698    \n",
            "      3893   201.12219     -148.17632      0             -148.07233      3486.7861    \n",
            "      3894   180.02708     -148.17011      0             -148.07703      2863.013     \n",
            "      3895   180.31346     -148.15556      0             -148.06234      2090.8296    \n",
            "      3896   201.21973     -148.16306      0             -148.05902      1088.8884    \n",
            "      3897   238.3763      -148.1906       0             -148.06735     -29.412908    \n",
            "      3898   285.14101     -148.1974       0             -148.04997     -1094.3878    \n",
            "      3899   333.62928     -148.24586      0             -148.07336     -2289.9363    \n",
            "      3900   376.76927     -148.24084      0             -148.04603     -3423.3833    \n",
            "      3901   409.94708     -148.28547      0             -148.07351     -4637.2983    \n",
            "      3902   431.6818      -148.31499      0             -148.09179     -5760.3944    \n",
            "      3903   443.55158     -148.3095       0             -148.08016     -6711.2356    \n",
            "      3904   449.9926      -148.27787      0             -148.04521     -7663.5697    \n",
            "      3905   456.41099     -148.30264      0             -148.06666     -8641.1806    \n",
            "      3906   467.53121     -148.30026      0             -148.05853     -9535.2261    \n",
            "      3907   486.3856      -148.30771      0             -148.05623     -10355.208    \n",
            "      3908   512.7534      -148.32605      0             -148.06094     -11137.034    \n",
            "      3909   543.34554     -148.35135      0             -148.07042     -11848.567    \n",
            "      3910   572.76355     -148.35346      0             -148.05732     -12388.686    \n",
            "      3911   594.59948     -148.36953      0             -148.0621      -12726.961    \n",
            "      3912   602.99219     -148.35894      0             -148.04717     -13024.007    \n",
            "      3913   594.39571     -148.36704      0             -148.05971     -12929.109    \n",
            "      3914   568.82791     -148.34844      0             -148.05434     -12809.202    \n",
            "      3915   529.84386     -148.32713      0             -148.05318     -12320.64     \n",
            "      3916   483.21107     -148.28701      0             -148.03718     -11751.993    \n",
            "      3917   435.82732     -148.25993      0             -148.03459     -11101.287    \n",
            "      3918   394.47657     -148.28131      0             -148.07735     -10239.644    \n",
            "      3919   363.72627     -148.22929      0             -148.04123     -9268.5099    \n",
            "      3920   345.33806     -148.22635      0             -148.0478      -8393.8028    \n",
            "      3921   338.22013     -148.21397      0             -148.0391      -7346.5876    \n",
            "      3922   338.57347     -148.20942      0             -148.03436     -6330.6044    \n",
            "      3923   341.35144     -148.20785      0             -148.03136     -5114.422     \n",
            "      3924   342.08617     -148.21533      0             -148.03846     -4066.9205    \n",
            "      3925   337.45629     -148.21816      0             -148.04368     -2777.2628    \n",
            "      3926   326.52317     -148.20595      0             -148.03712     -1563.1535    \n",
            "      3927   311.6232      -148.16485      0             -148.00373     -127.0927     \n",
            "      3928   297.77744     -148.20292      0             -148.04896      944.39128    \n",
            "      3929   290.8397      -148.16226      0             -148.01188      2215.9179    \n",
            "      3930   296.74968     -148.19378      0             -148.04035      3328.9761    \n",
            "      3931   319.9041      -148.19798      0             -148.03258      4228.6712    \n",
            "      3932   361.50257     -148.2415       0             -148.05459      4852.1261    \n",
            "      3933   419.28039     -148.27866      0             -148.06188      5410.3568    \n",
            "      3934   487.36594     -148.28306      0             -148.03108      5801.0948    \n",
            "      3935   557.23845     -148.30519      0             -148.01708      5931.241     \n",
            "      3936   619.51643     -148.35881      0             -148.0385       5905.8259    \n",
            "      3937   665.99975     -148.37909      0             -148.03474      5921.6674    \n",
            "      3938   690.94808     -148.37027      0             -148.01302      5596.7376    \n",
            "      3939   692.23684     -148.38589      0             -148.02798      5351.0604    \n",
            "      3940   671.99101     -148.3498       0             -148.00235      4943.5339    \n",
            "      3941   635.61901     -148.36827      0             -148.03963      4487.0331    \n",
            "      3942   590.17666     -148.32488      0             -148.01973      3952.6401    \n",
            "      3943   542.7601      -148.29026      0             -148.00963      3249.4517    \n",
            "      3944   498.75484     -148.26122      0             -148.00334      2548.6127    \n",
            "      3945   460.51956     -148.27516      0             -148.03705      1652.4382    \n",
            "      3946   427.36452     -148.23632      0             -148.01536      807.22053    \n",
            "      3947   396.47993     -148.20769      0             -148.0027       76.093263    \n",
            "      3948   363.60384     -148.19123      0             -148.00324     -787.68333    \n",
            "      3949   325.34471     -148.17621      0             -148.008       -1562.134     \n",
            "      3950   281.12233     -148.15307      0             -148.00771     -2069.7127    \n",
            "      3951   233.13765     -148.14129      0             -148.02075     -2485.6733    \n",
            "      3952   187.09589     -148.1261       0             -148.02937     -2666.9485    \n",
            "      3953   151.1334      -148.10022      0             -148.02208     -2692.4856    \n",
            "      3954   133.66988     -148.11017      0             -148.04105     -2778.6317    \n",
            "      3955   141.44948     -148.06328      0             -147.99014     -2579.4043    \n",
            "      3956   177.91106     -148.09022      0             -147.99823     -2285.8832    \n",
            "      3957   242.04336     -148.14813      0             -148.02298     -1968.0914    \n",
            "      3958   327.97663     -148.17307      0             -148.00349     -1556.9419    \n",
            "      3959   426.14237     -148.22588      0             -148.00555     -982.27267    \n",
            "      3960   525.00541     -148.28081      0             -148.00936     -154.0274     \n",
            "      3961   612.90828     -148.32261      0             -148.00571      775.60414    \n",
            "      3962   680.67187     -148.34862      0             -147.99669      1966.4394    \n",
            "      3963   723.50605     -148.38139      0             -148.00731      3234.966     \n",
            "      3964   740.72546     -148.40876      0             -148.02577      4718.8105    \n",
            "      3965   735.65451     -148.37922      0             -147.99886      6252.5444    \n",
            "      3966   714.3492      -148.36789      0             -147.99854      7868.5354    \n",
            "      3967   683.35163     -148.38371      0             -148.03039      9378.7058    \n",
            "      3968   647.57248     -148.34678      0             -148.01196      10781.911    \n",
            "      3969   609.5677      -148.33798      0             -148.02281      12179.716    \n",
            "      3970   569.23445     -148.29321      0             -147.99889      13385.937    \n",
            "      3971   524.40563     -148.31096      0             -148.03983      14324.268    \n",
            "      3972   472.54777     -148.25342      0             -148.00909      15199.679    \n",
            "      3973   412.58611     -148.20817      0             -147.99484      15826.235    \n",
            "      3974   345.97243     -148.20286      0             -148.02398      16286.274    \n",
            "      3975   277.01139     -148.14484      0             -148.00162      16655.57     \n",
            "      3976   212.84541     -148.14289      0             -148.03284      16760.097    \n",
            "      3977   162.55555     -148.10459      0             -148.02054      16762.488    \n",
            "      3978   134.89062     -148.10732      0             -148.03758      16460.046    \n",
            "      3979   136.33272     -148.09944      0             -148.02895      15771.403    \n",
            "      3980   169.82885     -148.11404      0             -148.02623      14890.319    \n",
            "      3981   233.52717     -148.13132      0             -148.01058      13848.77     \n",
            "      3982   320.69476     -148.16231      0             -147.9965       12502.574    \n",
            "      3983   420.62106     -148.22109      0             -148.00362      10881.532    \n",
            "      3984   521.08563     -148.28453      0             -148.01511      9106.0987    \n",
            "      3985   609.96812     -148.33002      0             -148.01464      7334.7219    \n",
            "      3986   677.94839     -148.34393      0             -147.99341      5483.7812    \n",
            "      3987   719.78953     -148.39302      0             -148.02086      3762.0806    \n",
            "      3988   734.89129     -148.41619      0             -148.03623      2069.8004    \n",
            "      3989   726.91888     -148.40493      0             -148.02908      290.71544    \n",
            "      3990   702.34266     -148.40269      0             -148.03955     -1298.92      \n",
            "      3991   669.09318     -148.37091      0             -148.02496     -2780.7753    \n",
            "      3992   633.81584     -148.35515      0             -148.02744     -4092.0796    \n",
            "      3993   600.51389     -148.34456      0             -148.03407     -5645.0281    \n",
            "      3994   570.53935     -148.29499      0             -148           -6674.7382    \n",
            "      3995   542.47807     -148.30637      0             -148.02589     -7775.6301    \n",
            "      3996   513.28916     -148.29048      0             -148.02509     -8538.6575    \n",
            "      3997   480.05894     -148.25284      0             -148.00463     -8979.7982    \n",
            "      3998   440.55518     -148.26714      0             -148.03935     -9203.6764    \n",
            "      3999   395.31112     -148.24658      0             -148.04219     -9016.7924    \n",
            "      4000   347.74128     -148.22171      0             -148.04191     -8657.8821    \n",
            "      4001   303.13095     -148.1946       0             -148.03787     -7956.0832    \n",
            "      4002   268.16619     -148.15806      0             -148.0194      -7100.3148    \n",
            "      4003   248.8078      -148.13838      0             -148.00974     -6110.753     \n",
            "      4004   249.22332     -148.15869      0             -148.02983     -4918.0701    \n",
            "      4005   270.46796     -148.18661      0             -148.04676     -3642.6537    \n",
            "      4006   309.8248      -148.18349      0             -148.0233      -2258.7625    \n",
            "      4007   361.21859     -148.22199      0             -148.03523     -960.39674    \n",
            "      4008   416.6295      -148.22076      0             -148.00534      567.00004    \n",
            "      4009   467.61038     -148.28647      0             -148.0447       1997.7245    \n",
            "      4010   506.52546     -148.30824      0             -148.04635      3553.6817    \n",
            "      4011   529.16369     -148.29425      0             -148.02065      5123.6011    \n",
            "      4012   534.87327     -148.31467      0             -148.03812      6729.7679    \n",
            "      4013   526.81996     -148.31713      0             -148.04474      8237.266     \n",
            "      4014   511.07984     -148.2969       0             -148.03265      9594.0443    \n",
            "      4015   494.73417     -148.31187      0             -148.05608      10833.091    \n",
            "      4016   484.3109      -148.28607      0             -148.03566      11734.154    \n",
            "      4017   483.85988     -148.2581       0             -148.00792      12577.2      \n",
            "      4018   494.20802     -148.2902       0             -148.03467      12905.688    \n",
            "      4019   512.56175     -148.30662      0             -148.0416       13133.805    \n",
            "      4020   533.44413     -148.35516      0             -148.07935      12844.582    \n",
            "      4021   549.89714     -148.32005      0             -148.03574      12502.395    \n",
            "      4022   555.76936     -148.33644      0             -148.04909      11963.177    \n",
            "      4023   546.73511     -148.3319       0             -148.04921      11274.452    \n",
            "      4024   522.07011     -148.32638      0             -148.05644      10396.861    \n",
            "      4025   484.66761     -148.28648      0             -148.03589      9439.6682    \n",
            "      4026   439.91527     -148.25999      0             -148.03253      8413.9745    \n",
            "      4027   395.13588     -148.23401      0             -148.02971      7140.2755    \n",
            "      4028   356.80185     -148.24914      0             -148.06465      5863.2162    \n",
            "      4029   329.51365     -148.21431      0             -148.04394      4355.6681    \n",
            "      4030   314.77938     -148.21891      0             -148.05615      2949.6633    \n",
            "      4031   310.49858     -148.20101      0             -148.04047      1408.4152    \n",
            "      4032   312.20322     -148.2432       0             -148.08177     -126.33814    \n",
            "      4033   314.36828     -148.22192      0             -148.05938     -1527.1225    \n",
            "      4034   312.24442     -148.21811      0             -148.05667     -2789.9643    \n",
            "      4035   303.14882     -148.19122      0             -148.03448     -3911.335     \n",
            "      4036   287.38843     -148.1953       0             -148.04671     -4846.6546    \n",
            "      4037   269.27423     -148.19524      0             -148.05601     -5693.9172    \n",
            "      4038   254.97701     -148.19764      0             -148.0658      -6320.708     \n",
            "      4039   251.32676     -148.18172      0             -148.05177     -6806.5095    \n",
            "      4040   264.04938     -148.20394      0             -148.06742     -7285.5817    \n",
            "      4041   296.01387     -148.23058      0             -148.07752     -7473.5142    \n",
            "      4042   346.29079     -148.25261      0             -148.07357     -7544.8752    \n",
            "      4043   409.6807      -148.29077      0             -148.07895     -7661.4776    \n",
            "      4044   478.03324     -148.30596      0             -148.05879     -7483.0648    \n",
            "      4045   541.6818      -148.37641      0             -148.09634     -7122.2557    \n",
            "      4046   591.34359     -148.38974      0             -148.08399     -6494.591     \n",
            "      4047   620.354       -148.3877       0             -148.06695     -5888.8657    \n",
            "      4048   625.44968     -148.41354      0             -148.09016     -4952.5639    \n",
            "      4049   607.80426     -148.39318      0             -148.07892     -3838.6814    \n",
            "      4050   572.01986     -148.38483      0             -148.08908     -2809.2242    \n",
            "      4051   524.63819     -148.36987      0             -148.09861     -1647.9559    \n",
            "      4052   472.33349     -148.3167       0             -148.07249     -462.15506    \n",
            "      4053   420.87342     -148.29213      0             -148.07452      620.88888    \n",
            "      4054   373.744       -148.26866      0             -148.07542      1588.3737    \n",
            "      4055   331.86343     -148.27691      0             -148.10533      2377.0085    \n",
            "      4056   294.34915     -148.22078      0             -148.06859      3242.4974    \n",
            "      4057   259.33889     -148.2238       0             -148.08971      3718.9493    \n",
            "      4058   225.1722      -148.20939      0             -148.09297      4285.665     \n",
            "      4059   191.7956      -148.19691      0             -148.09774      4573.3479    \n",
            "      4060   161.02329     -148.20307      0             -148.11982      4881.5569    \n",
            "      4061   136.46835     -148.16192      0             -148.09136      4916.7709    \n",
            "      4062   122.94682     -148.16663      0             -148.10306      4959.8244    \n",
            "      4063   125.40443     -148.1636       0             -148.09876      4685.6278    \n",
            "      4064   147.62023     -148.18129      0             -148.10497      4222.1187    \n",
            "      4065   190.89102     -148.17592      0             -148.07722      3522.3538    \n",
            "      4066   253.0975      -148.24561      0             -148.11475      2570.172     \n",
            "      4067   328.97508     -148.25854      0             -148.08845      1635.6508    \n",
            "      4068   410.57808     -148.30199      0             -148.0897       422.45945    \n",
            "      4069   488.40113     -148.35738      0             -148.10486     -946.66628    \n",
            "      4070   553.07802     -148.38714      0             -148.10117     -2423.3775    \n",
            "      4071   597.01712     -148.38576      0             -148.07708     -3843.263     \n",
            "      4072   615.83337     -148.44209      0             -148.12368     -5378.0946    \n",
            "      4073   608.7004      -148.43324      0             -148.11852     -6709.8957    \n",
            "      4074   578.72666     -148.40131      0             -148.10209     -8010.1889    \n",
            "      4075   532.22116     -148.37754      0             -148.10236     -9201.7502    \n",
            "      4076   476.71334     -148.37246      0             -148.12598     -10380.5      \n",
            "      4077   419.34263     -148.33966      0             -148.12285     -11404.526    \n",
            "      4078   365.71583     -148.30465      0             -148.11556     -12358.166    \n",
            "      4079   319.11415     -148.30948      0             -148.14449     -13267.607    \n",
            "      4080   280.37823     -148.26229      0             -148.11732     -14049.848    \n",
            "      4081   248.10441     -148.26196      0             -148.13368     -14650.824    \n",
            "      4082   220.15204     -148.24112      0             -148.12729     -14902.384    \n",
            "      4083   194.53336     -148.25105      0             -148.15047     -15182.117    \n",
            "      4084   170.31894     -148.21517      0             -148.1271      -15087.367    \n",
            "      4085   148.62254     -148.21951      0             -148.14267     -14919.085    \n",
            "      4086   132.29651     -148.20843      0             -148.14003     -14377.469    \n",
            "      4087   125.13195     -148.20709      0             -148.14239     -13597.524    \n",
            "      4088   130.98439     -148.20974      0             -148.14201     -12718.759    \n",
            "      4089   152.58741     -148.24477      0             -148.16588     -11999.542    \n",
            "      4090   190.41824     -148.24089      0             -148.14243     -10866.76     \n",
            "      4091   242.17927     -148.28701      0             -148.16179     -9714.5822    \n",
            "      4092   302.87627     -148.27847      0             -148.12187     -8361.3298    \n",
            "      4093   365.39926     -148.3402       0             -148.15127     -7075.9632    \n",
            "      4094   421.89553     -148.36562      0             -148.14749     -5767.6266    \n",
            "      4095   465.16541     -148.38789      0             -148.14738     -4171.9103    \n",
            "      4096   490.19167     -148.40003      0             -148.14658     -2678.6207    \n",
            "      4097   495.0617      -148.39211      0             -148.13615     -1042.8671    \n",
            "      4098   481.40526     -148.39652      0             -148.14761      529.34085    \n",
            "      4099   453.42846     -148.40815      0             -148.17371      2080.3799    \n",
            "      4100   417.48244     -148.34886      0             -148.133        3539.7016    \n",
            "      4101   380.2722      -148.34793      0             -148.15131      4850.6241    \n",
            "      4102   347.06007     -148.32984      0             -148.1504       5972.5459    \n",
            "      4103   321.30171     -148.30767      0             -148.14154      6801.5287    \n",
            "      4104   303.82434     -148.34296      0             -148.18587      7346.0536    \n",
            "      4105   293.00191     -148.28695      0             -148.13545      7849.798     \n",
            "      4106   285.83809     -148.31938      0             -148.17159      8058.0252    \n",
            "      4107   279.07442     -148.2909       0             -148.1466       7916.3123    \n",
            "      4108   270.1172      -148.31003      0             -148.17037      7922.9946    \n",
            "      4109   257.79295     -148.27539      0             -148.1421       7596.0326    \n",
            "      4110   243.16151     -148.29858      0             -148.17286      7145.0876    \n",
            "      4111   228.87925     -148.26779      0             -148.14945      6498.7368    \n",
            "      4112   218.41718     -148.30488      0             -148.19195      5657.6948    \n",
            "      4113   215.39134     -148.26913      0             -148.15777      4760.4494    \n",
            "      4114   221.96709     -148.28911      0             -148.17435      3754.9347    \n",
            "      4115   237.95218     -148.28666      0             -148.16363      2552.2133    \n",
            "      4116   261.04403     -148.32924      0             -148.19427      1266.2736    \n",
            "      4117   286.77055     -148.30537      0             -148.1571      -162.81135    \n",
            "      4118   309.87552     -148.34123      0             -148.18102     -1341.2045    \n",
            "      4119   325.58424     -148.33508      0             -148.16674     -2684.7535    \n",
            "      4120   330.60508     -148.36087      0             -148.18994     -3986.5453    \n",
            "      4121   323.99014     -148.34595      0             -148.17843     -4920.8761    \n",
            "      4122   307.76632     -148.33234      0             -148.17321     -5972.2709    \n",
            "      4123   286.19445     -148.32757      0             -148.1796      -6750.3755    \n",
            "      4124   265.1407      -148.32453      0             -148.18744     -7427.7754    \n",
            "      4125   250.54437     -148.30484      0             -148.1753      -7820.5431    \n",
            "      4126   246.39532     -148.31669      0             -148.18929     -8261.825     \n",
            "      4127   254.49541     -148.31517      0             -148.18359     -8504.7862    \n",
            "      4128   273.9458      -148.33936      0             -148.19772     -8571.6874    \n",
            "      4129   301.28291     -148.34827      0             -148.1925      -8453.9013    \n",
            "      4130   331.01447     -148.35537      0             -148.18422     -8277.0278    \n",
            "      4131   357.59251     -148.37068      0             -148.18579     -7698.4748    \n",
            "      4132   376.17471     -148.4119       0             -148.21741     -7033.2686    \n",
            "      4133   383.35869     -148.38935      0             -148.19114     -6169.3906    \n",
            "      4134   378.56946     -148.41611      0             -148.22037     -5152.9185    \n",
            "      4135   363.49161     -148.37067      0             -148.18273     -3944.6214    \n",
            "      4136   341.18408     -148.36551      0             -148.1891      -2514.4238    \n",
            "      4137   315.69373     -148.3468       0             -148.18357     -1149.98      \n",
            "      4138   290.80216     -148.36699      0             -148.21664      216.04959    \n",
            "      4139   268.96285     -148.3415       0             -148.20244      1548.0831    \n",
            "      4140   251.01728     -148.31721      0             -148.18743      3067.2198    \n",
            "      4141   236.14113     -148.32166      0             -148.19956      4289.8208    \n",
            "      4142   222.73662     -148.3258       0             -148.21063      5383.5763    \n",
            "      4143   209.05908     -148.34187      0             -148.23378      6464.3048    \n",
            "      4144   193.85296     -148.32345      0             -148.22322      7570.1151    \n",
            "      4145   177.61857     -148.26699      0             -148.17516      8515.65      \n",
            "      4146   162.27152     -148.29183      0             -148.20793      9218.0033    \n",
            "      4147   150.97093     -148.29814      0             -148.22008      9663.8584    \n",
            "      4148   147.80666     -148.28462      0             -148.20819      10133.234    \n",
            "      4149   156.13715     -148.28788      0             -148.20715      10385.178    \n",
            "      4150   178.04799     -148.31506      0             -148.223        10263.936    \n",
            "      4151   213.66049     -148.34613      0             -148.23566      9858.3693    \n",
            "      4152   260.3763      -148.31799      0             -148.18337      9352.2341    \n",
            "      4153   313.23298     -148.40236      0             -148.2404       8557.0317    \n",
            "      4154   365.48371     -148.42234      0             -148.23337      7497.8406    \n",
            "      4155   410.18935     -148.43625      0             -148.22416      6398.3699    \n",
            "      4156   441.26319     -148.46042      0             -148.23226      5078.9031    \n",
            "      4157   454.36156     -148.43299      0             -148.19807      3973.2433    \n",
            "      4158   448.17655     -148.45012      0             -148.2184       2741.6151    \n",
            "      4159   424.1465      -148.43528      0             -148.21598      1485.2706    \n",
            "      4160   386.37712     -148.41476      0             -148.21499      198.95472    \n",
            "      4161   340.10153     -148.43181      0             -148.25596     -990.80752    \n",
            "      4162   291.1131      -148.38586      0             -148.23534     -2034.5753    \n",
            "      4163   244.6905      -148.35821      0             -148.2317      -3103.5534    \n",
            "      4164   204.2913      -148.35154      0             -148.24591     -4115.4017    \n",
            "      4165   171.69224     -148.29454      0             -148.20577     -4822.8875    \n",
            "      4166   146.82164     -148.28847      0             -148.21256     -5443.1029    \n",
            "      4167   128.63836     -148.30257      0             -148.23606     -6107.5268    \n",
            "      4168   115.63428     -148.27625      0             -148.21646     -6234.6313    \n",
            "      4169   106.8989      -148.26567      0             -148.2104      -6356.3013    \n",
            "      4170   102.485       -148.30386      0             -148.25087     -6260.9419    \n",
            "      4171   103.30429     -148.31009      0             -148.25668     -5851.2157    \n",
            "      4172   111.77353     -148.28152      0             -148.22372     -5289.259     \n",
            "      4173   129.98154     -148.30635      0             -148.23914     -4519.8417    \n",
            "      4174   158.87804     -148.33098      0             -148.24884     -3526.0911    \n",
            "      4175   198.27885     -148.33688      0             -148.23436     -2455.7261    \n",
            "      4176   245.86667     -148.33288      0             -148.20576     -1115.3498    \n",
            "      4177   297.20946     -148.38334      0             -148.22967      161.7519     \n",
            "      4178   346.60656     -148.43579      0             -148.25658      1358.9531    \n",
            "      4179   387.59665     -148.4355       0             -148.2351       2946.4994    \n",
            "      4180   414.45873     -148.44936      0             -148.23507      4475.3413    \n",
            "      4181   423.39916     -148.46581      0             -148.24689      6003.522     \n",
            "      4182   413.24047     -148.45298      0             -148.23932      7654.3827    \n",
            "      4183   385.80496     -148.446        0             -148.24652      9018.0666    \n",
            "      4184   345.0448      -148.4143       0             -148.2359       10632.764    \n",
            "      4185   296.89499     -148.39499      0             -148.24149      11806.342    \n",
            "      4186   248.02137     -148.39582      0             -148.26759      12927.256    \n",
            "      4187   204.01047     -148.35535      0             -148.24986      13873.527    \n",
            "      4188   168.80852     -148.36111      0             -148.27383      14465.236    \n",
            "      4189   144.20793     -148.31028      0             -148.23571      15021.36     \n",
            "      4190   129.94851     -148.3128       0             -148.24561      15274.68     \n",
            "      4191   124.1338      -148.30343      0             -148.23925      15184.907    \n",
            "      4192   124.29175     -148.35064      0             -148.28637      14785.267    \n",
            "      4193   128.04752     -148.33254      0             -148.26633      14232.755    \n",
            "      4194   133.66591     -148.33077      0             -148.26166      13686.424    \n",
            "      4195   141.14333     -148.31958      0             -148.2466       12822.141    \n",
            "      4196   151.03986     -148.30785      0             -148.22976      11833.388    \n",
            "      4197   164.39754     -148.33035      0             -148.24535      10690.366    \n",
            "      4198   182.58461     -148.36209      0             -148.26769      9178.4333    \n",
            "      4199   205.8937      -148.37787      0             -148.27142      7731.4794    \n",
            "      4200   233.19919     -148.35882      0             -148.23825      6236.079     \n",
            "      4201   261.88479     -148.37774      0             -148.24234      4631.1667    \n",
            "      4202   288.22116     -148.40737      0             -148.25835      2887.5772    \n",
            "      4203   307.96616     -148.41925      0             -148.26002      1329.3117    \n",
            "      4204   317.70263     -148.41767      0             -148.25341     -218.06005    \n",
            "      4205   315.44354     -148.41037      0             -148.24728     -1563.1282    \n",
            "      4206   301.23333     -148.40872      0             -148.25297     -2965.1885    \n",
            "      4207   277.27448     -148.38974      0             -148.24638     -4054.6892    \n",
            "      4208   247.49205     -148.39864      0             -148.27068     -5133.8136    \n",
            "      4209   217.12342     -148.38084      0             -148.26857     -5908.5992    \n",
            "      4210   191.15939     -148.36158      0             -148.26275     -6621.6325    \n",
            "      4211   173.44382     -148.37379      0             -148.28411     -6983.0877    \n",
            "      4212   166.04026     -148.39964      0             -148.31379     -7362.8789    \n",
            "      4213   168.84175     -148.35494      0             -148.26764     -7490.6682    \n",
            "      4214   179.69189     -148.35956      0             -148.26665     -7408.3105    \n",
            "      4215   195.17196     -148.4038       0             -148.30289     -7269.5678    \n",
            "      4216   211.07822     -148.36383      0             -148.25469     -6948.9546    \n",
            "      4217   223.62244     -148.37396      0             -148.25834     -6254.0158    \n",
            "      4218   230.70413     -148.3839       0             -148.26462     -5459.8492    \n",
            "      4219   231.48723     -148.37192      0             -148.25224     -4624.3584    \n",
            "      4220   226.62379     -148.40878      0             -148.29161     -3658.5147    \n",
            "      4221   218.31909     -148.38736      0             -148.27448     -2586.197     \n",
            "      4222   209.12635     -148.38012      0             -148.27199     -1367.3165    \n",
            "      4223   201.34563     -148.37109      0             -148.26699     -199.53213    \n",
            "      4224   196.53073     -148.3837       0             -148.28208      901.86765    \n",
            "      4225   195.02149     -148.37379      0             -148.27295      1911.7732    \n",
            "      4226   195.92276     -148.38697      0             -148.28567      2957.5773    \n",
            "      4227   197.64724     -148.37655      0             -148.27436      3886.2232    \n",
            "      4228   198.24844     -148.36689      0             -148.26438      4722.5289    \n",
            "      4229   196.20438     -148.372        0             -148.27056      5324.0947    \n",
            "      4230   191.35499     -148.35554      0             -148.2566       5921.0975    \n",
            "      4231   184.44902     -148.3683       0             -148.27293      6397.4442    \n",
            "      4232   177.3724      -148.36655      0             -148.27484      6642.0322    \n",
            "      4233   173.00737     -148.3885       0             -148.29905      6718.5643    \n",
            "      4234   174.11048     -148.35882      0             -148.26879      6729.7777    \n",
            "      4235   182.65432     -148.37116      0             -148.27672      6499.6288    \n",
            "      4236   199.23937     -148.37419      0             -148.27118      6037.5307    \n",
            "      4237   222.74952     -148.36399      0             -148.24882      5278.4529    \n",
            "      4238   250.12998     -148.41395      0             -148.28463      4221.6883    \n",
            "      4239   277.01185     -148.40212      0             -148.25889      3346.6839    \n",
            "      4240   299.02475     -148.44696      0             -148.29235      2254.6922    \n",
            "      4241   312.26413     -148.42683      0             -148.26538      1101.8412    \n",
            "      4242   313.92199     -148.41464      0             -148.25233     -11.741216    \n",
            "      4243   303.02817     -148.42523      0             -148.26855     -1204.3939    \n",
            "      4244   280.75846     -148.42545      0             -148.28029     -2365.2327    \n",
            "      4245   250.28939     -148.41153      0             -148.28212     -3575.8539    \n",
            "      4246   215.8668      -148.41376      0             -148.30215     -4517.3988    \n",
            "      4247   181.91337     -148.3654       0             -148.27134     -5526.5324    \n",
            "      4248   152.19072     -148.35804      0             -148.27935     -6559.8972    \n",
            "      4249   129.1859      -148.31984      0             -148.25305     -7449.3652    \n",
            "      4250   113.80525     -148.36543      0             -148.30659     -8322.0148    \n",
            "      4251   105.43918     -148.32813      0             -148.27362     -8959.3311    \n",
            "      4252   102.59933     -148.33451      0             -148.28146     -9327.9198    \n",
            "      4253   103.57221     -148.33695      0             -148.2834      -9763.5977    \n",
            "      4254   106.92602     -148.34756      0             -148.29227     -10074.795    \n",
            "      4255   112.00084     -148.33782      0             -148.27991     -9986.8891    \n",
            "      4256   119.33692     -148.31231      0             -148.25061     -9783.2412    \n",
            "      4257   130.34066     -148.3217       0             -148.25431     -9485.8334    \n",
            "      4258   146.31921     -148.38736      0             -148.31171     -9066.0036    \n",
            "      4259   168.57694     -148.37638      0             -148.28922     -8477.9564    \n",
            "      4260   197.33311     -148.38661      0             -148.28458     -7754.0842    \n",
            "      4261   230.93537     -148.42183      0             -148.30243     -6817.9068    \n",
            "      4262   266.37466     -148.41933      0             -148.28161     -5917.1854    \n",
            "      4263   299.41246     -148.43314      0             -148.27833     -4999.7202    \n",
            "      4264   324.99904     -148.46103      0             -148.29299     -4015.0133    \n",
            "      4265   338.9712      -148.46484      0             -148.28958     -2833.908     \n",
            "      4266   338.67005     -148.46802      0             -148.29291     -1747.7494    \n",
            "      4267   323.12115     -148.45605      0             -148.28899     -534.06791    \n",
            "      4268   293.89199     -148.42041      0             -148.26846      647.25656    \n",
            "      4269   254.38856     -148.42057      0             -148.28904      1750.9458    \n",
            "      4270   209.3434      -148.38072      0             -148.27248      2839.7063    \n",
            "      4271   164.24083     -148.37519      0             -148.29027      3720.1955    \n",
            "      4272   124.00674     -148.33904      0             -148.27492      4406.1411    \n",
            "      4273   92.141808     -148.33666      0             -148.28902      5077.7275    \n",
            "      4274   70.497672     -148.33023      0             -148.29378      5595.3372    \n",
            "      4275   59.104989     -148.31918      0             -148.28862      5847.8178    \n",
            "      4276   56.610265     -148.29831      0             -148.26904      5921.6851    \n",
            "      4277   61.034629     -148.32106      0             -148.2895       5817.9998    \n",
            "      4278   70.305927     -148.33628      0             -148.29993      5568.4385    \n",
            "      4279   82.912751     -148.32034      0             -148.27747      5236.8888    \n",
            "      4280   98.237758     -148.32504      0             -148.27424      4590.7323    \n",
            "      4281   116.35095     -148.34045      0             -148.28029      3952.9354    \n",
            "      4282   137.71278     -148.35422      0             -148.28302      3204.8384    \n",
            "      4283   162.81809     -148.37073      0             -148.28655      2109.7751    \n",
            "      4284   191.73985     -148.39285      0             -148.29371      1167.4044    \n",
            "      4285   223.21041     -148.41267      0             -148.29726     -75.199755    \n",
            "      4286   254.82378     -148.43017      0             -148.29841     -1421.1776    \n",
            "      4287   283.05143     -148.44111      0             -148.29476     -2650.5296    \n",
            "      4288   303.9033      -148.43923      0             -148.2821      -3943.2351    \n",
            "      4289   314.0947      -148.46774      0             -148.30534     -5329.8223    \n",
            "      4290   311.23951     -148.44204      0             -148.28111     -6531.6805    \n",
            "      4291   294.7049      -148.45464      0             -148.30226     -7742.2617    \n",
            "      4292   266.05421     -148.41241      0             -148.27485     -8636.5947    \n",
            "      4293   228.8037      -148.4013       0             -148.283       -9609.0372    \n",
            "      4294   187.75884     -148.42078      0             -148.3237      -10512.906    \n",
            "      4295   148.17553     -148.35642      0             -148.2798      -10982.364    \n",
            "      4296   114.77345     -148.36232      0             -148.30297     -11532.707    \n",
            "      4297   90.867202     -148.3213       0             -148.27432     -11841.434    \n",
            "      4298   77.990547     -148.35173      0             -148.3114      -12109.173    \n",
            "      4299   75.709718     -148.33489      0             -148.29575     -12238.65     \n",
            "      4300   82.217699     -148.32702      0             -148.28451     -12121.653    \n",
            "      4301   94.724528     -148.35052      0             -148.30154     -11779.845    \n",
            "      4302   110.10487     -148.32044      0             -148.26351     -11327.021    \n",
            "      4303   125.75791     -148.34415      0             -148.27913     -10715.229    \n",
            "      4304   140.18031     -148.35458      0             -148.2821      -9916.9849    \n",
            "      4305   153.0796      -148.37596      0             -148.29681     -9000.5099    \n",
            "      4306   165.21328     -148.38378      0             -148.29835     -7904.4837    \n",
            "      4307   177.80877     -148.39729      0             -148.30536     -6696.3484    \n",
            "      4308   191.76107     -148.38212      0             -148.28298     -5410.6784    \n",
            "      4309   207.39199     -148.38332      0             -148.27609     -4059.0329    \n",
            "      4310   223.99826     -148.40974      0             -148.29392     -2785.4846    \n",
            "      4311   239.78806     -148.41188      0             -148.2879      -1495.6732    \n",
            "      4312   252.34568     -148.40659      0             -148.27612     -111.15076    \n",
            "      4313   259.09147     -148.43018      0             -148.29621      1186.6815    \n",
            "      4314   257.84467     -148.42964      0             -148.29633      2375.3714    \n",
            "      4315   248.00268     -148.39022      0             -148.26199      3422.9196    \n",
            "      4316   230.2484      -148.38808      0             -148.26903      4520.7496    \n",
            "      4317   206.80764     -148.37181      0             -148.26489      5471.7734    \n",
            "      4318   181.63812     -148.38511      0             -148.29119      6230.2885    \n",
            "      4319   158.95463     -148.35948      0             -148.2773       6875.8543    \n",
            "      4320   142.74576     -148.35589      0             -148.28209      7227.914     \n",
            "      4321   135.95428     -148.35907      0             -148.28877      7490.1797    \n",
            "      4322   139.72436     -148.35931      0             -148.28707      7410.9456    \n",
            "      4323   153.16832     -148.35928      0             -148.28008      7202.4818    \n",
            "      4324   173.46871     -148.37229      0             -148.2826       6741.4441    \n",
            "      4325   196.56285     -148.37512      0             -148.27348      6166.6754    \n",
            "      4326   218.36031     -148.3922       0             -148.2793       5467.0018    \n",
            "      4327   235.18277     -148.40857      0             -148.28697      4749.2365    \n",
            "      4328   244.2548      -148.4199       0             -148.29362      3742.1746    \n",
            "      4329   244.89544     -148.39903      0             -148.27241      2758.7328    \n",
            "      4330   238.42238     -148.41821      0             -148.29494      1608.1146    \n",
            "      4331   227.14424     -148.38464      0             -148.2672       555.80669    \n",
            "      4332   214.02635     -148.39619      0             -148.28553     -387.77469    \n",
            "      4333   201.94495     -148.36454      0             -148.26013     -1538.0639    \n",
            "      4334   192.72964     -148.37939      0             -148.27974     -2528.1593    \n",
            "      4335   186.8989      -148.35848      0             -148.26184     -3483.468     \n",
            "      4336   183.7785      -148.34808      0             -148.25305     -4517.4373    \n",
            "      4337   181.88528     -148.34811      0             -148.25407     -5302.6841    \n",
            "      4338   179.5351      -148.36592      0             -148.27309     -5999.6865    \n",
            "      4339   175.52434     -148.34494      0             -148.25418     -6528.0828    \n",
            "      4340   169.83772     -148.36487      0             -148.27705     -6735.203     \n",
            "      4341   163.82762     -148.3449       0             -148.26019     -6758.9621    \n",
            "      4342   159.91437     -148.37034      0             -148.28765     -6859.6775    \n",
            "      4343   160.79408     -148.37849      0             -148.29535     -6555.3191    \n",
            "      4344   168.95138     -148.36326      0             -148.27591     -6034.5669    \n",
            "      4345   185.90332     -148.37785      0             -148.28173     -5336.3256    \n",
            "      4346   211.35897     -148.39604      0             -148.28676     -4607.3256    \n",
            "      4347   243.03774     -148.39243      0             -148.26677     -3712.4014    \n",
            "      4348   276.76146     -148.42657      0             -148.28347     -2687.2653    \n",
            "      4349   307.24355     -148.43321      0             -148.27436     -1552.9772    \n",
            "      4350   329.29214     -148.42379      0             -148.25353     -132.93435    \n",
            "      4351   338.64122     -148.44994      0             -148.27485      1077.7756    \n",
            "      4352   332.94329     -148.42873      0             -148.25659      2685.6029    \n",
            "      4353   312.64862     -148.42614      0             -148.26449      4152.2922    \n",
            "      4354   280.31199     -148.41523      0             -148.2703       5774.5678    \n",
            "      4355   240.2152      -148.39491      0             -148.27071      7179.5141    \n",
            "      4356   197.83227     -148.37512      0             -148.27283      8594.5657    \n",
            "      4357   158.47564     -148.35714      0             -148.2752       9808.6764    \n",
            "      4358   126.30822     -148.32627      0             -148.26096      10969.975    \n",
            "      4359   103.90391     -148.35154      0             -148.29782      11819.248    \n",
            "      4360   91.977144     -148.32725      0             -148.27969      12460.423    \n",
            "      4361   89.558652     -148.32372      0             -148.27741      12960.465    \n",
            "      4362   94.878427     -148.31516      0             -148.2661       13358.053    \n",
            "      4363   105.76086     -148.30465      0             -148.24997      13402.652    \n",
            "      4364   120.48978     -148.32968      0             -148.26739      13241.792    \n",
            "      4365   138.71206     -148.33528      0             -148.26356      12959.806    \n",
            "      4366   160.25521     -148.33386      0             -148.251        12307.323    \n",
            "      4367   185.51553     -148.39249      0             -148.29657      11532.367    \n",
            "      4368   215.09745     -148.36895      0             -148.25773      10720.669    \n",
            "      4369   248.74678     -148.40321      0             -148.2746       9374.9517    \n",
            "      4370   284.90677     -148.41184      0             -148.26453      8142.1585    \n",
            "      4371   320.68551     -148.41298      0             -148.24717      6728.4858    \n",
            "      4372   351.73014     -148.4348       0             -148.25295      4882.9117    \n",
            "      4373   373.24592     -148.4598       0             -148.26682      3284.1258    \n",
            "      4374   381.06626     -148.46876      0             -148.27174      1544.7475    \n",
            "      4375   372.26784     -148.43603      0             -148.24355     -25.109684    \n",
            "      4376   346.05446     -148.42877      0             -148.24985     -1623.4377    \n",
            "      4377   304.41703     -148.40295      0             -148.24556     -3108.6611    \n",
            "      4378   251.57523     -148.39948      0             -148.26941     -4347.3888    \n",
            "      4379   193.52161     -148.36216      0             -148.2621      -5548.6692    \n",
            "      4380   137.31906     -148.33557      0             -148.26457     -6486.8005    \n",
            "      4381   89.374784     -148.29856      0             -148.25235     -7255.3905    \n",
            "      4382   54.723879     -148.28079      0             -148.25249     -7899.9084    \n",
            "      4383   36.241633     -148.26927      0             -148.25053     -8372.4157    \n",
            "      4384   34.432841     -148.27026      0             -148.25245     -8473.0012    \n",
            "      4385   47.545195     -148.30445      0             -148.27987     -8499.9768    \n",
            "      4386   71.985631     -148.28815      0             -148.25093     -8439.6623    \n",
            "      4387   103.71173     -148.29095      0             -148.23732     -7933.9556    \n",
            "      4388   138.77954     -148.35107      0             -148.27931     -7253.8719    \n",
            "      4389   174.22749     -148.34444      0             -148.25436     -6336.7015    \n",
            "      4390   208.11996     -148.3773       0             -148.26969     -5220.8072    \n",
            "      4391   239.79473     -148.35634      0             -148.23236     -3850.6991    \n",
            "      4392   269.55371     -148.39946      0             -148.26009     -2420.8074    \n",
            "      4393   297.51087     -148.40035      0             -148.24653     -842.0337     \n",
            "      4394   323.54447     -148.41269      0             -148.2454       762.56605    \n",
            "      4395   346.60615     -148.40835      0             -148.22914      2457.9037    \n",
            "      4396   364.38954     -148.46012      0             -148.27171      4027.5299    \n",
            "      4397   373.90654     -148.41277      0             -148.21944      5584.7519    \n",
            "      4398   372.06232     -148.4219       0             -148.22953      7097.4232    \n",
            "      4399   356.84673     -148.42903      0             -148.24452      8561.2467    \n",
            "      4400   327.79974     -148.40911      0             -148.23963      9877.8529    \n",
            "      4401   286.82735     -148.38565      0             -148.23735      11047.107    \n",
            "      4402   238.33208     -148.35394      0             -148.23071      12108.558    \n",
            "      4403   188.22316     -148.32556      0             -148.22824      12756.228    \n",
            "      4404   143.17771     -148.32909      0             -148.25506      13415.866    \n",
            "      4405   109.63887     -148.28926      0             -148.23257      13649.985    \n",
            "      4406   92.335629     -148.31389      0             -148.26615      13678.914    \n",
            "      4407   93.363808     -148.25285      0             -148.20457      13333.914    \n",
            "      4408   111.96066     -148.28826      0             -148.23038      12817.457    \n",
            "      4409   144.69392     -148.29381      0             -148.21899      12061.429    \n",
            "      4410   185.94062     -148.33566      0             -148.23952      11079.298    \n",
            "      4411   229.29634     -148.34346      0             -148.22491      9912.0641    \n",
            "      4412   268.94646     -148.39834      0             -148.25928      8413.0827    \n",
            "      4413   300.46787     -148.3758       0             -148.22045      7114.3272    \n",
            "      4414   321.81426     -148.40732      0             -148.24093      5597.2211    \n",
            "      4415   332.96994     -148.387        0             -148.21484      3966.3512    \n",
            "      4416   335.68271     -148.39757      0             -148.22401      2419.8691    \n",
            "      4417   333.26377     -148.38532      0             -148.21301      832.61275    \n",
            "      4418   328.70977     -148.37957      0             -148.20961     -825.54408    \n",
            "      4419   324.04845     -148.40072      0             -148.23318     -2398.5183    \n",
            "      4420   320.12302     -148.39214      0             -148.22662     -3953.7686    \n",
            "      4421   316.27413     -148.38938      0             -148.22586     -5368.4646    \n",
            "      4422   310.55986     -148.38882      0             -148.22825     -6736.3937    \n",
            "      4423   301.06893     -148.36531      0             -148.20964     -7698.7886    \n",
            "      4424   286.77111     -148.36421      0             -148.21594     -8617.418     \n",
            "      4425   267.92848     -148.32806      0             -148.18953     -9102.6646    \n",
            "      4426   246.95049     -148.33271      0             -148.20503     -9472.5303    \n",
            "      4427   227.69935     -148.32902      0             -148.21129     -9743.1778    \n",
            "      4428   214.35096     -148.3285       0             -148.21767     -9623.9425    \n",
            "      4429   211.07297     -148.3201       0             -148.21097     -9357.3488    \n",
            "      4430   220.56492     -148.33496      0             -148.22092     -9011.3128    \n",
            "      4431   243.02887     -148.32675      0             -148.2011      -8338.218     \n",
            "      4432   276.21732     -148.36369      0             -148.22087     -7655.0304    \n",
            "      4433   315.40223     -148.36301      0             -148.19993     -6683.8205    \n",
            "      4434   353.95594     -148.36773      0             -148.18472     -5739.1296    \n",
            "      4435   385.0969      -148.38474      0             -148.18563     -4527.488     \n",
            "      4436   403.60587     -148.39098      0             -148.1823      -3417.2119    \n",
            "      4437   406.11569     -148.40323      0             -148.19326     -2095.7495    \n",
            "      4438   392.2133      -148.39599      0             -148.1932      -704.80121    \n",
            "      4439   364.48276     -148.38913      0             -148.20067      743.4542     \n",
            "      4440   327.73774     -148.3643       0             -148.19485      2127.8643    \n",
            "      4441   288.28363     -148.33339      0             -148.18433      3397.2682    \n",
            "      4442   252.19822     -148.32259      0             -148.19219      4640.6471    \n",
            "      4443   224.2931      -148.30061      0             -148.18464      5682.84      \n",
            "      4444   207.34349     -148.2873       0             -148.1801       6398.3808    \n",
            "      4445   201.92552     -148.30219      0             -148.19779      6935.413     \n",
            "      4446   206.53306     -148.30776      0             -148.20098      7348.932     \n",
            "      4447   218.19324     -148.31028      0             -148.19747      7562.4356    \n",
            "      4448   233.79943     -148.29559      0             -148.17471      7416.6421    \n",
            "      4449   250.82124     -148.33129      0             -148.2016       7181.7081    \n",
            "      4450   268.1829      -148.3161       0             -148.17744      6894.6396    \n",
            "      4451   286.07906     -148.32278      0             -148.17487      6109.6444    \n",
            "      4452   305.43284     -148.33011      0             -148.17219      5269.6468    \n",
            "      4453   327.76548     -148.34205      0             -148.17258      4314.4954    \n",
            "      4454   354.07572     -148.3833       0             -148.20022      2987.4884    \n",
            "      4455   383.88047     -148.3805       0             -148.18202      1508.5625    \n",
            "      4456   414.98805     -148.37676      0             -148.1622      -56.172937    \n",
            "      4457   443.24543     -148.40538      0             -148.1762      -1791.1176    \n",
            "      4458   463.31351     -148.42603      0             -148.18647     -3645.9527    \n",
            "      4459   470.15592     -148.4025       0             -148.15941     -5327.2817    \n",
            "      4460   459.54816     -148.40796      0             -148.17035     -7012.4663    \n",
            "      4461   429.43712     -148.37452      0             -148.15249     -8681.8698    \n",
            "      4462   381.20157     -148.36405      0             -148.16695     -10074.638    \n",
            "      4463   318.86705     -148.34752      0             -148.18265     -11529.104    \n",
            "      4464   249.57293     -148.27398      0             -148.14494     -12653.322    \n",
            "      4465   181.69913     -148.26257      0             -148.16862     -13742.899    \n",
            "      4466   123.4795      -148.23365      0             -148.16981     -14486.718    \n",
            "      4467   81.896401     -148.20496      0             -148.16262     -15189.147    \n",
            "      4468   61.179347     -148.20512      0             -148.17349     -15578.907    \n",
            "      4469   62.416928     -148.20114      0             -148.16887     -16041.49     \n",
            "      4470   83.357569     -148.18726      0             -148.14416     -16204.587    \n",
            "      4471   119.64598     -148.24412      0             -148.18226     -16102.705    \n",
            "      4472   165.7979      -148.23304      0             -148.14731     -15809.587    \n",
            "      4473   216.04155     -148.27826      0             -148.16656     -15274.653    \n",
            "      4474   265.81704     -148.28153      0             -148.14409     -14334.613    \n",
            "      4475   312.32468     -148.2945       0             -148.13302     -13402.894    \n",
            "      4476   354.51425     -148.37113      0             -148.18783     -12105.661    \n",
            "      4477   392.64974     -148.35769      0             -148.15467     -10596.903    \n",
            "      4478   427.71972     -148.38307      0             -148.16192     -8990.5234    \n",
            "      4479   460.03812     -148.37813      0             -148.14027     -7422.9981    \n",
            "      4480   488.42832     -148.39461      0             -148.14207     -5597.2451    \n",
            "      4481   510.01487     -148.39312      0             -148.12942     -3864.8669    \n",
            "      4482   520.62688     -148.41726      0             -148.14808     -2056.1736    \n",
            "      4483   515.87215     -148.41959      0             -148.15287     -306.04506    \n",
            "      4484   492.17608     -148.40143      0             -148.14696      1403.7306    \n",
            "      4485   447.99357     -148.34713      0             -148.1155       3122.129     \n",
            "      4486   384.98933     -148.33598      0             -148.13692      4704.3926    \n",
            "      4487   308.37067     -148.27814      0             -148.1187       6179.5787    \n",
            "      4488   226.07207     -148.21439      0             -148.0975       7522.8334    \n",
            "      4489   147.92918     -148.21039      0             -148.13391      8485.8572    \n",
            "      4490   84.085744     -148.18321      0             -148.13974      9354.4514    \n",
            "      4491   42.716006     -148.12982      0             -148.10773      9914.821     \n",
            "      4492   29.053876     -148.13724      0             -148.12222      10170.072    \n",
            "      4493   44.363955     -148.15549      0             -148.13255      10103.475    \n",
            "      4494   85.764093     -148.17925      0             -148.13491      9722.8624    \n",
            "      4495   147.11245     -148.2054       0             -148.12934      9079.0253    \n",
            "      4496   219.92213     -148.22435      0             -148.11064      8272.5805    \n",
            "      4497   294.98703     -148.28183      0             -148.12931      7154.1422    \n",
            "      4498   364.2271      -148.31245      0             -148.12413      5831.3491    \n",
            "      4499   422.01646     -148.34197      0             -148.12377      4373.1222    \n",
            "      4500   465.91429     -148.34268      0             -148.10178      3056.351     \n",
            "      4501   496.04367     -148.38836      0             -148.13188      1388.6236    \n",
            "      4502   514.65499     -148.37818      0             -148.11209     -204.97361    \n",
            "      4503   524.70293     -148.38495      0             -148.11366     -1925.9971    \n",
            "      4504   528.36757     -148.40361      0             -148.13042     -3756.748     \n",
            "      4505   526.30827     -148.3951       0             -148.12298     -5488.0953    \n",
            "      4506   517.89762     -148.37886      0             -148.11109     -7190.3223    \n",
            "      4507   501.29554     -148.37741      0             -148.11822     -8861.3147    \n",
            "      4508   473.69614     -148.34612      0             -148.1012      -10195.177    \n",
            "      4509   433.76945     -148.33094      0             -148.10666     -11389.165    \n",
            "      4510   382.86408     -148.32328      0             -148.12533     -12356.42     \n",
            "      4511   324.85341     -148.26313      0             -148.09517     -12986.245    \n",
            "      4512   266.21828     -148.26499      0             -148.12735     -13383.483    \n",
            "      4513   214.74672     -148.1991       0             -148.08807     -13508.981    \n",
            "      4514   178.66195     -148.18042      0             -148.08805     -13384.982    \n",
            "      4515   164.49188     -148.17102      0             -148.08597     -13084.007    \n",
            "      4516   175.60784     -148.18105      0             -148.09025     -12499.786    \n",
            "      4517   211.47779     -148.1959       0             -148.08656     -11788.732    \n",
            "      4518   267.63506     -148.22593      0             -148.08756     -10797.639    \n",
            "      4519   336.14989     -148.27225      0             -148.09845     -9770.5729    \n",
            "      4520   407.02035     -148.28899      0             -148.07855     -8443.307     \n",
            "      4521   470.23633     -148.33306      0             -148.08993     -6840.8301    \n",
            "      4522   517.92998     -148.34473      0             -148.07694     -5115.4077    \n",
            "      4523   545.78798     -148.36643      0             -148.08423     -3121.3433    \n",
            "      4524   552.76493     -148.37501      0             -148.08921     -1049.1141    \n",
            "      4525   541.85251     -148.34742      0             -148.06726      1103.7292    \n",
            "      4526   518.52619     -148.35606      0             -148.08796      3347.6883    \n",
            "      4527   489.41977     -148.33885      0             -148.0858       5416.1941    \n",
            "      4528   460.55318     -148.34011      0             -148.10199      7403.3427    \n",
            "      4529   436.15726     -148.30136      0             -148.07585      9098.3508    \n",
            "      4530   418.20907     -148.29217      0             -148.07594      10636.146    \n",
            "      4531   406.29807     -148.29552      0             -148.08544      12075.077    \n",
            "      4532   398.28146     -148.27149      0             -148.06556      13093.364    \n",
            "      4533   391.61146     -148.29676      0             -148.09428      13893.988    \n",
            "      4534   384.58606     -148.27804      0             -148.07919      14386.969    \n",
            "      4535   377.10812     -148.2492       0             -148.05422      14626.629    \n",
            "      4536   370.78202     -148.26153      0             -148.06982      14487.084    \n",
            "      4537   368.80194     -148.26307      0             -148.07239      14129.077    \n",
            "      4538   375.23731     -148.26565      0             -148.07164      13492.463    \n",
            "      4539   393.11827     -148.27781      0             -148.07455      12415.658    \n",
            "      4540   423.33635     -148.26865      0             -148.04977      11176.36     \n",
            "      4541   463.87309     -148.30683      0             -148.06699      9573.2408    \n",
            "      4542   509.57085     -148.31172      0             -148.04825      7807.7403    \n",
            "      4543   553.03495     -148.32714      0             -148.0412       5743.222     \n",
            "      4544   585.51675     -148.37446      0             -148.07172      3656.5827    \n",
            "      4545   598.81506     -148.36356      0             -148.05395      1517.8778    \n",
            "      4546   587.63588     -148.3734       0             -148.06957     -532.10622    \n",
            "      4547   550.63012     -148.32364      0             -148.03894     -2394.8319    \n",
            "      4548   491.03743     -148.31331      0             -148.05942     -4118.3484    \n",
            "      4549   416.41926     -148.28302      0             -148.06771     -5817.2337    \n",
            "      4550   337.09321     -148.23133      0             -148.05704     -7098.046     \n",
            "      4551   264.29426     -148.19027      0             -148.05362     -8274.9227    \n",
            "      4552   208.53233     -148.16927      0             -148.06145     -9239.1582    \n",
            "      4553   177.11232     -148.15871      0             -148.06713     -9895.064     \n",
            "      4554   173.27653     -148.1403       0             -148.0507      -10416.091    \n",
            "      4555   195.7573      -148.15668      0             -148.05547     -10681.182    \n",
            "      4556   239.55266     -148.17854      0             -148.05469     -10582.448    \n",
            "      4557   297.4677      -148.1889       0             -148.03509     -10228.97     \n",
            "      4558   361.06138     -148.21966      0             -148.03298     -9437.5631    \n",
            "      4559   423.02866     -148.26284      0             -148.04412     -8145.502     \n",
            "      4560   478.24611     -148.27942      0             -148.03214     -6760.5305    \n",
            "      4561   524.06188     -148.32894      0             -148.05798     -4923.7159    \n",
            "      4562   560.2647      -148.34481      0             -148.05513     -2848.3662    \n",
            "      4563   587.93994     -148.34705      0             -148.04306     -461.57569    \n",
            "      4564   607.98251     -148.32574      0             -148.01139      1932.015     \n",
            "      4565   620.38093     -148.35318      0             -148.03242      4518.913     \n",
            "      4566   623.34146     -148.36378      0             -148.04149      7002.8334    \n",
            "      4567   613.1827      -148.34647      0             -148.02943      9576.7642    \n",
            "      4568   586.15332     -148.35301      0             -148.04994      11941.893    \n",
            "      4569   538.84645     -148.30389      0             -148.02529      14230.872    \n",
            "      4570   470.26748     -148.29429      0             -148.05114      16447.444    \n",
            "      4571   383.54989     -148.24921      0             -148.0509       18449.192    \n",
            "      4572   285.90062     -148.18445      0             -148.03663      20272.428    \n",
            "      4573   187.99096     -148.12453      0             -148.02733      21795.116    \n",
            "      4574   102.3054      -148.07844      0             -148.02555      22944.585    \n",
            "      4575   41.416468     -148.06121      0             -148.03979      23702.396    \n",
            "      4576   15.129836     -148.05094      0             -148.04312      24103.312    \n",
            "      4577   28.856354     -148.065        0             -148.05008      24021.363    \n",
            "      4578   82.19826      -148.08698      0             -148.04448      23570.058    \n",
            "      4579   168.96673     -148.14306      0             -148.0557       22394.776    \n",
            "      4580   278.05153     -148.18843      0             -148.04466      21052.312    \n",
            "      4581   395.5847      -148.23162      0             -148.02709      19443.144    \n",
            "      4582   507.44223     -148.28526      0             -148.02289      17468.697    \n",
            "      4583   601.4475      -148.34895      0             -148.03797      15337.046    \n",
            "      4584   669.51841     -148.37454      0             -148.02837      12932.403    \n",
            "      4585   707.90816     -148.38458      0             -148.01856      10455.053    \n",
            "      4586   717.87851     -148.40159      0             -148.03042      7947.3258    \n",
            "      4587   704.24865     -148.37951      0             -148.01539      5318.625     \n",
            "      4588   672.98752     -148.37039      0             -148.02243      2795.464     \n",
            "      4589   630.42087     -148.37235      0             -148.0464       224.61471    \n",
            "      4590   581.47337     -148.32485      0             -148.02421     -2171.4567    \n",
            "      4591   528.69395     -148.30425      0             -148.03089     -4365.6555    \n",
            "      4592   472.58046     -148.27057      0             -148.02623     -6473.6396    \n",
            "      4593   413.02479     -148.23792      0             -148.02437     -8231.3514    \n",
            "      4594   350.66428     -148.21562      0             -148.03431     -9667.4174    \n",
            "      4595   287.20063     -148.16629      0             -148.01779     -10653.765    \n",
            "      4596   226.96344     -148.16802      0             -148.05067     -11437.978    \n",
            "      4597   176.20319     -148.10679      0             -148.01569     -11647.078    \n",
            "      4598   142.36169     -148.10278      0             -148.02917     -11668.215    \n",
            "      4599   132.38531     -148.07199      0             -148.00354     -11345.988    \n",
            "      4600   151.18068     -148.11769      0             -148.03953     -10777.02     \n",
            "      4601   200.15973     -148.13338      0             -148.02989     -9951.8378    \n",
            "      4602   276.13387     -148.17499      0             -148.03222     -8957.6429    \n",
            "      4603   371.78819     -148.22143      0             -148.0292      -7854.3922    \n",
            "      4604   475.86154     -148.26795      0             -148.02191     -6388.0448    \n",
            "      4605   575.58327     -148.31894      0             -148.02134     -4663.4458    \n",
            "      4606   658.80608     -148.37248      0             -148.03185     -2886.6351    \n",
            "      4607   715.48269     -148.41181      0             -148.04187     -924.02481    \n",
            "      4608   740.45606     -148.43003      0             -148.04719      1165.603     \n",
            "      4609   733.83679     -148.42068      0             -148.04126      3269.2875    \n",
            "      4610   700.28447     -148.38241      0             -148.02033      5464.4313    \n",
            "      4611   648.15402     -148.35917      0             -148.02404      7394.3837    \n",
            "      4612   587.62273     -148.34288      0             -148.03905      9272.8946    \n",
            "      4613   528.17558     -148.29878      0             -148.02569      10861.292    \n",
            "      4614   477.18996     -148.28907      0             -148.04235      12105.381    \n",
            "      4615   438.4421      -148.27215      0             -148.04546      13083.629    \n",
            "      4616   412.47599     -148.25564      0             -148.04238      13796.808    \n",
            "      4617   397.28272     -148.22844      0             -148.02303      13967.874    \n",
            "      4618   389.38608     -148.20948      0             -148.00816      14010.615    \n",
            "      4619   385.47411     -148.22881      0             -148.0295       13637.774    \n",
            "      4620   383.34034     -148.21801      0             -148.0198       13038.187    \n",
            "      4621   382.9333      -148.22786      0             -148.02987      12245.51     \n",
            "      4622   386.0318      -148.25286      0             -148.05327      11078.334    \n",
            "      4623   395.30145     -148.23301      0             -148.02862      9770.7003    \n",
            "      4624   413.57618     -148.2494       0             -148.03556      8122.163     \n",
            "      4625   442.14886     -148.27002      0             -148.04141      6386.2315    \n",
            "      4626   479.89256     -148.29642      0             -148.04829      4374.9339    \n",
            "      4627   522.66755     -148.30774      0             -148.0375       2230.9357    \n",
            "      4628   564.0274      -148.36196      0             -148.07033      30.633693    \n",
            "      4629   596.13716     -148.33245      0             -148.02422     -2092.5513    \n",
            "      4630   611.48008     -148.35574      0             -148.03958     -4278.6113    \n",
            "      4631   604.91821     -148.33954      0             -148.02677     -6233.3223    \n",
            "      4632   575.11373     -148.3397       0             -148.04234     -7977.8934    \n",
            "      4633   524.91989     -148.3121       0             -148.0407      -9512.3979    \n",
            "      4634   461.22809     -148.27155      0             -148.03308     -10906.442    \n",
            "      4635   394.17101     -148.24465      0             -148.04085     -12005.021    \n",
            "      4636   334.63662     -148.20477      0             -148.03175     -12940.661    \n",
            "      4637   292.03312     -148.18705      0             -148.03605     -13602.204    \n",
            "      4638   272.74929     -148.1764       0             -148.03538     -14136.223    \n",
            "      4639   278.75225     -148.1826       0             -148.03847     -14441.621    \n",
            "      4640   307.31879     -148.21818      0             -148.05928     -14485.619    \n",
            "      4641   352.04551     -148.20973      0             -148.02771     -14298.993    \n",
            "      4642   404.2088      -148.25734      0             -148.04835     -13950.725    \n",
            "      4643   455.01342     -148.2727       0             -148.03743     -13227.553    \n",
            "      4644   497.33608     -148.28598      0             -148.02883     -12358.178    \n",
            "      4645   526.2687      -148.33384      0             -148.06174     -11132.743    \n",
            "      4646   540.75913     -148.32397      0             -148.04437     -9752.8896    \n",
            "      4647   542.9983      -148.3579       0             -148.07715     -8132.4125    \n",
            "      4648   536.81738     -148.29578      0             -148.01822     -6416.4705    \n",
            "      4649   525.92169     -148.33412      0             -148.0622      -4675.9391    \n",
            "      4650   513.05783     -148.34278      0             -148.07751     -2930.3226    \n",
            "      4651   498.78178     -148.31564      0             -148.05775     -1134.1074    \n",
            "      4652   481.08325     -148.30791      0             -148.05917      571.61281    \n",
            "      4653   456.9022      -148.30213      0             -148.06589      2216.5183    \n",
            "      4654   423.15772     -148.29079      0             -148.072        3720.631     \n",
            "      4655   378.13763     -148.27975      0             -148.08423      5105.9302    \n",
            "      4656   322.74464     -148.25808      0             -148.09121      6392.1504    \n",
            "      4657   260.78382     -148.18945      0             -148.05461      7606.3328    \n",
            "      4658   199.22083     -148.13345      0             -148.03045      8499.96      \n",
            "      4659   146.95717     -148.12745      0             -148.05146      9170.4619    \n",
            "      4660   112.921       -148.12417      0             -148.06578      9580.9966    \n",
            "      4661   104.11247     -148.12106      0             -148.06723      9739.0672    \n",
            "      4662   124.41309     -148.13637      0             -148.07204      9661.0398    \n",
            "      4663   173.24898     -148.18395      0             -148.09437      8994.1441    \n",
            "      4664   245.50738     -148.19695      0             -148.07001      8243.518     \n",
            "      4665   332.24868     -148.25034      0             -148.07856      7100.0025    \n",
            "      4666   422.57877     -148.30076      0             -148.08227      5812.5029    \n",
            "      4667   505.37853     -148.33348      0             -148.07218      4355.3259    \n",
            "      4668   570.76365     -148.36965      0             -148.07454      2590.3932    \n",
            "      4669   612.14708     -148.40522      0             -148.08871      966.12511    \n",
            "      4670   627.37362     -148.39792      0             -148.07354     -918.58511    \n",
            "      4671   618.06188     -148.39816      0             -148.0786      -2705.1631    \n",
            "      4672   589.27843     -148.37628      0             -148.0716      -4424.8919    \n",
            "      4673   547.60018     -148.36957      0             -148.08644     -6405.9535    \n",
            "      4674   499.27475     -148.34198      0             -148.08383     -8061.207     \n",
            "      4675   449.18821     -148.3312       0             -148.09895     -9830.7981    \n",
            "      4676   399.91088     -148.2966       0             -148.08983     -11495.497    \n",
            "      4677   352.09947     -148.2716       0             -148.08955     -12977.344    \n",
            "      4678   304.88799     -148.23546      0             -148.07782     -14389.401    \n",
            "      4679   257.19397     -148.21112      0             -148.07814     -15358.051    \n",
            "      4680   208.60437     -148.17922      0             -148.07136     -16270.672    \n",
            "      4681   161.0785      -148.15981      0             -148.07653     -16853.036    \n",
            "      4682   118.53221     -148.14837      0             -148.08709     -17273.39     \n",
            "      4683   86.395831     -148.13362      0             -148.08895     -17320.381    \n",
            "      4684   70.792592     -148.14508      0             -148.10848     -17192.197    \n",
            "      4685   76.917215     -148.10984      0             -148.07007     -16881.688    \n",
            "      4686   108.18012     -148.12875      0             -148.07282     -16355.102    \n",
            "      4687   164.3926      -148.1655       0             -148.0805      -15728.59     \n",
            "      4688   241.49724     -148.20353      0             -148.07867     -14825.652    \n",
            "      4689   332.1514      -148.23552      0             -148.06378     -13851.25     \n",
            "      4690   426.52088     -148.30151      0             -148.08098     -12700.165    \n",
            "      4691   513.66029     -148.3525       0             -148.08692     -11281.402    \n",
            "      4692   583.63511     -148.39695      0             -148.09519     -9720.7212    \n",
            "      4693   629.37595     -148.42765      0             -148.10223     -8023.1489    \n",
            "      4694   647.93181     -148.45897      0             -148.12396     -6235.1373    \n",
            "      4695   640.32464     -148.44109      0             -148.11001     -4359.9076    \n",
            "      4696   610.59312     -148.43355      0             -148.11785     -2259.8023    \n",
            "      4697   565.3685      -148.41505      0             -148.12274     -409.96206    \n",
            "      4698   512.30072     -148.37531      0             -148.11043      1460.8193    \n",
            "      4699   457.80167     -148.35316      0             -148.11646      3108.3569    \n",
            "      4700   406.52126     -148.32467      0             -148.11448      4508.7656    \n",
            "      4701   360.81764     -148.29329      0             -148.10673      5900.2804    \n",
            "      4702   321.12941     -148.27285      0             -148.10681      6944.4525    \n",
            "      4703   286.59386     -148.26569      0             -148.11751      7719.0167    \n",
            "      4704   255.9525      -148.2497       0             -148.11737      8345.3912    \n",
            "      4705   229.14284     -148.21577      0             -148.09729      8699.3335    \n",
            "      4706   207.12721     -148.23849      0             -148.1314       8933.0871    \n",
            "      4707   192.30201     -148.21454      0             -148.11511      8884.6991    \n",
            "      4708   188.05596     -148.20332      0             -148.10609      8649.8969    \n",
            "      4709   198.14255     -148.20972      0             -148.10727      8217.4896    \n",
            "      4710   224.82523     -148.24465      0             -148.1284       7405.8978    \n",
            "      4711   268.24682     -148.26716      0             -148.12846      6514.0681    \n",
            "      4712   325.71821     -148.29093      0             -148.12252      5249.1081    \n",
            "      4713   391.49542     -148.35053      0             -148.14811      4038.0852    \n",
            "      4714   457.54323     -148.37774      0             -148.14117      2476.0991    \n",
            "      4715   515.00994     -148.38196      0             -148.11568      900.54039    \n",
            "      4716   555.98909     -148.41436      0             -148.12689     -566.25199    \n",
            "      4717   574.84144     -148.41239      0             -148.11518     -2100.0314    \n",
            "      4718   569.30847     -148.42982      0             -148.13546     -3475.585     \n",
            "      4719   541.5043      -148.39201      0             -148.11203     -4931.9466    \n",
            "      4720   497.04047     -148.38641      0             -148.12942     -6011.1934    \n",
            "      4721   444.06153     -148.36341      0             -148.13381     -7026.2793    \n",
            "      4722   391.08091     -148.34577      0             -148.14356     -7982.5628    \n",
            "      4723   345.49721     -148.31308      0             -148.13445     -8776.3244    \n",
            "      4724   312.20095     -148.32342      0             -148.162       -9319.1553    \n",
            "      4725   292.79946     -148.3165       0             -148.16511     -9726.1917    \n",
            "      4726   285.97013     -148.29771      0             -148.14985     -9898.2068    \n",
            "      4727   288.00433     -148.30203      0             -148.15312     -9925.417     \n",
            "      4728   294.05582     -148.29727      0             -148.14523     -9620.8845    \n",
            "      4729   300.30336     -148.30547      0             -148.1502      -9257.6226    \n",
            "      4730   304.00888     -148.29231      0             -148.13513     -8330.5698    \n",
            "      4731   304.65552     -148.29212      0             -148.1346      -7428.8657    \n",
            "      4732   303.79929     -148.3169       0             -148.15983     -6223.5749    \n",
            "      4733   303.92977     -148.31739      0             -148.16024     -4657.7035    \n",
            "      4734   307.92176     -148.31784      0             -148.15863     -3204.7669    \n",
            "      4735   317.58694     -148.32764      0             -148.16344     -1655.2273    \n",
            "      4736   332.69581     -148.31366      0             -148.14164      57.997161    \n",
            "      4737   350.81443     -148.33755      0             -148.15617      1766.7813    \n",
            "      4738   367.79244     -148.32248      0             -148.13232      3468.6371    \n",
            "      4739   378.91718     -148.34373      0             -148.14781      5033.6877    \n",
            "      4740   379.66871     -148.36316      0             -148.16686      6851.2473    \n",
            "      4741   367.16276     -148.32712      0             -148.13728      8382.9377    \n",
            "      4742   341.70277     -148.35367      0             -148.17699      9845.2167    \n",
            "      4743   306.56322     -148.31118      0             -148.15268      11249.02     \n",
            "      4744   266.94886     -148.30738      0             -148.16936      12452.835    \n",
            "      4745   229.91737     -148.30969      0             -148.19081      13399.155    \n",
            "      4746   202.46419     -148.26153      0             -148.15685      14287.827    \n",
            "      4747   189.73059     -148.27885      0             -148.18075      14633.281    \n",
            "      4748   194.15564     -148.26418      0             -148.16379      14771.356    \n",
            "      4749   215.00712     -148.26421      0             -148.15304      14704.54     \n",
            "      4750   248.568       -148.34526      0             -148.21674      14178.264    \n",
            "      4751   288.75799     -148.33573      0             -148.18643      13479.873    \n",
            "      4752   328.52354     -148.33253      0             -148.16267      12561.193    \n",
            "      4753   361.3072      -148.36329      0             -148.17648      11282.805    \n",
            "      4754   382.33621     -148.37832      0             -148.18064      10035.197    \n",
            "      4755   389.35348     -148.43192      0             -148.2306       8596.9475    \n",
            "      4756   382.8897      -148.40186      0             -148.20389      7029.8135    \n",
            "      4757   365.42043     -148.37707      0             -148.18813      5519.02      \n",
            "      4758   340.62616     -148.3788       0             -148.20268      3731.4896    \n",
            "      4759   312.74435     -148.37268      0             -148.21098      2034.7745    \n",
            "      4760   285.18718     -148.32063      0             -148.17317      344.41164    \n",
            "      4761   259.72803     -148.35371      0             -148.21942     -1380.9252    \n",
            "      4762   236.34994     -148.31274      0             -148.19053     -2958.8421    \n",
            "      4763   214.04295     -148.30952      0             -148.19885     -4391.3908    \n",
            "      4764   191.29409     -148.30993      0             -148.21103     -5799.6313    \n",
            "      4765   167.34041     -148.28483      0             -148.19831     -6808.0774    \n",
            "      4766   142.5969      -148.26606      0             -148.19233     -7729.5296    \n",
            "      4767   118.87485     -148.28373      0             -148.22227     -8410.216     \n",
            "      4768   99.826607     -148.23461      0             -148.18299     -8550.1135    \n",
            "      4769   89.708361     -148.21239      0             -148.16601     -8812.232     \n",
            "      4770   92.730018     -148.24822      0             -148.20028     -8757.6829    \n",
            "      4771   111.87207     -148.26283      0             -148.20499     -8453.6925    \n",
            "      4772   147.77512     -148.28451      0             -148.2081      -7929.5877    \n",
            "      4773   198.37516     -148.28717      0             -148.1846      -7082.2379    \n",
            "      4774   258.74524     -148.31997      0             -148.18619     -6278.7619    \n",
            "      4775   321.90775     -148.35978      0             -148.19334     -5153.7684    \n",
            "      4776   380.13729     -148.40759      0             -148.21104     -3944.6721    \n",
            "      4777   425.87026     -148.4223       0             -148.20211     -2434.2224    \n",
            "      4778   453.28947     -148.43783      0             -148.20347     -965.38177    \n",
            "      4779   459.75001     -148.455        0             -148.21729      714.29407    \n",
            "      4780   445.25161     -148.43308      0             -148.20286      2500.582     \n",
            "      4781   413.15026     -148.43604      0             -148.22242      4159.0111    \n",
            "      4782   368.68088     -148.39496      0             -148.20434      5986.9513    \n",
            "      4783   318.17602     -148.37287      0             -148.20836      7721.5456    \n",
            "      4784   267.71496     -148.34543      0             -148.20701      9252.203     \n",
            "      4785   221.86461     -148.33922      0             -148.22451      10517.172    \n",
            "      4786   183.47054     -148.30888      0             -148.21402      11654.413    \n",
            "      4787   153.41415     -148.31857      0             -148.23924      12535.535    \n",
            "      4788   130.86593     -148.29852      0             -148.23086      13206.352    \n",
            "      4789   114.8157      -148.27562      0             -148.21626      13857.532    \n",
            "      4790   104.47722     -148.28178      0             -148.22776      14070.7      \n",
            "      4791   99.758832     -148.28852      0             -148.23694      14111.67     \n",
            "      4792   101.78732     -148.27179      0             -148.21917      14038.834    \n",
            "      4793   112.34543     -148.2624       0             -148.20431      13636.566    \n",
            "      4794   133.24782     -148.30044      0             -148.23155      13012.33     \n",
            "      4795   165.786       -148.29679      0             -148.21107      12334.544    \n",
            "      4796   209.5777      -148.31376      0             -148.2054       11170.984    \n",
            "      4797   262.05447     -148.35465      0             -148.21915      9978.7056    \n",
            "      4798   318.6503      -148.41864      0             -148.25389      8556.5881    \n",
            "      4799   373.27804     -148.3888       0             -148.1958       6951.3396    \n",
            "      4800   419.15191     -148.43121      0             -148.21449      5374.5075    \n",
            "      4801   450.0342      -148.45377      0             -148.22109      3814.9457    \n",
            "      4802   461.82484     -148.44904      0             -148.21026      2098.2542    \n",
            "      4803   453.00774     -148.47223      0             -148.238        601.33591    \n",
            "      4804   425.29676     -148.43415      0             -148.21425     -825.61799    \n",
            "      4805   383.23695     -148.39994      0             -148.20179     -2215.6313    \n",
            "      4806   332.8905      -148.40142      0             -148.2293      -3369.6712    \n",
            "      4807   281.18218     -148.39113      0             -148.24575     -4460.5922    \n",
            "      4808   234.45013     -148.37655      0             -148.25533     -5445.1623    \n",
            "      4809   197.02586     -148.34211      0             -148.24024     -6186.1951    \n",
            "      4810   170.90967     -148.31932      0             -148.23095     -6783.3089    \n",
            "      4811   155.66767     -148.29901      0             -148.21853     -7362.196     \n",
            "      4812   149.21052     -148.29819      0             -148.22104     -7727.6821    \n",
            "      4813   148.72812     -148.30962      0             -148.23272     -7641.9862    \n",
            "      4814   151.45001     -148.3088       0             -148.2305      -7710.3573    \n",
            "      4815   155.54493     -148.30489      0             -148.22447     -7411.2573    \n",
            "      4816   160.71969     -148.3343       0             -148.2512      -6944.1016    \n",
            "      4817   167.83906     -148.31752      0             -148.23074     -6219.7936    \n",
            "      4818   178.43807     -148.33569      0             -148.24343     -5384.9319    \n",
            "      4819   194.15806     -148.3436       0             -148.24321     -4514.7014    \n",
            "      4820   215.91395     -148.36519      0             -148.25356     -3608.5412    \n",
            "      4821   243.11599     -148.37106      0             -148.24536     -2526.9025    \n",
            "      4822   273.5838      -148.37373      0             -148.23227     -1643.5533    \n",
            "      4823   303.65948     -148.42044      0             -148.26344     -585.12411    \n",
            "      4824   328.64117     -148.41937      0             -148.24945      449.72142    \n",
            "      4825   344.35263     -148.42524      0             -148.2472       1562.7177    \n",
            "      4826   347.97424     -148.43862      0             -148.2587       2452.927     \n",
            "      4827   338.27665     -148.43269      0             -148.25778      3325.4531    \n",
            "      4828   316.62895     -148.42616      0             -148.26245      4326.6233    \n",
            "      4829   286.55455     -148.40087      0             -148.25271      5059.9004    \n",
            "      4830   252.82228     -148.3684       0             -148.23768      5589.7133    \n",
            "      4831   220.68722     -148.37803      0             -148.26393      6023.6237    \n",
            "      4832   194.84335     -148.3549       0             -148.25416      6324.6456    \n",
            "      4833   178.35938     -148.33152      0             -148.2393       6382.1153    \n",
            "      4834   172.15611     -148.35188      0             -148.26287      6138.0565    \n",
            "      4835   175.06146     -148.35036      0             -148.25984      5686.7017    \n",
            "      4836   184.1341      -148.3694       0             -148.27419      5119.2272    \n",
            "      4837   195.71082     -148.36041      0             -148.25922      4510.1903    \n",
            "      4838   206.18753     -148.35345      0             -148.24684      3687.0957    \n",
            "      4839   212.81364     -148.38486      0             -148.27483      2584.5112    \n",
            "      4840   214.08651     -148.3719       0             -148.26121      1616.334     \n",
            "      4841   210.17098     -148.36446      0             -148.25579      487.78747    \n",
            "      4842   202.87675     -148.34178      0             -148.23688     -706.11753    \n",
            "      4843   194.51432     -148.35367      0             -148.2531      -1952.6641    \n",
            "      4844   187.7127      -148.34462      0             -148.24756     -3086.174     \n",
            "      4845   184.28469     -148.35033      0             -148.25505     -4340.6839    \n",
            "      4846   184.67815     -148.34589      0             -148.2504      -5607.2527    \n",
            "      4847   188.04224     -148.36407      0             -148.26685     -6837.5589    \n",
            "      4848   192.37597     -148.37367      0             -148.27421     -8118.6212    \n",
            "      4849   195.35745     -148.37923      0             -148.27822     -9094.8344    \n",
            "      4850   194.85601     -148.37306      0             -148.27231     -9967.6198    \n",
            "      4851   189.66736     -148.3823       0             -148.28423     -10684.231    \n",
            "      4852   180.1197      -148.36299      0             -148.26986     -11278.278    \n",
            "      4853   167.7818      -148.37106      0             -148.28431     -11707.098    \n",
            "      4854   155.81517     -148.34936      0             -148.2688      -11901.43     \n",
            "      4855   147.74498     -148.32995      0             -148.25356     -11971.268    \n",
            "      4856   146.78833     -148.32057      0             -148.24468     -11745.315    \n",
            "      4857   155.04809     -148.3149       0             -148.23473     -11525.643    \n",
            "      4858   172.71541     -148.36252      0             -148.27322     -11300.034    \n",
            "      4859   197.86953     -148.37264      0             -148.27033     -10753.293    \n",
            "      4860   226.70184     -148.37083      0             -148.25362     -10057.874    \n",
            "      4861   254.43094     -148.40882      0             -148.27727     -9359.7314    \n",
            "      4862   276.4497      -148.36482      0             -148.22189     -8344.3469    \n",
            "      4863   288.84122     -148.43733      0             -148.28799     -7451.938     \n",
            "      4864   289.41239     -148.40595      0             -148.25631     -6334.1488    \n",
            "      4865   278.20137     -148.4417       0             -148.29786     -5125.2806    \n",
            "      4866   257.16072     -148.4213       0             -148.28834     -3848.0808    \n",
            "      4867   229.73419     -148.39682      0             -148.27804     -2500.1136    \n",
            "      4868   200.14167     -148.35936      0             -148.25588     -1244.369     \n",
            "      4869   172.32699     -148.35725      0             -148.26815     -134.91337    \n",
            "      4870   149.02656     -148.36263      0             -148.28558      1121.9802    \n",
            "      4871   131.70669     -148.36697      0             -148.29887      2176.1096    \n",
            "      4872   120.36914     -148.32086      0             -148.25862      3030.2482    \n",
            "      4873   113.61173     -148.33638      0             -148.27764      3875.0144    \n",
            "      4874   109.91323     -148.28522      0             -148.22839      4581.3075    \n",
            "      4875   108.15507     -148.32079      0             -148.26487      5022.809     \n",
            "      4876   107.72198     -148.30909      0             -148.2534       5389.4589    \n",
            "      4877   109.23259     -148.33821      0             -148.28173      5597.8743    \n",
            "      4878   114.44764     -148.31899      0             -148.25982      5769.7452    \n",
            "      4879   125.30258     -148.33833      0             -148.27354      5674.7553    \n",
            "      4880   143.63633     -148.35433      0             -148.28007      5491.9022    \n",
            "      4881   170.54989     -148.38518      0             -148.29699      4958.9265    \n",
            "      4882   205.21126     -148.36424      0             -148.25813      4337.3371    \n",
            "      4883   245.17417     -148.38079      0             -148.25403      3650.6763    \n",
            "      4884   286.46761     -148.40651      0             -148.25839      2773.1467    \n",
            "      4885   323.85537     -148.44087      0             -148.27342      1729.8953    \n",
            "      4886   352.15827     -148.41851      0             -148.23643      545.70194    \n",
            "      4887   367.37685     -148.4651       0             -148.27515     -592.93443    \n",
            "      4888   367.00147     -148.46474      0             -148.27498     -1649.7157    \n",
            "      4889   351.06187     -148.42813      0             -148.24662     -2777.7456    \n",
            "      4890   322.1628      -148.43883      0             -148.27226     -3902.4592    \n",
            "      4891   284.27573     -148.42236      0             -148.27538     -4877.5268    \n",
            "      4892   242.50809     -148.41403      0             -148.28864     -5953.6009    \n",
            "      4893   202.01867     -148.35877      0             -148.25432     -6788.6796    \n",
            "      4894   166.95064     -148.33113      0             -148.24481     -7557.5709    \n",
            "      4895   139.75605     -148.34005      0             -148.26779     -8237.8712    \n",
            "      4896   121.02853     -148.34086      0             -148.27828     -8901.3963    \n",
            "      4897   109.8687      -148.33835      0             -148.28154     -9448.6162    \n",
            "      4898   104.57841     -148.32608      0             -148.27201     -9722.7991    \n",
            "      4899   103.52856     -148.34201      0             -148.28848     -9910.3482    \n",
            "      4900   105.43692     -148.34102      0             -148.28651     -9877.9124    \n",
            "      4901   109.91764     -148.33698      0             -148.28015     -9702.2048    \n",
            "      4902   117.8486      -148.35187      0             -148.29094     -9410.756     \n",
            "      4903   130.822       -148.36         0             -148.29236     -8900.2055    \n",
            "      4904   150.34718     -148.37477      0             -148.29704     -8222.3368    \n",
            "      4905   177.36588     -148.37238      0             -148.28068     -7396.957     \n",
            "      4906   211.56901     -148.38261      0             -148.27322     -6570.8323    \n",
            "      4907   251.01216     -148.3738       0             -148.24401     -5635.5957    \n",
            "      4908   291.99896     -148.42149      0             -148.27051     -4671.9028    \n",
            "      4909   329.78607     -148.46725      0             -148.29674     -3718.9774    \n",
            "      4910   359.48132     -148.45798      0             -148.27211     -2458.6837    \n",
            "      4911   376.86107     -148.46627      0             -148.27142     -1396.9575    \n",
            "      4912   379.36386     -148.46654      0             -148.27039     -142.16964    \n",
            "      4913   366.49938     -148.46488      0             -148.27538      1056.8301    \n",
            "      4914   340.1123      -148.44542      0             -148.26957      2258.6108    \n",
            "      4915   304.10391     -148.42775      0             -148.27052      3312.92      \n",
            "      4916   263.46969     -148.40592      0             -148.26969      4424.8846    \n",
            "      4917   223.47235     -148.38154      0             -148.26599      5199.1431    \n",
            "      4918   188.55931     -148.38461      0             -148.28712      5900.5901    \n",
            "      4919   161.72529     -148.34876      0             -148.26515      6495.4813    \n",
            "      4920   144.04314     -148.36281      0             -148.28834      6702.034     \n",
            "      4921   134.80282     -148.34141      0             -148.27171      6908.6831    \n",
            "      4922   132.2285      -148.35072      0             -148.28235      6938.3059    \n",
            "      4923   133.99806     -148.35748      0             -148.2882       6870.2068    \n",
            "      4924   137.90813     -148.35978      0             -148.28847      6431.1046    \n",
            "      4925   142.60786     -148.33818      0             -148.26445      6055.0905    \n",
            "      4926   147.84823     -148.36127      0             -148.28483      5534.9708    \n",
            "      4927   154.19941     -148.37392      0             -148.29419      4759.9898    \n",
            "      4928   163.02106     -148.34061      0             -148.25632      4030.9017    \n",
            "      4929   175.68687     -148.35713      0             -148.2663       3271.2317    \n",
            "      4930   192.72144     -148.38332      0             -148.28368      2242.7774    \n",
            "      4931   213.70358     -148.38221      0             -148.27172      1284.4732    \n",
            "      4932   236.82925     -148.42096      0             -148.29851      120.11398    \n",
            "      4933   259.08434     -148.42054      0             -148.28658     -938.71299    \n",
            "      4934   276.97318     -148.41309      0             -148.26989     -1882.7417    \n",
            "      4935   287.40606     -148.42958      0             -148.28098     -2821.7072    \n",
            "      4936   288.17169     -148.41817      0             -148.26918     -3616.651     \n",
            "      4937   278.88318     -148.3921       0             -148.2479      -4256.3674    \n",
            "      4938   260.93254     -148.40755      0             -148.27264     -4875.0604    \n",
            "      4939   237.26724     -148.39127      0             -148.26859     -5235.7967    \n",
            "      4940   212.20913     -148.39317      0             -148.28345     -5400.1195    \n",
            "      4941   190.1631      -148.35758      0             -148.25926     -5316.9021    \n",
            "      4942   174.69529     -148.34036      0             -148.25003     -5187.7169    \n",
            "      4943   167.85646     -148.33389      0             -148.2471      -4921.7967    \n",
            "      4944   169.73296     -148.36955      0             -148.28179     -4678.6099    \n",
            "      4945   178.40875     -148.36862      0             -148.27637     -4121.4169    \n",
            "      4946   190.46184     -148.36689      0             -148.26841     -3307.2065    \n",
            "      4947   201.97197     -148.37817      0             -148.27374     -2450.2198    \n",
            "      4948   209.52323     -148.41171      0             -148.30338     -1509.0906    \n",
            "      4949   210.80467     -148.37582      0             -148.26683     -366.45086    \n",
            "      4950   204.94067     -148.39013      0             -148.28417      990.85123    \n",
            "      4951   193.27873     -148.37025      0             -148.27031      2269.757     \n",
            "      4952   178.52104     -148.341        0             -148.24869      3722.0371    \n",
            "      4953   163.96829     -148.34524      0             -148.26046      5138.2211    \n",
            "      4954   152.85422     -148.36199      0             -148.28296      6404.0171    \n",
            "      4955   147.21759     -148.35863      0             -148.28252      7675.0054    \n",
            "      4956   147.65854     -148.35048      0             -148.27414      8919.6287    \n",
            "      4957   153.25128     -148.34624      0             -148.26701      10089.652    \n",
            "      4958   161.71299     -148.33682      0             -148.2532       11039.449    \n",
            "      4959   170.68006     -148.32639      0             -148.23815      11850.182    \n",
            "      4960   178.18097     -148.35759      0             -148.26546      12450.119    \n",
            "      4961   182.90062     -148.35091      0             -148.25635      13012.333    \n",
            "      4962   185.15785     -148.36925      0             -148.27352      13407.037    \n",
            "      4963   186.72822     -148.3799       0             -148.28335      13488.383    \n",
            "      4964   190.05404     -148.35006      0             -148.25179      13575.121    \n",
            "      4965   197.75693     -148.36844      0             -148.26619      13231.997    \n",
            "      4966   211.67857     -148.40057      0             -148.29112      12764.993    \n",
            "      4967   231.90516     -148.39608      0             -148.27618      12111.827    \n",
            "      4968   256.84891     -148.42106      0             -148.28826      11177.219    \n",
            "      4969   283.24887     -148.40772      0             -148.26127      10235.048    \n",
            "      4970   306.50451     -148.43503      0             -148.27655      8937.4766    \n",
            "      4971   321.90871     -148.43454      0             -148.2681       7703.2799    \n",
            "      4972   325.66243     -148.43167      0             -148.26329      6210.4999    \n",
            "      4973   315.96672     -148.43483      0             -148.27146      4944.4444    \n",
            "      4974   293.201       -148.42445      0             -148.27286      3603.1369    \n",
            "      4975   259.9359      -148.39408      0             -148.25968      2145.7576    \n",
            "      4976   220.5888      -148.37813      0             -148.26408      790.48874    \n",
            "      4977   180.49559     -148.35421      0             -148.26089     -324.9917     \n",
            "      4978   144.88292     -148.34291      0             -148.268       -1545.3826    \n",
            "      4979   117.65179     -148.32159      0             -148.26076     -2646.9597    \n",
            "      4980   101.09242     -148.3193       0             -148.26703     -3609.4491    \n",
            "      4981   95.451862     -148.3189       0             -148.26955     -4532.1015    \n",
            "      4982   99.449074     -148.33288      0             -148.28147     -5102.92      \n",
            "      4983   110.75582     -148.31198      0             -148.25472     -5681.7998    \n",
            "      4984   126.64333     -148.30907      0             -148.24359     -5980.9126    \n",
            "      4985   145.05145     -148.35376      0             -148.27876     -6111.8538    \n",
            "      4986   164.73281     -148.37868      0             -148.29351     -6023.9546    \n",
            "      4987   185.42555     -148.36581      0             -148.26994     -5789.1722    \n",
            "      4988   208.13407     -148.3633       0             -148.25569     -5362.8637    \n",
            "      4989   234.11143     -148.3792       0             -148.25816     -4666.5183    \n",
            "      4990   263.82937     -148.39403      0             -148.25762     -3858.8767    \n",
            "      4991   296.95472     -148.41835      0             -148.26481     -3112.6099    \n",
            "      4992   331.63474     -148.4412       0             -148.26973     -2046.9055    \n",
            "      4993   364.41019     -148.4417       0             -148.25329     -1050.6781    \n",
            "      4994   390.76594     -148.47772      0             -148.27568     -73.820615    \n",
            "      4995   406.24914     -148.47782      0             -148.26778      1053.5642    \n",
            "      4996   407.19934     -148.46376      0             -148.25322      2214.4258    \n",
            "      4997   391.50045     -148.46114      0             -148.25872      3237.7185    \n",
            "      4998   359.83631     -148.42577      0             -148.23972      4442.1707    \n",
            "      4999   315.18258     -148.41224      0             -148.24927      5478.3323    \n",
            "      5000   262.64109     -148.38926      0             -148.25347      6406.6327    \n",
            "Loop time of 35.511 on 1 procs for 5000 steps with 5 atoms\n",
            "\n",
            "Performance: 1.217 ns/day, 19.728 hours/ns, 140.801 timesteps/s, 704.007 atom-step/s\n",
            "98.9% CPU use with 1 MPI tasks x 1 OpenMP threads\n",
            "\n",
            "MPI task timing breakdown:\n",
            "Section |  min time  |  avg time  |  max time  |%varavg| %total\n",
            "---------------------------------------------------------------\n",
            "Pair    | 35.29      | 35.29      | 35.29      |   0.0 | 99.38\n",
            "Neigh   | 0.00010064 | 0.00010064 | 0.00010064 |   0.0 |  0.00\n",
            "Comm    | 0.0096415  | 0.0096415  | 0.0096415  |   0.0 |  0.03\n",
            "Output  | 0.13775    | 0.13775    | 0.13775    |   0.0 |  0.39\n",
            "Modify  | 0.066698   | 0.066698   | 0.066698   |   0.0 |  0.19\n",
            "Other   |            | 0.007142   |            |       |  0.02\n",
            "\n",
            "Nlocal:              5 ave           5 max           5 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Nghost:             35 ave          35 max          35 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Neighs:             10 ave          10 max          10 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "FullNghs:           20 ave          20 max          20 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "Total # of neighbors = 20\n",
            "Ave neighs/atom = 4\n",
            "Neighbor list builds = 3\n",
            "Dangerous builds = 0\n",
            "Total wall time: 0:00:37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Radial Function Analysis for COARSENED Molecule."
      ],
      "metadata": {
        "id": "mrU7hDCVEZjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['font.size'] = 30\n",
        "\n",
        "def parse_lammps_rdf(rdffile):\n",
        "    \"\"\"\n",
        "    Parse the RDF file written by LAMMPS\n",
        "\n",
        "    copied from Boris' class code: https://github.com/bkoz37/labutil\n",
        "    \"\"\"\n",
        "    with open(rdffile, 'r') as rdfout:\n",
        "        rdfs = []\n",
        "        buffer = []\n",
        "        for line in rdfout:\n",
        "            values = line.split()\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            elif len(values) == 2:\n",
        "                nbins = values[1]\n",
        "            else:\n",
        "                buffer.append([float(values[1]), float(values[2])])\n",
        "                if len(buffer) == int(nbins):\n",
        "                    frame = np.transpose(np.array(buffer))\n",
        "                    rdfs.append(frame)\n",
        "                    buffer = []\n",
        "    return rdfs\n",
        "\n",
        "rdf = parse_lammps_rdf('./lammps_run/des_coa.rdf')\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "plt.plot(rdf[0][0], rdf[0][1], 'b', linewidth=5, label=\"Coa-NH4, T=300K\")\n",
        "plt.xlabel('r [$\\AA$]')\n",
        "plt.ylabel('g(r)')\n",
        "plt.title(\"Coarsened UNH N-H bond length: {:.3f}$\\AA$\".format(rdf[0][0][np.argmax(rdf[0][1])]))\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "uRBhi9PTEYnM",
        "outputId": "4e06cee0-0511-4e2b-f1d6-77a939236c4c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}