{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c982debe7a4f4e768e1959b7851e48d0": {
          "model_module": "nglview-js-widgets",
          "model_name": "ColormakerRegistryModel",
          "model_module_version": "3.0.8",
          "state": {
            "_dom_classes": [],
            "_model_module": "nglview-js-widgets",
            "_model_module_version": "3.0.8",
            "_model_name": "ColormakerRegistryModel",
            "_msg_ar": [],
            "_msg_q": [],
            "_ready": true,
            "_view_count": null,
            "_view_module": "nglview-js-widgets",
            "_view_module_version": "3.0.8",
            "_view_name": "ColormakerRegistryView",
            "layout": "IPY_MODEL_985c9ee4208341e894c815a69e5b977e"
          }
        },
        "985c9ee4208341e894c815a69e5b977e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8ccef826d924dd4b458096bfd3b5095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede6d765bfb64c05a8625235e38decc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a23a859c344431d8234d7aa56b319ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "003ea2d7ccde44159366b1c3ab8bea00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87621e85c9642d7a4f8d5d3fb22728c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "a3b366f3407049f7b59d01caf3a7d349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PlayModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PlayModel",
            "_playing": false,
            "_repeat": false,
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PlayView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "interval": 100,
            "layout": "IPY_MODEL_ede6d765bfb64c05a8625235e38decc7",
            "max": 109,
            "min": 0,
            "show_repeat": true,
            "step": 1,
            "style": "IPY_MODEL_6a23a859c344431d8234d7aa56b319ad",
            "value": 109,
            "playing": false
          }
        },
        "7044760e402f48fbb8b49d2994d951b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_003ea2d7ccde44159366b1c3ab8bea00",
            "max": 109,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_b87621e85c9642d7a4f8d5d3fb22728c",
            "value": 109
          }
        },
        "4480bc7963d74944b5f53bedb0972666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "736db268196343089bfa653c8d57d8d2": {
          "model_module": "nglview-js-widgets",
          "model_name": "NGLModel",
          "model_module_version": "3.0.8",
          "state": {
            "_camera_orientation": [
              25.39874519728198,
              0,
              0,
              0,
              0,
              25.39874519728198,
              0,
              0,
              0,
              0,
              25.39874519728198,
              0,
              -4.751999855041504,
              -4.751999855041504,
              -4.751999855041504,
              1
            ],
            "_camera_str": "orthographic",
            "_dom_classes": [],
            "_gui_theme": null,
            "_ibtn_fullscreen": "IPY_MODEL_c5c60d1c2e0b47ec89653f7f241d1131",
            "_igui": null,
            "_iplayer": "IPY_MODEL_e699d6f5565e40e39df900d76b123d7e",
            "_model_module": "nglview-js-widgets",
            "_model_module_version": "3.0.8",
            "_model_name": "NGLModel",
            "_ngl_color_dict": {},
            "_ngl_coordinate_resource": {},
            "_ngl_full_stage_parameters": {
              "impostor": true,
              "quality": "medium",
              "workerDefault": true,
              "sampleLevel": 0,
              "backgroundColor": "white",
              "rotateSpeed": 2,
              "zoomSpeed": 1.2,
              "panSpeed": 1,
              "clipNear": 0,
              "clipFar": 100,
              "clipDist": 0,
              "clipMode": "scene",
              "clipScale": "relative",
              "fogNear": 50,
              "fogFar": 100,
              "cameraFov": 40,
              "cameraEyeSep": 0.3,
              "cameraType": "orthographic",
              "lightColor": 14540253,
              "lightIntensity": 1,
              "ambientColor": 14540253,
              "ambientIntensity": 0.2,
              "hoverTimeout": 0,
              "tooltip": true,
              "mousePreset": "default"
            },
            "_ngl_msg_archive": [
              {
                "target": "Stage",
                "type": "call_method",
                "methodName": "loadFile",
                "reconstruc_color_scheme": false,
                "args": [
                  {
                    "type": "blob",
                    "data": "CRYST1   12.414   12.414   12.414  90.00  90.00  90.00 P 1\nMODEL     1\nATOM      1   Si MOL     1       0.000   0.000   0.000  1.00  0.00          SI  \nATOM      2   Si MOL     1       0.000   2.715   2.715  1.00  0.00          SI  \nATOM      3   Si MOL     1       2.715   2.715   0.000  1.00  0.00          SI  \nATOM      4   Si MOL     1       2.715   0.000   2.715  1.00  0.00          SI  \nATOM      5   Si MOL     1       4.073   1.358   4.073  1.00  0.00          SI  \nATOM      6   Si MOL     1       1.358   1.358   1.358  1.00  0.00          SI  \nATOM      7   Si MOL     1       1.358   4.073   4.073  1.00  0.00          SI  \nATOM      8   Si MOL     1       4.073   4.073   1.358  1.00  0.00          SI  \nATOM      9   Si MOL     1       0.000   0.000   5.431  1.00  0.00          SI  \nATOM     10   Si MOL     1       0.000   2.715   8.146  1.00  0.00          SI  \nATOM     11   Si MOL     1       2.715   2.715   5.431  1.00  0.00          SI  \nATOM     12   Si MOL     1       2.715   0.000   8.146  1.00  0.00          SI  \nATOM     13   Si MOL     1       4.073   1.358   9.504  1.00  0.00          SI  \nATOM     14   Si MOL     1       1.358   1.358   6.789  1.00  0.00          SI  \nATOM     15   Si MOL     1       1.358   4.073   9.504  1.00  0.00          SI  \nATOM     16   Si MOL     1       4.073   4.073   6.789  1.00  0.00          SI  \nATOM     17   Si MOL     1       0.000   5.431   0.000  1.00  0.00          SI  \nATOM     18   Si MOL     1       0.000   8.146   2.715  1.00  0.00          SI  \nATOM     19   Si MOL     1       2.715   8.146   0.000  1.00  0.00          SI  \nATOM     20   Si MOL     1       2.715   5.431   2.715  1.00  0.00          SI  \nATOM     21   Si MOL     1       4.073   6.789   4.073  1.00  0.00          SI  \nATOM     22   Si MOL     1       1.358   6.789   1.358  1.00  0.00          SI  \nATOM     23   Si MOL     1       1.358   9.504   4.073  1.00  0.00          SI  \nATOM     24   Si MOL     1       4.073   9.504   1.358  1.00  0.00          SI  \nATOM     25   Si MOL     1       0.000   5.431   5.431  1.00  0.00          SI  \nATOM     26   Si MOL     1       0.000   8.146   8.146  1.00  0.00          SI  \nATOM     27   Si MOL     1       2.715   8.146   5.431  1.00  0.00          SI  \nATOM     28   Si MOL     1       2.715   5.431   8.146  1.00  0.00          SI  \nATOM     29   Si MOL     1       4.073   6.789   9.504  1.00  0.00          SI  \nATOM     30   Si MOL     1       1.358   6.789   6.789  1.00  0.00          SI  \nATOM     31   Si MOL     1       1.358   9.504   9.504  1.00  0.00          SI  \nATOM     32   Si MOL     1       4.073   9.504   6.789  1.00  0.00          SI  \nATOM     33   Si MOL     1       5.431   0.000   0.000  1.00  0.00          SI  \nATOM     34   Si MOL     1       5.431   2.715   2.715  1.00  0.00          SI  \nATOM     35   Si MOL     1       8.146   2.715   0.000  1.00  0.00          SI  \nATOM     36   Si MOL     1       8.146   0.000   2.715  1.00  0.00          SI  \nATOM     37   Si MOL     1       9.504   1.358   4.073  1.00  0.00          SI  \nATOM     38   Si MOL     1       6.789   1.358   1.358  1.00  0.00          SI  \nATOM     39   Si MOL     1       6.789   4.073   4.073  1.00  0.00          SI  \nATOM     40   Si MOL     1       9.504   4.073   1.358  1.00  0.00          SI  \nATOM     41   Si MOL     1       5.431   0.000   5.431  1.00  0.00          SI  \nATOM     42   Si MOL     1       5.431   2.715   8.146  1.00  0.00          SI  \nATOM     43   Si MOL     1       8.146   2.715   5.431  1.00  0.00          SI  \nATOM     44   Si MOL     1       8.146   0.000   8.146  1.00  0.00          SI  \nATOM     45   Si MOL     1       9.504   1.358   9.504  1.00  0.00          SI  \nATOM     46   Si MOL     1       6.789   1.358   6.789  1.00  0.00          SI  \nATOM     47   Si MOL     1       6.789   4.073   9.504  1.00  0.00          SI  \nATOM     48   Si MOL     1       9.504   4.073   6.789  1.00  0.00          SI  \nATOM     49   Si MOL     1       5.431   5.431   0.000  1.00  0.00          SI  \nATOM     50   Si MOL     1       5.431   8.146   2.715  1.00  0.00          SI  \nATOM     51   Si MOL     1       8.146   8.146   0.000  1.00  0.00          SI  \nATOM     52   Si MOL     1       8.146   5.431   2.715  1.00  0.00          SI  \nATOM     53   Si MOL     1       9.504   6.789   4.073  1.00  0.00          SI  \nATOM     54   Si MOL     1       6.789   6.789   1.358  1.00  0.00          SI  \nATOM     55   Si MOL     1       6.789   9.504   4.073  1.00  0.00          SI  \nATOM     56   Si MOL     1       9.504   9.504   1.358  1.00  0.00          SI  \nATOM     57   Si MOL     1       5.431   5.431   5.431  1.00  0.00          SI  \nATOM     58   Si MOL     1       5.431   8.146   8.146  1.00  0.00          SI  \nATOM     59   Si MOL     1       8.146   8.146   5.431  1.00  0.00          SI  \nATOM     60   Si MOL     1       8.146   5.431   8.146  1.00  0.00          SI  \nATOM     61   Si MOL     1       9.504   6.789   9.504  1.00  0.00          SI  \nATOM     62   Si MOL     1       6.789   6.789   6.789  1.00  0.00          SI  \nATOM     63   Si MOL     1       6.789   9.504   9.504  1.00  0.00          SI  \nATOM     64   Si MOL     1       9.504   9.504   6.789  1.00  0.00          SI  \nENDMDL\n",
                    "binary": false
                  }
                ],
                "kwargs": {
                  "name": "nglview.adaptor.ASETrajectory",
                  "defaultRepresentation": false,
                  "ext": "pdb"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "setSize",
                "reconstruc_color_scheme": false,
                "args": [
                  "500px",
                  "500px"
                ],
                "kwargs": {}
              },
              {
                "component_index": 0,
                "target": "compList",
                "type": "call_method",
                "methodName": "addRepresentation",
                "reconstruc_color_scheme": false,
                "args": [
                  "unitcell"
                ],
                "kwargs": {
                  "sele": "all"
                }
              },
              {
                "component_index": 0,
                "target": "compList",
                "type": "call_method",
                "methodName": "addRepresentation",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill"
                ],
                "kwargs": {
                  "sele": "all"
                }
              },
              {
                "target": "Stage",
                "type": "call_method",
                "methodName": "setParameters",
                "reconstruc_color_scheme": false,
                "args": [],
                "kwargs": {
                  "cameraType": "orthographic"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "setParameters",
                "reconstruc_color_scheme": false,
                "args": [
                  {
                    "clipDist": 0
                  }
                ],
                "kwargs": {}
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.5,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.5,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.51,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.51,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.51,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.58,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.58,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.58,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.63,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.63,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.63,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.94,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.94,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.94,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.95,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.95,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.95,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.13,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.13,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.13,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.3,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.3,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.3,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.29,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.29,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.29,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.23,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.23,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.23,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.88,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.88,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.88,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.58,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.58,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.58,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.30999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.30999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.30999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.2,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.2,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.2,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.21,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.21,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.21,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.33,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.33,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.33,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.44,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.44,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.44,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.68,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.68,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.68,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.69,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.69,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.69,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.77,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.77,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.77,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.78,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.78,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.78,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.92,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.92,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.92,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.94,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.94,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.94,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9999999999999999,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9999999999999999,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9999999999999999,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.21,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.21,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.21,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.23,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.23,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.23,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.31,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.31,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.31,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9999999999999999,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9999999999999999,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9999999999999999,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.83,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.83,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.83,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.75,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.75,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.75,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.74,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.74,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.74,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.6199999999999999,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.6199999999999999,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.6199999999999999,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.52,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.52,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.52,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.51,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.51,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.51,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.44,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.44,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              }
            ],
            "_ngl_original_stage_parameters": {
              "impostor": true,
              "quality": "medium",
              "workerDefault": true,
              "sampleLevel": 0,
              "backgroundColor": "white",
              "rotateSpeed": 2,
              "zoomSpeed": 1.2,
              "panSpeed": 1,
              "clipNear": 0,
              "clipFar": 100,
              "clipDist": 10,
              "clipMode": "scene",
              "clipScale": "relative",
              "fogNear": 50,
              "fogFar": 100,
              "cameraFov": 40,
              "cameraEyeSep": 0.3,
              "cameraType": "perspective",
              "lightColor": 14540253,
              "lightIntensity": 1,
              "ambientColor": 14540253,
              "ambientIntensity": 0.2,
              "hoverTimeout": 0,
              "tooltip": true,
              "mousePreset": "default"
            },
            "_ngl_repr_dict": {
              "0": {
                "0": {
                  "type": "unitcell",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "radiusSize": 0.062069997275368154,
                    "sphereDetail": 1,
                    "radialSegments": 10,
                    "disableImpostor": false,
                    "radiusType": "vdw",
                    "radiusData": {},
                    "radiusScale": 1,
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "",
                    "colorReverse": false,
                    "colorValue": "orange",
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                },
                "1": {
                  "type": "unitcell",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "radiusSize": 0.062069997275368154,
                    "sphereDetail": 1,
                    "radialSegments": 10,
                    "disableImpostor": false,
                    "radiusType": "vdw",
                    "radiusData": {},
                    "radiusScale": 1,
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "",
                    "colorReverse": false,
                    "colorValue": "orange",
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                },
                "2": {
                  "type": "spacefill",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "sphereDetail": 1,
                    "disableImpostor": false,
                    "radiusType": "covalent",
                    "radiusData": {},
                    "radiusSize": 1,
                    "radiusScale": 0.52,
                    "assembly": "default",
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "rainbow",
                    "colorReverse": false,
                    "colorValue": 9474192,
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                },
                "3": {
                  "type": "spacefill",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "sphereDetail": 1,
                    "disableImpostor": false,
                    "radiusType": "covalent",
                    "radiusData": {},
                    "radiusSize": 1,
                    "radiusScale": 0.52,
                    "assembly": "default",
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "rainbow",
                    "colorReverse": false,
                    "colorValue": 9474192,
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                }
              },
              "1": {}
            },
            "_ngl_serialize": false,
            "_ngl_version": "2.0.0-dev.39",
            "_ngl_view_id": [
              "5ABFC1FA-4105-4F16-934D-BACA229297D0"
            ],
            "_player_dict": {},
            "_scene_position": {},
            "_scene_rotation": {},
            "_synced_model_ids": [],
            "_synced_repr_model_ids": [],
            "_view_count": null,
            "_view_height": "",
            "_view_module": "nglview-js-widgets",
            "_view_module_version": "3.0.8",
            "_view_name": "NGLView",
            "_view_width": "",
            "background": "white",
            "frame": 109,
            "gui_style": null,
            "layout": "IPY_MODEL_c8ccef826d924dd4b458096bfd3b5095",
            "max_frame": 109,
            "n_components": 2,
            "picked": {}
          }
        },
        "6234838457484b9f9e59efe247903ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "34px"
          }
        },
        "af126e0fd93646f0b3a35b852ffe523e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f1c2d8453c1d472796eb327025b335b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "700587f3288042febe8d695b059b33ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "8f078a272b1e407ab670297dcce9e8c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d75c118c25184eabb012d6015a5441b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90e416e72b26470fbddc49ec96168801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8434f01fd11b43c6adb35567cccae706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b38302e9432b477fba56af3730aece0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11407f19a7dc45f88408d54b524cb76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "02decbe630c240ac865a59ca03c5be3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "All",
              "Si"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Show",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_8f078a272b1e407ab670297dcce9e8c9",
            "style": "IPY_MODEL_d75c118c25184eabb012d6015a5441b4"
          }
        },
        "f6dba43537174e2c947f239a5edb252e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              " ",
              "picking",
              "random",
              "uniform",
              "atomindex",
              "residueindex",
              "chainindex",
              "modelindex",
              "sstruc",
              "element",
              "resname",
              "bfactor",
              "hydrophobicity",
              "value",
              "volume",
              "occupancy"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Color scheme",
            "description_tooltip": null,
            "disabled": false,
            "index": 9,
            "layout": "IPY_MODEL_90e416e72b26470fbddc49ec96168801",
            "style": "IPY_MODEL_8434f01fd11b43c6adb35567cccae706"
          }
        },
        "9e01913f2d9e4891b9b8c05814f3136a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Ball size",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b38302e9432b477fba56af3730aece0b",
            "max": 1.5,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.01,
            "style": "IPY_MODEL_11407f19a7dc45f88408d54b524cb76c",
            "value": 0.44
          }
        },
        "3f31ecaec7574d27b6c882f3a6295a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f1c2d8453c1d472796eb327025b335b9",
            "max": 109,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_700587f3288042febe8d695b059b33ed",
            "value": 79
          }
        },
        "69167a67f20647d2b8fe2b86557e190e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384b8de53a15417997f536a19449ec67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02decbe630c240ac865a59ca03c5be3e",
              "IPY_MODEL_f6dba43537174e2c947f239a5edb252e",
              "IPY_MODEL_9e01913f2d9e4891b9b8c05814f3136a",
              "IPY_MODEL_3f31ecaec7574d27b6c882f3a6295a9b"
            ],
            "layout": "IPY_MODEL_69167a67f20647d2b8fe2b86557e190e"
          }
        },
        "b51404b09ede4bb6905e119343806523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0ab632de7b41948ae792cd7a30464f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_736db268196343089bfa653c8d57d8d2",
              "IPY_MODEL_384b8de53a15417997f536a19449ec67"
            ],
            "layout": "IPY_MODEL_b51404b09ede4bb6905e119343806523"
          }
        },
        "c5c60d1c2e0b47ec89653f7f241d1131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "",
            "disabled": false,
            "icon": "compress",
            "layout": "IPY_MODEL_6234838457484b9f9e59efe247903ef6",
            "style": "IPY_MODEL_af126e0fd93646f0b3a35b852ffe523e",
            "tooltip": ""
          }
        },
        "e699d6f5565e40e39df900d76b123d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3b366f3407049f7b59d01caf3a7d349",
              "IPY_MODEL_7044760e402f48fbb8b49d2994d951b3"
            ],
            "layout": "IPY_MODEL_4480bc7963d74944b5f53bedb0972666"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XynMFbIsYBUz",
        "outputId": "03b89d6d-64f9-4da8-cfb3-83e5dee17413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Cloning into 'nequip'...\n",
            "remote: Enumerating objects: 218, done.\u001b[K\n",
            "remote: Counting objects: 100% (218/218), done.\u001b[K\n",
            "remote: Compressing objects: 100% (205/205), done.\u001b[K\n",
            "remote: Total 218 (delta 4), reused 90 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (218/218), 361.14 KiB | 16.42 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Processing ./nequip\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (1.26.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (4.66.6)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (6.0.2)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (2.5.1+cu121)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (0.1.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase->nequip==0.6.1) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip==0.6.1) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.0.2)\n",
            "Building wheels for collected packages: nequip\n",
            "  Building wheel for nequip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nequip: filename=nequip-0.6.1-py3-none-any.whl size=175386 sha256=179f168b022e502eeadb9a246bfa72fcf937ec9eb1f19196b22a26c94bd7a268\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-705ugxvu/wheels/11/64/44/9d30bacb0803dffa7821bb8685dbc60a0830cca339476e4e86\n",
            "Successfully built nequip\n",
            "Installing collected packages: nequip\n",
            "  Attempting uninstall: nequip\n",
            "    Found existing installation: nequip 0.6.1\n",
            "    Uninstalling nequip-0.6.1:\n",
            "      Successfully uninstalled nequip-0.6.1\n",
            "Successfully installed nequip-0.6.1\n",
            "Cloning into 'allegro'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 44 (delta 0), reused 24 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (44/44), 71.53 KiB | 17.88 MiB/s, done.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nequip>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from mir-allegro==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.66.6)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (0.1.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.0.2)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27432 sha256=59735dc77835322d9d8d1f4360e031762d0ae3e5b596622025ff394b45b72ddb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xgovlc4u/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: mir-allegro\n",
            "  Attempting uninstall: mir-allegro\n",
            "    Found existing installation: mir-allegro 0.2.0\n",
            "    Uninstalling mir-allegro-0.2.0:\n",
            "      Successfully uninstalled mir-allegro-0.2.0\n",
            "Successfully installed mir-allegro-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "\n",
        "# install nequip\n",
        "!rm -rf nequip\n",
        "!git clone --depth 1 \"https://github.com/mir-group/nequip.git\"\n",
        "!pip install nequip/\n",
        "\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "\n",
        "!rm -rf allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "print(\"-----------------------------\")\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "!which nvcc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZvdhYOaOp5I",
        "outputId": "68e9b03d-e41e-4725-acf2-14bd7e1644c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "True\n",
            "1\n",
            "NVIDIA A100-SXM4-40GB\n",
            "-----------------------------\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Mon Dec  9 22:31:48 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              45W / 400W |      5MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf lammps\n",
        "!git clone --depth 1 https://github.com/lammps/lammps.git\n",
        "# Stable Release (Simon )\n",
        "#!git clone -b stable_29Sep2021_update2 --depth 1 https://github.com/lammps/lammps.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJrTDEJJ8klU",
        "outputId": "f1e99ad8-0ffc-4d3c-84c4-0868157a8be0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lammps'...\n",
            "remote: Enumerating objects: 14161, done.\u001b[K\n",
            "remote: Counting objects: 100% (14161/14161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10269/10269), done.\u001b[K\n",
            "remote: Total 14161 (delta 4831), reused 7853 (delta 3656), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14161/14161), 129.17 MiB | 14.35 MiB/s, done.\n",
            "Resolving deltas: 100% (4831/4831), done.\n",
            "Updating files: 100% (13507/13507), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mkl-include"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBHpSXUc8wsa",
        "outputId": "7a20c375-4e10-4e99-da41-8dcb2cc5bc0a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mkl-include in /usr/local/lib/python3.10/dist-packages (2025.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
        "!sh ./cmake-3.23.1-linux-x86_64.sh --prefix=/usr/local --exclude-subdir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyifJb3q81U1",
        "outputId": "b5d813d8-e4c5-4c52-eec2-ab777cbfca35"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-09 22:32:21--  https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241209T223221Z&X-Amz-Expires=300&X-Amz-Signature=d6f167de463c0bfb32608e24ed9e8ea3adc265e24b630b2dfb615a70ab861910&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-09 22:32:21--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241209T223221Z&X-Amz-Expires=300&X-Amz-Signature=d6f167de463c0bfb32608e24ed9e8ea3adc265e24b630b2dfb615a70ab861910&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46004365 (44M) [application/octet-stream]\n",
            "Saving to: ‘cmake-3.23.1-linux-x86_64.sh.1’\n",
            "\n",
            "cmake-3.23.1-linux- 100%[===================>]  43.87M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-12-09 22:32:22 (313 MB/s) - ‘cmake-3.23.1-linux-x86_64.sh.1’ saved [46004365/46004365]\n",
            "\n",
            "CMake Installer Version: 3.23.1, Copyright (c) Kitware\n",
            "This is a self-extracting archive.\n",
            "The archive will be extracted to: /usr/local\n",
            "\n",
            "Using target directory: /usr/local\n",
            "Extracting, please wait...\n",
            "\n",
            "Unpacking finished successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf libtorch\n",
        "!wget wget https://download.pytorch.org/libtorch/cu124/libtorch-cxx11-abi-shared-with-deps-2.5.1%2Bcu124.zip\n",
        "!unzip libtorch-cxx11-abi-shared-with-deps-2.5.1+cu124.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L0y1M-c9HNG",
        "outputId": "114c7eed-1b3b-46b9-c385-475522c3e773"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/item.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/le.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/less.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/median.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/min.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/or.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/put.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/random.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/range.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/real.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/round.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/set.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/size.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/square.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/std.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/t.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/take.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/values.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/var.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/where.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_ops.h  \n",
            "   creating: libtorch/include/ATen/hip/\n",
            "   creating: libtorch/include/ATen/hip/impl/\n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPAllocatorMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPStreamMasqueradingAsCUDA.h  \n",
            "   creating: libtorch/include/ATen/mps/\n",
            "  inflating: libtorch/include/ATen/mps/EmptyTensor.h  \n",
            "  inflating: libtorch/include/ATen/mps/IndexKernels.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSAllocator.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSAllocatorInterface.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSDevice.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSEvent.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSGeneratorImpl.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSGuardImpl.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSHooks.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSProfiler.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSStream.h  \n",
            "   creating: libtorch/include/ATen/miopen/\n",
            "  inflating: libtorch/include/ATen/miopen/Descriptors.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Exceptions.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Handle.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Types.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Utils.h  \n",
            "  inflating: libtorch/include/ATen/miopen/miopen-wrapper.h  \n",
            "   creating: libtorch/include/ATen/detail/\n",
            "  inflating: libtorch/include/ATen/detail/AcceleratorHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/CUDAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/FunctionTraits.h  \n",
            "  inflating: libtorch/include/ATen/detail/HIPHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/IPUHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MAIAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MPSHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MTIAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/PrivateUse1HooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/XPUHooksInterface.h  \n",
            "   creating: libtorch/include/ATen/native/\n",
            "  inflating: libtorch/include/ATen/native/Activation.h  \n",
            "  inflating: libtorch/include/ATen/native/AdaptivePooling.h  \n",
            "  inflating: libtorch/include/ATen/native/AmpKernels.h  \n",
            "  inflating: libtorch/include/ATen/native/BatchLinearAlgebra.h  \n",
            "  inflating: libtorch/include/ATen/native/BinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/BucketizationUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/CPUBlas.h  \n",
            "  inflating: libtorch/include/ATen/native/CPUFallback.h  \n",
            "  inflating: libtorch/include/ATen/native/CanUse32BitIndexMath.h  \n",
            "  inflating: libtorch/include/ATen/native/ComplexHelper.h  \n",
            "  inflating: libtorch/include/ATen/native/CompositeRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/CompositeRandomAccessorCommon.h  \n",
            "  inflating: libtorch/include/ATen/native/ConvUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/ConvolutionMM3d.h  \n",
            "  inflating: libtorch/include/ATen/native/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/Cross.h  \n",
            "  inflating: libtorch/include/ATen/native/DilatedConvolutionUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/DispatchStub.h  \n",
            "  inflating: libtorch/include/ATen/native/Distance.h  \n",
            "  inflating: libtorch/include/ATen/native/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/Distributions.h  \n",
            "  inflating: libtorch/include/ATen/native/EmbeddingBag.h  \n",
            "  inflating: libtorch/include/ATen/native/Fill.h  \n",
            "  inflating: libtorch/include/ATen/native/ForeachUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/FractionalMaxPooling.h  \n",
            "  inflating: libtorch/include/ATen/native/FunctionOfAMatrixUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedAdagrad.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedAdam.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedSGD.h  \n",
            "  inflating: libtorch/include/ATen/native/GridSampler.h  \n",
            "  inflating: libtorch/include/ATen/native/GridSamplerUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Histogram.h  \n",
            "  inflating: libtorch/include/ATen/native/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/IndexingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Lerp.h  \n",
            "  inflating: libtorch/include/ATen/native/LinearAlgebra.h  \n",
            "  inflating: libtorch/include/ATen/native/LinearAlgebraUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/LossMulti.h  \n",
            "  inflating: libtorch/include/ATen/native/Math.h  \n",
            "  inflating: libtorch/include/ATen/native/MathBitFallThroughLists.h  \n",
            "  inflating: libtorch/include/ATen/native/MathBitsFallback.h  \n",
            "  inflating: libtorch/include/ATen/native/MaxPooling.h  \n",
            "  inflating: libtorch/include/ATen/native/NonEmptyUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/NonSymbolicBC.h  \n",
            "  inflating: libtorch/include/ATen/native/Normalization.h  \n",
            "  inflating: libtorch/include/ATen/native/Padding.h  \n",
            "  inflating: libtorch/include/ATen/native/PixelShuffle.h  \n",
            "  inflating: libtorch/include/ATen/native/PointwiseOps.h  \n",
            "  inflating: libtorch/include/ATen/native/Pool.h  \n",
            "  inflating: libtorch/include/ATen/native/Pow.h  \n",
            "  inflating: libtorch/include/ATen/native/RNN.h  \n",
            "  inflating: libtorch/include/ATen/native/RangeFactories.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceAllOps.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/ReductionType.h  \n",
            "  inflating: libtorch/include/ATen/native/Repeat.h  \n",
            "  inflating: libtorch/include/ATen/native/Resize.h  \n",
            "  inflating: libtorch/include/ATen/native/ResizeCommon.h  \n",
            "  inflating: libtorch/include/ATen/native/ScatterGatherChecks.h  \n",
            "  inflating: libtorch/include/ATen/native/SegmentReduce.h  \n",
            "  inflating: libtorch/include/ATen/native/SharedReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/SobolEngineOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Sorting.h  \n",
            "  inflating: libtorch/include/ATen/native/SortingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/SparseTensorUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/SpectralOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/StridedRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorAdvancedIndexing.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorAdvancedIndexingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorCompare.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorConversions.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorDimApply.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorFactories.h  \n",
            " extracting: libtorch/include/ATen/native/TensorIterator.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorIteratorDynamicCasting.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorProperties.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorShape.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorTransformations.h  \n",
            "  inflating: libtorch/include/ATen/native/TopKImpl.h  \n",
            "  inflating: libtorch/include/ATen/native/TransposeType.h  \n",
            "  inflating: libtorch/include/ATen/native/TriangularOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/TypeProperties.h  \n",
            "  inflating: libtorch/include/ATen/native/UnaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/Unfold2d.h  \n",
            "  inflating: libtorch/include/ATen/native/Unfold3d.h  \n",
            "  inflating: libtorch/include/ATen/native/UnfoldBackward.h  \n",
            "  inflating: libtorch/include/ATen/native/UpSample.h  \n",
            "  inflating: libtorch/include/ATen/native/batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/group_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/im2col.h  \n",
            "  inflating: libtorch/include/ATen/native/im2col_shape_check.h  \n",
            "  inflating: libtorch/include/ATen/native/layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/verbose_wrapper.h  \n",
            "  inflating: libtorch/include/ATen/native/vol2col.h  \n",
            "   creating: libtorch/include/ATen/native/cpu/\n",
            "  inflating: libtorch/include/ATen/native/cpu/AtomicAddFloat.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/CatKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/ChannelShuffleKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/CopyKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/DepthwiseConvKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/GridSamplerKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/IndexKernelUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Intrinsics.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/IsContiguous.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/LogAddExp.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Loops.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/MaxUnpoolKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/PixelShuffleKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Reduce.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/ReduceUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SampledAddmmKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SerialStackImpl.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SoftmaxKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SpmmReduceKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/StackKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/UpSampleKernelAVXAntialias.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/WeightNormKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/avx_mathfun.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/int_mm_kernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/mixed_data_type.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/moments_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/zmath.h  \n",
            "   creating: libtorch/include/ATen/native/cuda/\n",
            "  inflating: libtorch/include/ATen/native/cuda/Activation.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/BinaryInternal.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CompositeRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CuFFTPlanCache.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CuFFTUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Distributions.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/GridSampler.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/LaunchUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MiscUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Resize.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/RowwiseScaledMM.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ScanKernels.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Sort.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortStable.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Sorting.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorModeKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorTopK.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/jit_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/thread_constants.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CUDAJitLoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CUDALoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/DeviceSqrt.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/EmbeddingBackwardKernel.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ForeachFunctors.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ForeachMinMaxFunctors.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/GridSampler.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/JitLoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/KernelUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Loops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Math.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MemoryAccess.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MultiTensorApply.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Normalization.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/PersistentSoftmax.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Pow.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Randperm.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Reduce.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ScanUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortingCommon.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortingRadixSelect.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorModeKernel.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/UniqueCub.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/UpSample.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/block_reduce.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_amsgrad_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_utils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adamw_amsgrad_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adamw_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/im2col.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/reduction_template.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/vol2col.cuh  \n",
            "   creating: libtorch/include/ATen/native/mps/\n",
            "  inflating: libtorch/include/ATen/native/mps/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphSequoiaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphSonomaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphVenturaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/OperationUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/TensorFactory.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/UnaryConstants.h  \n",
            "   creating: libtorch/include/ATen/native/nested/\n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorBinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorMath.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorTransformerFunctions.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorTransformerUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorUtils.h  \n",
            "   creating: libtorch/include/ATen/native/quantized/\n",
            "  inflating: libtorch/include/ATen/native/quantized/AffineQuantizer.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/AffineQuantizerBase.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/ConvUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/FakeQuantAffine.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/PackedParams.h  \n",
            "   creating: libtorch/include/ATen/native/quantized/cpu/\n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/BinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/EmbeddingPackedParams.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/OnednnUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QnnpackUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QuantUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QuantizedOps.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/RuyUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/XnnpackUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/conv_serialization.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/fbgemm_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/init_qnnpack.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/qembeddingbag.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/qembeddingbag_prepack.h  \n",
            "   creating: libtorch/include/ATen/native/transformers/\n",
            "  inflating: libtorch/include/ATen/native/transformers/attention.h  \n",
            "  inflating: libtorch/include/ATen/native/transformers/sdp_utils_cpp.h  \n",
            "   creating: libtorch/include/ATen/native/utils/\n",
            "  inflating: libtorch/include/ATen/native/utils/Factory.h  \n",
            "  inflating: libtorch/include/ATen/native/utils/ParamUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/utils/ParamsHash.h  \n",
            "   creating: libtorch/include/ATen/quantized/\n",
            "  inflating: libtorch/include/ATen/quantized/QTensorImpl.h  \n",
            "  inflating: libtorch/include/ATen/quantized/Quantizer.h  \n",
            "   creating: libtorch/include/ATen/xpu/\n",
            "  inflating: libtorch/include/ATen/xpu/CachingHostAllocator.h  \n",
            "  inflating: libtorch/include/ATen/xpu/PinnedMemoryAllocator.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUContext.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUDevice.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUEvent.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUGeneratorImpl.h  \n",
            "   creating: libtorch/include/ATen/xpu/detail/\n",
            "  inflating: libtorch/include/ATen/xpu/detail/XPUHooks.h  \n",
            "   creating: libtorch/include/c10/\n",
            "   creating: libtorch/include/c10/xpu/\n",
            "  inflating: libtorch/include/c10/xpu/XPUCachingAllocator.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUDeviceProp.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUException.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUFunctions.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUMacros.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUStream.h  \n",
            "   creating: libtorch/include/c10/xpu/impl/\n",
            "  inflating: libtorch/include/c10/xpu/impl/XPUGuardImpl.h  \n",
            "   creating: libtorch/include/c10/macros/\n",
            "  inflating: libtorch/include/c10/macros/Export.h  \n",
            "  inflating: libtorch/include/c10/macros/Macros.h  \n",
            "  inflating: libtorch/include/c10/macros/cmake_macros.h  \n",
            "   creating: libtorch/include/c10/core/\n",
            "  inflating: libtorch/include/c10/core/Allocator.h  \n",
            "  inflating: libtorch/include/c10/core/AutogradState.h  \n",
            "  inflating: libtorch/include/c10/core/Backend.h  \n",
            "  inflating: libtorch/include/c10/core/CPUAllocator.h  \n",
            "  inflating: libtorch/include/c10/core/CachingDeviceAllocator.h  \n",
            "  inflating: libtorch/include/c10/core/CompileTimeFunctionPointer.h  \n",
            "  inflating: libtorch/include/c10/core/ConstantSymNodeImpl.h  \n",
            "  inflating: libtorch/include/c10/core/Contiguity.h  \n",
            "  inflating: libtorch/include/c10/core/CopyBytes.h  \n",
            "  inflating: libtorch/include/c10/core/DefaultDtype.h  \n",
            "  inflating: libtorch/include/c10/core/DefaultTensorOptions.h  \n",
            "  inflating: libtorch/include/c10/core/Device.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceArray.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceGuard.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceType.h  \n",
            "  inflating: libtorch/include/c10/core/DispatchKey.h  \n",
            "  inflating: libtorch/include/c10/core/DispatchKeySet.h  \n",
            "  inflating: libtorch/include/c10/core/DynamicCast.h  \n",
            "  inflating: libtorch/include/c10/core/Event.h  \n",
            "  inflating: libtorch/include/c10/core/GeneratorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/GradMode.h  \n",
            "  inflating: libtorch/include/c10/core/InferenceMode.h  \n",
            "  inflating: libtorch/include/c10/core/Layout.h  \n",
            "  inflating: libtorch/include/c10/core/MemoryFormat.h  \n",
            "  inflating: libtorch/include/c10/core/OptionalRef.h  \n",
            "  inflating: libtorch/include/c10/core/PyHandleCache.h  \n",
            "  inflating: libtorch/include/c10/core/QEngine.h  \n",
            "  inflating: libtorch/include/c10/core/QScheme.h  \n",
            "  inflating: libtorch/include/c10/core/RefcountedDeleter.h  \n",
            "  inflating: libtorch/include/c10/core/SafePyObject.h  \n",
            "  inflating: libtorch/include/c10/core/Scalar.h  \n",
            "  inflating: libtorch/include/c10/core/ScalarType.h  \n",
            "  inflating: libtorch/include/c10/core/ScalarTypeToTypeMeta.h  \n",
            "  inflating: libtorch/include/c10/core/Storage.h  \n",
            "  inflating: libtorch/include/c10/core/StorageImpl.h  \n",
            "  inflating: libtorch/include/c10/core/Stream.h  \n",
            "  inflating: libtorch/include/c10/core/StreamGuard.h  \n",
            "  inflating: libtorch/include/c10/core/SymBool.h  \n",
            "  inflating: libtorch/include/c10/core/SymFloat.h  \n",
            "  inflating: libtorch/include/c10/core/SymInt.h  \n",
            "  inflating: libtorch/include/c10/core/SymIntArrayRef.h  \n",
            "  inflating: libtorch/include/c10/core/SymNodeImpl.h  \n",
            "  inflating: libtorch/include/c10/core/SymbolicShapeMeta.h  \n",
            "  inflating: libtorch/include/c10/core/TensorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/TensorOptions.h  \n",
            "  inflating: libtorch/include/c10/core/UndefinedTensorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/WrapDimMinimal.h  \n",
            "  inflating: libtorch/include/c10/core/alignment.h  \n",
            "  inflating: libtorch/include/c10/core/thread_pool.h  \n",
            "   creating: libtorch/include/c10/core/impl/\n",
            "  inflating: libtorch/include/c10/core/impl/COW.h  \n",
            "  inflating: libtorch/include/c10/core/impl/COWDeleter.h  \n",
            "  inflating: libtorch/include/c10/core/impl/DeviceGuardImplInterface.h  \n",
            "  inflating: libtorch/include/c10/core/impl/FakeGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/core/impl/GPUTrace.h  \n",
            "  inflating: libtorch/include/c10/core/impl/HermeticPyObjectTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineDeviceGuard.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineEvent.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineStreamGuard.h  \n",
            "  inflating: libtorch/include/c10/core/impl/LocalDispatchKeySet.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PyInterpreter.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PyObjectSlot.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PythonDispatcherTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/SizesAndStrides.h  \n",
            "  inflating: libtorch/include/c10/core/impl/TorchDispatchModeTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/VirtualGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/core/impl/alloc_cpu.h  \n",
            "   creating: libtorch/include/c10/util/\n",
            "  inflating: libtorch/include/c10/util/AbortHandler.h  \n",
            "  inflating: libtorch/include/c10/util/AlignOf.h  \n",
            "  inflating: libtorch/include/c10/util/ApproximateClock.h  \n",
            "  inflating: libtorch/include/c10/util/Array.h  \n",
            "  inflating: libtorch/include/c10/util/ArrayRef.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16-inl.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16-math.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16.h  \n",
            "  inflating: libtorch/include/c10/util/Backtrace.h  \n",
            "  inflating: libtorch/include/c10/util/Bitset.h  \n",
            "  inflating: libtorch/include/c10/util/C++17.h  \n",
            "  inflating: libtorch/include/c10/util/CallOnce.h  \n",
            "  inflating: libtorch/include/c10/util/ConstexprCrc.h  \n",
            "  inflating: libtorch/include/c10/util/DeadlockDetection.h  \n",
            "  inflating: libtorch/include/c10/util/Deprecated.h  \n",
            "  inflating: libtorch/include/c10/util/DimVector.h  \n",
            "  inflating: libtorch/include/c10/util/DynamicCounter.h  \n",
            "  inflating: libtorch/include/c10/util/Exception.h  \n",
            "  inflating: libtorch/include/c10/util/ExclusivelyOwned.h  \n",
            "  inflating: libtorch/include/c10/util/ExclusivelyOwnedTensorTraits.h  \n",
            "  inflating: libtorch/include/c10/util/FbcodeMaps.h  \n",
            "  inflating: libtorch/include/c10/util/Flags.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fn-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fn.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fnuz-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fnuz.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2fnuz-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2fnuz.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_fnuz_cvt.h  \n",
            "  inflating: libtorch/include/c10/util/FunctionRef.h  \n",
            "  inflating: libtorch/include/c10/util/Gauge.h  \n",
            "  inflating: libtorch/include/c10/util/Half-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Half.h  \n",
            "  inflating: libtorch/include/c10/util/IdWrapper.h  \n",
            "  inflating: libtorch/include/c10/util/Lazy.h  \n",
            "  inflating: libtorch/include/c10/util/LeftRight.h  \n",
            "  inflating: libtorch/include/c10/util/Load.h  \n",
            "  inflating: libtorch/include/c10/util/Logging.h  \n",
            "  inflating: libtorch/include/c10/util/MathConstants.h  \n",
            "  inflating: libtorch/include/c10/util/MaybeOwned.h  \n",
            "  inflating: libtorch/include/c10/util/Metaprogramming.h  \n",
            "  inflating: libtorch/include/c10/util/NetworkFlow.h  \n",
            "  inflating: libtorch/include/c10/util/Optional.h  \n",
            "  inflating: libtorch/include/c10/util/OptionalArrayRef.h  \n",
            "  inflating: libtorch/include/c10/util/ParallelGuard.h  \n",
            "  inflating: libtorch/include/c10/util/Registry.h  \n",
            "  inflating: libtorch/include/c10/util/ScopeExit.h  \n",
            "  inflating: libtorch/include/c10/util/SmallBuffer.h  \n",
            "  inflating: libtorch/include/c10/util/SmallVector.h  \n",
            "  inflating: libtorch/include/c10/util/StringUtil.h  \n",
            "  inflating: libtorch/include/c10/util/Synchronized.h  \n",
            "  inflating: libtorch/include/c10/util/ThreadLocal.h  \n",
            "  inflating: libtorch/include/c10/util/ThreadLocalDebugInfo.h  \n",
            "  inflating: libtorch/include/c10/util/Type.h  \n",
            "  inflating: libtorch/include/c10/util/TypeCast.h  \n",
            "  inflating: libtorch/include/c10/util/TypeIndex.h  \n",
            "  inflating: libtorch/include/c10/util/TypeList.h  \n",
            "  inflating: libtorch/include/c10/util/TypeSafeSignMath.h  \n",
            "  inflating: libtorch/include/c10/util/TypeTraits.h  \n",
            "  inflating: libtorch/include/c10/util/Unicode.h  \n",
            "  inflating: libtorch/include/c10/util/UniqueVoidPtr.h  \n",
            "  inflating: libtorch/include/c10/util/Unroll.h  \n",
            "  inflating: libtorch/include/c10/util/WaitCounter.h  \n",
            "  inflating: libtorch/include/c10/util/accumulate.h  \n",
            "  inflating: libtorch/include/c10/util/bit_cast.h  \n",
            "  inflating: libtorch/include/c10/util/bits.h  \n",
            "  inflating: libtorch/include/c10/util/complex.h  \n",
            "  inflating: libtorch/include/c10/util/complex_math.h  \n",
            "  inflating: libtorch/include/c10/util/complex_utils.h  \n",
            "  inflating: libtorch/include/c10/util/copysign.h  \n",
            "  inflating: libtorch/include/c10/util/env.h  \n",
            "  inflating: libtorch/include/c10/util/flat_hash_map.h  \n",
            "  inflating: libtorch/include/c10/util/floating_point_utils.h  \n",
            "  inflating: libtorch/include/c10/util/generic_math.h  \n",
            "  inflating: libtorch/include/c10/util/hash.h  \n",
            "  inflating: libtorch/include/c10/util/int128.h  \n",
            "  inflating: libtorch/include/c10/util/intrusive_ptr.h  \n",
            "  inflating: libtorch/include/c10/util/irange.h  \n",
            "  inflating: libtorch/include/c10/util/llvmMathExtras.h  \n",
            "  inflating: libtorch/include/c10/util/logging_is_google_glog.h  \n",
            "  inflating: libtorch/include/c10/util/logging_is_not_google_glog.h  \n",
            "  inflating: libtorch/include/c10/util/numa.h  \n",
            "  inflating: libtorch/include/c10/util/order_preserving_flat_hash_map.h  \n",
            "  inflating: libtorch/include/c10/util/overloaded.h  \n",
            "  inflating: libtorch/include/c10/util/python_stub.h  \n",
            "  inflating: libtorch/include/c10/util/qint32.h  \n",
            "  inflating: libtorch/include/c10/util/qint8.h  \n",
            "  inflating: libtorch/include/c10/util/quint2x4.h  \n",
            "  inflating: libtorch/include/c10/util/quint4x2.h  \n",
            "  inflating: libtorch/include/c10/util/quint8.h  \n",
            "  inflating: libtorch/include/c10/util/safe_numerics.h  \n",
            "  inflating: libtorch/include/c10/util/signal_handler.h  \n",
            "  inflating: libtorch/include/c10/util/sparse_bitset.h  \n",
            "  inflating: libtorch/include/c10/util/ssize.h  \n",
            "  inflating: libtorch/include/c10/util/static_tracepoint.h  \n",
            "  inflating: libtorch/include/c10/util/static_tracepoint_elfx86.h  \n",
            "  inflating: libtorch/include/c10/util/strides.h  \n",
            "  inflating: libtorch/include/c10/util/string_utils.h  \n",
            "  inflating: libtorch/include/c10/util/string_view.h  \n",
            "  inflating: libtorch/include/c10/util/strong_type.h  \n",
            "  inflating: libtorch/include/c10/util/tempfile.h  \n",
            "  inflating: libtorch/include/c10/util/thread_name.h  \n",
            "  inflating: libtorch/include/c10/util/typeid.h  \n",
            "  inflating: libtorch/include/c10/util/win32-headers.h  \n",
            "   creating: libtorch/include/c10/cuda/\n",
            "  inflating: libtorch/include/c10/cuda/CUDAAllocatorConfig.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDACachingAllocator.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDADeviceAssertionHost.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAException.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAFunctions.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAGuard.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMacros.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMathCompat.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMiscFunctions.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAStream.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAAlgorithm.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDADeviceAssertion.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAGraphsC10Utils.h  \n",
            "  inflating: libtorch/include/c10/cuda/driver_api.h  \n",
            "   creating: libtorch/include/c10/cuda/impl/\n",
            "  inflating: libtorch/include/c10/cuda/impl/CUDAGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/cuda/impl/CUDATest.h  \n",
            "  inflating: libtorch/include/c10/cuda/impl/cuda_cmake_macros.h  \n",
            "   creating: libtorch/include/caffe2/\n",
            "   creating: libtorch/include/caffe2/serialize/\n",
            "  inflating: libtorch/include/caffe2/serialize/crc_alt.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/file_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/in_memory_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/inline_container.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/istream_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/read_adapter_interface.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/versions.h  \n",
            "  inflating: libtorch/include/clog.h  \n",
            "  inflating: libtorch/include/cpuinfo.h  \n",
            "  inflating: libtorch/include/dnnl_config.h  \n",
            "  inflating: libtorch/include/dnnl_debug.h  \n",
            "  inflating: libtorch/include/dnnl.h  \n",
            "  inflating: libtorch/include/dnnl_ocl.h  \n",
            "  inflating: libtorch/include/dnnl_sycl.h  \n",
            "  inflating: libtorch/include/dnnl_sycl_types.h  \n",
            "  inflating: libtorch/include/dnnl_threadpool.h  \n",
            "  inflating: libtorch/include/dnnl_types.h  \n",
            "  inflating: libtorch/include/dnnl_version.h  \n",
            "  inflating: libtorch/include/experiments-config.h  \n",
            "  inflating: libtorch/include/fp16.h  \n",
            "  inflating: libtorch/include/fxdiv.h  \n",
            "   creating: libtorch/include/kineto/\n",
            "  inflating: libtorch/include/kineto/AbstractConfig.h  \n",
            "  inflating: libtorch/include/kineto/ActivityProfilerInterface.h  \n",
            "  inflating: libtorch/include/kineto/ActivityTraceInterface.h  \n",
            "  inflating: libtorch/include/kineto/ActivityType.h  \n",
            "  inflating: libtorch/include/kineto/Config.h  \n",
            "  inflating: libtorch/include/kineto/ClientInterface.h  \n",
            "  inflating: libtorch/include/kineto/GenericTraceActivity.h  \n",
            "  inflating: libtorch/include/kineto/IActivityProfiler.h  \n",
            "  inflating: libtorch/include/kineto/ILoggerObserver.h  \n",
            "  inflating: libtorch/include/kineto/ITraceActivity.h  \n",
            "  inflating: libtorch/include/kineto/LoggingAPI.h  \n",
            "  inflating: libtorch/include/kineto/TraceSpan.h  \n",
            "  inflating: libtorch/include/kineto/ThreadUtil.h  \n",
            "  inflating: libtorch/include/kineto/libkineto.h  \n",
            "  inflating: libtorch/include/kineto/time_since_epoch.h  \n",
            "  inflating: libtorch/include/kineto/output_base.h  \n",
            "  inflating: libtorch/include/libshm.h  \n",
            "  inflating: libtorch/include/nnpack.h  \n",
            "  inflating: libtorch/include/psimd.h  \n",
            "  inflating: libtorch/include/pthreadpool.h  \n",
            "   creating: libtorch/include/pybind11/\n",
            "  inflating: libtorch/include/pybind11/attr.h  \n",
            "  inflating: libtorch/include/pybind11/buffer_info.h  \n",
            "  inflating: libtorch/include/pybind11/cast.h  \n",
            "  inflating: libtorch/include/pybind11/chrono.h  \n",
            "  inflating: libtorch/include/pybind11/common.h  \n",
            "  inflating: libtorch/include/pybind11/complex.h  \n",
            "  inflating: libtorch/include/pybind11/eigen.h  \n",
            "  inflating: libtorch/include/pybind11/embed.h  \n",
            "  inflating: libtorch/include/pybind11/eval.h  \n",
            "  inflating: libtorch/include/pybind11/functional.h  \n",
            "  inflating: libtorch/include/pybind11/gil.h  \n",
            "  inflating: libtorch/include/pybind11/gil_safe_call_once.h  \n",
            "  inflating: libtorch/include/pybind11/iostream.h  \n",
            "  inflating: libtorch/include/pybind11/numpy.h  \n",
            "  inflating: libtorch/include/pybind11/operators.h  \n",
            "  inflating: libtorch/include/pybind11/options.h  \n",
            "  inflating: libtorch/include/pybind11/pybind11.h  \n",
            "  inflating: libtorch/include/pybind11/pytypes.h  \n",
            "  inflating: libtorch/include/pybind11/stl.h  \n",
            "  inflating: libtorch/include/pybind11/stl_bind.h  \n",
            "  inflating: libtorch/include/pybind11/type_caster_pyobject_ptr.h  \n",
            "  inflating: libtorch/include/pybind11/typing.h  \n",
            "   creating: libtorch/include/pybind11/detail/\n",
            "  inflating: libtorch/include/pybind11/detail/class.h  \n",
            "  inflating: libtorch/include/pybind11/detail/common.h  \n",
            "  inflating: libtorch/include/pybind11/detail/descr.h  \n",
            "  inflating: libtorch/include/pybind11/detail/init.h  \n",
            "  inflating: libtorch/include/pybind11/detail/internals.h  \n",
            "  inflating: libtorch/include/pybind11/detail/type_caster_base.h  \n",
            "  inflating: libtorch/include/pybind11/detail/typeid.h  \n",
            "  inflating: libtorch/include/pybind11/detail/value_and_holder.h  \n",
            "   creating: libtorch/include/pybind11/eigen/\n",
            "  inflating: libtorch/include/pybind11/eigen/common.h  \n",
            "  inflating: libtorch/include/pybind11/eigen/matrix.h  \n",
            "  inflating: libtorch/include/pybind11/eigen/tensor.h  \n",
            "  inflating: libtorch/include/qnnpack_func.h  \n",
            "   creating: libtorch/include/tensorpipe/\n",
            "  inflating: libtorch/include/tensorpipe/tensorpipe.h  \n",
            "  inflating: libtorch/include/tensorpipe/config.h  \n",
            "  inflating: libtorch/include/tensorpipe/tensorpipe_cuda.h  \n",
            "  inflating: libtorch/include/tensorpipe/config_cuda.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/\n",
            "  inflating: libtorch/include/tensorpipe/channel/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/channel/error.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/basic/\n",
            "  inflating: libtorch/include/tensorpipe/channel/basic/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/cma/\n",
            "  inflating: libtorch/include/tensorpipe/channel/cma/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/mpt/\n",
            "  inflating: libtorch/include/tensorpipe/channel/mpt/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/xth/\n",
            "  inflating: libtorch/include/tensorpipe/channel/xth/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/common/\n",
            "  inflating: libtorch/include/tensorpipe/common/buffer.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/cpu_buffer.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/device.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/optional.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/cuda_buffer.h  \n",
            "   creating: libtorch/include/tensorpipe/core/\n",
            "  inflating: libtorch/include/tensorpipe/core/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/listener.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/message.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/pipe.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/\n",
            "   creating: libtorch/include/tensorpipe/transport/shm/\n",
            "  inflating: libtorch/include/tensorpipe/transport/shm/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/uv/\n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/factory.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/utility.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/error.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/ibv/\n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/factory.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/utility.h  \n",
            "   creating: libtorch/include/THC/\n",
            "  inflating: libtorch/include/THC/THCAtomics.cuh  \n",
            " extracting: libtorch/include/THC/THCDeviceUtils.cuh  \n",
            "   creating: libtorch/include/torch/\n",
            "  inflating: libtorch/include/torch/custom_class.h  \n",
            "  inflating: libtorch/include/torch/custom_class_detail.h  \n",
            "  inflating: libtorch/include/torch/library.h  \n",
            "  inflating: libtorch/include/torch/script.h  \n",
            "  inflating: libtorch/include/torch/extension.h  \n",
            "   creating: libtorch/include/torch/csrc/\n",
            "  inflating: libtorch/include/torch/csrc/Export.h  \n",
            "  inflating: libtorch/include/torch/csrc/CudaIPCTypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/DataLoader.h  \n",
            "  inflating: libtorch/include/torch/csrc/Device.h  \n",
            "  inflating: libtorch/include/torch/csrc/Dtype.h  \n",
            "  inflating: libtorch/include/torch/csrc/DynamicTypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/Exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/Generator.h  \n",
            "  inflating: libtorch/include/torch/csrc/Layout.h  \n",
            "  inflating: libtorch/include/torch/csrc/MemoryFormat.h  \n",
            "  inflating: libtorch/include/torch/csrc/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/PyInterpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/QScheme.h  \n",
            "  inflating: libtorch/include/torch/csrc/Size.h  \n",
            "  inflating: libtorch/include/torch/csrc/Storage.h  \n",
            "  inflating: libtorch/include/torch/csrc/StorageMethods.h  \n",
            "  inflating: libtorch/include/torch/csrc/StorageSharing.h  \n",
            "  inflating: libtorch/include/torch/csrc/Stream.h  \n",
            "  inflating: libtorch/include/torch/csrc/THConcat.h  \n",
            "  inflating: libtorch/include/torch/csrc/THP.h  \n",
            "  inflating: libtorch/include/torch/csrc/TypeInfo.h  \n",
            "  inflating: libtorch/include/torch/csrc/Types.h  \n",
            "  inflating: libtorch/include/torch/csrc/copy_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/itt_wrapper.h  \n",
            "  inflating: libtorch/include/torch/csrc/python_dimname.h  \n",
            "  inflating: libtorch/include/torch/csrc/python_headers.h  \n",
            "  inflating: libtorch/include/torch/csrc/serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/\n",
            "   creating: libtorch/include/torch/csrc/api/include/\n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/all.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/arg.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/cuda.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/enum.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/expanding_array.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/fft.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/imethod.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/linalg.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/mps.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nested.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/ordered_dict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/python.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/sparse.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/special.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/torch.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/xpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/version.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader_options.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/example.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/iterator.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/worker_exception.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateless.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/datasets/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/chunk.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/map.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/mnist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/shared.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/stateful.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/detail/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/data_shuttle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/queue.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/sequencers.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/samplers/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/custom_batch_request.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/distributed.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/random.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/sequential.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/stream.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/transforms/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/collate.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/lambda.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/stack.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/detail/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/detail/TensorDataContainer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/detail/static.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/cloneable.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/pimpl-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/pimpl.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/functional/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/vision.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/options/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/adaptive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/rnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformercoder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformerlayer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/vision.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/modules/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/adaptive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/common.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/rnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformercoder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformerlayer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any_module_holder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/functional.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/moduledict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/modulelist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/named_any.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/parameterdict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/parameterlist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/sequential.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/parallel/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/parallel/data_parallel.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/utils/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/clip_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/convert_parameters.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/rnn.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/optim/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adagrad.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adam.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adamw.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/lbfgs.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/optimizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/rmsprop.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/sgd.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/reduce_on_plateau_scheduler.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/step_lr.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/serialize/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/input-archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/output-archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/onnx/\n",
            "  inflating: libtorch/include/torch/csrc/onnx/back_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/onnx/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/onnx/onnx.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/api.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/collection.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/combined_traceback.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/containers.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/data_flow.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/events.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/kineto_shim.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/perf-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/perf.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/util.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/orchestration/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/python_tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/vulkan.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/standalone/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/execution_trace_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/itt_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/nvtx_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/privateuse1_observer.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/stubs/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/stubs/base.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/unwind/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/action.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/communicate.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/dwarf_enums.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/dwarf_symbolize_enums.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/eh_frame_hdr.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/fast_symbolizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/fde.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/lexer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/line_number_program.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/mem_file.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/range_table.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/sections.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwind.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwind_error.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwinder.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/python/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/combined_traceback.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/pybind.h  \n",
            "   creating: libtorch/include/torch/csrc/utils/\n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_numpy.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_new.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/byte_order.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/cpp_stacktraces.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/cuda_enabled.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/device_lazy_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/disable_torch_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/invalid_arguments.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/nested.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/numpy_stub.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/object_ptr.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/out_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pycfunction_helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pyobject_preservation.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_arg_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_dispatch.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_numbers.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_raii.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_scalars.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_strings.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_stub.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_symnode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_torch_function_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_tuples.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pythoncapi_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/schema_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/six.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/structseq.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_apply.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_dtypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_flatten.h  \n",
            " extracting: libtorch/include/torch/csrc/utils/tensor_layouts.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_memoryformats.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_qschemes.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/throughput_benchmark-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/throughput_benchmark.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/torch_dispatch_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/variadic.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/verbose.h  \n",
            "   creating: libtorch/include/torch/csrc/tensor/\n",
            "  inflating: libtorch/include/torch/csrc/tensor/python_tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/\n",
            "   creating: libtorch/include/torch/csrc/lazy/backend/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_data.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_device.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/lowering_context.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/cache.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/config.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/debug_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/dynamic_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/hash.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_builder.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_dump_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/lazy_graph_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/metrics.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/multi_wait.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/permutation_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/shape.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/shape_inference.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/thread_pool.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/trie.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/unique.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/util.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/internal_ops/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/internal_ops/ltc_ops.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/ops/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ops/arithmetic_ir_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ops/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/python/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/python/python_util.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/ts_backend/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/config.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/dynamic_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ir_builder.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/tensor_aten_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_autograd_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_backend_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_eager_fallback.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_lowering_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_node.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_node_lowering.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/FunctionsManual.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/InferenceMode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/VariableTypeUtils.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/anomaly_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/autograd_not_implemented_fallback.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/cpp_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/custom_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/edge.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/engine.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/forward_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/function_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/grad_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/graph_task.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/input_buffer.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/input_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/jit_decomp_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_kineto.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_legacy.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_anomaly_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_cpp_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_engine.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_enum_tag.h  \n",
            " extracting: libtorch/include/torch/csrc/autograd/python_fft_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_legacy_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_linalg_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_nested_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_nn_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_saved_variable_hooks.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_sparse_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_special_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_torch_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_variable_indexing.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/record_function_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/saved_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/saved_variable_hooks.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/symbolic.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/variable_info.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/functions/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/accumulate_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/basic_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/generated/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/python_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/python_return_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/VariableType.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/Functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/variable_factories.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/ViewFuncs.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/utils/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/error_messages.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/grad_layout_contract.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/lambda_post_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/python_arg_parsing.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/warnings.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/wrap_outputs.h  \n",
            "   creating: libtorch/include/torch/csrc/xpu/\n",
            "  inflating: libtorch/include/torch/csrc/xpu/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/xpu/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/xpu/Stream.h  \n",
            "   creating: libtorch/include/torch/csrc/cuda/\n",
            "  inflating: libtorch/include/torch/csrc/cuda/CUDAPluggableAllocator.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/GdsFile.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Stream.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/THCP.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/device_set.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/memory_snapshot.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/nccl.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/python_comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/python_nccl.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/\n",
            "   creating: libtorch/include/torch/csrc/distributed/c10d/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/CUDASymmetricMemory-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TraceUtils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/c10d.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/debug.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/error.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/logging.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/python_comm_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/socket.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Backend.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Backoff.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/DMAConnectivity.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/FakeProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/FileStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Functional.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/GlooDeviceFactory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/GroupRegistry.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/HashStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/NCCLUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/NanCheck.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ParamCommsUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/PrefixStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupGloo.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupMPI.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupUCC.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupWrapper.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/PyProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/RankLocal.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Store.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/SymmetricMemory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TCPStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TCPStoreBackend.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Types.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UCCTracing.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UCCUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UnixSockUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Utils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/WinSockUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Work.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/comm.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/default_comm_hooks.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/intra_node_comm.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/logger.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/reducer.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/reducer_timer.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/sequence_num.hpp  \n",
            "   creating: libtorch/include/torch/csrc/distributed/rpc/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/agent_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/message.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/py_rref.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_rpc_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback_no_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc_agent.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc_command_base.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_proto.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/tensorpipe_agent.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/tensorpipe_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/torchscript_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/unpickled_python_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/unpickled_python_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/\n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/context/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/context/container.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/context/context.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/functions/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/functions/recvrpc_backward.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/functions/sendrpc_backward.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/autograd_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rref_backward_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rref_backward_resp.h  \n",
            "   creating: libtorch/include/torch/csrc/dynamo/\n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cache_entry.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/compiled_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpp_shim.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpython_defs.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpython_includes.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/debug_macros.h  \n",
            " extracting: libtorch/include/torch/csrc/dynamo/eval_frame.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/extra_state.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/framelocals_mapping.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/guards.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/python_compiled_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/inductor_ops.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_runner/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner_cpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner_cuda.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/pybind.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_runtime/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/arrayref_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/device_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/model.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/model_container.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/scalar_to_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/thread_local.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/utils_cuda.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/mkldnn_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/oss_proxy_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/proxy_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/tensor_converter.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/c/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/c/shim.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/\n",
            "   creating: libtorch/include/torch/csrc/jit/api/\n",
            "  inflating: libtorch/include/torch/csrc/jit/api/compilation_unit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/function_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/method.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/object.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/serialization/\n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/callstack_debug_info_serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/source_range_serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/export.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/export_bytecode.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/flatbuffer_serializer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/flatbuffer_serializer_jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_read.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_source.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/mobile_bytecode_generated.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/onnx.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/pickle.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/pickler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/python_print.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/source_range_serialization_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/storage_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/type_name_uniquer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/unpickler.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/python/\n",
            "  inflating: libtorch/include/torch/csrc/jit/python/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/module_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/pybind_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_arg_flatten.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_custom_class.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_dict.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_ivalue.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_sugared_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_tree_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/script_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/update_graph_executor_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/utf8_decoding_ignore.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/mobile/\n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/code.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/file_format.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/flatbuffer_loader.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/frame.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/function.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import_data.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import_export_common.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/interpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/method.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/parse_bytecode.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/parse_operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/prim_ops_registery.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/profiler_edge.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/promoted_prim_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/quantization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/register_ops_common_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/upgrader_mobile.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/testing/\n",
            "  inflating: libtorch/include/torch/csrc/jit/testing/file_check.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/testing/hooks_for_testing.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/tensorexpr/\n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/block_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/bounds_inference.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/bounds_overlap.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cpp_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cpp_intrinsics.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cuda_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cuda_random.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/eval.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/expr.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions_core.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/fwd_decls.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/graph_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/half_support.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/hash_provider.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/intrinsic_symbols.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_cloner.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_mutator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_printer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_simplifier.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_verifier.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_visitor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/kernel.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/llvm_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/llvm_jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/loopnest.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/loopnest_randomization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/lowerings.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/mem_dependency_checker.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/reduction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/registerizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/stmt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/tensorexpr_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/unique_name_manager.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/var_substitutor.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/tensorexpr/operators/\n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/conv2d.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/matmul.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/misc.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/norm.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/pointwise.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/quantization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/reduction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/softmax.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/codegen/\n",
            "   creating: libtorch/include/torch/csrc/jit/codegen/cuda/\n",
            "  inflating: libtorch/include/torch/csrc/jit/codegen/cuda/interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/jit_log.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/jit_opt_limit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/resource_guard.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/backends/\n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_debug_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_detail.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_preprocess.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_resolver.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/add_if_then_else.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/annotate_warns.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/autocast.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/bailout_graph.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/batch_mm.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/canonicalize.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/canonicalize_graph_fuser_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/check_strict_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/clear_profiling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/clear_undefinedness.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/common_subexpression_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/concat_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/constant_pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/constant_propagation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/create_autodiff_subgraphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/create_functional_graphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/dead_code_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/decompose_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/device_type_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/dtype_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/eliminate_no_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/erase_number_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fixup_trace_scope_blocks.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fold_conv_bn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fold_linear_bn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/freeze_module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_concat_linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_conv_add_relu_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_conv_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_graph_optimizations.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_linear_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_linear_transpose.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_ops_to_mkldnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fuse_linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fuse_relu.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/graph_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/graph_rewrite_helper.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/guard_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/hoist_conv_packed_params.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_autodiff_subgraphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_fork_wait.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_forked_closures.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inliner.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inplace_check.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/insert_guards.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/integer_value_refinement.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lift_closures.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/liveness.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/loop_unrolling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_grad_of.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_graph.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_tuples.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/metal_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/mkldnn_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/mobile_optimizer_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/normalize_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/onednn_graph_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/onnx.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/pass_manager.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_alias_sensitive.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_dict_idioms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_list_idioms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_non_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/prepack_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/refine_tuple_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_expands.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_inplace_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_mutation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_redundant_profiles.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/replacement_of_old_operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/requires_grad_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/restore_mutation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/shape_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/specialize_autogradzero.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/subgraph_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_cache.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_runtime_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/tensorexpr_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/update_differentiable_graph_requires_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/value_refinement_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/variadic_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/vulkan_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/xnnpack_rewrite.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/quantization/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/dedup_module_uses.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/finalize.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/fusion_passes.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/helper.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/insert_observers.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/insert_quant_dequant.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/quantization_patterns.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/quantization_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/register_packed_params.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/utils/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/check_alias_annotation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/memory_dag.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/op_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/optimization_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/subgraph_utils.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/runtime/\n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/argument_spec.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/autodiff.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/calculate_necessary_args.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/custom_operator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/decomposition_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/decomposition_registry_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/exception_message.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_iterator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/instruction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/interpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/jit_exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/jit_trace.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/logging.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/operator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/operator_options.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/print_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/profiling_graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/profiling_record.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/register_ops_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/script_profile.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/serialized_shape_function_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/shape_function_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/simple_graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/slice_indices_adjust.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_script.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_shape_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_shape_registry_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/vararg_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/variable_tensor_list.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/ir/\n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/alias_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/attributes.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/graph_node_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/graph_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/ir_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/irparser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/named_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/node_hashing.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/scope.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/subgraph_matcher.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/type_hashing.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/frontend/\n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/source_range.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/lexer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/strtod.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parser_constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/function_schema_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parse_string_literal.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/schema_type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/error_report.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tree.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/builtin_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/canonicalize_modified_loop.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/concrete_module_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/convert_to_ssa.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/edit_distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/exit_transforms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/inline_loop_condition.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/ir_emitter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/mini_environment.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/name_mangler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/resolver.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/schema_matching.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/script_type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/source_ref.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/sugared_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tree_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/versioned_symbols.h  \n",
            "  inflating: libtorch/include/xnnpack.h  \n",
            "   creating: libtorch/share/\n",
            "   creating: libtorch/share/cmake/\n",
            "   creating: libtorch/share/cmake/ATen/\n",
            "  inflating: libtorch/share/cmake/ATen/ATenConfig.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Config.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUDAToolkit.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUSPARSELT.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUDSS.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindSYCLToolkit.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Targets.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Targets-release.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/public/\n",
            "  inflating: libtorch/share/cmake/Caffe2/public/cuda.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/xpu.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/glog.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/gflags.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/mkl.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/mkldnn.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/protobuf.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/utils.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/LoadHIP.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/FindCUDA.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/FindCUDNN.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/CMakeInitializeConfigs.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindPackageHandleStandardArgs.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindPackageMessage.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/make2cmake.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/parse_cubin.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/run_nvcc.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake  \n",
            "   creating: libtorch/share/cmake/Tensorpipe/\n",
            "  inflating: libtorch/share/cmake/Tensorpipe/TensorpipeTargets.cmake  \n",
            "  inflating: libtorch/share/cmake/Tensorpipe/TensorpipeTargets-release.cmake  \n",
            "   creating: libtorch/share/cmake/Torch/\n",
            "  inflating: libtorch/share/cmake/Torch/TorchConfigVersion.cmake  \n",
            "  inflating: libtorch/share/cmake/Torch/TorchConfig.cmake  \n",
            " extracting: libtorch/build-version  \n",
            "  inflating: libtorch/build-hash     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch;\n",
        "print(torch.utils.cmake_prefix_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaqxNAP8XG7p",
        "outputId": "792e5635-e5e5-4144-f11f-fdfde31fd3bb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/share/cmake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf pair_allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/pair_allegro.git\n",
        "# Latest is at -- now same as above. Mon Dec 9, 2024\n",
        "#!git clone --depth 1 https://github.com/mir-group/pair_allegro/tree/multicut"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoEfje93DXt3",
        "outputId": "8420740f-2b2f-436e-8f1f-59fafda243da"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pair_allegro'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 28 (delta 0), reused 20 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (28/28), 195.32 KiB | 19.53 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "!cd pair_allegro && bash patch_lammps.sh ../lammps/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaJUONWEDi--",
        "outputId": "b97170a8-da86-418b-a0d2-eb2162df1f32"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying files...\n",
            "Updating CMakeLists.txt...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "# Use Python 3.11 Libtorch\n",
        "#!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j$(nproc)\n",
        "# Use downloaded 12.2 CUDA libtorch\n",
        "!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=/content/libtorch -DMKL_INCLUDE_DIR=`python -c \"import sysconfig;from pathlib import Path;print(Path(sysconfig.get_paths()[\\\"include\\\"]).parent)\"` && make -j$(nproc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYwb5zHP9BYo",
        "outputId": "187054fa-4523-4d02-8e53-504e9f101e2d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "-- Running check for auto-generated files from make-based build system\n",
            "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n",
            "-- Found MPI: TRUE (found version \"3.1\")  \n",
            "-- Looking for C++ include omp.h\n",
            "-- Looking for C++ include omp.h - found\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\") found components: CXX \n",
            "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n",
            "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.37\") \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found GZIP: /usr/bin/gzip  \n",
            "-- Found FFMPEG: /usr/bin/ffmpeg  \n",
            "-- Looking for C++ include cmath\n",
            "-- Looking for C++ include cmath - found\n",
            "-- Generating style headers...\n",
            "-- Generating package headers...\n",
            "-- Generating lmpinstalledpkgs.h...\n",
            "-- Found Python3: /usr/local/bin/python (found version \"3.10.12\") found components: Interpreter \n",
            "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"11.0\")\n",
            "-- The following tools and libraries have been found and configured:\n",
            " * Git\n",
            " * MPI\n",
            " * OpenMP\n",
            " * JPEG\n",
            " * PNG\n",
            " * ZLIB\n",
            " * Python3\n",
            "\n",
            "-- <<< Build configuration >>>\n",
            "   LAMMPS Version:   20241119 cd16308-modified\n",
            "   Operating System: Linux Ubuntu\" 22.04\n",
            "   CMake Version:    3.23.1\n",
            "   Build type:       RelWithDebInfo\n",
            "   Install path:     /root/.local\n",
            "   Generator:        Unix Makefiles using /usr/bin/gmake\n",
            "-- Enabled packages: <None>\n",
            "-- <<< Compilers and Flags: >>>\n",
            "-- C++ Compiler:     /usr/bin/c++\n",
            "      Type:          GNU\n",
            "      Version:       11.4.0\n",
            "      C++ Standard:  17\n",
            "      C++ Flags:     -O2 -g -DNDEBUG\n",
            "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=4;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n",
            "-- <<< Linker flags: >>>\n",
            "-- Executable name:  lmp\n",
            "-- Static library flags:    \n",
            "-- <<< MPI flags >>>\n",
            "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
            "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi\n",
            "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.2\") \n",
            "-- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.2.140\") \n",
            "-- Looking for C++ include pthread.h\n",
            "-- Looking for C++ include pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Caffe2: CUDA detected: 12.2\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 12.2\n",
            "-- Found Python: /usr/local/bin/python (found version \"3.10.12\") found components: Interpreter \n",
            "\u001b[33mCMake Warning at /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:140 (message):\n",
            "  Failed to compute shorthash for libnvrtc.so\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/share/cmake-3.23/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
            "  The package name passed to `find_package_handle_standard_args` (nvtx3) does\n",
            "  not match the name of the calling package (Caffe2).  This can lead to\n",
            "  problems in calling code that expects `find_package` result variables\n",
            "  (e.g., `_FOUND`) to follow a certain pattern.\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:174 (find_package_handle_standard_args)\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could NOT find nvtx3 (missing: nvtx3_dir) \n",
            "\u001b[33mCMake Warning at /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:180 (message):\n",
            "  Cannot find NVTX3, find old NVTX instead\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- USE_CUDNN is set to 0. Compiling without cuDNN support\n",
            "-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\n",
            "-- USE_CUDSS is set to 0. Compiling without cuDSS support\n",
            "-- USE_CUFILE is set to 0. Compiling without cuFile support\n",
            "-- Autodetected CUDA architecture(s):  8.0\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_80,code=sm_80\n",
            "-- Found Torch: /content/libtorch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/lammps/build\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
            "-- Git Directory: /content/lammps/.git\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/command.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
            "[  3%] Built target timer.h\n",
            "[  3%] Built target angle.h\n",
            "[  3%] Built target bond.h\n",
            "[  3%] Built target atom.h\n",
            "[  3%] Built target comm.h\n",
            "[  3%] Built target citeme.h\n",
            "[  3%] Built target input.h\n",
            "[  3%] Built target command.h\n",
            "[  3%] Built target info.h\n",
            "[  3%] Built target compute.h\n",
            "[  3%] Built target kspace.h\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
            "-- Generating lmpgitversion.h...\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
            "[  4%] Built target lammps.h\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/exceptions.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
            "[  5%] Built target lattice.h\n",
            "[  5%] Built target library.h\n",
            "[  5%] Built target lmppython.h\n",
            "[  5%] Built target lmptype.h\n",
            "[  5%] Built target memory.h\n",
            "[  5%] Built target domain.h\n",
            "[  5%] Built target dihedral.h\n",
            "[  5%] Built target gitversion\n",
            "[  5%] Built target error.h\n",
            "[  5%] Built target exceptions.h\n",
            "[  5%] Built target fix.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/core.h\u001b[0m\n",
            "[  7%] Built target force.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/format.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
            "[  8%] Built target group.h\n",
            "[  8%] Built target universe.h\n",
            "[  8%] Built target update.h\n",
            "[  8%] Built target improper.h\n",
            "[  8%] Built target utils.h\n",
            "[  8%] Built target fmt_core.h\n",
            "[  8%] Built target variable.h\n",
            "[  8%] Built target modify.h\n",
            "[  8%] Built target fmt_format.h\n",
            "[  8%] Built target neighbor.h\n",
            "[  8%] Built target neigh_list.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/platform.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
            "[  9%] Built target output.h\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
            "[  9%] Built target pair.h\n",
            "[  9%] Built target platform.h\n",
            "[  9%] Built target pointers.h\n",
            "[  9%] Built target region.h\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_write.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_allegro.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:535\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/fmt/base.h:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/fmt/format.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/pointers.h:33\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/atom_vec.h:17\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/atom_vec.cpp:14\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvirtual void LAMMPS_NS::AtomVec::write_data_restricted_to_general()\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/lammps/src/atom_vec.cpp:2272:21\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin_memcpy(void*, const void*, long unsigned int)\u001b[m\u001b[K’ specified bound between 18446744056529682432 and 18446744073709551592 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "   30 | \u001b[01;35m\u001b[K                                 __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
            "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_count_type.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_grid.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_write.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid_vtk.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_grid.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_bond_history.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_pair.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_langevin.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_atom.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_global.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_local.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_table.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid2d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid3d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/label_map.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_deprecated.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin_ghost.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi_old.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq_ghost.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_bin.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_nsq.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_trim.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_ghost_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi_old.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_allegro.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_molecular.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/platform.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_ellipsoid.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_id.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_image.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_mol.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:820:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  820 |         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n",
            "      |         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
            "[ 98%] Built target lammps\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
            "[100%] Built target lmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -rf Si_data\n",
        "!rm -rf DES_Config\n",
        "# download DES data of SPICE modal from Vinay's google drive.\n",
        "# Simon Batzner's data for Si-Si lattice\n",
        "!gdown --folder --id --no-cookies https://drive.google.com/drive/folders/1FEwF4i8IDHGmAIQ3RilA0jG9_lEX4Yk0?usp=sharing\n",
        "# DES data from SPICE <downloading it to Si_data.\n",
        "\n",
        "!gdown  --folder --id --no-cookies https://drive.google.com/drive/folders/179oeQ9zSlMp_7FKmO2i8rtcZqqPMJN9n?usp=sharing\n",
        "# Allegro modal SPICE DES Dataset config yaml files.\n",
        "!gdown --folder --id --no-cookies https://drive.google.com/drive/folders/1U57qI1v5x26TnH66XZ1hq9cvdaQEqYdR?usp=sharing"
      ],
      "metadata": {
        "id": "eYnhz_xH-P36"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Si_data\n",
        "!ls DES_Config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35kPXyy3-2SY",
        "outputId": "78f0247c-9ae4-401b-8475-b608070d52cb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DES.xyz  sitraj.xyz\n",
            "DES_tutorial_nequip.yaml  DES_tutorial.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edit /content/allegro/configs/tutorial.yaml removing optimizer_params (unused)\n",
        "# *Comment out lines 94-100 of /content/allegro/configs/tutorial.yaml and default_dtype: float64 (line7) and change Line 14 ForceOutput to StressForceOutput*\n",
        "\n",
        "Don't activate conda as it will mess everything up!."
      ],
      "metadata": {
        "id": "7A4lfjdmA1hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!conda info --envs\n",
        "# conda, nglview is bad for allegro as it downgrades environment\n",
        "#!conda init\n",
        "#!conda deactivate"
      ],
      "metadata": {
        "id": "03iY1OAewWMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./results\n",
        "#!nequip-train allegro/configs/tutorial.yaml\n",
        "# Copy SPICE DES extxyz config file, to configs\n",
        "!cp DES_Config/DES_tutorial.yaml allegro/configs/"
      ],
      "metadata": {
        "id": "JL4h2O5Bqwzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Allegro\n",
        "!rm -rf results\n",
        "!nequip-train allegro/configs/DES_tutorial.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdlT2wo1-6C-",
        "outputId": "d0f9a54b-5143-4001-fee3-ad33ef858c26"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-mouse-355011484483095595\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241209_230937-vTyIxtvpXvbClkLtjOc6xYVOFu-gPYgMhomtLLVWyvc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDES\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-355011484483095595/allegro-tutorial?apiKey=de2ad34873b0017384457889094ffd14c84a0163\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-355011484483095595/allegro-tutorial/runs/vTyIxtvpXvbClkLtjOc6xYVOFu-gPYgMhomtLLVWyvc?apiKey=de2ad34873b0017384457889094ffd14c84a0163\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_ase_dataset.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  datas = [torch.load(d) for d in datas]\n",
            "Loaded data: Batch(atomic_numbers=[239550, 1], batch=[239550], cell=[18700, 3, 3], edge_cell_shift=[1429670, 3], edge_index=[2, 1429670], forces=[239550, 3], pbc=[18700, 3], pos=[239550, 3], ptr=[18701], total_energy=[18700, 1])\n",
            "    processed data size: ~70.78 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(18700)...\n",
            "Replace string dataset_per_atom_total_energy_mean to -35.92002647258334\n",
            "Atomic outputs are scaled by: [H, C, N, O, F, P, S, Cl, Br, I: None], shifted by [H, C, N, O, F, P, S, Cl, Br, I: -35.920026].\n",
            "Replace string dataset_forces_rms to 0.02045545683041229\n",
            "Initially outputs are globally scaled by: 0.02045545683041229, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 37928\n",
            "Number of trainable weights: 37928\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0     2     6.94e+05        0.893     6.94e+05       0.0147       0.0193          244           16\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    2.158    0.002        0.871     5.92e+05     5.92e+05       0.0141       0.0191          197         14.3\n",
            "Wall time: 2.158742269999493\n",
            "! Best model        0 591781.156\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10     6.59e+05         1.44     6.59e+05       0.0178       0.0245          166         16.6\n",
            "      1    20     1.45e+06        0.731     1.45e+06       0.0129       0.0175          344         24.6\n",
            "      1    30     1.26e+05        0.687     1.26e+05       0.0127        0.017         87.2         7.27\n",
            "      1    40     1.26e+05        0.876     1.26e+05       0.0147       0.0191         29.1         7.27\n",
            "      1    50     6.59e+05        0.771     6.59e+05       0.0138        0.018          266         16.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1     2     6.94e+05        0.747     6.94e+05       0.0134       0.0177          243           16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   11.195    0.002         0.94     2.79e+06     2.79e+06       0.0144       0.0204          268         23.5\n",
            "! Validation          1   11.195    0.002        0.781     5.91e+05     5.91e+05       0.0131       0.0181          197         14.2\n",
            "Wall time: 11.195731005999733\n",
            "! Best model        1 591269.395\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10     6.58e+05        0.753     6.58e+05       0.0135       0.0177          266         16.6\n",
            "      2    20     1.24e+06        0.648     1.24e+06        0.013       0.0165          342         22.8\n",
            "      2    30      9.7e+03        0.813      9.7e+03        0.014       0.0184         22.2         2.01\n",
            "      2    40     5.39e+06         3.53     5.39e+06       0.0259       0.0385          712         47.5\n",
            "      2    50     2.85e+06        0.961     2.85e+06       0.0163       0.0201          449         34.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2     2     6.92e+05        0.556     6.92e+05        0.011       0.0152          243           16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2   12.531    0.002        0.893     2.79e+06     2.79e+06       0.0136       0.0199          268         23.4\n",
            "! Validation          2   12.531    0.002        0.741      5.9e+05      5.9e+05       0.0117       0.0175          197         14.2\n",
            "Wall time: 12.531698044999757\n",
            "! Best model        2 590278.685\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10     9.83e+03        0.808     9.83e+03       0.0133       0.0184         22.3         2.03\n",
            "      3    20     2.85e+06         1.26     2.85e+06       0.0175       0.0229          449         34.5\n",
            "      3    30     3.03e+05        0.384     3.03e+05       0.0106       0.0127           90         11.3\n",
            "      3    40     6.54e+05         1.51     6.54e+05       0.0201       0.0252          165         16.5\n",
            "      3    50     1.19e+06         1.29     1.19e+06       0.0174       0.0232          223         22.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3     2      6.9e+05        0.896      6.9e+05       0.0148       0.0194          243         15.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3   13.893    0.002         1.31     2.79e+06     2.79e+06        0.017       0.0242          268         23.4\n",
            "! Validation          3   13.893    0.002         1.31     5.89e+05     5.89e+05       0.0165       0.0232          197         14.2\n",
            "Wall time: 13.893622540999786\n",
            "! Best model        3 588809.169\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10     1.35e+06         2.77     1.35e+06       0.0251        0.034          190         23.8\n",
            "      4    20     8.03e+05         7.94     8.03e+05       0.0471       0.0576          385         18.3\n",
            "      4    30     1.23e+06         12.9     1.23e+06       0.0589       0.0736          340         22.7\n",
            "      4    40     2.31e+05         11.9     2.31e+05       0.0542       0.0705          128         9.83\n",
            "      4    50     4.49e+05         24.8     4.49e+05       0.0866        0.102          192         13.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4     2     6.83e+05         9.79     6.83e+05       0.0501        0.064          241         15.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4   15.250    0.002         11.1     2.79e+06     2.79e+06       0.0519       0.0701          267         23.4\n",
            "! Validation          4   15.250    0.002         11.1     5.83e+05     5.83e+05       0.0524       0.0678          196         14.2\n",
            "Wall time: 15.250575455999751\n",
            "! Best model        4 583257.109\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10      1.2e+06          138      1.2e+06        0.195         0.24          201         22.4\n",
            "      5    20     7.34e+05          434     7.33e+05        0.354        0.426          210         17.5\n",
            "      5    30     1.27e+06          424     1.27e+06        0.321        0.421          185         23.1\n",
            "      5    40     2.29e+05          416     2.28e+05        0.329        0.417          147         9.78\n",
            "      5    50     8.61e+04          679     8.54e+04        0.462        0.533          102         5.98\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5     2     6.53e+05          230     6.53e+05        0.244         0.31          236         15.5\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5   16.586    0.002          359     2.78e+06     2.78e+06        0.288        0.389          264         23.2\n",
            "! Validation          5   16.586    0.002          239      5.6e+05      5.6e+05        0.249        0.316          192         13.9\n",
            "Wall time: 16.586807558000146\n",
            "! Best model        5 560470.142\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10     1.09e+06     2.52e+03     1.08e+06        0.828         1.03          192         21.3\n",
            "      6    20     1.09e+06     6.29e+03     1.08e+06         1.26         1.62          170         21.3\n",
            "      6    30     3.51e+05     8.07e+03     3.43e+05         1.48         1.84          156           12\n",
            "      6    40     7.01e+06     3.64e+03     7.01e+06         0.96         1.23          541         54.1\n",
            "      6    50     1.16e+06     5.89e+03     1.15e+06         1.14         1.57          286           22\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6     2     5.61e+05      3.6e+03     5.58e+05        0.971         1.23          217         14.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6   17.916    0.002     6.42e+03     2.74e+06     2.75e+06         1.24         1.68          253         22.5\n",
            "! Validation          6   17.916    0.002     3.74e+03     4.89e+05     4.93e+05        0.992         1.25          181         13.1\n",
            "Wall time: 17.916450652000094\n",
            "! Best model        6 492932.387\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10     5.14e+05     2.07e+04     4.94e+05         2.25         2.94          201         14.4\n",
            "      7    20     3.33e+05     3.15e+04     3.01e+05         2.77         3.63          191         11.2\n",
            "      7    30      3.5e+05     3.04e+04     3.19e+05         2.91         3.57          185         11.6\n",
            "      7    40     5.43e+04     1.76e+04     3.67e+04         2.31         2.71         15.7         3.92\n",
            "      7    50     2.93e+05     2.23e+04     2.71e+05         2.26         3.05          202         10.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7     2     4.62e+05     1.41e+04     4.48e+05         1.92         2.43          193         12.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7   19.245    0.002     2.28e+04     2.69e+06     2.72e+06         2.43         3.15          236         21.3\n",
            "! Validation          7   19.245    0.002     1.49e+04      4.1e+05     4.25e+05         1.97         2.49          166           12\n",
            "Wall time: 19.245468418999735\n",
            "! Best model        7 424953.170\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10     5.28e+04     1.44e+04     3.84e+04         1.91         2.45         32.1         4.01\n",
            "      8    20      5.1e+05     2.34e+04     4.86e+05         2.52         3.13          257         14.3\n",
            "      8    30     7.68e+05     2.73e+04     7.41e+05         2.72         3.38          176         17.6\n",
            "      8    40     8.18e+05     4.05e+04     7.77e+05          3.1         4.11          144           18\n",
            "      8    50     1.34e+06     1.65e+04     1.32e+06          1.9         2.63          306         23.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8     2     4.27e+05     2.01e+04     4.07e+05         2.29          2.9          183         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8   20.573    0.002     2.36e+04     2.68e+06      2.7e+06         2.46         3.21          235         21.2\n",
            "! Validation          8   20.573    0.002     2.16e+04     3.82e+05     4.03e+05         2.37            3          160         11.6\n",
            "Wall time: 20.573546873999476\n",
            "! Best model        8 403134.399\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10     4.74e+05     3.04e+04     4.44e+05         2.89         3.56          245         13.6\n",
            "      9    20      5.7e+06     4.96e+03     5.69e+06        0.891         1.44          732         48.8\n",
            "      9    30     2.92e+05     5.22e+04      2.4e+05         2.89         4.67          100           10\n",
            "      9    40     6.24e+05      4.1e+04     5.83e+05         3.32         4.14          328         15.6\n",
            "      9    50     7.54e+05     4.02e+04     7.14e+05         3.36          4.1          156         17.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9     2     4.13e+05      2.3e+04      3.9e+05         2.45          3.1          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9   21.907    0.002     2.61e+04     2.68e+06      2.7e+06         2.63         3.41          232         21.1\n",
            "! Validation          9   21.907    0.002     2.47e+04      3.7e+05     3.94e+05         2.55         3.21          158         11.4\n",
            "Wall time: 21.90802658499979\n",
            "! Best model        9 394472.715\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10     5.84e+06     9.92e+03     5.83e+06         1.24         2.04          741         49.4\n",
            "     10    20     3.63e+06      4.5e+04     3.59e+06         3.19         4.34          504         38.7\n",
            "     10    30     4.07e+05     4.06e+04     3.67e+05         3.51         4.12          149         12.4\n",
            "     10    40     7.49e+05     3.98e+04     7.09e+05         3.38         4.08          155         17.2\n",
            "     10    50     2.12e+05     4.71e+04     1.65e+05         3.61         4.44          116         8.31\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10     2     3.82e+05     3.07e+04     3.52e+05         2.85         3.59          169           11\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10   23.247    0.002     4.02e+04     2.65e+06     2.69e+06         3.28         4.23          223         20.5\n",
            "! Validation         10   23.247    0.002     3.31e+04     3.44e+05     3.77e+05         2.96         3.71          152           11\n",
            "Wall time: 23.24776669199946\n",
            "! Best model       10 376744.375\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10     8.24e+05     5.27e+04     7.71e+05         4.07          4.7          252           18\n",
            "     11    20     1.94e+06     2.55e+04     1.91e+06         2.61         3.26          452         28.3\n",
            "     11    30     3.78e+05     2.07e+04     3.57e+05         2.39         2.94          196         12.2\n",
            "     11    40     6.38e+07          314     6.38e+07        0.244        0.362          654          163\n",
            "     11    50     7.54e+05     4.96e+04     7.05e+05         3.67         4.56          258         17.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11     2     3.84e+05     2.96e+04     3.55e+05         2.79         3.52          170         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11   24.580    0.002      3.2e+04     2.66e+06     2.69e+06         2.91         3.75          228         20.7\n",
            "! Validation         11   24.580    0.002     3.18e+04     3.46e+05     3.78e+05          2.9         3.64          152           11\n",
            "Wall time: 24.580953869999576\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10     9.33e+04     5.81e+04     3.53e+04         4.11         4.93         61.5         3.84\n",
            "     12    20     3.24e+05     4.55e+04     2.79e+05         3.63         4.36          108         10.8\n",
            "     12    30     2.34e+05     3.98e+04     1.94e+05         3.17         4.08          171         9.02\n",
            "     12    40     7.27e+05     6.47e+04     6.62e+05         4.03          5.2          133         16.6\n",
            "     12    50     9.11e+05     4.43e+04     8.67e+05         3.76         4.31          209           19\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12     2     3.74e+05     3.24e+04     3.41e+05         2.93         3.68          167         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12   25.922    0.002     3.72e+04     2.65e+06     2.69e+06         3.17         4.08          223         20.5\n",
            "! Validation         12   25.922    0.002     3.47e+04     3.37e+05     3.72e+05         3.04          3.8          150         10.9\n",
            "Wall time: 25.922553694999806\n",
            "! Best model       12 371760.855\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10      1.9e+06     2.29e+04     1.88e+06         2.46          3.1          448           28\n",
            "     13    20     7.57e+05     3.39e+04     7.24e+05         3.11         3.77          157         17.4\n",
            "     13    30     4.94e+04     1.15e+04     3.79e+04         1.93          2.2         15.9         3.98\n",
            "     13    40     2.34e+05     3.66e+04     1.97e+05         3.04         3.91          173         9.09\n",
            "     13    50     3.54e+06     3.51e+04      3.5e+06         2.77         3.83          498         38.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13     2     3.71e+05     3.23e+04     3.39e+05         2.92         3.68          166         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13   27.287    0.002      3.5e+04     2.64e+06     2.68e+06         3.04         3.96          224         20.5\n",
            "! Validation         13   27.287    0.002     3.48e+04     3.36e+05     3.71e+05         3.03         3.81          149         10.9\n",
            "Wall time: 27.288129828999445\n",
            "! Best model       13 370588.637\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10     4.31e+05     3.74e+04     3.94e+05         3.39         3.95          180         12.8\n",
            "     14    20     3.95e+04     2.19e+04     1.76e+04         2.66         3.03         46.2         2.71\n",
            "     14    30      2.3e+05     3.77e+04     1.93e+05         3.12         3.97          171         8.98\n",
            "     14    40     3.96e+05     6.57e+04     3.31e+05         4.04         5.24          165         11.8\n",
            "     14    50     2.67e+05     3.56e+04     2.31e+05         3.21         3.86          216         9.84\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14     2      3.7e+05     3.25e+04     3.37e+05         2.93         3.69          166         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14   28.631    0.002     3.47e+04     2.64e+06     2.68e+06         3.05         3.94          223         20.4\n",
            "! Validation         14   28.631    0.002     3.47e+04     3.35e+05     3.69e+05         3.03          3.8          149         10.8\n",
            "Wall time: 28.631318548999843\n",
            "! Best model       14 369429.657\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10     2.07e+07     2.29e+03     2.07e+07        0.737        0.979          465           93\n",
            "     15    20     7.16e+06     2.35e+04     7.13e+06         2.31         3.14          546         54.6\n",
            "     15    30     4.03e+04     1.57e+04     2.46e+04          2.1         2.56         25.7         3.21\n",
            "     15    40     4.46e+05     2.01e+04     4.26e+05         2.28          2.9          174         13.3\n",
            "     15    50     7.22e+05      6.3e+04     6.59e+05         3.93         5.14          133         16.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15     2     3.69e+05     3.21e+04     3.37e+05         2.91         3.67          166         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15   29.963    0.002     3.37e+04     2.64e+06     2.67e+06         3.02         3.89          223         20.4\n",
            "! Validation         15   29.963    0.002     3.42e+04     3.35e+05     3.69e+05         3.01         3.77          149         10.8\n",
            "Wall time: 29.96380018599939\n",
            "! Best model       15 368903.061\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10     4.65e+05     2.26e+04     4.42e+05         2.42         3.08          177         13.6\n",
            "     16    20     4.34e+05     8.61e+04     3.48e+05         4.82            6          145         12.1\n",
            "     16    30     6.44e+05     6.18e+04     5.82e+05         4.08         5.08          156         15.6\n",
            "     16    40     5.02e+05     8.65e+04     4.15e+05         4.65         6.02          198         13.2\n",
            "     16    50     2.08e+07      2.8e+03     2.08e+07        0.818         1.08          466         93.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16     2     3.64e+05     3.32e+04     3.31e+05         2.95         3.73          164         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16   31.295    0.002     3.63e+04     2.63e+06     2.66e+06         3.12         4.04          220         20.3\n",
            "! Validation         16   31.295    0.002      3.5e+04     3.31e+05     3.66e+05         3.05         3.82          147         10.8\n",
            "Wall time: 31.29566125099973\n",
            "! Best model       16 365966.310\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10     3.79e+05     5.73e+04     3.21e+05         4.12          4.9          162         11.6\n",
            "     17    20     2.28e+05     8.91e+04     1.39e+05         5.09         6.11          130         7.62\n",
            "     17    30     5.91e+05     1.15e+05     4.76e+05         5.51         6.94          212         14.1\n",
            "     17    40     3.29e+05     5.25e+04     2.76e+05         3.87         4.69          194         10.8\n",
            "     17    50     6.43e+05     8.33e+04     5.59e+05         4.49         5.91          122         15.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17     2     3.46e+05     3.82e+04     3.08e+05         3.15            4          158         10.4\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17   32.636    0.002     4.88e+04     2.61e+06     2.66e+06          3.6         4.72          213         19.8\n",
            "! Validation         17   32.636    0.002     4.01e+04     3.17e+05     3.57e+05         3.25         4.09          143         10.5\n",
            "Wall time: 32.63656652099962\n",
            "! Best model       17 356725.659\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10     7.03e+05     3.25e+04      6.7e+05         3.18         3.69          151         16.7\n",
            "     18    20     2.08e+05     4.73e+04     1.61e+05         3.67         4.45          181         8.21\n",
            "     18    30     5.96e+05      7.4e+04     5.22e+05         4.36         5.56          148         14.8\n",
            "     18    40     6.85e+05     5.85e+04     6.26e+05         3.94         4.95          243         16.2\n",
            "     18    50     4.15e+05     8.04e+03     4.07e+05         1.46         1.83          170           13\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18     2     3.47e+05     3.65e+04     3.11e+05         3.05         3.91          159         10.4\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18   33.979    0.002     3.66e+04     2.61e+06     2.65e+06         3.08         4.08          217         20.1\n",
            "! Validation         18   33.979    0.002     3.81e+04     3.19e+05     3.57e+05         3.15         3.99          144         10.5\n",
            "Wall time: 33.97959860299943\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10     1.16e+05     2.23e+04     9.35e+04         2.47         3.06         68.8         6.26\n",
            "     19    20        7e+06     1.55e+04     6.98e+06         1.81         2.55          541         54.1\n",
            "     19    30     8.64e+05     3.45e+04     8.29e+05         3.23          3.8          373         18.6\n",
            "     19    40     4.58e+05      4.6e+04     4.12e+05         3.32         4.39          210         13.1\n",
            "     19    50     2.07e+07     2.43e+03     2.07e+07        0.777         1.01          465         93.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19     2     3.55e+05      3.3e+04     3.22e+05         2.88         3.71          162         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19   35.299    0.002     2.82e+04     2.61e+06     2.64e+06          2.7         3.57          221         20.3\n",
            "! Validation         19   35.299    0.002     3.41e+04     3.26e+05      3.6e+05         2.97         3.77          145         10.7\n",
            "Wall time: 35.29953902399939\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10     2.01e+05     3.88e+04     1.62e+05         3.16         4.03          156         8.23\n",
            "     20    20     3.32e+04     8.85e+03     2.43e+04         1.62         1.92         38.3         3.19\n",
            "     20    30     3.08e+05     3.49e+04     2.73e+05         3.28         3.82          107         10.7\n",
            "     20    40     6.54e+05     3.12e+04     6.23e+05         3.16         3.61          145         16.1\n",
            "     20    50     6.58e+05     2.69e+04     6.31e+05         2.71         3.35          244         16.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20     2     3.41e+05     3.64e+04     3.04e+05         2.99          3.9          158         10.4\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20   36.621    0.002     4.37e+04     2.59e+06     2.63e+06         3.31         4.43          210         19.5\n",
            "! Validation         20   36.621    0.002     3.72e+04     3.15e+05     3.52e+05         3.08         3.95          142         10.4\n",
            "Wall time: 36.62134319699999\n",
            "! Best model       20 352381.354\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10     3.24e+05     3.81e+04     2.86e+05         3.26            4          197         10.9\n",
            "     21    20     6.31e+07     1.32e+03     6.31e+07        0.558        0.742          650          162\n",
            "     21    30     1.81e+05     1.02e+05     7.89e+04         5.62         6.54         97.7         5.75\n",
            "     21    40     2.85e+05     3.93e+04     2.46e+05         3.45         4.05          101         10.1\n",
            "     21    50     3.56e+05     4.87e+04     3.07e+05         3.71         4.51          159         11.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21     2      3.3e+05      3.8e+04     2.92e+05         3.03         3.99          154         10.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21   37.974    0.002     4.22e+04     2.58e+06     2.62e+06         3.25         4.37          208         19.4\n",
            "! Validation         21   37.974    0.002     3.85e+04     3.08e+05     3.47e+05         3.11         4.01          140         10.3\n",
            "Wall time: 37.97488127499946\n",
            "! Best model       21 346534.199\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10     3.47e+05     2.19e+04     3.25e+05         2.53         3.03          117         11.7\n",
            "     22    20     1.53e+05     5.49e+04     9.86e+04         3.93         4.79         89.9         6.42\n",
            "     22    30     6.95e+06     2.19e+04     6.92e+06         2.34         3.03          538         53.8\n",
            "     22    40      1.4e+05     2.49e+04     1.15e+05         2.56         3.23         76.3         6.93\n",
            "     22    50      3.5e+06      4.2e+04     3.46e+06         3.38         4.19          494           38\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22     2     3.35e+05     3.47e+04        3e+05         2.85         3.81          157         10.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22   39.355    0.002     2.99e+04     2.56e+06     2.59e+06          2.7         3.68          215         19.9\n",
            "! Validation         22   39.355    0.002     3.47e+04     3.13e+05     3.48e+05         2.91         3.81          141         10.4\n",
            "Wall time: 39.3551896959998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10     3.75e+05      1.1e+05     2.65e+05         5.46         6.79          221         10.5\n",
            "     23    20      8.3e+05     4.41e+04     7.86e+05         3.23         4.29          145         18.1\n",
            "     23    30     3.39e+04      5.5e+03     2.84e+04         1.09         1.52         27.6         3.45\n",
            "     23    40     3.08e+05        4e+04     2.68e+05          3.4         4.09          222         10.6\n",
            "     23    50     6.06e+05     5.62e+04      5.5e+05         3.54         4.85          121         15.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23     2     3.17e+05     3.78e+04     2.79e+05         2.94         3.98          151         9.96\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23   40.689    0.002     4.95e+04     2.49e+06     2.54e+06         3.48         4.68          201         18.9\n",
            "! Validation         23   40.689    0.002     3.74e+04     3.01e+05     3.38e+05         2.98         3.96          136         10.1\n",
            "Wall time: 40.68979157499962\n",
            "! Best model       23 338238.883\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10     5.46e+04     1.38e+04     4.08e+04         1.89          2.4         49.6         4.13\n",
            "     24    20     6.28e+04     2.11e+04     4.16e+04         2.26         2.97         20.9         4.17\n",
            "     24    30     1.31e+05     1.11e+05     1.92e+04         5.65         6.83         39.7         2.84\n",
            "     24    40     5.76e+07     2.05e+05     5.74e+07         6.44         9.26          620          155\n",
            "     24    50     1.74e+05     6.55e+04     1.09e+05         3.95         5.23         74.2         6.74\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24     2     3.11e+05     3.81e+04     2.73e+05         2.93         3.99          149         9.88\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24   42.027    0.002     5.59e+04     2.39e+06     2.45e+06         3.65         4.83          201         18.8\n",
            "! Validation         24   42.027    0.002     3.72e+04     2.97e+05     3.34e+05         2.93         3.95          135         9.99\n",
            "Wall time: 42.02798998599974\n",
            "! Best model       24 334303.971\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10      3.3e+05     5.92e+04     2.71e+05         4.02         4.98          149         10.6\n",
            "     25    20      2.8e+06     1.05e+05      2.7e+06          4.9         6.64          437         33.6\n",
            "     25    30     3.89e+05     1.67e+05     2.22e+05         7.24         8.36          145         9.64\n",
            "     25    40     2.08e+05     3.89e+04     1.69e+05         3.15         4.03          118         8.41\n",
            "     25    50     3.01e+05     7.15e+04      2.3e+05         4.19         5.47           98          9.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25     2     3.12e+05     4.32e+04     2.69e+05         3.19         4.25          148         9.84\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25   43.365    0.002     1.17e+05     2.21e+06     2.32e+06         4.81         6.72          195         18.2\n",
            "! Validation         25   43.365    0.002     4.09e+04     2.95e+05     3.35e+05         3.11         4.14          134         9.94\n",
            "Wall time: 43.36593233700023\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10     2.87e+05     1.73e+05     1.13e+05         7.03         8.51          145         6.89\n",
            "     26    20     5.99e+06     9.32e+05     5.06e+06         12.1         19.7          460           46\n",
            "     26    30     2.28e+06     8.56e+04     2.19e+06         4.48         5.99          273         30.3\n",
            "     26    40      2.5e+06     1.59e+05     2.34e+06         6.55         8.15          407         31.3\n",
            "     26    50     7.01e+06     1.53e+05     6.85e+06         7.18         8.01          535         53.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26     2     2.98e+05     6.08e+04     2.38e+05         3.83         5.04          139         9.26\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26   44.708    0.002      2.1e+05     1.98e+06     2.19e+06            6         8.62          173         16.6\n",
            "! Validation         26   44.708    0.002     5.56e+04     2.77e+05     3.33e+05         3.65         4.84          130         9.72\n",
            "Wall time: 44.708914672999526\n",
            "! Best model       26 332761.581\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10     6.07e+05     1.41e+05     4.67e+05         6.48         7.67          140           14\n",
            "     27    20     2.93e+05     8.78e+04     2.06e+05         5.04         6.06          130         9.27\n",
            "     27    30     8.14e+04     7.82e+04     3.24e+03         4.61         5.72         19.8         1.16\n",
            "     27    40     1.06e+06     5.29e+05     5.27e+05         11.1         14.9          193         14.8\n",
            "     27    50     7.25e+05     1.12e+05     6.13e+05         5.35         6.84          224           16\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27     2     3.12e+05     7.97e+04     2.33e+05         4.43         5.77          137         9.19\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27   46.042    0.002     2.98e+05     1.81e+06     2.11e+06         6.34          9.9          175         16.5\n",
            "! Validation         27   46.042    0.002     6.97e+04     2.73e+05     3.43e+05         4.14         5.42          130         9.68\n",
            "Wall time: 46.04228756700013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10      3.1e+05      2.6e+05     5.03e+04         7.77         10.4         36.7         4.59\n",
            "     28    20      2.7e+05      1.9e+05     7.99e+04         7.43         8.92         69.4         5.78\n",
            "     28    30     5.57e+05     2.38e+05     3.19e+05         6.77         9.99          185         11.5\n",
            "     28    40     1.88e+05     8.91e+04     9.86e+04         4.95          6.1          122         6.42\n",
            "     28    50     1.36e+06     1.74e+05     1.19e+06         6.07         8.54          356         22.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28     2     3.14e+05     9.63e+04     2.18e+05         4.87         6.35          132         8.87\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28   47.368    0.002     4.36e+05     1.58e+06     2.01e+06         6.67         11.4          161         15.3\n",
            "! Validation         28   47.368    0.002     8.39e+04     2.65e+05     3.49e+05         4.55         5.95          127         9.52\n",
            "Wall time: 47.36851253199984\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10     3.09e+06     1.72e+06     1.38e+06         13.7         26.8          312           24\n",
            "     29    20     3.23e+05     1.07e+05     2.16e+05         4.99         6.69          152          9.5\n",
            "     29    30      1.8e+06     2.42e+05     1.56e+06         7.63         10.1          332         25.5\n",
            "     29    40     1.43e+05     5.24e+04     9.05e+04         3.21         4.68         86.2         6.15\n",
            "     29    50     9.18e+04     5.93e+04     3.25e+04         3.62         4.98         18.4         3.69\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29     2     3.01e+05     9.75e+04     2.04e+05         4.88         6.39          127         8.54\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29   48.690    0.002     3.08e+05     1.62e+06     1.92e+06         5.83         9.78          160         15.2\n",
            "! Validation         29   48.690    0.002     8.69e+04     2.57e+05     3.44e+05         4.62         6.05          125         9.34\n",
            "Wall time: 48.69113414899948\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10     1.17e+06     1.22e+05     1.05e+06         5.57         7.15          189           21\n",
            "     30    20     3.08e+06     1.43e+06     1.65e+06         13.2         24.4          263         26.3\n",
            "     30    30     3.37e+05     1.29e+05     2.08e+05          5.5         7.34          196         9.33\n",
            "     30    40     3.58e+06     2.57e+06     1.01e+06           15         32.8          267         20.6\n",
            "     30    50     2.07e+07     1.34e+04     2.07e+07         1.61         2.37          465           93\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30     2     2.97e+05     1.03e+05     1.94e+05         5.01         6.55          124         8.32\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30   50.025    0.002     4.31e+05     1.41e+06     1.84e+06          6.3         11.2          154         14.6\n",
            "! Validation         30   50.025    0.002     9.36e+04     2.51e+05     3.45e+05         4.77         6.27          123         9.21\n",
            "Wall time: 50.02551901799961\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10     7.21e+06     3.61e+05     6.85e+06         9.59         12.3          535         53.5\n",
            "     31    20     5.13e+05     1.06e+05     4.07e+05            5         6.66          130           13\n",
            "     31    30      8.1e+04     5.79e+04     2.32e+04         3.55         4.92         15.6         3.11\n",
            "     31    40     4.36e+05     1.36e+05     3.01e+05         6.09         7.53         89.7         11.2\n",
            "     31    50     3.05e+06     2.45e+06     5.98e+05         15.7           32          206         15.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31     2     2.91e+05     1.02e+05     1.89e+05         4.96         6.54          121         8.17\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31   51.425    0.002     4.38e+05     1.33e+06     1.76e+06         6.42         11.7          147         13.9\n",
            "! Validation         31   51.425    0.002     9.91e+04     2.47e+05     3.46e+05         4.81         6.44          122         9.11\n",
            "Wall time: 51.425815431000046\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10     5.99e+05     1.18e+05     4.81e+05         5.46         7.02          284         14.2\n",
            "     32    20     2.71e+05     1.26e+05     1.45e+05          5.5         7.26         77.8         7.78\n",
            "     32    30     2.74e+06     1.82e+06     9.16e+05           18         27.6          196         19.6\n",
            "     32    40     2.45e+04     2.02e+04     4.21e+03         2.37         2.91         15.9         1.33\n",
            "     32    50     3.97e+05     1.04e+05     2.93e+05         4.67         6.59          111         11.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32     2     2.69e+05        1e+05     1.69e+05         4.92         6.48          114         7.67\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32   52.744    0.002     4.36e+05     1.23e+06     1.67e+06         6.55         11.4          137         13.1\n",
            "! Validation         32   52.744    0.002     1.05e+05     2.37e+05     3.42e+05         4.87         6.63          118         8.88\n",
            "Wall time: 52.744355435000216\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10      2.5e+05     8.31e+04     1.67e+05         4.83          5.9          117         8.36\n",
            "     33    20     2.06e+07     1.19e+04     2.06e+07         1.46         2.23          464         92.8\n",
            "     33    30     1.81e+05     1.71e+05     9.88e+03         5.93         8.47         28.5         2.03\n",
            "     33    40     1.18e+05      3.6e+04     8.18e+04         3.09         3.88         81.9         5.85\n",
            "     33    50     2.98e+05     9.89e+04     1.99e+05         5.16         6.43          146         9.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33     2     2.59e+05     9.31e+04     1.66e+05         4.73         6.24          113         7.57\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33   54.068    0.002     3.64e+05     1.21e+06     1.57e+06         5.97         10.4          139         13.2\n",
            "! Validation         33   54.068    0.002     1.05e+05     2.35e+05      3.4e+05         4.75         6.61          117         8.81\n",
            "Wall time: 54.068399001000216\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10     5.35e+05     1.46e+05     3.89e+05         5.64         7.81          255         12.8\n",
            "     34    20     8.72e+05     3.75e+04     8.34e+05         3.17         3.96          149         18.7\n",
            "     34    30     6.72e+04      4.2e+04     2.52e+04         3.37         4.19           13         3.25\n",
            "     34    40     1.82e+05     1.08e+05     7.33e+04         4.86         6.73         55.4         5.54\n",
            "     34    50     5.59e+04     5.14e+04     4.52e+03         3.58         4.64           11         1.38\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34     2     2.43e+05     9.64e+04     1.46e+05          4.8         6.35          105         7.05\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34   55.389    0.002     4.47e+05     1.04e+06     1.49e+06         6.78         11.8          123         11.8\n",
            "! Validation         34   55.389    0.002     1.18e+05     2.24e+05     3.42e+05         4.91         6.99          114         8.56\n",
            "Wall time: 55.389235177000046\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10     4.84e+04     4.81e+04          379         3.48         4.48         6.77        0.398\n",
            "     35    20      2.5e+06      9.3e+05     1.57e+06         13.2         19.7          385         25.7\n",
            "     35    30     2.98e+05      1.2e+05     1.78e+05         5.13         7.07          181         8.64\n",
            "     35    40     1.07e+05     4.36e+04     6.32e+04          3.4         4.27           72         5.14\n",
            "     35    50     2.12e+05     1.93e+05      1.9e+04         7.26         8.98         28.2         2.82\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35     2     2.34e+05     9.56e+04     1.38e+05         4.76         6.33          101         6.82\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35   56.706    0.002     3.73e+05     1.06e+06     1.43e+06         6.38         10.9          128         12.2\n",
            "! Validation         35   56.706    0.002     1.26e+05     2.19e+05     3.45e+05         4.95         7.22          112         8.44\n",
            "Wall time: 56.70710874799988\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10     2.99e+05     2.99e+05         77.6         6.68         11.2         2.34         0.18\n",
            "     36    20     1.36e+06     1.05e+06     3.05e+05         12.9           21          147         11.3\n",
            "     36    30     2.47e+06     1.03e+06     1.44e+06         13.8         20.8          368         24.5\n",
            "     36    40     3.65e+05     1.29e+05     2.37e+05         6.35         7.34         79.6         9.95\n",
            "     36    50     1.12e+05     9.23e+04     1.94e+04         4.68         6.21         54.1         2.85\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36     2     2.24e+05     9.83e+04     1.26e+05         4.81         6.41         95.5         6.47\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36   58.032    0.002     3.33e+05        1e+06     1.34e+06         6.26         10.4          119         11.4\n",
            "! Validation         36   58.032    0.002     1.34e+05     2.13e+05     3.47e+05         5.06         7.44          110         8.27\n",
            "Wall time: 58.03290218700022\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10     2.06e+07     2.16e+04     2.06e+07         1.81            3          464         92.8\n",
            "     37    20     4.73e+05     1.86e+05     2.86e+05         6.16         8.83          219         10.9\n",
            "     37    30     4.15e+05     5.67e+04     3.58e+05         3.69         4.87          135         12.2\n",
            "     37    40     5.65e+04      5.5e+04     1.54e+03         3.66          4.8         9.64        0.804\n",
            "     37    50     2.37e+05     1.15e+05     1.23e+05         5.88         6.92          115         7.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37     2     2.14e+05     1.06e+05     1.08e+05         4.99         6.67         87.1         5.93\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37   59.360    0.002     4.34e+05     8.33e+05     1.27e+06         7.09           12          105         10.1\n",
            "! Validation         37   59.360    0.002     1.52e+05     2.04e+05     3.56e+05         5.34         7.91          106         8.02\n",
            "Wall time: 59.36046479000015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10     2.24e+05     1.69e+05     5.52e+04         5.93          8.4          101         4.81\n",
            "     38    20     1.22e+05     1.19e+05     2.95e+03         5.41         7.06         8.88         1.11\n",
            "     38    30     9.74e+04     7.82e+04     1.92e+04         4.23         5.72         62.4         2.84\n",
            "     38    40      9.1e+04     8.42e+04     6.79e+03         4.19         5.94         8.43         1.69\n",
            "     38    50     1.54e+05     9.57e+04     5.85e+04         4.27         6.33         49.5         4.95\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38     2     2.09e+05     1.07e+05     1.03e+05         4.98         6.68         84.8         5.78\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38   60.682    0.002     3.38e+05     8.86e+05     1.22e+06         6.62           11          109         10.5\n",
            "! Validation         38   60.682    0.002     1.58e+05     2.01e+05     3.58e+05         5.41         8.05          105         7.93\n",
            "Wall time: 60.68307924599958\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10     1.15e+07     3.84e+06     7.62e+06         28.8         40.1          226         56.5\n",
            "     39    20     1.01e+05     1.01e+05         49.8         4.75         6.51         2.74        0.144\n",
            "     39    30     3.35e+05     3.05e+05     2.98e+04         7.19         11.3         45.9         3.53\n",
            "     39    40     1.13e+06     1.13e+06     2.91e+03         16.2         21.8           11          1.1\n",
            "     39    50     1.36e+05     1.12e+05     2.48e+04          5.2         6.84         67.6         3.22\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39     2     1.98e+05     1.09e+05     8.87e+04         5.05         6.76         77.4         5.29\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39   62.133    0.002     3.34e+05     8.56e+05     1.19e+06         6.73         10.9         99.5         9.77\n",
            "! Validation         39   62.133    0.002     1.62e+05     1.94e+05     3.56e+05         5.48         8.16          101          7.7\n",
            "Wall time: 62.133228371999394\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10     2.06e+05     9.54e+04      1.1e+05          4.9         6.32          122         6.79\n",
            "     40    20      3.7e+04      3.7e+04         4.48         3.06         3.93        0.346       0.0433\n",
            "     40    30     9.88e+05      4.2e+05     5.68e+05         8.22         13.3          200         15.4\n",
            "     40    40     1.22e+05     7.59e+04     4.62e+04         4.67         5.64         52.7         4.39\n",
            "     40    50     1.67e+05      6.4e+04     1.03e+05         4.06         5.18         78.7         6.55\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40     2     1.86e+05     1.05e+05     8.14e+04         4.95         6.63         73.3         5.02\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40   63.482    0.002     2.71e+05     8.63e+05     1.13e+06         6.12         9.77          105         10.2\n",
            "! Validation         40   63.482    0.002     1.53e+05     1.91e+05     3.44e+05         5.35         7.93         99.5         7.56\n",
            "Wall time: 63.48305472099946\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10     2.06e+07     3.92e+04     2.05e+07         2.32         4.05          463         92.7\n",
            "     41    20     9.38e+05      4.1e+04     8.97e+05          2.8         4.14          155         19.4\n",
            "     41    30     4.22e+05     3.04e+04     3.91e+05         2.94         3.57          141         12.8\n",
            "     41    40     3.85e+05     1.54e+05     2.31e+05          5.8         8.04          147         9.83\n",
            "     41    50     2.27e+05     8.45e+04     1.42e+05         4.36         5.95         77.1         7.71\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41     2     1.84e+05     1.01e+05     8.27e+04         4.88         6.51         74.2         5.08\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41   64.815    0.002     2.96e+05     8.16e+05     1.11e+06         6.18         10.2          107         10.2\n",
            "! Validation         41   64.815    0.002     1.47e+05     1.91e+05     3.39e+05         5.27         7.79         99.6         7.57\n",
            "Wall time: 64.81594072499956\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10     1.37e+05     1.09e+05     2.87e+04         4.85         6.74         41.6         3.46\n",
            "     42    20     1.61e+05     1.14e+05     4.69e+04         5.38          6.9         44.3         4.43\n",
            "     42    30     3.26e+05     1.41e+05     1.85e+05         5.55         7.69          123          8.8\n",
            "     42    40        2e+05      1.9e+05        1e+04         6.06         8.91         28.6         2.05\n",
            "     42    50     2.06e+07      3.7e+04     2.05e+07         2.35         3.94          464         92.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42     2     1.76e+05     1.07e+05     6.86e+04         5.02         6.69         65.6         4.53\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42   66.118    0.002     3.28e+05     7.49e+05     1.08e+06         6.68         10.9         91.5         8.95\n",
            "! Validation         42   66.118    0.002     1.54e+05     1.86e+05     3.39e+05          5.4         7.96           96         7.31\n",
            "Wall time: 66.11910414500016\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10     9.23e+04     8.39e+04     8.38e+03         4.54         5.92           15         1.87\n",
            "     43    20      5.5e+05      3.3e+04     5.17e+05         3.13         3.71          162         14.7\n",
            "     43    30     2.41e+05     3.76e+04     2.04e+05         3.02         3.97         73.9         9.23\n",
            "     43    40      1.5e+05     8.14e+04     6.86e+04         4.34         5.84           75         5.36\n",
            "     43    50     1.99e+05     1.82e+05     1.72e+04         5.67         8.73         37.5         2.68\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43     2     1.76e+05     9.97e+04     7.59e+04         4.85         6.46         70.3         4.84\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43   67.430    0.002     2.78e+05     8.04e+05     1.08e+06          6.1         9.95          110         10.2\n",
            "! Validation         43   67.430    0.002     1.44e+05     1.88e+05     3.32e+05         5.23          7.7         97.6         7.43\n",
            "Wall time: 67.43057716600015\n",
            "! Best model       43 331630.527\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10     8.68e+06     1.65e+06     7.03e+06         20.5         26.3          542         54.2\n",
            "     44    20     3.58e+04     3.57e+04         83.5         2.94         3.86         2.24        0.187\n",
            "     44    30     9.24e+04     7.97e+04     1.27e+04         4.12         5.77         11.5          2.3\n",
            "     44    40      1.6e+06      1.3e+06     2.98e+05         15.6         23.3          145         11.2\n",
            "     44    50     4.86e+05     4.34e+05     5.19e+04         10.8         13.5         69.9         4.66\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44     2      1.7e+05     9.91e+04     7.09e+04         4.84         6.44         67.3         4.64\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44   68.747    0.002     2.95e+05      7.5e+05     1.04e+06         6.29         10.3         94.1         9.13\n",
            "! Validation         44   68.747    0.002     1.42e+05     1.86e+05     3.28e+05         5.22         7.64         96.3         7.34\n",
            "Wall time: 68.74789130199952\n",
            "! Best model       44 327555.572\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10     1.84e+05     1.08e+05     7.55e+04         5.21         6.73          101         5.62\n",
            "     45    20      6.4e+04     6.32e+04          813         3.96         5.14         12.8        0.583\n",
            "     45    30     3.05e+05     2.03e+04     2.85e+05         2.06         2.91          120         10.9\n",
            "     45    40      2.9e+04      2.8e+04          956         2.58         3.42         7.59        0.633\n",
            "     45    50     1.82e+05     4.95e+04     1.33e+05         3.23         4.55         59.6         7.45\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45     2     1.62e+05     9.81e+04     6.42e+04          4.8         6.41         62.9         4.35\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45   70.064    0.002     2.97e+05     7.26e+05     1.02e+06         6.19         10.2         94.3         9.02\n",
            "! Validation         45   70.064    0.002     1.39e+05     1.84e+05     3.23e+05         5.18         7.57         94.4          7.2\n",
            "Wall time: 70.06419062199984\n",
            "! Best model       45 323074.201\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10     2.02e+05     1.95e+05     7.22e+03          6.3         9.03         22.6         1.74\n",
            "     46    20     1.53e+05     1.47e+05     5.74e+03         5.39         7.84         21.7         1.55\n",
            "     46    30     8.99e+04     7.28e+04     1.72e+04         3.93         5.52         13.4         2.68\n",
            "     46    40     3.95e+04     3.93e+04          184         3.13         4.06         3.33        0.277\n",
            "     46    50     8.68e+06     1.81e+06     6.87e+06         20.7         27.5          536         53.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46     2     1.58e+05     9.36e+04     6.42e+04         4.67         6.26           63         4.36\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46   71.385    0.002     2.56e+05     7.69e+05     1.02e+06         5.93         9.62           98         9.43\n",
            "! Validation         46   71.385    0.002      1.3e+05     1.84e+05     3.13e+05         5.05         7.31         94.4         7.19\n",
            "Wall time: 71.38608492399999\n",
            "! Best model       46 313357.309\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10     8.48e+04     8.34e+04     1.31e+03          4.4         5.91         5.93        0.741\n",
            "     47    20     2.09e+05     6.05e+04     1.49e+05         3.71         5.03         78.9         7.89\n",
            "     47    30     5.66e+04     4.26e+04      1.4e+04         3.33         4.22         9.69         2.42\n",
            "     47    40     7.05e+04     6.33e+04     7.15e+03         3.98         5.15         38.1         1.73\n",
            "     47    50      1.7e+05     5.78e+04     1.12e+05         3.77         4.92          110         6.85\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47     2     1.51e+05     9.07e+04     6.04e+04         4.58         6.16         60.4         4.19\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47   72.709    0.002     2.99e+05     7.11e+05     1.01e+06         6.09         10.1         95.9         9.04\n",
            "! Validation         47   72.709    0.002     1.25e+05     1.83e+05     3.07e+05         4.96         7.17         93.3         7.11\n",
            "Wall time: 72.70919664899975\n",
            "! Best model       47 307234.727\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10     3.01e+05      9.2e+04     2.09e+05         4.86         6.21          140         9.36\n",
            "     48    20     8.35e+04     7.86e+04     4.96e+03          4.2         5.73         11.5         1.44\n",
            "     48    30     7.08e+04     3.51e+04     3.57e+04         2.89         3.83         54.1         3.87\n",
            "     48    40     1.33e+05     1.21e+05      1.2e+04         5.72         7.11         22.4         2.24\n",
            "     48    50     4.57e+04     3.41e+04     1.16e+04         3.05         3.78         8.83         2.21\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48     2     1.55e+05     8.66e+04     6.81e+04         4.49         6.02         65.7         4.54\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48   74.045    0.002     2.43e+05     7.81e+05     1.02e+06         5.67         9.42          104         9.84\n",
            "! Validation         48   74.045    0.002      1.2e+05     1.84e+05     3.04e+05         4.88         7.04         95.2         7.25\n",
            "Wall time: 74.04566547299964\n",
            "! Best model       48 304293.776\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10     2.14e+05     1.36e+05     7.72e+04         5.74         7.55         85.3         5.68\n",
            "     49    20     3.41e+05     1.19e+05     2.22e+05         5.08         7.05          135         9.63\n",
            "     49    30     1.18e+05     6.16e+04     5.61e+04         3.84         5.08         67.8         4.85\n",
            "     49    40     7.19e+04     7.01e+04     1.84e+03         4.26         5.41         19.3        0.877\n",
            "     49    50     7.77e+04     7.74e+04          291         4.05         5.69         4.19        0.349\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49     2     1.48e+05     8.55e+04     6.29e+04         4.43         5.98         62.1         4.31\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49   75.414    0.002     2.47e+05      7.5e+05     9.97e+05         5.68         9.31         93.7         9.05\n",
            "! Validation         49   75.414    0.002     1.14e+05     1.83e+05     2.98e+05          4.8         6.87         93.8         7.15\n",
            "Wall time: 75.41452955099976\n",
            "! Best model       49 297590.740\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10     1.42e+06     1.12e+06     3.03e+05         14.6         21.6          146         11.3\n",
            "     50    20     9.47e+04     7.04e+04     2.42e+04         3.84         5.43         15.9         3.18\n",
            "     50    30     5.81e+04     5.79e+04          190         3.41         4.92         3.39        0.282\n",
            "     50    40     5.63e+04     5.61e+04          191         3.79         4.85         6.22        0.283\n",
            "     50    50     4.91e+04     2.83e+04     2.08e+04         2.74         3.44         11.8         2.95\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50     2     1.44e+05     8.11e+04     6.26e+04          4.3         5.83         61.9         4.29\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50   76.773    0.002     2.76e+05     7.03e+05     9.79e+05         5.73         9.63         95.2         8.88\n",
            "! Validation         50   76.773    0.002      1.1e+05     1.83e+05     2.93e+05         4.69         6.74         93.6         7.14\n",
            "Wall time: 76.77358913199987\n",
            "! Best model       50 292733.692\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10     9.44e+04      6.7e+04     2.74e+04         3.92          5.3           71         3.38\n",
            "     51    20     7.64e+04     5.97e+04     1.67e+04         3.38            5         13.2         2.65\n",
            "     51    30     6.75e+04     6.75e+04          2.4          4.2         5.32        0.697       0.0317\n",
            "     51    40     1.57e+05     1.15e+05     4.19e+04          4.7         6.94         41.9         4.19\n",
            "     51    50     2.81e+05     1.24e+05     1.56e+05         5.02         7.21          121         8.09\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51     2     1.44e+05     7.84e+04     6.52e+04         4.22         5.73         63.6         4.41\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51   78.082    0.002     2.73e+05     7.56e+05     1.03e+06         5.74          9.8         98.6         9.45\n",
            "! Validation         51   78.082    0.002     1.06e+05     1.83e+05     2.89e+05         4.62         6.61         94.1         7.18\n",
            "Wall time: 78.08290405699972\n",
            "! Best model       51 289093.394\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10     4.58e+04     4.57e+04         3.18         3.03         4.38        0.292       0.0365\n",
            "     52    20     1.11e+05      7.1e+04     4.01e+04         3.58         5.45         20.5          4.1\n",
            "     52    30     3.12e+04     1.74e+04     1.37e+04         2.25          2.7         9.59          2.4\n",
            "     52    40     3.34e+05     2.36e+05     9.79e+04         8.21         9.94           96          6.4\n",
            "     52    50     6.14e+04     4.57e+04     1.57e+04         3.59         4.37         43.5         2.56\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52     2     1.37e+05     7.41e+04     6.27e+04         4.05         5.57         61.8         4.29\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52   79.397    0.002     2.21e+05     7.59e+05      9.8e+05          5.3         8.74           93         9.05\n",
            "! Validation         52   79.397    0.002      9.5e+04     1.83e+05     2.78e+05         4.41         6.27         93.6         7.13\n",
            "Wall time: 79.39773599399996\n",
            "! Best model       52 277969.800\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10      4.3e+04     3.68e+04      6.2e+03          3.1         3.92         27.4         1.61\n",
            "     53    20      9.2e+04     5.91e+04     3.29e+04         3.62         4.97         77.9         3.71\n",
            "     53    30     1.53e+05     6.54e+04     8.71e+04         3.76         5.23         60.4         6.04\n",
            "     53    40     4.65e+05     3.67e+05     9.76e+04         8.83         12.4         57.5         6.39\n",
            "     53    50     2.07e+05     6.85e+04     1.38e+05         4.27         5.35          137         7.61\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53     2      1.4e+05     7.25e+04     6.73e+04         4.04         5.51           65         4.49\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53   80.700    0.002     2.51e+05     7.11e+05     9.62e+05          5.5          9.3         98.5         9.26\n",
            "! Validation         53   80.700    0.002     9.53e+04     1.83e+05     2.78e+05         4.41         6.28         94.5          7.2\n",
            "Wall time: 80.70023681200018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10     5.51e+04      5.5e+04         4.66         3.53          4.8        0.839       0.0441\n",
            "     54    20     1.28e+05     1.21e+05     6.67e+03         5.25         7.11         26.7         1.67\n",
            "     54    30     4.08e+06      2.9e+06     1.18e+06         27.5         34.8           89         22.2\n",
            "     54    40     1.34e+05     7.73e+04     5.71e+04         4.05         5.69         48.9         4.89\n",
            "     54    50     6.21e+04      6.2e+04         12.5         3.93          5.1         1.59       0.0723\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54     2     1.34e+05     7.48e+04     5.89e+04          4.1         5.59         59.5         4.14\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54   82.006    0.002     2.65e+05      6.9e+05     9.55e+05         5.65         9.58         88.6         8.55\n",
            "! Validation         54   82.006    0.002     9.68e+04      1.8e+05     2.77e+05         4.46         6.33         92.2         7.03\n",
            "Wall time: 82.00628951199997\n",
            "! Best model       54 277202.537\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10     9.61e+05     3.71e+04     9.23e+05         2.61         3.94          157         19.7\n",
            "     55    20     2.39e+04      2.2e+04     1.93e+03         2.28         3.03         10.8          0.9\n",
            "     55    30     5.84e+04     3.63e+04      2.2e+04         2.83          3.9         15.2         3.04\n",
            "     55    40     4.12e+05     5.25e+04      3.6e+05         3.96         4.69          110         12.3\n",
            "     55    50     4.78e+04     4.69e+04          806         3.16         4.43         4.64        0.581\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55     2     1.37e+05     7.08e+04     6.58e+04         3.99         5.44           64         4.43\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55   83.317    0.002      2.4e+05     7.09e+05     9.49e+05         5.28         9.03          100         9.29\n",
            "! Validation         55   83.317    0.002     9.23e+04     1.82e+05     2.74e+05         4.35         6.18         93.9         7.15\n",
            "Wall time: 83.31750363299943\n",
            "! Best model       55 274163.204\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10     2.49e+06     9.47e+05     1.55e+06         13.4         19.9          382         25.4\n",
            "     56    20     1.55e+05     7.87e+04     7.63e+04         4.22         5.74         79.1         5.65\n",
            "     56    30     1.47e+05     7.15e+04     7.56e+04         3.97         5.47         56.3         5.63\n",
            "     56    40     8.85e+04      5.5e+04     3.35e+04         3.59          4.8         52.5         3.75\n",
            "     56    50     8.92e+06      1.9e+06     7.02e+06         19.4         28.2          542         54.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56     2     1.33e+05     7.16e+04     6.18e+04            4         5.48         61.2         4.25\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56   84.623    0.002     2.37e+05     7.29e+05     9.66e+05         5.34         9.07         98.5         9.31\n",
            "! Validation         56   84.623    0.002     9.24e+04      1.8e+05     2.73e+05         4.35         6.18         92.7         7.06\n",
            "Wall time: 84.62366838599974\n",
            "! Best model       56 272768.728\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10     4.07e+06     2.86e+06     1.21e+06         27.3         34.6           90         22.5\n",
            "     57    20     1.11e+06     1.02e+06     8.56e+04         14.6         20.7         59.8         5.98\n",
            "     57    30     1.91e+05     1.02e+05     8.91e+04          4.7         6.52         91.6         6.11\n",
            "     57    40     8.04e+04     6.75e+04     1.29e+04         3.88         5.31         48.8         2.32\n",
            "     57    50     1.07e+05     4.98e+04      5.7e+04         3.34         4.57          103         4.89\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57     2      1.3e+05     7.01e+04     5.95e+04         3.94         5.42         59.9         4.17\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57   85.934    0.002     2.46e+05     6.91e+05     9.37e+05         5.34         9.17         90.3         8.67\n",
            "! Validation         57   85.934    0.002     8.99e+04      1.8e+05      2.7e+05         4.29          6.1         92.1         7.02\n",
            "Wall time: 85.93508162699982\n",
            "! Best model       57 269712.905\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10     1.13e+05     4.72e+04     6.56e+04         3.32         4.44          110         5.24\n",
            "     58    20     4.48e+04     4.01e+04      4.7e+03         3.08         4.09         26.7          1.4\n",
            "     58    30     3.87e+04     2.53e+04     1.34e+04         2.51         3.25         40.3         2.37\n",
            "     58    40     3.63e+04     3.59e+04          308         3.11         3.88          6.1        0.359\n",
            "     58    50     4.75e+05     3.93e+05     8.14e+04         8.66         12.8         52.5         5.83\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58     2     1.27e+05     6.65e+04        6e+04          3.8         5.27         60.3          4.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58   87.259    0.002     2.03e+05     7.41e+05     9.43e+05         4.94         8.37         94.3         9.09\n",
            "! Validation         58   87.259    0.002     8.29e+04      1.8e+05     2.63e+05         4.13         5.86         92.2         7.03\n",
            "Wall time: 87.25990379099949\n",
            "! Best model       58 263072.443\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10     3.07e+05     1.28e+05     1.79e+05         4.92         7.31          173         8.65\n",
            "     59    20     8.56e+05     3.62e+05     4.94e+05          9.2         12.3          187         14.4\n",
            "     59    30     6.66e+04     6.44e+04     2.24e+03          3.8         5.19         7.74        0.968\n",
            "     59    40     1.52e+05     5.79e+04     9.43e+04         3.96         4.92          101         6.28\n",
            "     59    50     1.43e+05     9.96e+04     4.36e+04         4.56         6.46         76.9         4.27\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59     2     1.27e+05     6.83e+04     5.82e+04         3.87         5.35         59.1         4.13\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59   88.643    0.002     2.63e+05     6.72e+05     9.35e+05         5.47         9.46         92.7         8.72\n",
            "! Validation         59   88.643    0.002     8.59e+04     1.79e+05     2.65e+05         4.21         5.97         91.6         6.98\n",
            "Wall time: 88.64328794699941\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10     3.98e+04     2.42e+04     1.56e+04         2.39         3.18         35.7         2.55\n",
            "     60    20     2.89e+05     1.35e+05     1.54e+05         4.86         7.52          161         8.03\n",
            "     60    30     8.82e+04     4.59e+04     4.23e+04         2.87         4.38           21          4.2\n",
            "     60    40     4.38e+05     2.12e+04     4.17e+05         2.49         2.98          145         13.2\n",
            "     60    50     3.94e+04      2.9e+04     1.04e+04         2.71         3.48         35.5         2.09\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60     2     1.25e+05      6.7e+04     5.77e+04         3.83         5.29         58.9         4.12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60   89.947    0.002     2.23e+05     7.13e+05     9.36e+05         5.16         8.85         89.9         8.75\n",
            "! Validation         60   89.947    0.002     8.31e+04     1.79e+05     2.62e+05         4.15         5.87         91.5         6.97\n",
            "Wall time: 89.94720443999995\n",
            "! Best model       60 261835.228\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10     1.29e+05     4.09e+04     8.83e+04         3.11         4.14          128         6.08\n",
            "     61    20     1.32e+05     6.32e+04     6.85e+04         3.36         5.14         53.5         5.35\n",
            "     61    30     3.42e+05        3e+05     4.19e+04          9.3         11.2         62.8         4.19\n",
            "     61    40     3.28e+04     1.42e+04     1.86e+04         2.01         2.43         11.2         2.79\n",
            "     61    50     3.21e+04     3.09e+04     1.18e+03          2.7         3.59         5.62        0.702\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61     2     1.26e+05     6.49e+04      6.1e+04         3.76         5.21         60.7         4.23\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61   91.259    0.002     2.37e+05     6.84e+05     9.21e+05         5.17            9           96         8.99\n",
            "! Validation         61   91.259    0.002      8.2e+04     1.79e+05     2.61e+05          4.1         5.83           92         7.01\n",
            "Wall time: 91.2598046889998\n",
            "! Best model       61 260697.976\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10     1.46e+05      3.3e+04     1.13e+05         2.76         3.71         68.9         6.89\n",
            "     62    20     4.57e+04     4.39e+04     1.75e+03          3.1         4.29         6.84        0.855\n",
            "     62    30     1.29e+05     9.02e+04     3.87e+04         4.27         6.15         56.4         4.03\n",
            "     62    40     2.86e+04     1.18e+04     1.69e+04         1.87         2.22         10.6         2.66\n",
            "     62    50     2.79e+05     2.21e+05      5.8e+04         7.06         9.61         64.1         4.93\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62     2     1.24e+05     6.46e+04     5.99e+04         3.75          5.2         60.2          4.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62   92.575    0.002     2.23e+05     7.01e+05     9.24e+05         5.11         8.81           92         8.84\n",
            "! Validation         62   92.575    0.002     8.12e+04     1.78e+05     2.59e+05         4.08          5.8         91.6         6.98\n",
            "Wall time: 92.5756891029996\n",
            "! Best model       62 259418.797\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10     7.89e+04     3.24e+04     4.65e+04         2.98         3.68         52.9         4.41\n",
            "     63    20     1.35e+05     9.15e+04     4.33e+04          4.4         6.19         59.6         4.25\n",
            "     63    30     2.09e+05     6.45e+04     1.44e+05         3.92         5.19          140         7.77\n",
            "     63    40     7.28e+04     3.31e+04     3.97e+04         2.58         3.72         20.4         4.08\n",
            "     63    50     4.61e+05     4.39e+05     2.22e+04         10.4         13.6         39.6         3.05\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63     2     1.24e+05     6.44e+04     5.93e+04         3.74         5.19         59.8         4.18\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63   93.885    0.002     2.33e+05     6.89e+05     9.23e+05         5.13         8.95         92.1         8.81\n",
            "! Validation         63   93.885    0.002     8.08e+04     1.78e+05     2.59e+05         4.07         5.78         91.4         6.97\n",
            "Wall time: 93.88523336100025\n",
            "! Best model       63 258753.597\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10     1.36e+05     1.03e+05     3.25e+04         4.58         6.58         66.4         3.69\n",
            "     64    20     5.71e+05     2.42e+05      3.3e+05         7.31         10.1          188         11.7\n",
            "     64    30      7.5e+04     5.75e+04     1.75e+04         3.79          4.9         59.5         2.71\n",
            "     64    40     4.44e+04     4.27e+04     1.64e+03         3.01         4.23         9.94        0.829\n",
            "     64    50     4.42e+04     3.91e+04     5.02e+03         2.96         4.05         27.5         1.45\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64     2     1.19e+05     6.41e+04     5.53e+04         3.72         5.18         57.4         4.04\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              64   95.198    0.002     2.38e+05     6.69e+05     9.07e+05          5.2         9.01         88.1         8.41\n",
            "! Validation         64   95.198    0.002     7.96e+04     1.77e+05     2.57e+05         4.04         5.74         90.5         6.91\n",
            "Wall time: 95.19891515899963\n",
            "! Best model       64 256516.681\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65    10     6.82e+04     2.79e+04     4.03e+04         2.32         3.42         20.5         4.11\n",
            "     65    20     1.39e+05     1.38e+05          531         5.87          7.6         6.13        0.471\n",
            "     65    30     3.88e+04     3.83e+04          502         2.89            4          5.5        0.458\n",
            "     65    40      5.1e+04     5.01e+04          923         3.35         4.58         4.97        0.622\n",
            "     65    50     1.02e+06     5.24e+04     9.72e+05         3.36         4.68          161         20.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65     2     1.17e+05      6.2e+04     5.54e+04         3.65         5.09         57.4         4.04\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              65   96.526    0.002     2.12e+05     7.12e+05     9.24e+05         4.86         8.48           91          8.8\n",
            "! Validation         65   96.526    0.002     7.54e+04     1.77e+05     2.52e+05         3.95         5.59         90.5         6.91\n",
            "Wall time: 96.52630332099943\n",
            "! Best model       65 252125.622\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66    10     4.14e+04     1.95e+04     2.19e+04         2.19         2.86         42.4         3.03\n",
            "     66    20     3.44e+04     2.43e+04     1.01e+04         2.53         3.19         34.9         2.05\n",
            "     66    30     7.78e+04     3.46e+04     4.33e+04         2.81          3.8         89.4         4.26\n",
            "     66    40      2.1e+04     2.09e+04          122         2.38         2.96         2.71        0.226\n",
            "     66    50     1.99e+05     6.39e+04     1.35e+05          3.9         5.17          135         7.51\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66     2     1.18e+05     6.17e+04     5.66e+04         3.67         5.08         58.2         4.09\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              66   97.846    0.002     2.52e+05      6.7e+05     9.22e+05         5.16         9.22         92.3          8.7\n",
            "! Validation         66   97.846    0.002     7.67e+04     1.76e+05     2.53e+05         3.97         5.64         90.6         6.91\n",
            "Wall time: 97.84625555299954\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67    10     1.03e+05      3.7e+04     6.62e+04         2.93         3.93          110         5.26\n",
            "     67    20     1.54e+05        8e+04     7.43e+04         4.19         5.79          100         5.57\n",
            "     67    30     2.59e+05     7.66e+04     1.83e+05          3.9         5.66          131         8.74\n",
            "     67    40     2.67e+05     1.06e+05     1.61e+05         4.57         6.66          115          8.2\n",
            "     67    50     1.09e+05     9.02e+04     1.87e+04          3.9         6.14         33.6          2.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67     2     1.18e+05     6.02e+04     5.76e+04         3.61         5.02         58.8         4.12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              67   99.173    0.002     2.25e+05      6.8e+05     9.05e+05         5.01         8.76         94.2         8.84\n",
            "! Validation         67   99.173    0.002     7.53e+04     1.76e+05     2.51e+05         3.92         5.58         90.6         6.92\n",
            "Wall time: 99.17381473499972\n",
            "! Best model       67 251049.483\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68    10     5.92e+05     5.92e+05          367         11.5         15.7         3.92        0.392\n",
            "     68    20     1.93e+05     4.54e+04     1.48e+05          3.2         4.36         62.9         7.86\n",
            "     68    30     7.12e+05     1.81e+05     5.32e+05         6.47          8.7          194         14.9\n",
            "     68    40     1.99e+07     6.81e+04     1.98e+07         3.75         5.34          456         91.1\n",
            "     68    50     8.83e+06     2.26e+06     6.57e+06         21.7         30.8          524         52.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68     2     1.14e+05     5.89e+04     5.49e+04         3.55         4.97         57.2         4.03\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              68  100.544    0.002     1.99e+05     7.21e+05      9.2e+05         4.72         8.27         90.3         8.83\n",
            "! Validation         68  100.544    0.002     7.13e+04     1.76e+05     2.47e+05         3.83         5.44         90.2         6.88\n",
            "Wall time: 100.54499105499963\n",
            "! Best model       68 246864.942\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69    10     8.74e+06     2.31e+06     6.43e+06           22         31.1          519         51.9\n",
            "     69    20     5.64e+04     4.37e+04     1.28e+04         3.08         4.27         43.9         2.31\n",
            "     69    30     1.17e+06     1.02e+06     1.49e+05         12.6         20.7          103          7.9\n",
            "     69    40      7.4e+04        6e+04      1.4e+04         3.58         5.01         50.9         2.42\n",
            "     69    50     5.98e+05     2.08e+05     3.89e+05         6.82         9.33          204         12.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69     2     1.19e+05      5.9e+04     5.96e+04         3.59         4.97         59.9         4.19\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              69  101.864    0.002     2.48e+05      6.5e+05     8.98e+05         5.06         9.07           91         8.52\n",
            "! Validation         69  101.864    0.002     7.37e+04     1.75e+05     2.49e+05         3.88         5.52         90.9         6.93\n",
            "Wall time: 101.86449522000021\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70    10     1.59e+05     2.64e+04     1.33e+05         2.39         3.32         74.5         7.45\n",
            "     70    20     3.76e+04     3.53e+04     2.23e+03         2.84         3.85         7.73        0.967\n",
            "     70    30     4.06e+05     3.05e+05     1.01e+05         8.36         11.3         84.5          6.5\n",
            "     70    40     1.06e+05     7.67e+04     2.89e+04         3.57         5.66         41.7         3.48\n",
            "     70    50     2.14e+06     9.13e+05     1.23e+06         12.5         19.5          340         22.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70     2     1.18e+05     5.83e+04     5.92e+04         3.56         4.94         59.5         4.17\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              70  103.188    0.002     2.08e+05     6.97e+05     9.05e+05         4.93         8.55         91.4         8.83\n",
            "! Validation         70  103.188    0.002     7.27e+04     1.75e+05     2.48e+05         3.86         5.49         90.8         6.92\n",
            "Wall time: 103.18871481899987\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71    10     2.32e+05     7.52e+04     1.57e+05         3.92         5.61          122         8.11\n",
            "     71    20     3.13e+04     2.93e+04     2.06e+03         2.62          3.5         13.9        0.927\n",
            "     71    30     1.35e+05     1.14e+05     2.16e+04         4.71         6.89         42.1         3.01\n",
            "     71    40     5.94e+04     5.53e+04     4.14e+03          3.7         4.81           29         1.32\n",
            "     71    50     3.22e+04     2.38e+04     8.38e+03         2.52         3.16         31.8         1.87\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71     2     1.16e+05     5.78e+04     5.84e+04         3.54         4.92           59         4.14\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              71  104.498    0.002     2.16e+05     6.76e+05     8.92e+05         4.89         8.57         89.7         8.62\n",
            "! Validation         71  104.498    0.002      7.2e+04     1.75e+05     2.47e+05         3.84         5.46         90.5         6.91\n",
            "Wall time: 104.49838658799945\n",
            "! Best model       71 246588.284\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72    10     2.98e+06     2.49e+06     4.84e+05         25.2         32.3         56.9         14.2\n",
            "     72    20     4.91e+05     4.09e+05     8.18e+04         9.91         13.1           76         5.85\n",
            "     72    30      3.6e+04     2.94e+04     6.52e+03         2.63         3.51         24.8         1.65\n",
            "     72    40     1.11e+06     9.91e+05     1.16e+05         14.4         20.4         69.8         6.98\n",
            "     72    50     2.65e+05     9.51e+04      1.7e+05         4.39         6.31          118         8.43\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72     2     1.15e+05     5.84e+04     5.71e+04         3.57         4.94         58.1         4.09\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              72  105.819    0.002     2.34e+05     6.51e+05     8.85e+05         5.03         8.95         88.9         8.42\n",
            "! Validation         72  105.819    0.002     7.39e+04     1.74e+05     2.48e+05         3.87         5.53           90         6.87\n",
            "Wall time: 105.81967936699948\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73    10     1.33e+05     9.98e+04     3.37e+04         4.29         6.46         37.5         3.75\n",
            "     73    20     4.39e+05     1.46e+04     4.24e+05         1.96         2.47          147         13.3\n",
            "     73    30     3.26e+05     3.73e+04     2.88e+05         3.19         3.95         98.9           11\n",
            "     73    40     6.46e+05     4.66e+05      1.8e+05         10.6           14          113         8.68\n",
            "     73    50     5.08e+05     2.55e+05     2.53e+05         7.38         10.3          165         10.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73     2     1.14e+05      5.9e+04     5.48e+04         3.59         4.97         56.7         4.01\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              73  107.120    0.002     2.17e+05     6.93e+05     9.09e+05         4.98         8.67         89.6         8.68\n",
            "! Validation         73  107.120    0.002     7.42e+04     1.73e+05     2.47e+05         3.88         5.54         89.3         6.82\n",
            "Wall time: 107.12079221899967\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74    10     8.61e+06      2.1e+06      6.5e+06         20.3         29.7          522         52.2\n",
            "     74    20     1.91e+05     1.65e+05     2.65e+04         6.66          8.3         43.3         3.33\n",
            "     74    30     9.63e+05     7.49e+05     2.14e+05         10.8         17.7          123         9.47\n",
            "     74    40     2.13e+06     7.55e+05     1.37e+06         11.4         17.8          359         23.9\n",
            "     74    50     5.82e+05     1.64e+05     4.17e+05         6.32         8.29          211         13.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74     2     1.13e+05     5.73e+04     5.61e+04         3.52          4.9         57.4         4.05\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              74  108.426    0.002     2.14e+05     6.63e+05     8.77e+05          4.8         8.44           91         8.63\n",
            "! Validation         74  108.426    0.002     7.09e+04     1.73e+05     2.44e+05          3.8         5.42         89.6         6.84\n",
            "Wall time: 108.42649177299973\n",
            "! Best model       74 243984.507\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75    10     1.31e+05     8.61e+04     4.47e+04         4.01            6         43.3         4.33\n",
            "     75    20     1.32e+05     1.03e+05     2.96e+04         4.44         6.55         49.3         3.52\n",
            "     75    30     1.96e+05     4.98e+04     1.46e+05          3.5         4.57         62.5         7.81\n",
            "     75    40     3.45e+05     3.45e+05          210         8.99           12         3.85        0.296\n",
            "     75    50      2.2e+04      2.2e+04         71.2         2.34         3.03         2.93        0.173\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75     2     1.12e+05     5.55e+04     5.67e+04         3.45         4.82         57.7         4.07\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              75  109.724    0.002     1.92e+05     6.83e+05     8.75e+05         4.65         8.13         88.8         8.57\n",
            "! Validation         75  109.724    0.002     6.69e+04     1.73e+05      2.4e+05          3.7         5.27         89.7         6.85\n",
            "Wall time: 109.72453721700003\n",
            "! Best model       75 240059.961\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76    10      2.4e+05     5.71e+04     1.83e+05         3.72         4.89          157         8.75\n",
            "     76    20     4.23e+04     1.39e+04     2.83e+04         1.95         2.41         48.2         3.44\n",
            "     76    30     1.72e+05     2.65e+04     1.45e+05         2.63         3.33           78          7.8\n",
            "     76    40     1.37e+05      9.5e+04     4.23e+04         4.18          6.3         63.1         4.21\n",
            "     76    50     1.28e+05      9.8e+04     2.98e+04         4.24         6.41         35.3         3.53\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76     2     1.16e+05     5.59e+04     5.97e+04         3.49         4.83         59.4         4.18\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              76  111.048    0.002     2.25e+05     6.57e+05     8.82e+05         4.93         8.77         92.5         8.74\n",
            "! Validation         76  111.048    0.002     6.84e+04     1.73e+05     2.41e+05         3.74         5.33         90.2         6.88\n",
            "Wall time: 111.04831187499985\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77    10      3.9e+04     3.81e+04          901         3.06         3.99         7.37        0.614\n",
            "     77    20     2.56e+05     1.97e+05     5.87e+04         7.29         9.08         64.4         4.96\n",
            "     77    30     4.77e+05     3.93e+05     8.38e+04         8.59         12.8         53.3         5.92\n",
            "     77    40     6.45e+04     2.23e+04     4.22e+04         2.07         3.05           21          4.2\n",
            "     77    50      6.3e+04     5.52e+04     7.83e+03         3.62         4.81         39.8         1.81\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77     2     1.13e+05     5.69e+04     5.57e+04         3.52         4.88         56.9         4.03\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              77  112.388    0.002     2.23e+05     6.48e+05     8.71e+05         4.98         8.77         84.5         8.14\n",
            "! Validation         77  112.388    0.002     6.94e+04     1.72e+05     2.42e+05         3.76         5.37         89.2         6.81\n",
            "Wall time: 112.3888156969997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78    10     3.88e+05     1.82e+04     3.69e+05         2.11         2.76          137         12.4\n",
            "     78    20     7.38e+04        6e+04     1.38e+04         3.91         5.01         52.8          2.4\n",
            "     78    30     3.19e+05     7.79e+04     2.41e+05         4.29         5.71          141           10\n",
            "     78    40     3.01e+04     2.53e+04     4.87e+03         2.39         3.25         21.4         1.43\n",
            "     78    50     9.93e+05     7.99e+05     1.94e+05         10.8         18.3          117            9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78     2     1.13e+05     5.65e+04     5.61e+04         3.52         4.86         57.1         4.04\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              78  113.724    0.002     2.27e+05     6.82e+05     9.09e+05         4.95          8.8         90.7         8.77\n",
            "! Validation         78  113.724    0.002     6.93e+04     1.72e+05     2.41e+05         3.76         5.36         89.2         6.81\n",
            "Wall time: 113.72475822399974\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79    10     2.79e+05     2.17e+05     6.17e+04            6         9.53           66         5.08\n",
            "     79    20        1e+06     9.06e+04     9.12e+05          4.2         6.16          156         19.5\n",
            "     79    30     7.07e+05     2.88e+05     4.19e+05         8.16           11          172         13.2\n",
            "     79    40     2.23e+05     4.34e+04     1.79e+05         3.28         4.26         69.3         8.66\n",
            "     79    50     1.42e+05     2.13e+04     1.21e+05         2.22         2.98         71.2         7.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79     2     1.14e+05     5.48e+04     5.96e+04         3.47         4.79           59         4.16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              79  115.051    0.002     2.12e+05     6.71e+05     8.83e+05         4.82         8.54         93.6         8.95\n",
            "! Validation         79  115.051    0.002     6.76e+04     1.72e+05      2.4e+05          3.7         5.29         89.8         6.86\n",
            "Wall time: 115.0515428990002\n",
            "! Best model       79 239522.312\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80    10     3.26e+04     3.06e+04     1.98e+03         2.58         3.58         7.28        0.909\n",
            "     80    20        4e+04     1.64e+04     2.36e+04         2.06         2.62         53.4         3.14\n",
            "     80    30     2.22e+05     5.27e+04      1.7e+05         3.52         4.69          152         8.42\n",
            "     80    40     3.16e+05     3.12e+05     4.29e+03         8.64         11.4         17.4         1.34\n",
            "     80    50     2.29e+05     6.36e+04     1.65e+05         3.66         5.16          125         8.31\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80     2     1.12e+05     5.33e+04     5.92e+04         3.37         4.72         58.6         4.14\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              80  116.376    0.002     1.79e+05     7.01e+05      8.8e+05         4.55         7.88         90.5         8.76\n",
            "! Validation         80  116.376    0.002     6.32e+04     1.73e+05     2.36e+05         3.59         5.12         89.8         6.86\n",
            "Wall time: 116.37632998699974\n",
            "! Best model       80 235833.351\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81    10     1.72e+05     8.68e+04     8.52e+04         4.17         6.03         95.5         5.97\n",
            "     81    20     2.24e+05     1.24e+05        1e+05         4.84         7.19          130         6.48\n",
            "     81    30      1.1e+06     9.84e+05     1.12e+05         12.1         20.3         88.8         6.83\n",
            "     81    40     4.49e+05     3.81e+05     6.83e+04         8.39         12.6         48.1         5.35\n",
            "     81    50     1.02e+05     6.77e+04     3.47e+04         3.45         5.32         45.7         3.81\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81     2     1.13e+05     5.48e+04     5.78e+04         3.46         4.79         57.7         4.09\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              81  117.693    0.002     2.24e+05     6.47e+05     8.71e+05         4.99         8.83         86.6         8.38\n",
            "! Validation         81  117.693    0.002     6.58e+04     1.71e+05     2.37e+05         3.66         5.23         89.2         6.81\n",
            "Wall time: 117.69399325699942\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82    10      2.3e+04     1.38e+04     9.26e+03         1.93          2.4         27.6         1.97\n",
            "     82    20     5.09e+05     4.38e+05     7.11e+04         9.02         13.5         49.1         5.45\n",
            "     82    30        1e+06     8.69e+05     1.32e+05         11.8         19.1         96.7         7.44\n",
            "     82    40        2e+04        2e+04         60.6         2.25         2.89         2.71        0.159\n",
            "     82    50      2.5e+05     9.93e+04     1.51e+05         4.34         6.45          159         7.94\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82     2     1.12e+05      5.5e+04     5.65e+04         3.48          4.8         56.9         4.04\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              82  118.982    0.002     2.21e+05      6.4e+05      8.6e+05         4.89         8.69         87.4         8.32\n",
            "! Validation         82  118.982    0.002     6.72e+04      1.7e+05     2.38e+05         3.69         5.28         88.7         6.78\n",
            "Wall time: 118.98255509199953\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83    10     6.14e+04     5.91e+04     2.33e+03         3.65         4.97         13.8        0.987\n",
            "     83    20     3.13e+05     7.34e+04      2.4e+05         4.18         5.54          140           10\n",
            "     83    30     3.06e+04      1.3e+04     1.76e+04          1.9         2.33           38         2.71\n",
            "     83    40     1.57e+05     8.34e+04      7.4e+04         4.14         5.91           89         5.56\n",
            "     83    50      2.4e+05     1.19e+05     1.22e+05          4.6         7.05          143         7.14\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83     2     1.13e+05     5.48e+04     5.83e+04         3.49         4.79         57.8          4.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              83  120.281    0.002     2.23e+05     6.64e+05     8.87e+05         4.92          8.8         90.1          8.7\n",
            "! Validation         83  120.281    0.002     6.69e+04      1.7e+05     2.37e+05         3.69         5.27           89          6.8\n",
            "Wall time: 120.28124747899983\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84    10     3.22e+04     1.62e+04      1.6e+04         2.11         2.61         43.9         2.58\n",
            "     84    20     5.09e+04     2.93e+04     2.16e+04          2.8          3.5         36.1         3.01\n",
            "     84    30     2.15e+05     1.78e+05     3.75e+04         6.93         8.63         39.6         3.96\n",
            "     84    40     1.15e+05     6.15e+04     5.34e+04         3.87         5.07          104         4.73\n",
            "     84    50     2.93e+04     2.92e+04         61.7         2.76          3.5         1.93        0.161\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84     2     1.09e+05     5.54e+04     5.38e+04         3.49         4.82           55         3.93\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              84  121.592    0.002      2.1e+05     6.72e+05     8.82e+05          4.8          8.6         86.2         8.44\n",
            "! Validation         84  121.592    0.002     6.61e+04     1.69e+05     2.35e+05         3.67         5.24         87.9         6.73\n",
            "Wall time: 121.59285072900002\n",
            "! Best model       84 235360.861\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85    10     2.57e+05     2.05e+05     5.21e+04          7.5         9.25           70         4.67\n",
            "     85    20     3.45e+04     2.87e+04     5.82e+03         2.64         3.46         12.5         1.56\n",
            "     85    30     8.42e+05     6.32e+05      2.1e+05         9.51         16.3          122         9.37\n",
            "     85    40     6.39e+05     1.91e+05     4.48e+05         6.52         8.94          178         13.7\n",
            "     85    50     2.08e+05     1.68e+05        4e+04         5.34         8.39         53.2         4.09\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85     2     1.11e+05     5.21e+04     5.85e+04         3.36         4.67         57.9         4.11\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              85  122.908    0.002     1.79e+05     6.85e+05     8.64e+05         4.51         7.85           91         8.76\n",
            "! Validation         85  122.908    0.002     6.23e+04      1.7e+05     2.32e+05         3.55         5.08         88.9          6.8\n",
            "Wall time: 122.90919297300024\n",
            "! Best model       85 232342.946\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86    10     2.63e+05     1.14e+04     2.52e+05         1.47         2.19          113         10.3\n",
            "     86    20     1.11e+05     8.88e+04      2.2e+04         4.08          6.1         42.5         3.04\n",
            "     86    30     3.33e+04     1.22e+04     2.11e+04         1.91         2.26         11.9         2.97\n",
            "     86    40     7.65e+05     3.13e+05     4.51e+05         8.42         11.4          179         13.7\n",
            "     86    50     4.56e+04     4.44e+04     1.15e+03         2.96         4.31         13.2        0.692\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86     2     1.11e+05     5.48e+04      5.6e+04         3.49         4.79         56.2         4.01\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              86  124.321    0.002     2.36e+05      6.4e+05     8.76e+05         5.07         9.05         86.3         8.33\n",
            "! Validation         86  124.321    0.002     6.75e+04     1.68e+05     2.36e+05         3.69         5.29         87.9         6.73\n",
            "Wall time: 124.32164563100014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87    10     5.35e+04     2.25e+04      3.1e+04         2.42         3.07         75.6          3.6\n",
            "     87    20     1.05e+05     5.61e+04     4.93e+04         3.18         4.85         45.4         4.54\n",
            "     87    30     3.41e+04     2.13e+04     1.27e+04         2.23         2.99         34.6         2.31\n",
            "     87    40     1.64e+05     1.44e+05     2.02e+04         6.07         7.76           29          2.9\n",
            "     87    50     9.14e+04     7.41e+04     1.74e+04         3.51         5.57         32.4          2.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87     2      1.1e+05     5.31e+04     5.72e+04         3.39         4.71         56.7         4.04\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              87  125.683    0.002     1.86e+05     6.72e+05     8.58e+05         4.55         8.02           88         8.54\n",
            "! Validation         87  125.683    0.002     6.33e+04     1.69e+05     2.32e+05         3.58         5.13         88.3         6.76\n",
            "Wall time: 125.68359628399958\n",
            "! Best model       87 232232.540\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88    10     1.66e+04     1.31e+04     3.49e+03         1.93         2.34         14.5         1.21\n",
            "     88    20     7.32e+04     2.63e+04      4.7e+04         2.54         3.31         62.1         4.43\n",
            "     88    30     8.75e+06     2.23e+06     6.51e+06           21         30.6          522         52.2\n",
            "     88    40     4.27e+05     2.59e+04     4.01e+05         2.34         3.29          142         12.9\n",
            "     88    50     1.94e+07      1.1e+05     1.93e+07         5.46         6.79          450         89.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88     2      1.1e+05     5.42e+04     5.56e+04         3.47         4.76         55.7         3.99\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              88  127.011    0.002     2.16e+05     6.29e+05     8.46e+05         4.82         8.58         85.9         8.14\n",
            "! Validation         88  127.011    0.002     6.47e+04     1.68e+05     2.33e+05         3.63         5.18         87.8         6.72\n",
            "Wall time: 127.01219640899944\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89    10     6.47e+05     2.18e+05      4.3e+05         6.91         9.54          174         13.4\n",
            "     89    20     3.29e+05     2.93e+04     2.99e+05          2.9          3.5          101         11.2\n",
            "     89    30     1.72e+05     5.16e+04     1.21e+05         3.68         4.64         56.9         7.11\n",
            "     89    40     2.88e+04     7.14e+03     2.16e+04         1.49         1.73           12         3.01\n",
            "     89    50     1.07e+05     6.01e+04     4.66e+04         3.49         5.02         44.1         4.41\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89     2     1.09e+05     5.25e+04     5.67e+04         3.38         4.68         56.3         4.02\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              89  128.323    0.002     1.89e+05     6.86e+05     8.75e+05         4.62         8.08         91.7         8.77\n",
            "! Validation         89  128.323    0.002     6.19e+04     1.69e+05     2.31e+05         3.54         5.07         88.1         6.74\n",
            "Wall time: 128.32337332599946\n",
            "! Best model       89 230718.999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90    10     2.45e+06     2.36e+06     8.51e+04         25.6         31.4         23.9         5.97\n",
            "     90    20     1.09e+05     9.12e+04     1.76e+04         4.22         6.18         48.9         2.71\n",
            "     90    30     8.33e+04     8.33e+04         47.3         4.09          5.9         2.95        0.141\n",
            "     90    40     2.03e+04     1.63e+04     3.92e+03         2.02         2.61         21.8         1.28\n",
            "     90    50     4.72e+04      3.8e+04     9.24e+03         2.66         3.99         37.4         1.97\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90     2     1.09e+05     5.24e+04     5.65e+04         3.39         4.68         56.1         4.01\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              90  129.629    0.002     2.02e+05      6.4e+05     8.42e+05         4.66         8.23         87.8         8.24\n",
            "! Validation         90  129.629    0.002      6.2e+04     1.68e+05      2.3e+05         3.55         5.07         87.9         6.73\n",
            "Wall time: 129.62990259599974\n",
            "! Best model       90 230345.386\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91    10     8.23e+06     1.79e+06     6.44e+06         18.3         27.4          519         51.9\n",
            "     91    20     1.13e+05     7.61e+04     3.69e+04          3.9         5.64         70.7         3.93\n",
            "     91    30     5.01e+05     4.88e+05     1.31e+04         8.88         14.3         21.1         2.34\n",
            "     91    40     1.07e+06     1.41e+05     9.24e+05         5.32         7.69          157         19.7\n",
            "     91    50     9.46e+04      6.4e+04     3.06e+04         3.52         5.17         35.8         3.58\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91     2     1.13e+05     5.21e+04     6.08e+04         3.41         4.67         58.5         4.16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              91  130.947    0.002     1.86e+05     6.73e+05     8.59e+05         4.63          8.1         91.3         8.83\n",
            "! Validation         91  130.947    0.002     6.27e+04     1.68e+05     2.31e+05         3.57          5.1         88.7         6.78\n",
            "Wall time: 130.94729806299983\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92    10      1.1e+05     1.07e+05      2.6e+03         4.35         6.71         18.8         1.04\n",
            "     92    20     1.96e+05     7.85e+04     1.17e+05            4         5.73          112            7\n",
            "     92    30     3.05e+06     1.75e+06      1.3e+06         22.1         27.1         93.3         23.3\n",
            "     92    40     6.84e+05     2.67e+05     4.17e+05         7.99         10.6          172         13.2\n",
            "     92    50     2.88e+05     1.77e+05     1.11e+05         5.59         8.61         88.5         6.81\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92     2     1.11e+05     5.24e+04     5.87e+04         3.43         4.68         57.2         4.08\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              92  132.250    0.002     1.91e+05     6.52e+05     8.43e+05         4.63         8.16         84.8         8.17\n",
            "! Validation         92  132.250    0.002     6.22e+04     1.68e+05      2.3e+05         3.57         5.08         88.2         6.75\n",
            "Wall time: 132.2509775489998\n",
            "! Best model       92 229933.424\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93    10     1.02e+06      1.4e+05     8.83e+05         5.02         7.64          154         19.2\n",
            "     93    20     2.91e+05     1.28e+04     2.78e+05          1.7         2.32          119         10.8\n",
            "     93    30     2.94e+06      1.8e+06     1.14e+06         23.1         27.4         87.2         21.8\n",
            "     93    40     9.41e+04     5.46e+04     3.95e+04         3.61         4.78         89.5         4.07\n",
            "     93    50      4.8e+04     4.43e+04     3.76e+03         2.92          4.3         23.8         1.25\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93     2     1.12e+05     5.21e+04     5.98e+04          3.4         4.67         57.7         4.12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              93  133.563    0.002     1.97e+05     6.49e+05     8.45e+05         4.71         8.29         90.2         8.62\n",
            "! Validation         93  133.563    0.002     6.21e+04     1.68e+05      2.3e+05         3.56         5.08         88.3         6.75\n",
            "Wall time: 133.56370976299968\n",
            "! Best model       93 229706.700\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94    10     1.12e+05     1.01e+05     1.09e+04         4.22         6.49           32         2.13\n",
            "     94    20     1.61e+04     1.32e+04     2.91e+03         1.88         2.35         15.4          1.1\n",
            "     94    30     4.95e+04        2e+04     2.96e+04         2.39         2.89         59.8         3.52\n",
            "     94    40     1.58e+05     1.43e+05     1.51e+04         6.08         7.74         25.1         2.51\n",
            "     94    50     1.05e+06     1.21e+05     9.28e+05         4.64         7.12          158         19.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94     2     1.09e+05     5.33e+04     5.54e+04         3.44         4.72         54.9         3.95\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              94  134.873    0.002      2.1e+05     6.24e+05     8.34e+05         4.81         8.51           83         7.94\n",
            "! Validation         94  134.873    0.002      6.3e+04     1.67e+05      2.3e+05         3.59         5.11         87.2         6.68\n",
            "Wall time: 134.87381650599946\n",
            "! Best model       94 229700.460\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95    10     6.55e+05     1.91e+05     4.64e+05         6.62         8.94          181         13.9\n",
            "     95    20      7.8e+04      7.6e+04     1.92e+03         3.62         5.64         14.3        0.897\n",
            "     95    30     1.54e+06     1.01e+06     5.25e+05         15.2         20.6          148         14.8\n",
            "     95    40      1.8e+04     9.79e+03     8.16e+03         1.67         2.02         22.2         1.85\n",
            "     95    50     5.67e+04     1.88e+04     3.79e+04         2.18          2.8         19.9         3.98\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95     2     1.13e+05     5.23e+04     6.06e+04         3.43         4.68         58.1         4.15\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              95  136.211    0.002     2.09e+05     6.83e+05     8.92e+05         4.81         8.61         94.4         9.04\n",
            "! Validation         95  136.211    0.002     6.22e+04     1.67e+05     2.29e+05         3.58         5.08         88.3         6.76\n",
            "Wall time: 136.2112362859998\n",
            "! Best model       95 229138.483\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96    10     8.27e+06     1.72e+06     6.55e+06         17.8         26.8          524         52.4\n",
            "     96    20     7.87e+04     6.62e+04     1.25e+04         3.79         5.26         48.1         2.29\n",
            "     96    30     3.32e+05     2.34e+04     3.09e+05         2.48         3.13          102         11.4\n",
            "     96    40     9.96e+04     4.69e+04     5.26e+04         2.94         4.43         46.9         4.69\n",
            "     96    50     2.65e+05     1.76e+05      8.9e+04         5.47         8.58         79.3          6.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96     2     1.13e+05     5.01e+04     6.26e+04         3.34         4.58         59.4         4.23\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              96  137.568    0.002     1.61e+05     6.87e+05     8.48e+05         4.26         7.48         88.9         8.65\n",
            "! Validation         96  137.568    0.002     5.77e+04     1.68e+05     2.26e+05         3.46          4.9         89.1         6.81\n",
            "Wall time: 137.56884919999993\n",
            "! Best model       96 226036.893\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97    10      1.4e+04     1.24e+04      1.6e+03         1.87         2.27         9.82        0.819\n",
            "     97    20     1.82e+05     7.75e+04     1.04e+05          4.1         5.69          106          6.6\n",
            "     97    30     3.24e+04     3.05e+04     1.89e+03         2.69         3.57         10.7         0.89\n",
            "     97    40     2.54e+06     1.92e+06     6.27e+05         23.2         28.3         64.8         16.2\n",
            "     97    50     8.96e+05     8.23e+05     7.35e+04         10.9         18.6         72.1         5.55\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97     2      1.1e+05     5.21e+04     5.75e+04         3.41         4.67         55.8         4.01\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              97  138.879    0.002     2.04e+05     6.32e+05     8.36e+05         4.78         8.44         84.3          8.1\n",
            "! Validation         97  138.879    0.002     6.02e+04     1.66e+05     2.27e+05         3.53            5         87.5          6.7\n",
            "Wall time: 138.87916762799978\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98    10     1.79e+04     1.79e+04         14.6         2.14         2.74         1.33       0.0783\n",
            "     98    20     8.07e+04     5.96e+04     2.11e+04          3.2         4.99         35.6         2.97\n",
            "     98    30     9.54e+05      7.1e+05     2.44e+05         12.7         17.2          101         10.1\n",
            "     98    40     2.35e+05     5.32e+04     1.82e+05         3.56         4.72          157         8.72\n",
            "     98    50     4.62e+04     4.16e+04     4.66e+03         2.83         4.17         26.5          1.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98     2     1.14e+05     5.07e+04     6.36e+04         3.39         4.61         60.1         4.28\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              98  140.181    0.002     1.96e+05     6.38e+05     8.34e+05         4.63         8.25         91.1         8.58\n",
            "! Validation         98  140.181    0.002     6.03e+04     1.67e+05     2.27e+05         3.53            5           89          6.8\n",
            "Wall time: 140.18200583399994\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99    10     5.27e+05     3.85e+05     1.43e+05         8.32         12.7         69.5         7.73\n",
            "     99    20     2.32e+05     2.25e+05      6.7e+03         7.53          9.7         25.1         1.67\n",
            "     99    30     2.13e+05     5.17e+04     1.62e+05         3.65         4.65         65.8         8.22\n",
            "     99    40     8.34e+05      7.5e+05     8.41e+04         10.6         17.7         77.1         5.93\n",
            "     99    50     1.14e+05     7.72e+04      3.7e+04          3.8         5.68           59         3.94\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99     2     1.12e+05     5.08e+04     6.15e+04         3.38         4.61         58.4         4.17\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              99  141.492    0.002     1.87e+05     6.56e+05     8.42e+05         4.63         8.13         86.3         8.41\n",
            "! Validation         99  141.492    0.002     5.98e+04     1.67e+05     2.26e+05         3.52         4.98         88.3         6.76\n",
            "Wall time: 141.4931694830002\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100    10     2.89e+04     2.23e+04     6.64e+03         2.31         3.05           35         1.67\n",
            "    100    20     7.55e+04     6.23e+04     1.32e+04         3.36          5.1         23.5         2.35\n",
            "    100    30     1.73e+04     1.45e+04     2.84e+03         1.88         2.46         8.73         1.09\n",
            "    100    40      1.5e+05     6.69e+04     8.34e+04         3.88         5.29         94.5         5.91\n",
            "    100    50     1.42e+05     1.33e+05     9.84e+03         5.89         7.45         20.3         2.03\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100     2     1.08e+05     5.23e+04     5.58e+04         3.41         4.68         54.4         3.94\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train             100  142.789    0.002     1.95e+05     6.39e+05     8.34e+05         4.72         8.25         78.6         7.78\n",
            "! Validation        100  142.789    0.002     6.07e+04     1.66e+05     2.26e+05         3.55         5.02         86.7         6.65\n",
            "Wall time: 142.78926284099998\n",
            "! Stop training: max epochs\n",
            "Wall time: 142.79928590599957\n",
            "Cumulative wall time: 142.79928590599957\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mDES\u001b[0m at: \u001b[34mhttps://wandb.ai/anony-mouse-355011484483095595/allegro-tutorial/runs/vTyIxtvpXvbClkLtjOc6xYVOFu-gPYgMhomtLLVWyvc\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241209_230937-vTyIxtvpXvbClkLtjOc6xYVOFu-gPYgMhomtLLVWyvc/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!nequip-deploy build --train-dir results/silicon-tutorial/si si-deployed.pth\n",
        "!nequip-deploy build --train-dir results/DES-tutorial/DES as-deployed.pth\n",
        "!ls *pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnHEECs1CY4k",
        "outputId": "a22978c9-a72b-42cf-8f29-3b1e66f36770"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "INFO:root:Loading best_model.pth from training session...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/deploy.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\n",
            "INFO:root:Compiled & optimized model.\n",
            "as-deployed.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!nequip-evaluate --train-dir results/silicon-tutorial/si --batch-size 5\n",
        "!nequip-evaluate --train-dir results/DES-tutorial/DES --batch-size 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nElT2PbgCgF3",
        "outputId": "81f4ac30-0629-4ebf-aaa3-dac0e7192be3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/evaluate.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  trainer = torch.load(\n",
            "Using device: cuda\n",
            "Please note that _all_ machine learning models running on CUDA hardware are generally somewhat nondeterministic and that this can manifest in small, generally unimportant variation in the final test errors.\n",
            "Loading model... \n",
            "/usr/local/lib/python3.10/dist-packages/nequip/train/trainer.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_state_dict = torch.load(traindir + \"/\" + model_name, map_location=device)\n",
            "    loaded model\n",
            "Loading original dataset...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (18700 frames) minus training (50 frames) and validation frames (10 frames), yielding a test set size of 18640 frames.\n",
            "Starting...\n",
            "  0% 0/18640 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  0% 5/18640 [00:01<1:12:39,  4.27it/s]\n",
            "  0% 10/18640 [00:01<46:45,  6.64it/s] \n",
            "  0% 15/18640 [00:03<1:25:51,  3.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  0% 50/18640 [00:03<16:08, 19.20it/s]  \n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  0% 85/18640 [00:03<07:53, 39.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 120/18640 [00:04<04:51, 63.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 160/18640 [00:04<03:11, 96.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 200/18640 [00:04<02:19, 132.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 235/18640 [00:04<01:51, 164.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 275/18640 [00:04<01:31, 201.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 315/18640 [00:04<01:18, 234.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 355/18640 [00:04<01:09, 262.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 390/18640 [00:04<01:04, 282.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 425/18640 [00:04<01:00, 298.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 465/18640 [00:05<00:57, 314.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 505/18640 [00:05<00:55, 325.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 545/18640 [00:05<00:53, 335.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 585/18640 [00:05<00:52, 341.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 625/18640 [00:05<00:52, 343.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 665/18640 [00:05<00:51, 346.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 705/18640 [00:05<00:51, 346.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 745/18640 [00:05<00:51, 345.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 785/18640 [00:05<00:51, 347.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 820/18640 [00:06<00:51, 345.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 855/18640 [00:06<00:51, 343.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 890/18640 [00:06<00:51, 343.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 925/18640 [00:06<00:51, 344.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 960/18640 [00:06<00:51, 341.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 1000/18640 [00:06<00:51, 344.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1035/18640 [00:06<00:50, 345.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1070/18640 [00:06<00:51, 343.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1105/18640 [00:06<00:51, 343.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1140/18640 [00:06<00:50, 345.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1175/18640 [00:07<00:50, 344.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1210/18640 [00:07<00:50, 343.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1245/18640 [00:07<00:50, 345.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1285/18640 [00:07<00:50, 346.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1320/18640 [00:07<00:50, 344.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1355/18640 [00:07<00:49, 346.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1395/18640 [00:07<00:49, 349.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1430/18640 [00:07<00:49, 347.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1465/18640 [00:07<00:49, 347.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1505/18640 [00:08<00:48, 349.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1545/18640 [00:08<00:48, 352.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1585/18640 [00:08<00:48, 354.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1625/18640 [00:08<00:48, 352.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1665/18640 [00:08<00:48, 349.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1705/18640 [00:08<00:48, 349.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1740/18640 [00:08<00:48, 345.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1780/18640 [00:08<00:48, 346.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1815/18640 [00:08<00:48, 346.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1850/18640 [00:09<00:48, 345.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1890/18640 [00:09<00:48, 348.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1930/18640 [00:09<00:47, 350.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 1970/18640 [00:09<00:47, 352.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2010/18640 [00:09<00:47, 351.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2050/18640 [00:09<00:47, 352.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2090/18640 [00:09<00:46, 352.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2130/18640 [00:09<00:46, 353.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2170/18640 [00:09<00:46, 354.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2210/18640 [00:10<00:46, 351.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2250/18640 [00:10<00:46, 352.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2290/18640 [00:10<00:46, 354.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2330/18640 [00:10<00:45, 354.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2370/18640 [00:10<00:46, 353.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2410/18640 [00:10<00:46, 352.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2450/18640 [00:10<00:45, 353.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2490/18640 [00:10<00:45, 353.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2530/18640 [00:10<00:45, 352.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2570/18640 [00:11<00:45, 352.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2610/18640 [00:11<00:45, 351.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2650/18640 [00:11<00:45, 353.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2690/18640 [00:11<00:44, 354.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2730/18640 [00:11<00:45, 350.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2770/18640 [00:11<00:44, 353.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2810/18640 [00:11<00:44, 354.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2850/18640 [00:11<00:44, 355.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2890/18640 [00:11<00:44, 351.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2930/18640 [00:12<00:44, 352.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2970/18640 [00:12<00:44, 352.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 3010/18640 [00:12<00:44, 353.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 3050/18640 [00:12<00:43, 354.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3090/18640 [00:12<00:43, 353.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3130/18640 [00:12<00:44, 347.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3165/18640 [00:12<00:44, 347.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3205/18640 [00:12<00:44, 350.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3245/18640 [00:12<00:43, 351.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3285/18640 [00:13<00:43, 352.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3325/18640 [00:13<00:43, 354.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3365/18640 [00:13<00:43, 354.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3405/18640 [00:13<00:43, 350.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3445/18640 [00:13<00:43, 345.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3480/18640 [00:13<00:44, 343.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3515/18640 [00:13<00:43, 345.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3555/18640 [00:13<00:43, 349.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3590/18640 [00:13<00:43, 348.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3625/18640 [00:14<00:43, 346.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3665/18640 [00:14<00:43, 347.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3700/18640 [00:14<00:43, 346.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3735/18640 [00:14<00:43, 341.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3770/18640 [00:14<00:43, 339.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3805/18640 [00:14<00:44, 336.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3840/18640 [00:14<00:43, 337.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3875/18640 [00:14<00:43, 339.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3910/18640 [00:14<00:43, 339.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3945/18640 [00:15<00:42, 342.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3980/18640 [00:15<00:43, 333.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4015/18640 [00:15<00:43, 336.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4050/18640 [00:15<00:43, 338.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4090/18640 [00:15<00:42, 342.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4125/18640 [00:15<00:42, 340.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4165/18640 [00:15<00:42, 343.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4205/18640 [00:15<00:41, 348.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4245/18640 [00:15<00:41, 350.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4285/18640 [00:16<00:40, 352.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4325/18640 [00:16<00:40, 349.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4360/18640 [00:16<00:40, 349.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4400/18640 [00:16<00:40, 350.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4440/18640 [00:16<00:40, 352.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4480/18640 [00:16<00:40, 350.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4520/18640 [00:16<00:40, 351.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4560/18640 [00:16<00:39, 353.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4600/18640 [00:16<00:39, 355.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4640/18640 [00:17<00:39, 354.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4680/18640 [00:17<00:39, 352.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4720/18640 [00:17<00:39, 354.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4760/18640 [00:17<00:38, 356.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4800/18640 [00:17<00:39, 353.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4840/18640 [00:17<00:39, 352.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4880/18640 [00:17<00:38, 353.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4920/18640 [00:17<00:38, 354.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 4960/18640 [00:17<00:38, 354.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5000/18640 [00:18<00:38, 354.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5040/18640 [00:18<00:38, 354.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5080/18640 [00:18<00:38, 355.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5120/18640 [00:18<00:38, 355.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5160/18640 [00:18<00:37, 354.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5200/18640 [00:18<00:38, 353.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5240/18640 [00:18<00:37, 353.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5280/18640 [00:18<00:37, 353.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5320/18640 [00:18<00:37, 353.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5360/18640 [00:19<00:37, 353.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5400/18640 [00:19<00:37, 352.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5440/18640 [00:19<00:37, 354.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5480/18640 [00:19<00:37, 355.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5520/18640 [00:19<00:37, 354.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5560/18640 [00:19<00:36, 354.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5600/18640 [00:19<00:37, 352.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5640/18640 [00:19<00:36, 351.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5680/18640 [00:19<00:36, 354.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5720/18640 [00:20<00:36, 354.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5760/18640 [00:20<00:36, 356.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5800/18640 [00:20<00:35, 357.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5840/18640 [00:20<00:35, 357.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5880/18640 [00:20<00:35, 357.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5920/18640 [00:20<00:35, 357.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5960/18640 [00:20<00:35, 354.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 6000/18640 [00:20<00:35, 353.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 6040/18640 [00:20<00:35, 352.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6080/18640 [00:21<00:35, 351.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6120/18640 [00:21<00:35, 351.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6160/18640 [00:21<00:35, 353.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6200/18640 [00:21<00:35, 352.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6240/18640 [00:21<00:35, 351.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6280/18640 [00:21<00:35, 351.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6320/18640 [00:21<00:35, 351.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6360/18640 [00:21<00:34, 353.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6400/18640 [00:21<00:34, 353.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6440/18640 [00:22<00:34, 353.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6480/18640 [00:22<00:34, 353.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6520/18640 [00:22<00:34, 354.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6560/18640 [00:22<00:33, 356.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6600/18640 [00:22<00:33, 354.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6640/18640 [00:22<00:33, 355.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6680/18640 [00:22<00:33, 357.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6720/18640 [00:22<00:33, 356.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6760/18640 [00:22<00:33, 355.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6800/18640 [00:23<00:33, 353.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6840/18640 [00:23<00:33, 352.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6880/18640 [00:23<00:33, 352.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6920/18640 [00:23<00:33, 352.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6960/18640 [00:23<00:33, 346.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 6995/18640 [00:23<00:33, 347.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7035/18640 [00:23<00:33, 348.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7075/18640 [00:23<00:33, 350.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7115/18640 [00:24<00:32, 351.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7155/18640 [00:24<00:32, 352.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7195/18640 [00:24<00:32, 354.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7235/18640 [00:24<00:32, 355.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7275/18640 [00:24<00:32, 352.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7315/18640 [00:24<00:32, 350.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7355/18640 [00:24<00:32, 350.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7395/18640 [00:24<00:32, 350.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7435/18640 [00:24<00:31, 352.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7475/18640 [00:25<00:31, 353.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7515/18640 [00:25<00:31, 354.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7555/18640 [00:25<00:31, 355.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7595/18640 [00:25<00:31, 350.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7635/18640 [00:25<00:31, 349.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7675/18640 [00:25<00:31, 350.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7715/18640 [00:25<00:31, 350.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7755/18640 [00:25<00:30, 352.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7795/18640 [00:25<00:30, 353.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7835/18640 [00:26<00:30, 352.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7875/18640 [00:26<00:30, 352.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7915/18640 [00:26<00:30, 347.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 7950/18640 [00:26<00:30, 346.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 7985/18640 [00:26<00:31, 341.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8020/18640 [00:26<00:31, 342.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8060/18640 [00:26<00:30, 347.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8100/18640 [00:26<00:30, 351.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8140/18640 [00:26<00:29, 352.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8180/18640 [00:27<00:29, 354.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8220/18640 [00:27<00:29, 353.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8260/18640 [00:27<00:29, 356.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8300/18640 [00:27<00:28, 358.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8340/18640 [00:27<00:28, 357.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8380/18640 [00:27<00:40, 253.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8420/18640 [00:27<00:36, 279.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8460/18640 [00:27<00:33, 300.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8500/18640 [00:28<00:32, 315.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8540/18640 [00:28<00:30, 328.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8580/18640 [00:28<00:29, 336.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8620/18640 [00:28<00:29, 341.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8660/18640 [00:28<00:28, 345.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8700/18640 [00:28<00:28, 351.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8740/18640 [00:28<00:28, 352.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8780/18640 [00:28<00:27, 353.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8820/18640 [00:28<00:27, 354.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8860/18640 [00:29<00:27, 355.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8900/18640 [00:29<00:27, 357.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8940/18640 [00:29<00:27, 357.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8980/18640 [00:29<00:26, 358.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 9020/18640 [00:29<00:27, 353.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9060/18640 [00:29<00:26, 355.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9100/18640 [00:29<00:26, 355.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9140/18640 [00:29<00:26, 356.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9180/18640 [00:29<00:26, 357.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9220/18640 [00:30<00:26, 357.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9260/18640 [00:30<00:26, 357.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9300/18640 [00:30<00:26, 358.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9340/18640 [00:30<00:25, 357.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9380/18640 [00:30<00:26, 355.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9420/18640 [00:30<00:26, 354.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9460/18640 [00:30<00:25, 353.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9500/18640 [00:30<00:25, 353.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9540/18640 [00:31<00:25, 356.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9580/18640 [00:31<00:25, 358.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9620/18640 [00:31<00:25, 360.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9660/18640 [00:31<00:24, 361.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9700/18640 [00:31<00:24, 362.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9740/18640 [00:31<00:24, 362.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9780/18640 [00:31<00:24, 362.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9820/18640 [00:31<00:24, 361.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9860/18640 [00:31<00:24, 361.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9900/18640 [00:31<00:24, 361.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9940/18640 [00:32<00:24, 361.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 9980/18640 [00:32<00:23, 362.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10020/18640 [00:32<00:23, 364.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10060/18640 [00:32<00:23, 363.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10100/18640 [00:32<00:23, 359.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10140/18640 [00:32<00:23, 359.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10180/18640 [00:32<00:23, 359.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10220/18640 [00:32<00:23, 361.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10260/18640 [00:32<00:23, 361.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10300/18640 [00:33<00:23, 359.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10340/18640 [00:33<00:22, 361.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10380/18640 [00:33<00:22, 361.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10420/18640 [00:33<00:22, 360.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10460/18640 [00:33<00:22, 362.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10500/18640 [00:33<00:22, 362.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10540/18640 [00:33<00:22, 363.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10580/18640 [00:33<00:22, 364.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10620/18640 [00:33<00:21, 365.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10660/18640 [00:34<00:21, 365.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10700/18640 [00:34<00:21, 366.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10740/18640 [00:34<00:21, 366.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10780/18640 [00:34<00:21, 366.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10820/18640 [00:34<00:21, 365.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10860/18640 [00:34<00:21, 365.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10900/18640 [00:34<00:21, 362.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 10940/18640 [00:34<00:21, 359.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 10980/18640 [00:34<00:21, 359.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 11020/18640 [00:35<00:21, 361.34it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 11060/18640 [00:35<00:20, 361.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11100/18640 [00:35<00:20, 361.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11140/18640 [00:35<00:20, 363.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11180/18640 [00:35<00:20, 363.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11220/18640 [00:35<00:20, 363.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11260/18640 [00:35<00:20, 361.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11300/18640 [00:35<00:20, 360.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11340/18640 [00:35<00:20, 358.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11380/18640 [00:36<00:20, 360.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11420/18640 [00:36<00:20, 360.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11460/18640 [00:36<00:19, 360.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11500/18640 [00:36<00:19, 359.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11540/18640 [00:36<00:20, 352.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11580/18640 [00:36<00:19, 353.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11620/18640 [00:36<00:19, 354.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11660/18640 [00:36<00:19, 355.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11700/18640 [00:36<00:19, 356.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11740/18640 [00:37<00:19, 351.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11780/18640 [00:37<00:19, 344.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11820/18640 [00:37<00:19, 346.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11860/18640 [00:37<00:19, 348.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11895/18640 [00:37<00:19, 348.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11935/18640 [00:37<00:19, 351.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11975/18640 [00:37<00:19, 350.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 12015/18640 [00:37<00:18, 354.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12055/18640 [00:37<00:18, 356.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12095/18640 [00:38<00:18, 355.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12135/18640 [00:38<00:18, 356.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12175/18640 [00:38<00:18, 354.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12215/18640 [00:38<00:18, 350.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12255/18640 [00:38<00:18, 343.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12290/18640 [00:38<00:18, 335.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12325/18640 [00:38<00:18, 338.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12360/18640 [00:38<00:18, 340.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12395/18640 [00:38<00:18, 342.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12430/18640 [00:39<00:18, 342.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12470/18640 [00:39<00:17, 348.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12510/18640 [00:39<00:17, 354.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12550/18640 [00:39<00:17, 356.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12590/18640 [00:39<00:16, 356.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12630/18640 [00:39<00:16, 359.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12670/18640 [00:39<00:16, 354.34it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12710/18640 [00:39<00:16, 354.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12750/18640 [00:39<00:16, 354.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12790/18640 [00:40<00:16, 356.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12830/18640 [00:40<00:16, 358.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12870/18640 [00:40<00:16, 357.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12910/18640 [00:40<00:15, 358.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12950/18640 [00:40<00:16, 354.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 12990/18640 [00:40<00:15, 355.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13030/18640 [00:40<00:15, 356.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13070/18640 [00:40<00:15, 356.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13110/18640 [00:40<00:15, 358.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13150/18640 [00:41<00:15, 354.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13190/18640 [00:41<00:15, 355.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13230/18640 [00:41<00:15, 356.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13270/18640 [00:41<00:14, 358.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13310/18640 [00:41<00:14, 356.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13350/18640 [00:41<00:14, 356.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13390/18640 [00:41<00:14, 357.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13430/18640 [00:41<00:14, 358.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13470/18640 [00:41<00:14, 356.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13510/18640 [00:42<00:14, 357.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13550/18640 [00:42<00:14, 358.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13590/18640 [00:42<00:14, 360.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13630/18640 [00:42<00:13, 361.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13670/18640 [00:42<00:13, 362.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13710/18640 [00:42<00:13, 360.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13750/18640 [00:42<00:13, 361.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13790/18640 [00:42<00:13, 362.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13830/18640 [00:42<00:13, 363.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13870/18640 [00:43<00:13, 364.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13910/18640 [00:43<00:12, 364.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13950/18640 [00:43<00:12, 364.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13990/18640 [00:43<00:12, 363.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 14030/18640 [00:43<00:12, 361.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 14070/18640 [00:43<00:12, 360.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14110/18640 [00:43<00:12, 359.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14150/18640 [00:43<00:12, 359.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14190/18640 [00:43<00:12, 360.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14230/18640 [00:44<00:12, 361.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14270/18640 [00:44<00:12, 360.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14310/18640 [00:44<00:11, 361.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14350/18640 [00:44<00:11, 361.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14390/18640 [00:44<00:11, 358.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14430/18640 [00:44<00:11, 358.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14470/18640 [00:44<00:11, 358.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14510/18640 [00:44<00:11, 357.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14550/18640 [00:44<00:11, 358.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14590/18640 [00:45<00:11, 357.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14630/18640 [00:45<00:11, 356.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14670/18640 [00:45<00:11, 359.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14710/18640 [00:45<00:10, 359.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14750/18640 [00:45<00:11, 352.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14790/18640 [00:45<00:10, 356.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14830/18640 [00:45<00:10, 357.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14870/18640 [00:45<00:10, 359.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14910/18640 [00:45<00:10, 358.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14950/18640 [00:46<00:10, 357.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14990/18640 [00:46<00:10, 357.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15030/18640 [00:46<00:10, 358.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15070/18640 [00:46<00:09, 357.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15110/18640 [00:46<00:09, 356.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15150/18640 [00:46<00:09, 354.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15190/18640 [00:46<00:09, 355.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15230/18640 [00:46<00:09, 358.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15270/18640 [00:47<00:09, 360.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15310/18640 [00:47<00:09, 360.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15350/18640 [00:47<00:09, 361.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15390/18640 [00:47<00:08, 362.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15430/18640 [00:47<00:08, 364.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15470/18640 [00:47<00:08, 360.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15510/18640 [00:47<00:08, 359.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15550/18640 [00:47<00:08, 359.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15590/18640 [00:47<00:08, 359.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15630/18640 [00:48<00:08, 360.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15670/18640 [00:48<00:08, 358.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15710/18640 [00:48<00:08, 358.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15750/18640 [00:48<00:08, 358.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15790/18640 [00:48<00:07, 357.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15830/18640 [00:48<00:07, 354.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15870/18640 [00:48<00:07, 357.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15910/18640 [00:48<00:07, 354.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 15950/18640 [00:48<00:07, 357.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 15990/18640 [00:49<00:07, 360.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16030/18640 [00:49<00:07, 357.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16070/18640 [00:49<00:07, 353.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16110/18640 [00:49<00:07, 353.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16150/18640 [00:49<00:07, 355.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16190/18640 [00:49<00:06, 356.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16230/18640 [00:49<00:06, 356.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16270/18640 [00:49<00:06, 352.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16310/18640 [00:49<00:06, 354.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16350/18640 [00:50<00:06, 353.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16390/18640 [00:50<00:06, 351.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16430/18640 [00:50<00:06, 351.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16470/18640 [00:50<00:06, 346.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16505/18640 [00:50<00:06, 345.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16540/18640 [00:50<00:06, 345.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16575/18640 [00:50<00:06, 334.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16610/18640 [00:50<00:06, 326.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16650/18640 [00:50<00:05, 334.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16690/18640 [00:51<00:05, 339.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16730/18640 [00:51<00:05, 342.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16770/18640 [00:51<00:05, 348.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16810/18640 [00:51<00:05, 352.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16850/18640 [00:51<00:05, 353.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16890/18640 [00:51<00:04, 354.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16930/18640 [00:51<00:04, 354.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16970/18640 [00:51<00:04, 352.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 17010/18640 [00:51<00:04, 354.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 17050/18640 [00:52<00:04, 355.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17090/18640 [00:52<00:04, 355.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17130/18640 [00:52<00:04, 355.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17170/18640 [00:52<00:04, 355.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17210/18640 [00:52<00:04, 355.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17250/18640 [00:52<00:03, 355.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17290/18640 [00:52<00:03, 355.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17330/18640 [00:52<00:03, 353.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17370/18640 [00:52<00:03, 354.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17410/18640 [00:53<00:03, 354.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17450/18640 [00:53<00:03, 356.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17490/18640 [00:53<00:03, 358.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17530/18640 [00:53<00:03, 359.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17570/18640 [00:53<00:02, 360.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17610/18640 [00:53<00:02, 357.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17650/18640 [00:53<00:02, 358.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17690/18640 [00:53<00:02, 357.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17730/18640 [00:53<00:02, 358.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17770/18640 [00:54<00:02, 357.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17810/18640 [00:54<00:02, 358.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17850/18640 [00:54<00:02, 357.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17890/18640 [00:54<00:02, 359.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17930/18640 [00:54<00:01, 360.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17970/18640 [00:54<00:01, 361.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18010/18640 [00:54<00:01, 360.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18050/18640 [00:54<00:01, 357.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18090/18640 [00:54<00:01, 357.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18130/18640 [00:55<00:01, 357.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18170/18640 [00:55<00:01, 359.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18210/18640 [00:55<00:01, 357.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18250/18640 [00:55<00:01, 359.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18290/18640 [00:55<00:00, 357.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18330/18640 [00:55<00:00, 359.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18370/18640 [00:55<00:00, 360.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18410/18640 [00:55<00:00, 359.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18450/18640 [00:55<00:00, 359.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18490/18640 [00:56<00:00, 359.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18530/18640 [00:56<00:00, 361.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18570/18640 [00:56<00:00, 360.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18610/18640 [00:56<00:00, 349.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18640/18640 [00:56<00:00, 329.89it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  4.645455           \n",
            "              f_rmse =  9.186818           \n",
            "               e_mae =  263.217928         \n",
            "             e/N_mae =  30.208962          \n",
            "               f_mae =  4.645455           \n",
            "              f_rmse =  9.186818           \n",
            "               e_mae =  263.217928         \n",
            "             e/N_mae =  30.208962          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time for Ground Truth using LAMMPS**"
      ],
      "metadata": {
        "id": "eOEE44cTCvSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dynasor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PDJ2kGvhOdQS",
        "outputId": "c9dd57e5-dbe2-4ec2-ef14-7126d1ecb595"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dynasor\n",
            "  Downloading dynasor-2.0.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from dynasor) (3.23.0)\n",
            "Collecting mdanalysis (from dynasor)\n",
            "  Downloading MDAnalysis-2.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.55 in /usr/local/lib/python3.10/dist-packages (from dynasor) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from dynasor) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dynasor) (2.2.2)\n",
            "Collecting importlib-metadata<5 (from dynasor)\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<5->dynasor) (3.21.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->dynasor) (0.43.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ase->dynasor) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase->dynasor) (3.8.0)\n",
            "Collecting GridDataFormats>=0.4.0 (from mdanalysis->dynasor)\n",
            "  Downloading GridDataFormats-1.0.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting mmtf-python>=1.0.0 (from mdanalysis->dynasor)\n",
            "  Downloading mmtf_python-1.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.10/dist-packages (from mdanalysis->dynasor) (1.4.2)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from mdanalysis->dynasor) (4.66.6)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from mdanalysis->dynasor) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mdanalysis->dynasor) (24.2)\n",
            "Collecting fasteners (from mdanalysis->dynasor)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting mda-xdrlib (from mdanalysis->dynasor)\n",
            "  Downloading mda_xdrlib-0.2.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting waterdynamics (from mdanalysis->dynasor)\n",
            "  Downloading waterdynamics-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting pathsimanalysis (from mdanalysis->dynasor)\n",
            "  Downloading pathsimanalysis-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting mdahole2 (from mdanalysis->dynasor)\n",
            "  Downloading mdahole2-0.5.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dynasor) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dynasor) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->dynasor) (2024.2)\n",
            "Collecting mrcfile (from GridDataFormats>=0.4.0->mdanalysis->dynasor)\n",
            "  Downloading mrcfile-1.5.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->dynasor) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->dynasor) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->dynasor) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->dynasor) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->dynasor) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->dynasor) (3.2.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mmtf-python>=1.0.0->mdanalysis->dynasor) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dynasor) (1.16.0)\n",
            "Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Downloading MDAnalysis-2.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GridDataFormats-1.0.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmtf_python-1.1.3-py2.py3-none-any.whl (25 kB)\n",
            "Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading mda_xdrlib-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading mdahole2-0.5.0-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathsimanalysis-1.2.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading waterdynamics-1.2.0-py3-none-any.whl (35 kB)\n",
            "Downloading mrcfile-1.5.3-py2.py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: dynasor\n",
            "  Building wheel for dynasor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dynasor: filename=dynasor-2.0-py3-none-any.whl size=48301 sha256=22bc05f46c86cd7626e1fa7f401de469fae29eaf3ab16d18803d25e4db22ab85\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/21/cf/9fd7222ada34b80f2ecd549c963cec41b06a13e74bb499eedb\n",
            "Successfully built dynasor\n",
            "Installing collected packages: mrcfile, mmtf-python, mda-xdrlib, importlib-metadata, fasteners, GridDataFormats, waterdynamics, pathsimanalysis, mdahole2, mdanalysis, dynasor\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-api 1.28.2 requires importlib-metadata<=8.5.0,>=6.0, but you have importlib-metadata 4.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GridDataFormats-1.0.2 dynasor-2.0 fasteners-0.19 importlib-metadata-4.13.0 mda-xdrlib-0.2.0 mdahole2-0.5.0 mdanalysis-2.8.0 mmtf-python-1.1.3 mrcfile-1.5.3 pathsimanalysis-1.2.0 waterdynamics-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "8839481c5f8f4b6cbfebd050cd215bc6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ase.io import read, write\n",
        "from numpy import load\n",
        "\n",
        "#example_atoms = read('./Si_data/sitraj.xyz', index=0)\n",
        "example_atoms = read('./Si_data/DES.xyz', index=0)\n",
        "#example_atoms = load('./benchmark_data/aspirin_ccsd-train.npz')\n",
        "write('./si.data', example_atoms, format='lammps-data')"
      ],
      "metadata": {
        "id": "-YAmau_8D6bU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "941a5163-0851-4f37-edde-74eb53f934e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "Singular matrix",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cee06a21fa01>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexample_atoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Si_data/DES.xyz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#example_atoms = load('./benchmark_data/aspirin_ccsd-train.npz')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./si.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_atoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lammps-data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/io/formats.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(filename, images, format, parallel, append, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ioformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m     return _write(filename, fd, format, io, images,\n\u001b[0m\u001b[1;32m    693\u001b[0m                   parallel=parallel, append=append, **kwargs)\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/parallel.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 not kwargs.pop('parallel', True)):\n\u001b[1;32m    270\u001b[0m             \u001b[0;31m# Disable:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/io/formats.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(filename, fd, format, io, images, parallel, append, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;31m# XXX remember to re-enable compressed open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m                 \u001b[0;31m# fd = io.open(filename, mode)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopen_new\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/io/formats.py\u001b[0m in \u001b[0;36m_write_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Cannot write to {self.name}-format'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/utils/__init__.py\u001b[0m in \u001b[0;36miofunc\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/io/lammpsdata.py\u001b[0m in \u001b[0;36mwrite_lammps_data\u001b[0;34m(fd, atoms, specorder, reduce_cell, force_skew, prismobj, write_image_flags, masses, velocities, units, bonds, atom_style)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprismobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mprismobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_cell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;31m# Get cell parameters and convert from ASE units to LAMMPS units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/calculators/lammps/coordinatetransform.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, pbc, reduce_cell, tolerance)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# The diagonals of `lammps_tilt` are always positive by construction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlammps_tilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_rotated_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlammps_tilt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mase_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolerance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DD->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'dd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lammps_input = \"\"\"\n",
        "units\tmetal\n",
        "atom_style atomic\n",
        "dimension 3\n",
        "\n",
        "# set newton on for pair_allegro (off for pair_nequip)\n",
        "newton on\n",
        "boundary p p p\n",
        "read_data ../si.data\n",
        "\n",
        "# if you want to run a larger system, simply replicate the system in space\n",
        "# replicate 3 3 3\n",
        "\n",
        "# allegro pair style\n",
        "pair_style\tallegro\n",
        "pair_coeff\t* * ../si-deployed.pth Si\n",
        "\n",
        "mass 1 28.0855\n",
        "\n",
        "velocity all create 300.0 1234567 loop geom\n",
        "\n",
        "neighbor 1.0 bin\n",
        "neigh_modify delay 5 every 1\n",
        "\n",
        "timestep 0.001\n",
        "thermo 10\n",
        "\n",
        "# nose-hoover thermostat, 300K\n",
        "fix  1 all nvt temp 300 300 $(100*dt)\n",
        "\n",
        "# compute rdf and average after some equilibration\n",
        "comm_modify cutoff 7.0\n",
        "compute rdfall all rdf 1000 cutoff 5.0\n",
        "fix 2 all ave/time 1 2500 5000 c_rdfall[*] file si.rdf mode vector\n",
        "\n",
        "# run 5ps\n",
        "run 5000\n",
        "\"\"\"\n",
        "!rm -rf ./lammps_run\n",
        "!mkdir lammps_run\n",
        "with open(\"lammps_run/si_rdf.in\", \"w\") as f:\n",
        "    f.write(lammps_input)"
      ],
      "metadata": {
        "id": "5RQGnRCsC0gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "!cd lammps_run/ && ../lammps/build/lmp -in si_rdf.in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St_X2LxSDAKk",
        "outputId": "9fb67a38-9c26-4f6a-80be-134aafd904e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAMMPS (19 Nov 2024 - Development - cd16308-modified)\n",
            "OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:99)\n",
            "  using 1 OpenMP thread(s) per MPI task\n",
            "Reading data file ...\n",
            "  orthogonal box = (0 0 0) to (10.862 10.862 10.862)\n",
            "  1 by 1 by 1 MPI processor grid\n",
            "  reading atoms ...\n",
            "  64 atoms\n",
            "  read_data CPU = 0.001 seconds\n",
            "Allegro is using input precision f and output precision d\n",
            "Allegro: Loading model from ../si-deployed.pth\n",
            "Allegro: Freezing TorchScript model...\n",
            "Type mapping:\n",
            "Allegro type | Allegro name | LAMMPS type | LAMMPS name\n",
            "0 | Si | 1 | Si\n",
            "Neighbor list info ...\n",
            "  update: every = 1 steps, delay = 5 steps, check = yes\n",
            "  max neighbors/atom: 2000, page size: 100000\n",
            "  master list distance cutoff = 6\n",
            "  ghost atom cutoff = 7\n",
            "  binsize = 3, bins = 4 4 4\n",
            "  2 neighbor lists, perpetual/occasional/extra = 1 1 0\n",
            "  (1) pair allegro, perpetual\n",
            "      attributes: full, newton on, ghost\n",
            "      pair build: full/bin/ghost\n",
            "      stencil: full/ghost/bin/3d\n",
            "      bin: standard\n",
            "  (2) compute rdf, occasional\n",
            "      attributes: half, newton on\n",
            "      pair build: half/bin/atomonly/newton\n",
            "      stencil: half/bin/3d\n",
            "      bin: standard\n",
            "Setting up Verlet run ...\n",
            "  Unit style    : metal\n",
            "  Current step  : 0\n",
            "  Time step     : 0.001\n",
            "Per MPI rank memory allocation (min/avg/max) = 3.719 | 3.719 | 3.719 Mbytes\n",
            "   Step          Temp          E_pair         E_mol          TotEng         Press     \n",
            "         0   300           -8324.1978      0             -8321.7548      123772.48    \n",
            "        10   189.27021     -8323.2832      0             -8321.7419      124289.58    \n",
            "        20   78.719835     -8322.378       0             -8321.7369      123984.49    \n",
            "        30   147.0925      -8322.9166      0             -8321.7187      121786.76    \n",
            "        40   190.97895     -8323.226       0             -8321.6708      120721.16    \n",
            "        50   117.05569     -8322.5723      0             -8321.6191      121562.45    \n",
            "        60   99.199339     -8322.3898      0             -8321.582       121657.03    \n",
            "        70   167.22383     -8322.8901      0             -8321.5283      120886.65    \n",
            "        80   169.62241     -8322.8459      0             -8321.4646      121526.54    \n",
            "        90   144.31278     -8322.5944      0             -8321.4193      122693.11    \n",
            "       100   194.44737     -8322.9535      0             -8321.37        122370.91    \n",
            "       110   194.44528     -8322.9029      0             -8321.3194      121931.69    \n",
            "       120   121.84454     -8322.2653      0             -8321.273       122553.91    \n",
            "       130   152.326       -8322.4781      0             -8321.2376      122765.16    \n",
            "       140   247.04311     -8323.186       0             -8321.1742      122643.33    \n",
            "       150   211.92622     -8322.8202      0             -8321.0944      123831.97    \n",
            "       160   156.57154     -8322.3123      0             -8321.0373      124981.78    \n",
            "       170   238.26071     -8322.9145      0             -8320.9743      124432.1     \n",
            "       180   307.38381     -8323.3883      0             -8320.8851      123729.64    \n",
            "       190   220.44838     -8322.5941      0             -8320.7989      123877.47    \n",
            "       200   150.11678     -8321.9702      0             -8320.7477      122863.93    \n",
            "       210   198.75078     -8322.3076      0             -8320.6891      120363.72    \n",
            "       220   190.75928     -8322.1703      0             -8320.6168      120030.95    \n",
            "       230   130.69934     -8321.6232      0             -8320.5589      121798.61    \n",
            "       240   195.90929     -8322.0925      0             -8320.4972      122138.16    \n",
            "       250   285.97784     -8322.7313      0             -8320.4025      121769.75    \n",
            "       260   238.77911     -8322.2495      0             -8320.305       123070.93    \n",
            "       270   187.2298      -8321.7621      0             -8320.2375      124692.23    \n",
            "       280   283.45862     -8322.4716      0             -8320.1633      124157.03    \n",
            "       290   337.76635     -8322.8163      0             -8320.0658      123399.4     \n",
            "       300   238.80272     -8321.9311      0             -8319.9864      124413.99    \n",
            "       310   218.30774     -8321.706       0             -8319.9282      124679.53    \n",
            "       320   299.67598     -8322.2984      0             -8319.858       123835.24    \n",
            "       330   292.7327      -8322.1521      0             -8319.7683      124495.34    \n",
            "       340   268.51164     -8321.889       0             -8319.7024      125597.29    \n",
            "       350   329.99664     -8322.3155      0             -8319.6282      124792.18    \n",
            "       360   350.15325     -8322.4013      0             -8319.5499      123181.37    \n",
            "       370   244.9736      -8321.4908      0             -8319.4959      122450.88    \n",
            "       380   195.3374      -8321.0386      0             -8319.4479      121419.7     \n",
            "       390   273.5302      -8321.6195      0             -8319.392       119876.42    \n",
            "       400   241.15327     -8321.2887      0             -8319.3249      120705.65    \n",
            "       410   174.0639      -8320.6837      0             -8319.2662      122710.86    \n",
            "       420   323.90796     -8321.8333      0             -8319.1956      123020.65    \n",
            "       430   426.72533     -8322.5616      0             -8319.0866      124504.42    \n",
            "       440   359.92939     -8321.9398      0             -8319.0087      127194.03    \n",
            "       450   300.1507      -8321.4044      0             -8318.9601      128106.03    \n",
            "       460   377.38003     -8321.9843      0             -8318.9111      125739.43    \n",
            "       470   391.87202     -8322.0635      0             -8318.8724      124027.3     \n",
            "       480   287.92979     -8321.197       0             -8318.8523      124470.46    \n",
            "       490   262.64045     -8320.9769      0             -8318.8382      124481.07    \n",
            "       500   358.52685     -8321.7367      0             -8318.8171      122882.14    \n",
            "       510   334.82725     -8321.5251      0             -8318.7985      123083.45    \n",
            "       520   286.80106     -8321.1306      0             -8318.7951      124562.08    \n",
            "       530   336.91513     -8321.5268      0             -8318.7832      124799.95    \n",
            "       540   380.12159     -8321.8752      0             -8318.7798      123298.27    \n",
            "       550   324.73201     -8321.4305      0             -8318.7861      121687.98    \n",
            "       560   236.02997     -8320.7226      0             -8318.8005      121640.88    \n",
            "       570   256.23164     -8320.8876      0             -8318.801       121905.21    \n",
            "       580   297.42751     -8321.2209      0             -8318.7988      123103.77    \n",
            "       590   339.58654     -8321.5607      0             -8318.7953      124382.66    \n",
            "       600   411.72221     -8322.1538      0             -8318.801       124744.07    \n",
            "       610   373.06764     -8321.8617      0             -8318.8237      126204.01    \n",
            "       620   306.10918     -8321.352       0             -8318.8593      127590.1     \n",
            "       630   373.33279     -8321.9422      0             -8318.902       126674.91    \n",
            "       640   420.81426     -8322.3946      0             -8318.9678      124647.39    \n",
            "       650   311.05059     -8321.578       0             -8319.045       123584.83    \n",
            "       660   192.63373     -8320.663       0             -8319.0943      123424.56    \n",
            "       670   260.65343     -8321.2567      0             -8319.1341      122291.59    \n",
            "       680   339.95957     -8321.9491      0             -8319.1807      121827.02    \n",
            "       690   283.03179     -8321.5259      0             -8319.221       123019.7     \n",
            "       700   242.01042     -8321.2304      0             -8319.2596      123977.39    \n",
            "       710   310.60811     -8321.815       0             -8319.2856      123406.88    \n",
            "       720   339.685       -8322.0906      0             -8319.3244      122967.76    \n",
            "       730   297.60019     -8321.7876      0             -8319.3642      123390.38    \n",
            "       740   288.8018      -8321.7501      0             -8319.3982      123259.48    \n",
            "       750   267.27838     -8321.6117      0             -8319.4352      123137.86    \n",
            "       760   227.72073     -8321.3092      0             -8319.4547      123866.54    \n",
            "       770   283.60224     -8321.7834      0             -8319.4739      124759.12    \n",
            "       780   388.44289     -8322.657       0             -8319.4938      125344.13    \n",
            "       790   368.90039     -8322.5375      0             -8319.5334      125939.38    \n",
            "       800   257.81254     -8321.6748      0             -8319.5754      125918.98    \n",
            "       810   270.67888     -8321.8081      0             -8319.6038      124076.66    \n",
            "       820   314.74015     -8322.2014      0             -8319.6384      122474.58    \n",
            "       830   245.80435     -8321.6667      0             -8319.665       122262.28    \n",
            "       840   190.25059     -8321.2314      0             -8319.6822      121922.73    \n",
            "       850   242.18708     -8321.6627      0             -8319.6905      121136.32    \n",
            "       860   264.86614     -8321.8434      0             -8319.6865      122104.64    \n",
            "       870   233.9546      -8321.5846      0             -8319.6794      124549.89    \n",
            "       880   310.20168     -8322.1909      0             -8319.6648      124912.58    \n",
            "       890   371.08979     -8322.6694      0             -8319.6475      124028.72    \n",
            "       900   283.8312      -8321.9469      0             -8319.6355      124519.29    \n",
            "       910   225.69327     -8321.4666      0             -8319.6287      124949.34    \n",
            "       920   288.00532     -8321.9544      0             -8319.609       123830.95    \n",
            "       930   324.32338     -8322.2249      0             -8319.5838      122870.74    \n",
            "       940   245.09093     -8321.5507      0             -8319.5548      123693.99    \n",
            "       950   242.97251     -8321.5075      0             -8319.5289      124406.8     \n",
            "       960   353.49427     -8322.3648      0             -8319.4861      124181.58    \n",
            "       970   352.47982     -8322.3107      0             -8319.4404      124831.2     \n",
            "       980   288.93742     -8321.7583      0             -8319.4053      124966.05    \n",
            "       990   278.28145     -8321.6402      0             -8319.3741      123423.58    \n",
            "      1000   266.83023     -8321.5136      0             -8319.3407      122044.51    \n",
            "      1010   226.39642     -8321.1445      0             -8319.3008      121927.99    \n",
            "      1020   264.70192     -8321.408       0             -8319.2524      121912.66    \n",
            "      1030   338.02749     -8321.9437      0             -8319.191       121920.07    \n",
            "      1040   287.52875     -8321.4659      0             -8319.1244      123544.46    \n",
            "      1050   239.94045     -8321.0312      0             -8319.0773      125249.18    \n",
            "      1060   361.98305     -8321.9641      0             -8319.0164      125018.09    \n",
            "      1070   453.20481     -8322.6361      0             -8318.9454      124940.25    \n",
            "      1080   355.22655     -8321.8056      0             -8318.9129      125843.83    \n",
            "      1090   254.08212     -8320.9659      0             -8318.8968      125498.25    \n",
            "      1100   314.36458     -8321.4399      0             -8318.8799      122876.7     \n",
            "      1110   360.15834     -8321.7986      0             -8318.8657      121560.31    \n",
            "      1120   270.21412     -8321.0584      0             -8318.8579      123348.95    \n",
            "      1130   270.00313     -8321.0464      0             -8318.8476      124406.42    \n",
            "      1140   380.77905     -8321.9398      0             -8318.839       123501.45    \n",
            "      1150   372.21377     -8321.871       0             -8318.84        123539.21    \n",
            "      1160   290.37507     -8321.2141      0             -8318.8494      124493.42    \n",
            "      1170   299.48685     -8321.2959      0             -8318.857       124014.45    \n",
            "      1180   351.4324      -8321.7299      0             -8318.868       122331.86    \n",
            "      1190   298.52529     -8321.3168      0             -8318.8857      122068.55    \n",
            "      1200   233.06576     -8320.7976      0             -8318.8996      123280.48    \n",
            "      1210   311.77149     -8321.4461      0             -8318.9072      123811.9     \n",
            "      1220   370.2113      -8321.9334      0             -8318.9187      124768.15    \n",
            "      1230   353.00034     -8321.8126      0             -8318.938       125632.64    \n",
            "      1240   355.72893     -8321.8678      0             -8318.971       125339.13    \n",
            "      1250   344.06595     -8321.8148      0             -8319.013       125271.14    \n",
            "      1260   331.42406     -8321.7647      0             -8319.0658      124838.48    \n",
            "      1270   300.50168     -8321.5587      0             -8319.1116      123789.55    \n",
            "      1280   302.83272     -8321.6273      0             -8319.1612      121856.25    \n",
            "      1290   261.72306     -8321.3392      0             -8319.2079      121403.6     \n",
            "      1300   196.7867      -8320.8375      0             -8319.235       122829.67    \n",
            "      1310   298.37114     -8321.6875      0             -8319.2578      123128.63    \n",
            "      1320   399.12481     -8322.5388      0             -8319.2885      123463.37    \n",
            "      1330   339.92073     -8322.096       0             -8319.3278      124496.97    \n",
            "      1340   255.69913     -8321.4487      0             -8319.3664      124596.31    \n",
            "      1350   263.18466     -8321.5315      0             -8319.3883      123417.03    \n",
            "      1360   306.67894     -8321.9081      0             -8319.4107      122247.96    \n",
            "      1370   275.23843     -8321.6729      0             -8319.4315      122806.29    \n",
            "      1380   256.24228     -8321.5248      0             -8319.4381      123601.01    \n",
            "      1390   291.41577     -8321.8192      0             -8319.4461      123521.98    \n",
            "      1400   295.29877     -8321.8487      0             -8319.444       124043.37    \n",
            "      1410   307.68644     -8321.9486      0             -8319.443       124936.5     \n",
            "      1420   340.63532     -8322.2245      0             -8319.4506      125476.76    \n",
            "      1430   324.26772     -8322.0971      0             -8319.4565      125078.02    \n",
            "      1440   276.1281      -8321.7162      0             -8319.4676      123847.56    \n",
            "      1450   267.28696     -8321.6435      0             -8319.4669      122604.7     \n",
            "      1460   282.55268     -8321.7676      0             -8319.4667      121874.59    \n",
            "      1470   245.81763     -8321.4636      0             -8319.4618      122209.56    \n",
            "      1480   221.62018     -8321.2578      0             -8319.4531      122819.64    \n",
            "      1490   297.07691     -8321.8572      0             -8319.438       122863.58    \n",
            "      1500   348.00976     -8322.2458      0             -8319.4119      123685.02    \n",
            "      1510   299.8444      -8321.8379      0             -8319.3962      125209.13    \n",
            "      1520   309.61408     -8321.9079      0             -8319.3866      124770.94    \n",
            "      1530   346.28017     -8322.1975      0             -8319.3776      123039.11    \n",
            "      1540   273.7132      -8321.601       0             -8319.372       122897.51    \n",
            "      1550   204.51673     -8321.035       0             -8319.3695      123653.31    \n",
            "      1560   268.41255     -8321.5334      0             -8319.3476      123401.69    \n",
            "      1570   358.93654     -8322.2475      0             -8319.3246      122819.78    \n",
            "      1580   321.53185     -8321.9192      0             -8319.3008      123653.52    \n",
            "      1590   255.13267     -8321.3644      0             -8319.2868      124987.57    \n",
            "      1600   299.76712     -8321.7056      0             -8319.2644      125150.95    \n",
            "      1610   356.5172      -8322.1421      0             -8319.2388      124557.32    \n",
            "      1620   362.8928      -8322.1806      0             -8319.2254      123231.35    \n",
            "      1630   282.2538      -8321.513       0             -8319.2145      122483.98    \n",
            "      1640   212.53286     -8320.9389      0             -8319.2082      122360.4     \n",
            "      1650   245.51328     -8321.1935      0             -8319.1942      122314.61    \n",
            "      1660   323.99257     -8321.8006      0             -8319.1622      122713.13    \n",
            "      1670   360.98911     -8322.0643      0             -8319.1247      123614.81    \n",
            "      1680   321.54166     -8321.7176      0             -8319.0992      124774.38    \n",
            "      1690   317.31634     -8321.6582      0             -8319.0741      124529.94    \n",
            "      1700   357.25945     -8321.9624      0             -8319.0531      123198.51    \n",
            "      1710   327.29005     -8321.6996      0             -8319.0343      123058.79    \n",
            "      1720   255.22156     -8321.1067      0             -8319.0283      123615.3     \n",
            "      1730   236.06443     -8320.9396      0             -8319.0172      123437.84    \n",
            "      1740   312.60754     -8321.534       0             -8318.9883      122426.26    \n",
            "      1750   364.08602     -8321.9159      0             -8318.951       122906.04    \n",
            "      1760   348.21908     -8321.7589      0             -8318.9232      124859.8     \n",
            "      1770   322.88231     -8321.5295      0             -8318.9001      125761.11    \n",
            "      1780   330.48954     -8321.5704      0             -8318.8791      124638.86    \n",
            "      1790   349.65779     -8321.712       0             -8318.8646      123307.19    \n",
            "      1800   334.12353     -8321.5706      0             -8318.8497      123304.98    \n",
            "      1810   321.01354     -8321.4544      0             -8318.8402      122945.03    \n",
            "      1820   311.99647     -8321.386       0             -8318.8452      121771.11    \n",
            "      1830   268.94991     -8321.0311      0             -8318.8409      121517.76    \n",
            "      1840   275.46789     -8321.0806      0             -8318.8374      122366.52    \n",
            "      1850   330.76923     -8321.5218      0             -8318.8282      123956.34    \n",
            "      1860   389.12908     -8321.9852      0             -8318.8163      124738.71    \n",
            "      1870   388.00571     -8321.9853      0             -8318.8256      124189.08    \n",
            "      1880   337.39734     -8321.5995      0             -8318.8519      123134.67    \n",
            "      1890   292.19112     -8321.266       0             -8318.8866      122811.52    \n",
            "      1900   253.01165     -8320.9756      0             -8318.9153      123422.31    \n",
            "      1910   285.10349     -8321.2618      0             -8318.9401      122986.17    \n",
            "      1920   352.80271     -8321.85        0             -8318.977       122073.52    \n",
            "      1930   331.861       -8321.7282      0             -8319.0258      123078.76    \n",
            "      1940   272.61855     -8321.2918      0             -8319.0717      124971.98    \n",
            "      1950   311.18033     -8321.6542      0             -8319.1201      125287.74    \n",
            "      1960   389.18295     -8322.3539      0             -8319.1847      124205.06    \n",
            "      1970   356.09164     -8322.1724      0             -8319.2726      123606.94    \n",
            "      1980   255.8088      -8321.4261      0             -8319.3429      123372.23    \n",
            "      1990   244.98223     -8321.3942      0             -8319.3992      122246.22    \n",
            "      2000   289.12135     -8321.8072      0             -8319.4528      121288.01    \n",
            "      2010   262.276       -8321.6302      0             -8319.4943      122092.51    \n",
            "      2020   226.71432     -8321.3714      0             -8319.5252      123084.45    \n",
            "      2030   266.17627     -8321.7173      0             -8319.5497      123121.83    \n",
            "      2040   320.02389     -8322.1746      0             -8319.5685      123400.35    \n",
            "      2050   351.78077     -8322.4556      0             -8319.5909      123670.15    \n",
            "      2060   317.01996     -8322.2039      0             -8319.6223      123884.64    \n",
            "      2070   250.98148     -8321.6918      0             -8319.6479      123821.12    \n",
            "      2080   210.47208     -8321.3771      0             -8319.6632      123514.44    \n",
            "      2090   249.007       -8321.6972      0             -8319.6694      123016.79    \n",
            "      2100   319.03662     -8322.2636      0             -8319.6656      122991.14    \n",
            "      2110   303.34856     -8322.1377      0             -8319.6675      124007.99    \n",
            "      2120   278.3419      -8321.9343      0             -8319.6677      124208.09    \n",
            "      2130   284.68072     -8321.9851      0             -8319.6669      123392.71    \n",
            "      2140   275.83424     -8321.9078      0             -8319.6615      123062.04    \n",
            "      2150   269.90697     -8321.8513      0             -8319.6534      123035.07    \n",
            "      2160   274.20945     -8321.8756      0             -8319.6426      122584.04    \n",
            "      2170   290.80266     -8321.9876      0             -8319.6194      121463.62    \n",
            "      2180   247.54569     -8321.6152      0             -8319.5993      121715.35    \n",
            "      2190   207.16703     -8321.2602      0             -8319.5732      123206       \n",
            "      2200   276.70288     -8321.7873      0             -8319.534       123574.13    \n",
            "      2210   343.00189     -8322.2646      0             -8319.4714      123566.96    \n",
            "      2220   337.23299     -8322.1514      0             -8319.4052      124056.34    \n",
            "      2230   307.76717     -8321.8536      0             -8319.3473      124368.45    \n",
            "      2240   295.72467     -8321.7014      0             -8319.2932      123985.33    \n",
            "      2250   311.70833     -8321.7682      0             -8319.2298      123161.04    \n",
            "      2260   314.86824     -8321.7345      0             -8319.1704      122481.46    \n",
            "      2270   273.65376     -8321.3419      0             -8319.1134      122676.68    \n",
            "      2280   261.01541     -8321.1859      0             -8319.0604      123275.17    \n",
            "      2290   325.67163     -8321.6529      0             -8319.0009      123687.86    \n",
            "      2300   382.19108     -8322.0443      0             -8318.9319      123994.85    \n",
            "      2310   348.30663     -8321.7162      0             -8318.8798      123909.94    \n",
            "      2320   275.86766     -8321.0946      0             -8318.8481      123404.51    \n",
            "      2330   301.15412     -8321.274       0             -8318.8216      122168.13    \n",
            "      2340   354.23206     -8321.6816      0             -8318.7969      121528.97    \n",
            "      2350   329.48692     -8321.4634      0             -8318.7802      122208.51    \n",
            "      2360   289.26885     -8321.1286      0             -8318.7729      122895.85    \n",
            "      2370   268.54632     -8320.9518      0             -8318.7649      123298.97    \n",
            "      2380   311.21588     -8321.29        0             -8318.7556      123485.6     \n",
            "      2390   379.60682     -8321.8385      0             -8318.7473      124537.81    \n",
            "      2400   427.19531     -8322.2342      0             -8318.7554      125555.57    \n",
            "      2410   410.12099     -8322.129       0             -8318.7892      125125.47    \n",
            "      2420   310.37597     -8321.3531      0             -8318.8256      124055.84    \n",
            "      2430   285.01779     -8321.1807      0             -8318.8597      122536.3     \n",
            "      2440   296.19625     -8321.2956      0             -8318.8835      122114.23    \n",
            "      2450   274.76811     -8321.1455      0             -8318.9079      122537.09    \n",
            "      2460   287.42729     -8321.2594      0             -8318.9188      122075.57    \n",
            "      2470   329.37528     -8321.6111      0             -8318.9288      121695.74    \n",
            "      2480   336.19139     -8321.6811      0             -8318.9434      122595.31    \n",
            "      2490   298.33181     -8321.3909      0             -8318.9615      123962.04    \n",
            "      2500   316.25928     -8321.5539      0             -8318.9784      123633.35    \n",
            "      2510   356.50408     -8321.9027      0             -8318.9995      122461.99    \n",
            "      2520   320.63918     -8321.6427      0             -8319.0316      122532.03    \n",
            "      2530   273.04521     -8321.283       0             -8319.0595      123476.77    \n",
            "      2540   296.76818     -8321.4976      0             -8319.0809      123748.42    \n",
            "      2550   346.2255      -8321.9258      0             -8319.1063      123346.15    \n",
            "      2560   325.87823     -8321.7917      0             -8319.138       123483.51    \n",
            "      2570   299.29638     -8321.61        0             -8319.1727      124045.89    \n",
            "      2580   316.04722     -8321.7789      0             -8319.2052      124359.28    \n",
            "      2590   343.38524     -8322.0376      0             -8319.2413      123605.01    \n",
            "      2600   340.6013      -8322.0626      0             -8319.289       122173.12    \n",
            "      2610   256.29278     -8321.4218      0             -8319.3347      121796.32    \n",
            "      2620   202.24787     -8321.0092      0             -8319.3622      121802.17    \n",
            "      2630   247.51253     -8321.394       0             -8319.3784      121496.28    \n",
            "      2640   306.72541     -8321.8887      0             -8319.3909      121969.45    \n",
            "      2650   329.07266     -8322.0858      0             -8319.4061      122987.08    \n",
            "      2660   287.6027      -8321.7673      0             -8319.4253      123976.81    \n",
            "      2670   254.16003     -8321.5054      0             -8319.4356      123865.69    \n",
            "      2680   298.6197      -8321.8765      0             -8319.4448      122996.23    \n",
            "      2690   342.01893     -8322.243       0             -8319.4578      123091.18    \n",
            "      2700   317.42412     -8322.058       0             -8319.4731      123816.85    \n",
            "      2710   265.92155     -8321.6587      0             -8319.4932      123752.88    \n",
            "      2720   267.49277     -8321.6815      0             -8319.5032      122869.16    \n",
            "      2730   301.79742     -8321.9727      0             -8319.5151      122524.7     \n",
            "      2740   295.22819     -8321.9311      0             -8319.5269      123444.39    \n",
            "      2750   270.64587     -8321.7401      0             -8319.5361      124289.75    \n",
            "      2760   298.97003     -8321.9802      0             -8319.5455      123425.52    \n",
            "      2770   327.46724     -8322.2243      0             -8319.5576      121749.5     \n",
            "      2780   267.40305     -8321.7513      0             -8319.5738      121544.93    \n",
            "      2790   205.98358     -8321.2631      0             -8319.5857      122035.4     \n",
            "      2800   234.33993     -8321.4966      0             -8319.5883      121499.25    \n",
            "      2810   259.37871     -8321.6938      0             -8319.5816      121299.79    \n",
            "      2820   264.66562     -8321.723       0             -8319.5677      122691.62    \n",
            "      2830   303.19761     -8322.0066      0             -8319.5375      124588.53    \n",
            "      2840   347.0044      -8322.3279      0             -8319.5021      125163.17    \n",
            "      2850   341.10355     -8322.2451      0             -8319.4674      124503.08    \n",
            "      2860   304.21214     -8321.9127      0             -8319.4354      123888.57    \n",
            "      2870   289.23306     -8321.7578      0             -8319.4024      123646.94    \n",
            "      2880   300.13825     -8321.8105      0             -8319.3664      123118.82    \n",
            "      2890   299.63142     -8321.7599      0             -8319.3198      122182.5     \n",
            "      2900   295.08566     -8321.6802      0             -8319.2772      121193.35    \n",
            "      2910   275.38357     -8321.4776      0             -8319.2351      121179.65    \n",
            "      2920   235.29409     -8321.1064      0             -8319.1903      122577.37    \n",
            "      2930   281.34854     -8321.4279      0             -8319.1367      123304.07    \n",
            "      2940   386.30488     -8322.2096      0             -8319.0638      122536.44    \n",
            "      2950   352.47308     -8321.871       0             -8319.0007      122291.52    \n",
            "      2960   246.77133     -8320.9699      0             -8318.9603      122816.24    \n",
            "      2970   244.92331     -8320.9144      0             -8318.9199      123043.6     \n",
            "      2980   334.38513     -8321.5929      0             -8318.8699      122914.45    \n",
            "      2990   384.97711     -8321.9478      0             -8318.8128      123143.15    \n",
            "      3000   324.59927     -8321.4166      0             -8318.7733      124504.86    \n",
            "      3010   318.29744     -8321.34        0             -8318.748       125071.65    \n",
            "      3020   372.05568     -8321.7572      0             -8318.7274      124656.43    \n",
            "      3030   393.75051     -8321.9161      0             -8318.7096      124442.83    \n",
            "      3040   357.7837      -8321.6291      0             -8318.7156      124667.93    \n",
            "      3050   312.23458     -8321.2753      0             -8318.7327      124195.44    \n",
            "      3060   309.86501     -8321.2689      0             -8318.7456      122425.61    \n",
            "      3070   321.63488     -8321.3877      0             -8318.7685      121126.41    \n",
            "      3080   312.5167      -8321.3378      0             -8318.7929      121436.91    \n",
            "      3090   290.67566     -8321.1843      0             -8318.8172      122246.71    \n",
            "      3100   267.27971     -8321.0123      0             -8318.8357      122810.38    \n",
            "      3110   298.13224     -8321.2848      0             -8318.857       122876.47    \n",
            "      3120   352.54071     -8321.7489      0             -8318.878       123283.51    \n",
            "      3130   363.29323     -8321.8757      0             -8318.9173      124005.87    \n",
            "      3140   336.68704     -8321.7092      0             -8318.9674      124129.58    \n",
            "      3150   310.81671     -8321.552       0             -8319.0209      123924.61    \n",
            "      3160   298.17094     -8321.5045      0             -8319.0764      124108.24    \n",
            "      3170   291.20982     -8321.5019      0             -8319.1304      124631.48    \n",
            "      3180   319.56227     -8321.7883      0             -8319.1859      124725.71    \n",
            "      3190   357.58689     -8322.1639      0             -8319.2519      124339.26    \n",
            "      3200   346.99425     -8322.1555      0             -8319.3298      123690.33    \n",
            "      3210   278.26672     -8321.6668      0             -8319.4008      123476.53    \n",
            "      3220   248.80081     -8321.4868      0             -8319.4607      123185.09    \n",
            "      3230   277.35296     -8321.7603      0             -8319.5017      122175.48    \n",
            "      3240   282.31859     -8321.8399      0             -8319.5409      121228.26    \n",
            "      3250   242.84341     -8321.5437      0             -8319.5662      121417.78    \n",
            "      3260   212.5024      -8321.3113      0             -8319.5808      122455.88    \n",
            "      3270   261.24678     -8321.7104      0             -8319.583       123063.32    \n",
            "      3280   318.29613     -8322.1713      0             -8319.5792      123617.25    \n",
            "      3290   320.26144     -8322.192       0             -8319.584       124605.05    \n",
            "      3300   293.47275     -8321.9725      0             -8319.5827      125307.12    \n",
            "      3310   285.11236     -8321.9031      0             -8319.5813      125174.59    \n",
            "      3320   315.78907     -8322.1556      0             -8319.5841      124397.86    \n",
            "      3330   325.41445     -8322.2337      0             -8319.5837      123940.47    \n",
            "      3340   269.40341     -8321.7822      0             -8319.5883      123939.12    \n",
            "      3350   232.57496     -8321.4814      0             -8319.5874      123374.55    \n",
            "      3360   273.08123     -8321.8084      0             -8319.5846      122378.39    \n",
            "      3370   315.97941     -8322.1433      0             -8319.5701      122488.33    \n",
            "      3380   291.24604     -8321.929       0             -8319.5573      123378.04    \n",
            "      3390   249.49097     -8321.5773      0             -8319.5456      123305.81    \n",
            "      3400   255.45963     -8321.6111      0             -8319.5308      122142.47    \n",
            "      3410   277.98269     -8321.7721      0             -8319.5083      121435.81    \n",
            "      3420   272.6636      -8321.7021      0             -8319.4817      122124.41    \n",
            "      3430   271.33796     -8321.666       0             -8319.4564      122975.67    \n",
            "      3440   286.68244     -8321.757       0             -8319.4225      123087.94    \n",
            "      3450   288.81787     -8321.7433      0             -8319.3914      123340.39    \n",
            "      3460   291.60162     -8321.731       0             -8319.3564      124390.2     \n",
            "      3470   328.37022     -8321.9997      0             -8319.3257      125353.07    \n",
            "      3480   365.17301     -8322.2637      0             -8319.29        125274.63    \n",
            "      3490   336.06559     -8322.0033      0             -8319.2666      124558.55    \n",
            "      3500   309.90304     -8321.782       0             -8319.2583      123619.54    \n",
            "      3510   295.78584     -8321.6624      0             -8319.2537      123561.6     \n",
            "      3520   261.6243      -8321.3748      0             -8319.2442      123997.23    \n",
            "      3530   279.53715     -8321.5061      0             -8319.2297      122774.22    \n",
            "      3540   306.94182     -8321.7115      0             -8319.212       121232.23    \n",
            "      3550   282.39486     -8321.4975      0             -8319.1979      121504.54    \n",
            "      3560   250.29759     -8321.2195      0             -8319.1812      122649.19    \n",
            "      3570   279.85718     -8321.4407      0             -8319.1617      122649.28    \n",
            "      3580   352.39019     -8322.0043      0             -8319.1346      121836.14    \n",
            "      3590   324.76267     -8321.758       0             -8319.1133      122470.27    \n",
            "      3600   247.14983     -8321.1122      0             -8319.0996      123948.61    \n",
            "      3610   304.3502      -8321.5539      0             -8319.0755      123899.82    \n",
            "      3620   372.68242     -8322.0878      0             -8319.0529      123969.76    \n",
            "      3630   350.52951     -8321.8969      0             -8319.0424      125211.18    \n",
            "      3640   317.62623     -8321.628       0             -8319.0414      126032.83    \n",
            "      3650   331.06066     -8321.7392      0             -8319.0433      125376.72    \n",
            "      3660   358.15707     -8321.9695      0             -8319.0529      124009.55    \n",
            "      3670   332.99562     -8321.7817      0             -8319.07        123535.73    \n",
            "      3680   292.02462     -8321.467       0             -8319.0889      123251.48    \n",
            "      3690   274.40127     -8321.3471      0             -8319.1126      122378.68    \n",
            "      3700   248.47652     -8321.1593      0             -8319.1358      121737.17    \n",
            "      3710   247.63745     -8321.1625      0             -8319.1458      121540.77    \n",
            "      3720   300.54758     -8321.6064      0             -8319.159       121959.15    \n",
            "      3730   330.50624     -8321.8569      0             -8319.1655      123085.36    \n",
            "      3740   292.41393     -8321.5677      0             -8319.1865      124061.31    \n",
            "      3750   290.55058     -8321.5821      0             -8319.2161      123865.59    \n",
            "      3760   336.08613     -8321.994       0             -8319.2571      123557.65    \n",
            "      3770   325.75695     -8321.975       0             -8319.3222      124273.58    \n",
            "      3780   292.26591     -8321.7837      0             -8319.4036      124714.03    \n",
            "      3790   305.19849     -8321.9792      0             -8319.4939      123893.17    \n",
            "      3800   304.40269     -8322.0786      0             -8319.5997      123373.62    \n",
            "      3810   255.6368      -8321.7727      0             -8319.6909      124204.93    \n",
            "      3820   245.50297     -8321.7593      0             -8319.76        124539.43    \n",
            "      3830   308.50511     -8322.3426      0             -8319.8303      123073.62    \n",
            "      3840   303.33657     -8322.3677      0             -8319.8975      122095.95    \n",
            "      3850   209.87533     -8321.6536      0             -8319.9445      122756.5     \n",
            "      3860   193.47711     -8321.5476      0             -8319.9721      123035.44    \n",
            "      3870   274.62662     -8322.2244      0             -8319.9881      121951.13    \n",
            "      3880   273.55249     -8322.2332      0             -8320.0056      121475.34    \n",
            "      3890   186.26637     -8321.5262      0             -8320.0093      122533.65    \n",
            "      3900   197.04435     -8321.6117      0             -8320.0071      123294.22    \n",
            "      3910   303.97897     -8322.4608      0             -8319.9854      123385.92    \n",
            "      3920   329.85972     -8322.6478      0             -8319.9616      124052.04    \n",
            "      3930   280.70086     -8322.2286      0             -8319.9427      124484.68    \n",
            "      3940   261.38575     -8322.0492      0             -8319.9206      124041.97    \n",
            "      3950   272.88885     -8322.1159      0             -8319.8937      123666.16    \n",
            "      3960   279.04871     -8322.1344      0             -8319.862       123798.66    \n",
            "      3970   287.05152     -8322.1636      0             -8319.8261      123613.59    \n",
            "      3980   274.33899     -8322.0211      0             -8319.7871      122858.12    \n",
            "      3990   215.87061     -8321.5057      0             -8319.7478      122520.25    \n",
            "      4000   228.74235     -8321.5711      0             -8319.7083      122118.65    \n",
            "      4010   317.03396     -8322.2282      0             -8319.6465      121441.42    \n",
            "      4020   309.57777     -8322.0985      0             -8319.5775      121835.25    \n",
            "      4030   225.0952      -8321.3553      0             -8319.5223      122495.12    \n",
            "      4040   219.12852     -8321.2607      0             -8319.4763      122092.33    \n",
            "      4050   301.11591     -8321.8685      0             -8319.4164      121484.23    \n",
            "      4060   324.40817     -8321.9895      0             -8319.3477      122672.69    \n",
            "      4070   276.3867      -8321.5411      0             -8319.2904      124688.5     \n",
            "      4080   297.29343     -8321.6637      0             -8319.2427      124961.91    \n",
            "      4090   366.62629     -8322.1822      0             -8319.1966      124248.15    \n",
            "      4100   356.79508     -8322.0643      0             -8319.1588      124595.11    \n",
            "      4110   311.79271     -8321.678       0             -8319.1389      125062.13    \n",
            "      4120   326.20413     -8321.7771      0             -8319.1207      123920.8     \n",
            "      4130   323.33081     -8321.7425      0             -8319.1095      122527.65    \n",
            "      4140   279.36284     -8321.3787      0             -8319.1037      122310.78    \n",
            "      4150   267.85778     -8321.2774      0             -8319.0962      122509.37    \n",
            "      4160   296.1345      -8321.4943      0             -8319.0828      121907.69    \n",
            "      4170   299.21221     -8321.5096      0             -8319.073       121498.91    \n",
            "      4180   291.14604     -8321.4267      0             -8319.0557      121908.97    \n",
            "      4190   301.32054     -8321.4943      0             -8319.0406      122638.17    \n",
            "      4200   311.52482     -8321.562       0             -8319.0252      123160.93    \n",
            "      4210   319.7152      -8321.6129      0             -8319.0094      123160.57    \n",
            "      4220   337.41589     -8321.7451      0             -8318.9974      122726.34    \n",
            "      4230   328.93613     -8321.6718      0             -8318.9931      122955.49    \n",
            "      4240   284.8365      -8321.3116      0             -8318.9921      124101.77    \n",
            "      4250   287.45054     -8321.3306      0             -8318.9898      124845.97    \n",
            "      4260   366.84669     -8321.9733      0             -8318.9859      124561.61    \n",
            "      4270   393.72807     -8322.2004      0             -8318.9941      124395.97    \n",
            "      4280   316.43757     -8321.5935      0             -8319.0167      124822.74    \n",
            "      4290   294.51873     -8321.443       0             -8319.0446      124209.62    \n",
            "      4300   347.11818     -8321.9006      0             -8319.0739      122541.97    \n",
            "      4310   332.31071     -8321.8333      0             -8319.1272      121743.89    \n",
            "      4320   249.20502     -8321.1998      0             -8319.1704      121916       \n",
            "      4330   221.14509     -8321.0066      0             -8319.2057      121250.28    \n",
            "      4340   261.29618     -8321.366       0             -8319.2382      120423.23    \n",
            "      4350   296.86094     -8321.6878      0             -8319.2703      121147.73    \n",
            "      4360   299.78261     -8321.75        0             -8319.3087      122868.76    \n",
            "      4370   303.22605     -8321.8193      0             -8319.35        123806.23    \n",
            "      4380   312.05992     -8321.9356      0             -8319.3944      123598.56    \n",
            "      4390   310.55016     -8321.9801      0             -8319.4511      123319.61    \n",
            "      4400   306.36687     -8322.0059      0             -8319.5111      123773.27    \n",
            "      4410   295.22026     -8321.9818      0             -8319.5777      124365.37    \n",
            "      4420   268.12562     -8321.8228      0             -8319.6394      124153.26    \n",
            "      4430   277.43398     -8321.9533      0             -8319.6941      123011.89    \n",
            "      4440   294.00083     -8322.1471      0             -8319.7529      122471.42    \n",
            "      4450   272.33403     -8322.0243      0             -8319.8066      123049.44    \n",
            "      4460   243.54543     -8321.8313      0             -8319.848       123344.69    \n",
            "      4470   260.8459      -8322.0059      0             -8319.8818      122365.52    \n",
            "      4480   279.44551     -8322.1892      0             -8319.9136      121299.26    \n",
            "      4490   230.53898     -8321.8117      0             -8319.9344      121827.51    \n",
            "      4500   202.01909     -8321.5876      0             -8319.9425      122517.74    \n",
            "      4510   260.79906     -8322.0651      0             -8319.9413      121875.31    \n",
            "      4520   290.34129     -8322.3013      0             -8319.9369      121580.28    \n",
            "      4530   236.86965     -8321.8527      0             -8319.9238      122824.21    \n",
            "      4540   229.5343      -8321.7821      0             -8319.9129      124041.91    \n",
            "      4550   307.4593      -8322.39        0             -8319.8862      123819.11    \n",
            "      4560   329.69951     -8322.5433      0             -8319.8584      123440.43    \n",
            "      4570   273.79089     -8322.0677      0             -8319.8381      123599.96    \n",
            "      4580   245.59409     -8321.8149      0             -8319.8149      123442.32    \n",
            "      4590   263.43833     -8321.9378      0             -8319.7925      122840.33    \n",
            "      4600   275.6029      -8322.0052      0             -8319.7609      122413.15    \n",
            "      4610   276.61299     -8321.982       0             -8319.7294      122334.94    \n",
            "      4620   270.6445      -8321.8952      0             -8319.6912      122444.45    \n",
            "      4630   243.99283     -8321.6486      0             -8319.6617      122838.82    \n",
            "      4640   246.58486     -8321.6385      0             -8319.6305      122838.45    \n",
            "      4650   314.2202      -8322.1565      0             -8319.5977      121997.38    \n",
            "      4660   322.31334     -8322.189       0             -8319.5643      121869.23    \n",
            "      4670   246.04027     -8321.5446      0             -8319.541       122532.63    \n",
            "      4680   224.00339     -8321.3453      0             -8319.5212      122474.44    \n",
            "      4690   283.00058     -8321.8024      0             -8319.4978      122117.12    \n",
            "      4700   309.32811     -8321.9916      0             -8319.4726      122842.01    \n",
            "      4710   279.16516     -8321.723       0             -8319.4496      124247.98    \n",
            "      4720   291.30758     -8321.8041      0             -8319.4318      124700.04    \n",
            "      4730   350.24408     -8322.2631      0             -8319.4109      123990.01    \n",
            "      4740   337.63561     -8322.1553      0             -8319.4058      124042.59    \n",
            "      4750   290.80477     -8321.7682      0             -8319.4         124261.7     \n",
            "      4760   284.87996     -8321.7147      0             -8319.3948      123186.69    \n",
            "      4770   280.92896     -8321.6803      0             -8319.3926      121811.26    \n",
            "      4780   250.10553     -8321.4219      0             -8319.3852      121970.33    \n",
            "      4790   249.17736     -8321.4002      0             -8319.3711      122898.73    \n",
            "      4800   309.40956     -8321.8697      0             -8319.35        122978.19    \n",
            "      4810   324.75103     -8321.9708      0             -8319.3263      122998.77    \n",
            "      4820   281.43914     -8321.5971      0             -8319.3052      123482.47    \n",
            "      4830   281.87911     -8321.5843      0             -8319.2888      123885.14    \n",
            "      4840   322.35645     -8321.8902      0             -8319.2652      123973.65    \n",
            "      4850   327.79043     -8321.9071      0             -8319.2378      123708.65    \n",
            "      4860   303.28645     -8321.6954      0             -8319.2257      123183.76    \n",
            "      4870   279.2748      -8321.481       0             -8319.2068      122939.23    \n",
            "      4880   273.73261     -8321.4131      0             -8319.184       123190.65    \n",
            "      4890   300.67271     -8321.6127      0             -8319.1642      123737.5     \n",
            "      4900   358.34497     -8322.0538      0             -8319.1357      123758.27    \n",
            "      4910   366.71513     -8322.0983      0             -8319.112       123529.16    \n",
            "      4920   289.54979     -8321.4604      0             -8319.1025      123899.58    \n",
            "      4930   251.70109     -8321.1406      0             -8319.0909      123933.28    \n",
            "      4940   338.34585     -8321.8267      0             -8319.0714      122602.17    \n",
            "      4950   360.91563     -8321.995       0             -8319.0559      122164.95    \n",
            "      4960   270.87327     -8321.2573      0             -8319.0514      123162.42    \n",
            "      4970   235.73387     -8320.9573      0             -8319.0376      123416.39    \n",
            "      4980   320.61768     -8321.6296      0             -8319.0186      122444.68    \n",
            "      4990   364.0137      -8321.9562      0             -8318.9919      122809       \n",
            "      5000   307.34411     -8321.4764      0             -8318.9735      124562.24    \n",
            "Loop time of 36.1998 on 1 procs for 5000 steps with 64 atoms\n",
            "\n",
            "Performance: 11.934 ns/day, 2.011 hours/ns, 138.122 timesteps/s, 8.840 katom-step/s\n",
            "99.0% CPU use with 1 MPI tasks x 1 OpenMP threads\n",
            "\n",
            "MPI task timing breakdown:\n",
            "Section |  min time  |  avg time  |  max time  |%varavg| %total\n",
            "---------------------------------------------------------------\n",
            "Pair    | 35.988     | 35.988     | 35.988     |   0.0 | 99.42\n",
            "Neigh   | 0          | 0          | 0          |   0.0 |  0.00\n",
            "Comm    | 0.025789   | 0.025789   | 0.025789   |   0.0 |  0.07\n",
            "Output  | 0.021504   | 0.021504   | 0.021504   |   0.0 |  0.06\n",
            "Modify  | 0.15377    | 0.15377    | 0.15377    |   0.0 |  0.42\n",
            "Other   |            | 0.01059    |            |       |  0.03\n",
            "\n",
            "Nlocal:             64 ave          64 max          64 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Nghost:            801 ave         801 max         801 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Neighs:           1390 ave        1390 max        1390 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "FullNghs:         2944 ave        2944 max        2944 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "Total # of neighbors = 2944\n",
            "Ave neighs/atom = 46\n",
            "Neighbor list builds = 0\n",
            "Dangerous builds = 0\n",
            "Total wall time: 0:00:38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['font.size'] = 30\n",
        "\n",
        "def parse_lammps_rdf(rdffile):\n",
        "    \"\"\"\n",
        "    Parse the RDF file written by LAMMPS\n",
        "\n",
        "    copied from Boris' class code: https://github.com/bkoz37/labutil\n",
        "    \"\"\"\n",
        "    with open(rdffile, 'r') as rdfout:\n",
        "        rdfs = []\n",
        "        buffer = []\n",
        "        for line in rdfout:\n",
        "            values = line.split()\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            elif len(values) == 2:\n",
        "                nbins = values[1]\n",
        "            else:\n",
        "                buffer.append([float(values[1]), float(values[2])])\n",
        "                if len(buffer) == int(nbins):\n",
        "                    frame = np.transpose(np.array(buffer))\n",
        "                    rdfs.append(frame)\n",
        "                    buffer = []\n",
        "    return rdfs\n",
        "\n",
        "rdf = parse_lammps_rdf('./lammps_run/si.rdf')\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "plt.plot(rdf[0][0], rdf[0][1], 'b', linewidth=5, label=\"Allegro, T=300K\")\n",
        "plt.xlabel('r [$\\AA$]')\n",
        "plt.ylabel('g(r)')\n",
        "plt.title(\"Des bond length: {:.3f}$\\AA$\".format(rdf[0][0][np.argmax(rdf[0][1])]))\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "7aKj3yzhomge",
        "outputId": "e0e9c04d-4aae-4488-81cd-faa5000279ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './lammps_run/si.rdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e9c3aefe042d>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mrdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_lammps_rdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./lammps_run/si.rdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e9c3aefe042d>\u001b[0m in \u001b[0;36mparse_lammps_rdf\u001b[0;34m(rdffile)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcopied\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBoris\u001b[0m\u001b[0;31m'\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgithub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbkoz37\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlabutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdffile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrdfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mrdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lammps_run/si.rdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Line Done"
      ],
      "metadata": {
        "id": "6bs71WeFor0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have SI-SI Bond matching."
      ],
      "metadata": {
        "id": "wkn7Nn-Yo3pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhance modal:\n",
        "from ase.visualize import view"
      ],
      "metadata": {
        "id": "XFJ-li4cb6VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#si_traj = read('./Si_data/sitraj.xyz', index=\"::\")\n",
        "si_traj = read('./Si_data/DES.extxyz', index=\"::\")"
      ],
      "metadata": {
        "id": "rzbb9J6WmR9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "si_cell_prod = np.array([12.4138, 12.4138, 12.4138])\n",
        "for i in range(len(si_traj)):\n",
        "  si_traj[i].set_cell(si_cell_prod)\n",
        "  si_traj[i].set_pbc([True, True, True])\n",
        "  si_traj[i].wrap()"
      ],
      "metadata": {
        "id": "F7bJ5b7BnQ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nglview\n",
        "!pip install ipywidgets==7.7.2 nglview\n",
        "#!pip install pytraj\n",
        "#!jupyter nbextension enable --py widgetsnbextension\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3kdzwhamhIk",
        "outputId": "a246465f-aa8b-48b2-ce22-5639e1434e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets==7.7.2 in /usr/local/lib/python3.10/dist-packages (7.7.2)\n",
            "Requirement already satisfied: nglview in /usr/local/lib/python3.10/dist-packages (3.0.8)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (6.29.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (1.1.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nglview) (1.26.4)\n",
            "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (8.6.3)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (7.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.7.2) (0.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.7.2) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets==7.7.2) (4.3.6)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.14.2)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.27.3)\n",
            "Requirement already satisfied: jupyterlab<4.4,>=4.3.2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (4.3.2)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.2.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.7.2) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.7.2) (0.2.13)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (23.1.0)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.1.4)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.10.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.5.3)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (7.16.4)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (5.10.4)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (7.7.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.21.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.8.0)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab<4.4,>=4.3.2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.0.4)\n",
            "Requirement already satisfied: httpx~=0.28.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab<4.4,>=4.3.2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.28.0)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab<4.4,>=4.3.2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.2.5)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab<4.4,>=4.3.2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.2.1)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.16.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.10.0)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (4.23.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.7.2) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.2.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from async-lru>=1.0.0->jupyterlab<4.4,>=4.3.2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (4.12.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx~=0.28.0->jupyterlab<4.4,>=4.3.2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx~=0.28.0->jupyterlab<4.4,>=4.3.2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx~=0.28.0->jupyterlab<4.4,>=4.3.2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.22.3)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (6.0.2)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.1.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.10.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.21.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.2.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.5.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.0.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (24.11.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.22)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.9.0.20241206)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart Run-time for Nglview to work"
      ],
      "metadata": {
        "id": "Vs8gZotd92Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q condacolab\n",
        "#import condacolab\n",
        "#condacolab.install()\n"
      ],
      "metadata": {
        "id": "5VmvwWCusUHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import condacolab\n",
        "#condacolab.check()"
      ],
      "metadata": {
        "id": "_EgtW7t5sX18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "import nglview as nv\n",
        "view(si_traj, viewer='ngl')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517,
          "referenced_widgets": [
            "c982debe7a4f4e768e1959b7851e48d0",
            "985c9ee4208341e894c815a69e5b977e",
            "c8ccef826d924dd4b458096bfd3b5095",
            "ede6d765bfb64c05a8625235e38decc7",
            "6a23a859c344431d8234d7aa56b319ad",
            "003ea2d7ccde44159366b1c3ab8bea00",
            "b87621e85c9642d7a4f8d5d3fb22728c",
            "a3b366f3407049f7b59d01caf3a7d349",
            "7044760e402f48fbb8b49d2994d951b3",
            "4480bc7963d74944b5f53bedb0972666",
            "736db268196343089bfa653c8d57d8d2",
            "6234838457484b9f9e59efe247903ef6",
            "af126e0fd93646f0b3a35b852ffe523e",
            "f1c2d8453c1d472796eb327025b335b9",
            "700587f3288042febe8d695b059b33ed",
            "8f078a272b1e407ab670297dcce9e8c9",
            "d75c118c25184eabb012d6015a5441b4",
            "90e416e72b26470fbddc49ec96168801",
            "8434f01fd11b43c6adb35567cccae706",
            "b38302e9432b477fba56af3730aece0b",
            "11407f19a7dc45f88408d54b524cb76c",
            "02decbe630c240ac865a59ca03c5be3e",
            "f6dba43537174e2c947f239a5edb252e",
            "9e01913f2d9e4891b9b8c05814f3136a",
            "3f31ecaec7574d27b6c882f3a6295a9b",
            "69167a67f20647d2b8fe2b86557e190e",
            "384b8de53a15417997f536a19449ec67",
            "b51404b09ede4bb6905e119343806523",
            "7e0ab632de7b41948ae792cd7a30464f",
            "c5c60d1c2e0b47ec89653f7f241d1131",
            "e699d6f5565e40e39df900d76b123d7e"
          ]
        },
        "id": "DoH2ueMa4NzW",
        "outputId": "3e342819-a91c-44ee-e881-c934037a076f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c982debe7a4f4e768e1959b7851e48d0"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(NGLWidget(max_frame=109), VBox(children=(Dropdown(description='Show', options=('All', 'Si'), va…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e0ab632de7b41948ae792cd7a30464f"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "Vovoan395GyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install MDAnalysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNAsY6jxs5Ks",
        "outputId": "f5f2058d-73bb-4b9b-e400-005b7f984574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MDAnalysis\n",
            "  Using cached MDAnalysis-2.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (108 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.10/dist-packages (from MDAnalysis) (1.26.4)\n",
            "Collecting GridDataFormats>=0.4.0 (from MDAnalysis)\n",
            "  Downloading GridDataFormats-1.0.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting mmtf-python>=1.0.0 (from MDAnalysis)\n",
            "  Downloading mmtf_python-1.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.10/dist-packages (from MDAnalysis) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from MDAnalysis) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from MDAnalysis) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from MDAnalysis) (4.66.6)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from MDAnalysis) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from MDAnalysis) (24.2)\n",
            "Collecting fasteners (from MDAnalysis)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting mda-xdrlib (from MDAnalysis)\n",
            "  Downloading mda_xdrlib-0.2.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting waterdynamics (from MDAnalysis)\n",
            "  Downloading waterdynamics-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting pathsimanalysis (from MDAnalysis)\n",
            "  Downloading pathsimanalysis-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting mdahole2 (from MDAnalysis)\n",
            "  Downloading mdahole2-0.5.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting mrcfile (from GridDataFormats>=0.4.0->MDAnalysis)\n",
            "  Downloading mrcfile-1.5.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5.1->MDAnalysis) (2.8.2)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mmtf-python>=1.0.0->MDAnalysis) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=1.5.1->MDAnalysis) (1.16.0)\n",
            "Downloading MDAnalysis-2.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GridDataFormats-1.0.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmtf_python-1.1.3-py2.py3-none-any.whl (25 kB)\n",
            "Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading mda_xdrlib-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading mdahole2-0.5.0-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathsimanalysis-1.2.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading waterdynamics-1.2.0-py3-none-any.whl (35 kB)\n",
            "Downloading mrcfile-1.5.3-py2.py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mrcfile, mmtf-python, mda-xdrlib, fasteners, GridDataFormats, waterdynamics, pathsimanalysis, mdahole2, MDAnalysis\n",
            "Successfully installed GridDataFormats-1.0.2 MDAnalysis-2.8.0 fasteners-0.19 mda-xdrlib-0.2.0 mdahole2-0.5.0 mmtf-python-1.1.3 mrcfile-1.5.3 pathsimanalysis-1.2.0 waterdynamics-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import rdf"
      ],
      "metadata": {
        "id": "6SJCOKJf-gNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = mda.coordinates.XYZ.XYZReader('./Si_data/sitraj.xyz')\n",
        "topology = mda.topology.XYZParser.XYZParser(\"./Si_data/sitraj.xyz\")\n",
        "\n",
        "u = mda.Universe('./Si_data/sitraj.xyz')\n",
        "u.dimensions = [si_cell_prod[0], si_cell_prod[1], si_cell_prod[2],90,90,90]"
      ],
      "metadata": {
        "id": "q2EL7PQj-ste"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al ./lammps_run/\n",
        "!cat ./lammps_run/si.rdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMonVramArPu",
        "outputId": "7e931881-dfdf-4d01-ac23-4926bd1e0715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 84\n",
            "drwxr-xr-x 2 root root  4096 Dec  9 05:47 .\n",
            "drwxr-xr-x 1 root root  4096 Dec  9 05:47 ..\n",
            "-rw-r--r-- 1 root root 46659 Dec  9 05:48 log.lammps\n",
            "-rw-r--r-- 1 root root 20068 Dec  9 05:48 si.rdf\n",
            "-rw-r--r-- 1 root root   722 Dec  9 05:47 si_rdf.in\n",
            "# Time-averaged data for fix 2\n",
            "# TimeStep Number-of-rows\n",
            "# Row c_rdfall[1] c_rdfall[2] c_rdfall[3]\n",
            "5000 1000\n",
            "1 0.0025 0 0\n",
            "2 0.0075 0 0\n",
            "3 0.0125 0 0\n",
            "4 0.0175 0 0\n",
            "5 0.0225 0 0\n",
            "6 0.0275 0 0\n",
            "7 0.0325 0 0\n",
            "8 0.0375 0 0\n",
            "9 0.0425 0 0\n",
            "10 0.0475 0 0\n",
            "11 0.0525 0 0\n",
            "12 0.0575 0 0\n",
            "13 0.0625 0 0\n",
            "14 0.0675 0 0\n",
            "15 0.0725 0 0\n",
            "16 0.0775 0 0\n",
            "17 0.0825 0 0\n",
            "18 0.0875 0 0\n",
            "19 0.0925 0 0\n",
            "20 0.0975 0 0\n",
            "21 0.1025 0 0\n",
            "22 0.1075 0 0\n",
            "23 0.1125 0 0\n",
            "24 0.1175 0 0\n",
            "25 0.1225 0 0\n",
            "26 0.1275 0 0\n",
            "27 0.1325 0 0\n",
            "28 0.1375 0 0\n",
            "29 0.1425 0 0\n",
            "30 0.1475 0 0\n",
            "31 0.1525 0 0\n",
            "32 0.1575 0 0\n",
            "33 0.1625 0 0\n",
            "34 0.1675 0 0\n",
            "35 0.1725 0 0\n",
            "36 0.1775 0 0\n",
            "37 0.1825 0 0\n",
            "38 0.1875 0 0\n",
            "39 0.1925 0 0\n",
            "40 0.1975 0 0\n",
            "41 0.2025 0 0\n",
            "42 0.2075 0 0\n",
            "43 0.2125 0 0\n",
            "44 0.2175 0 0\n",
            "45 0.2225 0 0\n",
            "46 0.2275 0 0\n",
            "47 0.2325 0 0\n",
            "48 0.2375 0 0\n",
            "49 0.2425 0 0\n",
            "50 0.2475 0 0\n",
            "51 0.2525 0 0\n",
            "52 0.2575 0 0\n",
            "53 0.2625 0 0\n",
            "54 0.2675 0 0\n",
            "55 0.2725 0 0\n",
            "56 0.2775 0 0\n",
            "57 0.2825 0 0\n",
            "58 0.2875 0 0\n",
            "59 0.2925 0 0\n",
            "60 0.2975 0 0\n",
            "61 0.3025 0 0\n",
            "62 0.3075 0 0\n",
            "63 0.3125 0 0\n",
            "64 0.3175 0 0\n",
            "65 0.3225 0 0\n",
            "66 0.3275 0 0\n",
            "67 0.3325 0 0\n",
            "68 0.3375 0 0\n",
            "69 0.3425 0 0\n",
            "70 0.3475 0 0\n",
            "71 0.3525 0 0\n",
            "72 0.3575 0 0\n",
            "73 0.3625 0 0\n",
            "74 0.3675 0 0\n",
            "75 0.3725 0 0\n",
            "76 0.3775 0 0\n",
            "77 0.3825 0 0\n",
            "78 0.3875 0 0\n",
            "79 0.3925 0 0\n",
            "80 0.3975 0 0\n",
            "81 0.4025 0 0\n",
            "82 0.4075 0 0\n",
            "83 0.4125 0 0\n",
            "84 0.4175 0 0\n",
            "85 0.4225 0 0\n",
            "86 0.4275 0 0\n",
            "87 0.4325 0 0\n",
            "88 0.4375 0 0\n",
            "89 0.4425 0 0\n",
            "90 0.4475 0 0\n",
            "91 0.4525 0 0\n",
            "92 0.4575 0 0\n",
            "93 0.4625 0 0\n",
            "94 0.4675 0 0\n",
            "95 0.4725 0 0\n",
            "96 0.4775 0 0\n",
            "97 0.4825 0 0\n",
            "98 0.4875 0 0\n",
            "99 0.4925 0 0\n",
            "100 0.4975 0 0\n",
            "101 0.5025 0 0\n",
            "102 0.5075 0 0\n",
            "103 0.5125 0 0\n",
            "104 0.5175 0 0\n",
            "105 0.5225 0 0\n",
            "106 0.5275 0 0\n",
            "107 0.5325 0 0\n",
            "108 0.5375 0 0\n",
            "109 0.5425 0 0\n",
            "110 0.5475 0 0\n",
            "111 0.5525 0 0\n",
            "112 0.5575 0 0\n",
            "113 0.5625 0 0\n",
            "114 0.5675 0 0\n",
            "115 0.5725 0 0\n",
            "116 0.5775 0 0\n",
            "117 0.5825 0 0\n",
            "118 0.5875 0 0\n",
            "119 0.5925 0 0\n",
            "120 0.5975 0 0\n",
            "121 0.6025 0 0\n",
            "122 0.6075 0 0\n",
            "123 0.6125 0 0\n",
            "124 0.6175 0 0\n",
            "125 0.6225 0 0\n",
            "126 0.6275 0 0\n",
            "127 0.6325 0 0\n",
            "128 0.6375 0 0\n",
            "129 0.6425 0 0\n",
            "130 0.6475 0 0\n",
            "131 0.6525 0 0\n",
            "132 0.6575 0 0\n",
            "133 0.6625 0 0\n",
            "134 0.6675 0 0\n",
            "135 0.6725 0 0\n",
            "136 0.6775 0 0\n",
            "137 0.6825 0 0\n",
            "138 0.6875 0 0\n",
            "139 0.6925 0 0\n",
            "140 0.6975 0 0\n",
            "141 0.7025 0 0\n",
            "142 0.7075 0 0\n",
            "143 0.7125 0 0\n",
            "144 0.7175 0 0\n",
            "145 0.7225 0 0\n",
            "146 0.7275 0 0\n",
            "147 0.7325 0 0\n",
            "148 0.7375 0 0\n",
            "149 0.7425 0 0\n",
            "150 0.7475 0 0\n",
            "151 0.7525 0 0\n",
            "152 0.7575 0 0\n",
            "153 0.7625 0 0\n",
            "154 0.7675 0 0\n",
            "155 0.7725 0 0\n",
            "156 0.7775 0 0\n",
            "157 0.7825 0 0\n",
            "158 0.7875 0 0\n",
            "159 0.7925 0 0\n",
            "160 0.7975 0 0\n",
            "161 0.8025 0 0\n",
            "162 0.8075 0 0\n",
            "163 0.8125 0 0\n",
            "164 0.8175 0 0\n",
            "165 0.8225 0 0\n",
            "166 0.8275 0 0\n",
            "167 0.8325 0 0\n",
            "168 0.8375 0 0\n",
            "169 0.8425 0 0\n",
            "170 0.8475 0 0\n",
            "171 0.8525 0 0\n",
            "172 0.8575 0 0\n",
            "173 0.8625 0 0\n",
            "174 0.8675 0 0\n",
            "175 0.8725 0 0\n",
            "176 0.8775 0 0\n",
            "177 0.8825 0 0\n",
            "178 0.8875 0 0\n",
            "179 0.8925 0 0\n",
            "180 0.8975 0 0\n",
            "181 0.9025 0 0\n",
            "182 0.9075 0 0\n",
            "183 0.9125 0 0\n",
            "184 0.9175 0 0\n",
            "185 0.9225 0 0\n",
            "186 0.9275 0 0\n",
            "187 0.9325 0 0\n",
            "188 0.9375 0 0\n",
            "189 0.9425 0 0\n",
            "190 0.9475 0 0\n",
            "191 0.9525 0 0\n",
            "192 0.9575 0 0\n",
            "193 0.9625 0 0\n",
            "194 0.9675 0 0\n",
            "195 0.9725 0 0\n",
            "196 0.9775 0 0\n",
            "197 0.9825 0 0\n",
            "198 0.9875 0 0\n",
            "199 0.9925 0 0\n",
            "200 0.9975 0 0\n",
            "201 1.0025 0 0\n",
            "202 1.0075 0 0\n",
            "203 1.0125 0 0\n",
            "204 1.0175 0 0\n",
            "205 1.0225 0 0\n",
            "206 1.0275 0 0\n",
            "207 1.0325 0 0\n",
            "208 1.0375 0 0\n",
            "209 1.0425 0 0\n",
            "210 1.0475 0 0\n",
            "211 1.0525 0 0\n",
            "212 1.0575 0 0\n",
            "213 1.0625 0 0\n",
            "214 1.0675 0 0\n",
            "215 1.0725 0 0\n",
            "216 1.0775 0 0\n",
            "217 1.0825 0 0\n",
            "218 1.0875 0 0\n",
            "219 1.0925 0 0\n",
            "220 1.0975 0 0\n",
            "221 1.1025 0 0\n",
            "222 1.1075 0 0\n",
            "223 1.1125 0 0\n",
            "224 1.1175 0 0\n",
            "225 1.1225 0 0\n",
            "226 1.1275 0 0\n",
            "227 1.1325 0 0\n",
            "228 1.1375 0 0\n",
            "229 1.1425 0 0\n",
            "230 1.1475 0 0\n",
            "231 1.1525 0 0\n",
            "232 1.1575 0 0\n",
            "233 1.1625 0 0\n",
            "234 1.1675 0 0\n",
            "235 1.1725 0 0\n",
            "236 1.1775 0 0\n",
            "237 1.1825 0 0\n",
            "238 1.1875 0 0\n",
            "239 1.1925 0 0\n",
            "240 1.1975 0 0\n",
            "241 1.2025 0 0\n",
            "242 1.2075 0 0\n",
            "243 1.2125 0 0\n",
            "244 1.2175 0 0\n",
            "245 1.2225 0 0\n",
            "246 1.2275 0 0\n",
            "247 1.2325 0 0\n",
            "248 1.2375 0 0\n",
            "249 1.2425 0 0\n",
            "250 1.2475 0 0\n",
            "251 1.2525 0 0\n",
            "252 1.2575 0 0\n",
            "253 1.2625 0 0\n",
            "254 1.2675 0 0\n",
            "255 1.2725 0 0\n",
            "256 1.2775 0 0\n",
            "257 1.2825 0 0\n",
            "258 1.2875 0 0\n",
            "259 1.2925 0 0\n",
            "260 1.2975 0 0\n",
            "261 1.3025 0 0\n",
            "262 1.3075 0 0\n",
            "263 1.3125 0 0\n",
            "264 1.3175 0 0\n",
            "265 1.3225 0 0\n",
            "266 1.3275 0 0\n",
            "267 1.3325 0 0\n",
            "268 1.3375 0 0\n",
            "269 1.3425 0 0\n",
            "270 1.3475 0 0\n",
            "271 1.3525 0 0\n",
            "272 1.3575 0 0\n",
            "273 1.3625 0 0\n",
            "274 1.3675 0 0\n",
            "275 1.3725 0 0\n",
            "276 1.3775 0 0\n",
            "277 1.3825 0 0\n",
            "278 1.3875 0 0\n",
            "279 1.3925 0 0\n",
            "280 1.3975 0 0\n",
            "281 1.4025 0 0\n",
            "282 1.4075 0 0\n",
            "283 1.4125 0 0\n",
            "284 1.4175 0 0\n",
            "285 1.4225 0 0\n",
            "286 1.4275 0 0\n",
            "287 1.4325 0 0\n",
            "288 1.4375 0 0\n",
            "289 1.4425 0 0\n",
            "290 1.4475 0 0\n",
            "291 1.4525 0 0\n",
            "292 1.4575 0 0\n",
            "293 1.4625 0 0\n",
            "294 1.4675 0 0\n",
            "295 1.4725 0 0\n",
            "296 1.4775 0 0\n",
            "297 1.4825 0 0\n",
            "298 1.4875 0 0\n",
            "299 1.4925 0 0\n",
            "300 1.4975 0 0\n",
            "301 1.5025 0 0\n",
            "302 1.5075 0 0\n",
            "303 1.5125 0 0\n",
            "304 1.5175 0 0\n",
            "305 1.5225 0 0\n",
            "306 1.5275 0 0\n",
            "307 1.5325 0 0\n",
            "308 1.5375 0 0\n",
            "309 1.5425 0 0\n",
            "310 1.5475 0 0\n",
            "311 1.5525 0 0\n",
            "312 1.5575 0 0\n",
            "313 1.5625 0 0\n",
            "314 1.5675 0 0\n",
            "315 1.5725 0 0\n",
            "316 1.5775 0 0\n",
            "317 1.5825 0 0\n",
            "318 1.5875 0 0\n",
            "319 1.5925 0 0\n",
            "320 1.5975 0 0\n",
            "321 1.6025 0 0\n",
            "322 1.6075 0 0\n",
            "323 1.6125 0 0\n",
            "324 1.6175 0 0\n",
            "325 1.6225 0 0\n",
            "326 1.6275 0 0\n",
            "327 1.6325 0 0\n",
            "328 1.6375 0 0\n",
            "329 1.6425 0 0\n",
            "330 1.6475 0 0\n",
            "331 1.6525 0 0\n",
            "332 1.6575 0 0\n",
            "333 1.6625 0 0\n",
            "334 1.6675 0 0\n",
            "335 1.6725 0 0\n",
            "336 1.6775 0 0\n",
            "337 1.6825 0 0\n",
            "338 1.6875 0 0\n",
            "339 1.6925 0 0\n",
            "340 1.6975 0 0\n",
            "341 1.7025 0 0\n",
            "342 1.7075 0 0\n",
            "343 1.7125 0 0\n",
            "344 1.7175 0 0\n",
            "345 1.7225 0 0\n",
            "346 1.7275 0 0\n",
            "347 1.7325 0 0\n",
            "348 1.7375 0 0\n",
            "349 1.7425 0 0\n",
            "350 1.7475 0 0\n",
            "351 1.7525 0 0\n",
            "352 1.7575 0 0\n",
            "353 1.7625 0 0\n",
            "354 1.7675 0 0\n",
            "355 1.7725 0 0\n",
            "356 1.7775 0 0\n",
            "357 1.7825 0 0\n",
            "358 1.7875 0 0\n",
            "359 1.7925 0 0\n",
            "360 1.7975 0 0\n",
            "361 1.8025 0 0\n",
            "362 1.8075 0 0\n",
            "363 1.8125 0 0\n",
            "364 1.8175 0 0\n",
            "365 1.8225 0 0\n",
            "366 1.8275 0 0\n",
            "367 1.8325 0 0\n",
            "368 1.8375 0 0\n",
            "369 1.8425 0 0\n",
            "370 1.8475 0 0\n",
            "371 1.8525 0 0\n",
            "372 1.8575 0 0\n",
            "373 1.8625 0 0\n",
            "374 1.8675 0 0\n",
            "375 1.8725 0 0\n",
            "376 1.8775 0 0\n",
            "377 1.8825 0 0\n",
            "378 1.8875 0 0\n",
            "379 1.8925 0 0\n",
            "380 1.8975 0 0\n",
            "381 1.9025 0 0\n",
            "382 1.9075 0 0\n",
            "383 1.9125 0 0\n",
            "384 1.9175 0 0\n",
            "385 1.9225 0 0\n",
            "386 1.9275 0 0\n",
            "387 1.9325 0 0\n",
            "388 1.9375 0 0\n",
            "389 1.9425 0 0\n",
            "390 1.9475 0 0\n",
            "391 1.9525 0 0\n",
            "392 1.9575 0 0\n",
            "393 1.9625 0 0\n",
            "394 1.9675 0 0\n",
            "395 1.9725 0 0\n",
            "396 1.9775 0 0\n",
            "397 1.9825 0 0\n",
            "398 1.9875 0 0\n",
            "399 1.9925 0 0\n",
            "400 1.9975 0 0\n",
            "401 2.0025 0 0\n",
            "402 2.0075 0 0\n",
            "403 2.0125 0 0\n",
            "404 2.0175 0 0\n",
            "405 2.0225 0 0\n",
            "406 2.0275 0 0\n",
            "407 2.0325 0 0\n",
            "408 2.0375 0 0\n",
            "409 2.0425 0 0\n",
            "410 2.0475 0 0\n",
            "411 2.0525 0 0\n",
            "412 2.0575 0 0\n",
            "413 2.0625 0 0\n",
            "414 2.0675 0 0\n",
            "415 2.0725 0 0\n",
            "416 2.0775 0 0\n",
            "417 2.0825 0 0\n",
            "418 2.0875 0 0\n",
            "419 2.0925 0 0\n",
            "420 2.0975 0 0\n",
            "421 2.1025 0 0\n",
            "422 2.1075 0 0\n",
            "423 2.1125 0 0\n",
            "424 2.1175 0 0\n",
            "425 2.1225 0 0\n",
            "426 2.1275 0 0\n",
            "427 2.1325 0 0\n",
            "428 2.1375 0 0\n",
            "429 2.1425 0 0\n",
            "430 2.1475 0.00438755 6.25e-05\n",
            "431 2.1525 0.00174688 8.75e-05\n",
            "432 2.1575 0.000869395 0.0001\n",
            "433 2.1625 0.000865379 0.0001125\n",
            "434 2.1675 0.00258417 0.00015\n",
            "435 2.1725 0.00514459 0.000225\n",
            "436 2.1775 0.0230444 0.0005625\n",
            "437 2.1825 0.0280365 0.000975\n",
            "438 2.1875 0.0363656 0.0015125\n",
            "439 2.1925 0.0614557 0.002425\n",
            "440 2.1975 0.0670426 0.003425\n",
            "441 2.2025 0.0817548 0.00465\n",
            "442 2.2075 0.111281 0.006325\n",
            "443 2.2125 0.181049 0.0090625\n",
            "444 2.2175 0.217268 0.0123625\n",
            "445 2.2225 0.312967 0.0171375\n",
            "446 2.2275 0.438799 0.0238625\n",
            "447 2.2325 0.55863 0.0324625\n",
            "448 2.2375 0.736395 0.04385\n",
            "449 2.2425 0.956832 0.0587125\n",
            "450 2.2475 1.16809 0.0769375\n",
            "451 2.2525 1.35673 0.0982\n",
            "452 2.2575 1.62945 0.12385\n",
            "453 2.2625 1.9622 0.154875\n",
            "454 2.2675 2.33372 0.191938\n",
            "455 2.2725 2.68315 0.234737\n",
            "456 2.2775 3.14262 0.285087\n",
            "457 2.2825 3.61513 0.343263\n",
            "458 2.2875 4.18402 0.410888\n",
            "459 2.2925 4.94274 0.491125\n",
            "460 2.2975 5.31761 0.577825\n",
            "461 2.3025 5.74034 0.671825\n",
            "462 2.3075 6.24067 0.774463\n",
            "463 2.3125 6.8615 0.8878\n",
            "464 2.3175 7.35711 1.00985\n",
            "465 2.3225 7.81463 1.14005\n",
            "466 2.3275 8.29355 1.27883\n",
            "467 2.3325 8.45366 1.42089\n",
            "468 2.3375 8.69528 1.56764\n",
            "469 2.3425 8.78284 1.7165\n",
            "470 2.3475 8.92465 1.86841\n",
            "471 2.3525 8.90138 2.02058\n",
            "472 2.3575 8.80468 2.17172\n",
            "473 2.3625 8.79428 2.32334\n",
            "474 2.3675 8.70807 2.4741\n",
            "475 2.3725 8.23716 2.61731\n",
            "476 2.3775 7.87464 2.7548\n",
            "477 2.3825 7.34756 2.88362\n",
            "478 2.3875 6.87238 3.00462\n",
            "479 2.3925 6.08721 3.11225\n",
            "480 2.3975 5.5852 3.21141\n",
            "481 2.4025 5.27242 3.30541\n",
            "482 2.4075 4.89585 3.39306\n",
            "483 2.4125 4.43057 3.47271\n",
            "484 2.4175 4.01272 3.54515\n",
            "485 2.4225 3.55277 3.60955\n",
            "486 2.4275 3.07459 3.66551\n",
            "487 2.4325 2.68717 3.71462\n",
            "488 2.4375 2.30903 3.757\n",
            "489 2.4425 2.04046 3.7946\n",
            "490 2.4475 1.79298 3.82777\n",
            "491 2.4525 1.5051 3.85574\n",
            "492 2.4575 1.3127 3.88022\n",
            "493 2.4625 1.10783 3.90097\n",
            "494 2.4675 0.991684 3.91962\n",
            "495 2.4725 0.789083 3.93452\n",
            "496 2.4775 0.630962 3.94649\n",
            "497 2.4825 0.513508 3.95626\n",
            "498 2.4875 0.448659 3.96484\n",
            "499 2.4925 0.388235 3.97229\n",
            "500 2.4975 0.337374 3.97879\n",
            "501 2.5025 0.263652 3.98389\n",
            "502 2.5075 0.178286 3.98735\n",
            "503 2.5125 0.139754 3.99008\n",
            "504 2.5175 0.137283 3.99276\n",
            "505 2.5225 0.108756 3.9949\n",
            "506 2.5275 0.0525793 3.99594\n",
            "507 2.5325 0.0662536 3.99725\n",
            "508 2.5375 0.0383386 3.99801\n",
            "509 2.5425 0.0262934 3.99854\n",
            "510 2.5475 0.0230724 3.999\n",
            "511 2.5525 0.0198764 3.9994\n",
            "512 2.5575 0.0105181 3.99961\n",
            "513 2.5625 0.0012326 3.99964\n",
            "514 2.5675 0.0018417 3.99967\n",
            "515 2.5725 0.00244607 3.99973\n",
            "516 2.5775 0.00243658 3.99978\n",
            "517 2.5825 0.00364074 3.99985\n",
            "518 2.5875 0.00120889 3.99987\n",
            "519 2.5925 0.00120424 3.9999\n",
            "520 2.5975 0.00179941 3.99994\n",
            "521 2.6025 0.0029875 4\n",
            "522 2.6075 0 4\n",
            "523 2.6125 0 4\n",
            "524 2.6175 0 4\n",
            "525 2.6225 0 4\n",
            "526 2.6275 0 4\n",
            "527 2.6325 0 4\n",
            "528 2.6375 0 4\n",
            "529 2.6425 0 4\n",
            "530 2.6475 0 4\n",
            "531 2.6525 0 4\n",
            "532 2.6575 0 4\n",
            "533 2.6625 0 4\n",
            "534 2.6675 0 4\n",
            "535 2.6725 0 4\n",
            "536 2.6775 0 4\n",
            "537 2.6825 0 4\n",
            "538 2.6875 0 4\n",
            "539 2.6925 0 4\n",
            "540 2.6975 0 4\n",
            "541 2.7025 0 4\n",
            "542 2.7075 0 4\n",
            "543 2.7125 0 4\n",
            "544 2.7175 0 4\n",
            "545 2.7225 0 4\n",
            "546 2.7275 0 4\n",
            "547 2.7325 0 4\n",
            "548 2.7375 0 4\n",
            "549 2.7425 0 4\n",
            "550 2.7475 0 4\n",
            "551 2.7525 0 4\n",
            "552 2.7575 0 4\n",
            "553 2.7625 0 4\n",
            "554 2.7675 0 4\n",
            "555 2.7725 0 4\n",
            "556 2.7775 0 4\n",
            "557 2.7825 0 4\n",
            "558 2.7875 0 4\n",
            "559 2.7925 0 4\n",
            "560 2.7975 0 4\n",
            "561 2.8025 0 4\n",
            "562 2.8075 0 4\n",
            "563 2.8125 0 4\n",
            "564 2.8175 0 4\n",
            "565 2.8225 0 4\n",
            "566 2.8275 0 4\n",
            "567 2.8325 0 4\n",
            "568 2.8375 0 4\n",
            "569 2.8425 0 4\n",
            "570 2.8475 0 4\n",
            "571 2.8525 0 4\n",
            "572 2.8575 0 4\n",
            "573 2.8625 0 4\n",
            "574 2.8675 0 4\n",
            "575 2.8725 0 4\n",
            "576 2.8775 0 4\n",
            "577 2.8825 0 4\n",
            "578 2.8875 0 4\n",
            "579 2.8925 0 4\n",
            "580 2.8975 0 4\n",
            "581 2.9025 0 4\n",
            "582 2.9075 0 4\n",
            "583 2.9125 0 4\n",
            "584 2.9175 0 4\n",
            "585 2.9225 0 4\n",
            "586 2.9275 0 4\n",
            "587 2.9325 0 4\n",
            "588 2.9375 0 4\n",
            "589 2.9425 0 4\n",
            "590 2.9475 0 4\n",
            "591 2.9525 0 4\n",
            "592 2.9575 0 4\n",
            "593 2.9625 0 4\n",
            "594 2.9675 0 4\n",
            "595 2.9725 0 4\n",
            "596 2.9775 0 4\n",
            "597 2.9825 0 4\n",
            "598 2.9875 0 4\n",
            "599 2.9925 0 4\n",
            "600 2.9975 0 4\n",
            "601 3.0025 0 4\n",
            "602 3.0075 0 4\n",
            "603 3.0125 0 4\n",
            "604 3.0175 0 4\n",
            "605 3.0225 0 4\n",
            "606 3.0275 0 4\n",
            "607 3.0325 0 4\n",
            "608 3.0375 0 4\n",
            "609 3.0425 0 4\n",
            "610 3.0475 0 4\n",
            "611 3.0525 0 4\n",
            "612 3.0575 0 4\n",
            "613 3.0625 0 4\n",
            "614 3.0675 0 4\n",
            "615 3.0725 0 4\n",
            "616 3.0775 0 4\n",
            "617 3.0825 0 4\n",
            "618 3.0875 0 4\n",
            "619 3.0925 0 4\n",
            "620 3.0975 0 4\n",
            "621 3.1025 0 4\n",
            "622 3.1075 0 4\n",
            "623 3.1125 0 4\n",
            "624 3.1175 0 4\n",
            "625 3.1225 0 4\n",
            "626 3.1275 0 4\n",
            "627 3.1325 0 4\n",
            "628 3.1375 0 4\n",
            "629 3.1425 0 4\n",
            "630 3.1475 0 4\n",
            "631 3.1525 0 4\n",
            "632 3.1575 0 4\n",
            "633 3.1625 0 4\n",
            "634 3.1675 0 4\n",
            "635 3.1725 0 4\n",
            "636 3.1775 0 4\n",
            "637 3.1825 0 4\n",
            "638 3.1875 0 4\n",
            "639 3.1925 0 4\n",
            "640 3.1975 0 4\n",
            "641 3.2025 0 4\n",
            "642 3.2075 0 4\n",
            "643 3.2125 0 4\n",
            "644 3.2175 0 4\n",
            "645 3.2225 0 4\n",
            "646 3.2275 0 4\n",
            "647 3.2325 0 4\n",
            "648 3.2375 0 4\n",
            "649 3.2425 0 4\n",
            "650 3.2475 0 4\n",
            "651 3.2525 0 4\n",
            "652 3.2575 0 4\n",
            "653 3.2625 0 4\n",
            "654 3.2675 0 4\n",
            "655 3.2725 0 4\n",
            "656 3.2775 0 4\n",
            "657 3.2825 0 4\n",
            "658 3.2875 0 4\n",
            "659 3.2925 0 4\n",
            "660 3.2975 0 4\n",
            "661 3.3025 0 4\n",
            "662 3.3075 0 4\n",
            "663 3.3125 0 4\n",
            "664 3.3175 0 4\n",
            "665 3.3225 0 4\n",
            "666 3.3275 0 4\n",
            "667 3.3325 0 4\n",
            "668 3.3375 0 4\n",
            "669 3.3425 0 4\n",
            "670 3.3475 0 4\n",
            "671 3.3525 0 4\n",
            "672 3.3575 0 4\n",
            "673 3.3625 0 4\n",
            "674 3.3675 0 4\n",
            "675 3.3725 0 4\n",
            "676 3.3775 0 4\n",
            "677 3.3825 0 4\n",
            "678 3.3875 0 4\n",
            "679 3.3925 0 4\n",
            "680 3.3975 0 4\n",
            "681 3.4025 0 4\n",
            "682 3.4075 0 4\n",
            "683 3.4125 0 4\n",
            "684 3.4175 0 4\n",
            "685 3.4225 0 4\n",
            "686 3.4275 0.00137792 4.00005\n",
            "687 3.4325 0.00103043 4.00009\n",
            "688 3.4375 0.000684957 4.00011\n",
            "689 3.4425 0.000341485 4.00012\n",
            "690 3.4475 0.000680989 4.00015\n",
            "691 3.4525 0.000339509 4.00016\n",
            "692 3.4575 0.00270822 4.00026\n",
            "693 3.4625 0.00742612 4.00054\n",
            "694 3.4675 0.00605841 4.00076\n",
            "695 3.4725 0.00671219 4.00101\n",
            "696 3.4775 0.0066929 4.00126\n",
            "697 3.4825 0.00533896 4.00146\n",
            "698 3.4875 0.00698731 4.00173\n",
            "699 3.4925 0.00597198 4.00195\n",
            "700 3.4975 0.0109174 4.00236\n",
            "701 3.5025 0.0168241 4.003\n",
            "702 3.5075 0.0131578 4.0035\n",
            "703 3.5125 0.0164005 4.00413\n",
            "704 3.5175 0.0196247 4.00488\n",
            "705 3.5225 0.0159813 4.00549\n",
            "706 3.5275 0.0208144 4.00629\n",
            "707 3.5325 0.0347007 4.00762\n",
            "708 3.5375 0.0397769 4.00916\n",
            "709 3.5425 0.0445019 4.01089\n",
            "710 3.5475 0.051451 4.01289\n",
            "711 3.5525 0.0590022 4.01519\n",
            "712 3.5575 0.0565982 4.0174\n",
            "713 3.5625 0.0707884 4.02018\n",
            "714 3.5675 0.0833091 4.02345\n",
            "715 3.5725 0.0795881 4.02659\n",
            "716 3.5775 0.101816 4.03061\n",
            "717 3.5825 0.118874 4.03533\n",
            "718 3.5875 0.140239 4.0409\n",
            "719 3.5925 0.15396 4.04704\n",
            "720 3.5975 0.227327 4.05612\n",
            "721 3.6025 0.252266 4.06624\n",
            "722 3.6075 0.283286 4.07763\n",
            "723 3.6125 0.316923 4.0904\n",
            "724 3.6175 0.369238 4.10532\n",
            "725 3.6225 0.397208 4.12143\n",
            "726 3.6275 0.450241 4.13973\n",
            "727 3.6325 0.481512 4.15935\n",
            "728 3.6375 0.512916 4.18031\n",
            "729 3.6425 0.593252 4.20463\n",
            "730 3.6475 0.722119 4.2343\n",
            "731 3.6525 0.802047 4.26735\n",
            "732 3.6575 0.888796 4.30408\n",
            "733 3.6625 0.972353 4.34436\n",
            "734 3.6675 1.08283 4.38935\n",
            "735 3.6725 1.1591 4.43764\n",
            "736 3.6775 1.27504 4.4909\n",
            "737 3.6825 1.36589 4.54811\n",
            "738 3.6875 1.50266 4.61123\n",
            "739 3.6925 1.62088 4.67949\n",
            "740 3.6975 1.77634 4.7545\n",
            "741 3.7025 1.94365 4.8368\n",
            "742 3.7075 2.07502 4.9249\n",
            "743 3.7125 2.15047 5.01645\n",
            "744 3.7175 2.33738 5.11623\n",
            "745 3.7225 2.57759 5.22655\n",
            "746 3.7275 2.75912 5.34496\n",
            "747 3.7325 2.90976 5.47018\n",
            "748 3.7375 3.15663 5.60637\n",
            "749 3.7425 3.25857 5.74735\n",
            "750 3.7475 3.52248 5.90015\n",
            "751 3.7525 3.63956 6.05845\n",
            "752 3.7575 3.86348 6.22694\n",
            "753 3.7625 4.04561 6.40384\n",
            "754 3.7675 4.23616 6.58956\n",
            "755 3.7725 4.451 6.78522\n",
            "756 3.7775 4.53991 6.98532\n",
            "757 3.7825 4.67867 7.19209\n",
            "758 3.7875 4.7755 7.40369\n",
            "759 3.7925 4.98042 7.62495\n",
            "760 3.7975 5.11604 7.85284\n",
            "761 3.8025 5.23106 8.08646\n",
            "762 3.8075 5.31392 8.32441\n",
            "763 3.8125 5.31864 8.5632\n",
            "764 3.8175 5.46495 8.8092\n",
            "765 3.8225 5.45509 9.0554\n",
            "766 3.8275 5.5414 9.30615\n",
            "767 3.8325 5.6198 9.56111\n",
            "768 3.8375 5.60104 9.81589\n",
            "769 3.8425 5.58236 10.0705\n",
            "770 3.8475 5.62746 10.3278\n",
            "771 3.8525 5.64667 10.5867\n",
            "772 3.8575 5.55127 10.8418\n",
            "773 3.8625 5.49676 11.0951\n",
            "774 3.8675 5.4317 11.346\n",
            "775 3.8725 5.31702 11.5923\n",
            "776 3.8775 5.23926 11.8356\n",
            "777 3.8825 5.0816 12.0723\n",
            "778 3.8875 5.02007 12.3066\n",
            "779 3.8925 4.92251 12.537\n",
            "780 3.8975 4.76124 12.7604\n",
            "781 3.9025 4.49103 12.9716\n",
            "782 3.9075 4.36133 13.1773\n",
            "783 3.9125 4.16989 13.3745\n",
            "784 3.9175 3.97493 13.5629\n",
            "785 3.9225 3.85591 13.7462\n",
            "786 3.9275 3.62257 13.9187\n",
            "787 3.9325 3.37288 14.0799\n",
            "788 3.9375 3.20274 14.2332\n",
            "789 3.9425 2.98738 14.3767\n",
            "790 3.9475 2.83906 14.5133\n",
            "791 3.9525 2.56584 14.6371\n",
            "792 3.9575 2.38236 14.7524\n",
            "793 3.9625 2.28512 14.8632\n",
            "794 3.9675 2.20917 14.9706\n",
            "795 3.9725 2.00026 15.0681\n",
            "796 3.9775 1.78548 15.1554\n",
            "797 3.9825 1.69067 15.2382\n",
            "798 3.9875 1.57139 15.3154\n",
            "799 3.9925 1.42224 15.3854\n",
            "800 3.9975 1.25382 15.4473\n",
            "801 4.0025 1.15318 15.5043\n",
            "802 4.0075 1.08706 15.5583\n",
            "803 4.0125 1.00794 15.6084\n",
            "804 4.0175 0.939738 15.6552\n",
            "805 4.0225 0.796843 15.6951\n",
            "806 4.0275 0.689582 15.7296\n",
            "807 4.0325 0.615203 15.7605\n",
            "808 4.0375 0.587862 15.7901\n",
            "809 4.0425 0.506669 15.8157\n",
            "810 4.0475 0.451566 15.8385\n",
            "811 4.0525 0.398705 15.8588\n",
            "812 4.0575 0.361588 15.8772\n",
            "813 4.0625 0.338385 15.8944\n",
            "814 4.0675 0.302086 15.9098\n",
            "815 4.0725 0.242784 15.9223\n",
            "816 4.0775 0.211763 15.9331\n",
            "817 4.0825 0.187692 15.9428\n",
            "818 4.0875 0.160589 15.9511\n",
            "819 4.0925 0.132652 15.958\n",
            "820 4.0975 0.116179 15.964\n",
            "821 4.1025 0.11782 15.9701\n",
            "822 4.1075 0.0904285 15.9748\n",
            "823 4.1125 0.0744162 15.9787\n",
            "824 4.1175 0.0785322 15.9828\n",
            "825 4.1225 0.063102 15.9861\n",
            "826 4.1275 0.0624742 15.9894\n",
            "827 4.1325 0.0585316 15.9925\n",
            "828 4.1375 0.0491707 15.9951\n",
            "829 4.1425 0.0400906 15.9972\n",
            "830 4.1475 0.0388177 15.9993\n",
            "831 4.1525 0.0300407 16.0009\n",
            "832 4.1575 0.0353534 16.0028\n",
            "833 4.1625 0.0341007 16.0046\n",
            "834 4.1675 0.0300578 16.0062\n",
            "835 4.1725 0.0304507 16.0079\n",
            "836 4.1775 0.0336244 16.0097\n",
            "837 4.1825 0.0282232 16.0112\n",
            "838 4.1875 0.035541 16.0131\n",
            "839 4.1925 0.0490402 16.0158\n",
            "840 4.1975 0.0523688 16.0186\n",
            "841 4.2025 0.0554522 16.0217\n",
            "842 4.2075 0.06355 16.0251\n",
            "843 4.2125 0.0745739 16.0292\n",
            "844 4.2175 0.0773549 16.0335\n",
            "845 4.2225 0.093741 16.0386\n",
            "846 4.2275 0.112314 16.0448\n",
            "847 4.2325 0.139383 16.0526\n",
            "848 4.2375 0.154605 16.0611\n",
            "849 4.2425 0.164584 16.0703\n",
            "850 4.2475 0.193357 16.0811\n",
            "851 4.2525 0.228708 16.0938\n",
            "852 4.2575 0.235539 16.107\n",
            "853 4.2625 0.250355 16.1211\n",
            "854 4.2675 0.268657 16.1362\n",
            "855 4.2725 0.326778 16.1546\n",
            "856 4.2775 0.337294 16.1737\n",
            "857 4.2825 0.376667 16.195\n",
            "858 4.2875 0.424661 16.2191\n",
            "859 4.2925 0.460352 16.2453\n",
            "860 4.2975 0.535974 16.2759\n",
            "861 4.3025 0.604904 16.3105\n",
            "862 4.3075 0.646685 16.3475\n",
            "863 4.3125 0.673257 16.3862\n",
            "864 4.3175 0.755281 16.4297\n",
            "865 4.3225 0.828477 16.4775\n",
            "866 4.3275 0.898955 16.5295\n",
            "867 4.3325 0.945175 16.5843\n",
            "868 4.3375 1.00409 16.6427\n",
            "869 4.3425 1.04512 16.7035\n",
            "870 4.3475 1.1592 16.7712\n",
            "871 4.3525 1.24433 16.844\n",
            "872 4.3575 1.33824 16.9225\n",
            "873 4.3625 1.47531 17.0092\n",
            "874 4.3675 1.57419 17.102\n",
            "875 4.3725 1.67558 17.2009\n",
            "876 4.3775 1.76953 17.3057\n",
            "877 4.3825 1.85294 17.4156\n",
            "878 4.3875 1.92861 17.5303\n",
            "879 4.3925 2.0465 17.6522\n",
            "880 4.3975 2.14774 17.7805\n",
            "881 4.4025 2.31575 17.9192\n",
            "882 4.4075 2.45382 18.0664\n",
            "883 4.4125 2.54678 18.2196\n",
            "884 4.4175 2.63953 18.3787\n",
            "885 4.4225 2.74302 18.5444\n",
            "886 4.4275 2.85037 18.717\n",
            "887 4.4325 2.99822 18.8989\n",
            "888 4.4375 3.12546 19.089\n",
            "889 4.4425 3.23285 19.2861\n",
            "890 4.4475 3.3911 19.4933\n",
            "891 4.4525 3.38226 19.7004\n",
            "892 4.4575 3.44413 19.9118\n",
            "893 4.4625 3.48336 20.126\n",
            "894 4.4675 3.63231 20.3499\n",
            "895 4.4725 3.67335 20.5769\n",
            "896 4.4775 3.60924 20.8004\n",
            "897 4.4825 3.72284 21.0315\n",
            "898 4.4875 3.71153 21.2623\n",
            "899 4.4925 3.76042 21.4967\n",
            "900 4.4975 3.77127 21.7324\n",
            "901 4.5025 3.68106 21.9629\n",
            "902 4.5075 3.69819 22.195\n",
            "903 4.5125 3.70729 22.4281\n",
            "904 4.5175 3.68957 22.6607\n",
            "905 4.5225 3.68359 22.8934\n",
            "906 4.5275 3.59787 23.1212\n",
            "907 4.5325 3.61693 23.3507\n",
            "908 4.5375 3.51147 23.574\n",
            "909 4.5425 3.45138 23.794\n",
            "910 4.5475 3.37197 24.0094\n",
            "911 4.5525 3.37921 24.2257\n",
            "912 4.5575 3.24828 24.4341\n",
            "913 4.5625 3.10799 24.634\n",
            "914 4.5675 3.01759 24.8284\n",
            "915 4.5725 2.88015 25.0144\n",
            "916 4.5775 2.84122 25.1983\n",
            "917 4.5825 2.73173 25.3755\n",
            "918 4.5875 2.65328 25.548\n",
            "919 4.5925 2.55253 25.7143\n",
            "920 4.5975 2.46561 25.8752\n",
            "921 4.6025 2.30226 26.0259\n",
            "922 4.6075 2.25285 26.1736\n",
            "923 4.6125 2.08705 26.3107\n",
            "924 4.6175 1.98193 26.4413\n",
            "925 4.6225 1.84791 26.5632\n",
            "926 4.6275 1.70332 26.6759\n",
            "927 4.6325 1.58668 26.7811\n",
            "928 4.6375 1.51571 26.8818\n",
            "929 4.6425 1.4165 26.9761\n",
            "930 4.6475 1.32727 27.0646\n",
            "931 4.6525 1.27169 27.1496\n",
            "932 4.6575 1.17979 27.2287\n",
            "933 4.6625 1.08158 27.3013\n",
            "934 4.6675 0.992883 27.3681\n",
            "935 4.6725 0.894927 27.4285\n",
            "936 4.6775 0.841779 27.4854\n",
            "937 4.6825 0.735515 27.5352\n",
            "938 4.6875 0.6728 27.5808\n",
            "939 4.6925 0.608329 27.6222\n",
            "940 4.6975 0.583927 27.662\n",
            "941 4.7025 0.520281 27.6975\n",
            "942 4.7075 0.490506 27.7311\n",
            "943 4.7125 0.449558 27.762\n",
            "944 4.7175 0.401872 27.7896\n",
            "945 4.7225 0.360556 27.8144\n",
            "946 4.7275 0.334262 27.8375\n",
            "947 4.7325 0.277181 27.8567\n",
            "948 4.7375 0.251533 27.8741\n",
            "949 4.7425 0.23121 27.8902\n",
            "950 4.7475 0.211512 27.9049\n",
            "951 4.7525 0.181861 27.9176\n",
            "952 4.7575 0.150905 27.9281\n",
            "953 4.7625 0.128821 27.9372\n",
            "954 4.7675 0.114129 27.9452\n",
            "955 4.7725 0.0964777 27.952\n",
            "956 4.7775 0.095212 27.9587\n",
            "957 4.7825 0.0812123 27.9644\n",
            "958 4.7875 0.0734505 27.9696\n",
            "959 4.7925 0.0657209 27.9743\n",
            "960 4.7975 0.0566167 27.9783\n",
            "961 4.8025 0.0471994 27.9817\n",
            "962 4.8075 0.032218 27.984\n",
            "963 4.8125 0.0323258 27.9863\n",
            "964 4.8175 0.0331306 27.9886\n",
            "965 4.8225 0.0276676 27.9906\n",
            "966 4.8275 0.0246583 27.9924\n",
            "967 4.8325 0.0240874 27.9942\n",
            "968 4.8375 0.0157369 27.9953\n",
            "969 4.8425 0.0127706 27.9962\n",
            "970 4.8475 0.0118832 27.9971\n",
            "971 4.8525 0.0073902 27.9976\n",
            "972 4.8575 0.00977616 27.9983\n",
            "973 4.8625 0.00239623 27.9985\n",
            "974 4.8675 0.00187888 27.9986\n",
            "975 4.8725 0.00255686 27.9988\n",
            "976 4.8775 0.00238151 27.999\n",
            "977 4.8825 0.00220688 27.9992\n",
            "978 4.8875 0.0013553 27.9993\n",
            "979 4.8925 0.00118346 27.9993\n",
            "980 4.8975 0.00168721 27.9995\n",
            "981 4.9025 0.000841887 27.9995\n",
            "982 4.9075 0.000672138 27.9996\n",
            "983 4.9125 0.00100616 27.9997\n",
            "984 4.9175 0.00100411 27.9997\n",
            "985 4.9225 0.00200414 27.9999\n",
            "986 4.9275 0.000833366 27.9999\n",
            "987 4.9325 0.000665342 28\n",
            "988 4.9375 0 28\n",
            "989 4.9425 0 28\n",
            "990 4.9475 0 28\n",
            "991 4.9525 0 28\n",
            "992 4.9575 0 28\n",
            "993 4.9625 0 28\n",
            "994 4.9675 0 28\n",
            "995 4.9725 0 28\n",
            "996 4.9775 0 28\n",
            "997 4.9825 0 28\n",
            "998 4.9875 0 28\n",
            "999 4.9925 0 28\n",
            "1000 4.9975 0 28\n"
          ]
        }
      ]
    }
  ]
}