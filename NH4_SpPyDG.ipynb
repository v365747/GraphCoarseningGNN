{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "994c35d93f0041ed99aff4e12a524337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PlayModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PlayModel",
            "_playing": false,
            "_repeat": false,
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PlayView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "interval": 100,
            "layout": "IPY_MODEL_699a8050e045453291c43987fd8e7c62",
            "max": 18699,
            "min": 0,
            "show_repeat": true,
            "step": 1,
            "style": "IPY_MODEL_f245accbecbb46bdb89635c7f73eaeb5",
            "value": 4135,
            "playing": true
          }
        },
        "a99c252d856a48d989d99e81742dacd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e9ad173779214d28b8e41f86f5ae5994",
            "max": 18699,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_3fa251860a264a76b8fcbbe604b9f1c4",
            "value": 11025
          }
        },
        "d1526029201349b589f342e5783fc96d": {
          "model_module": "nglview-js-widgets",
          "model_name": "NGLModel",
          "model_module_version": "3.0.8",
          "state": {
            "_camera_orientation": [
              22.002573948597277,
              9.414961747108197,
              17.10149766137556,
              0,
              6.2525559009418705,
              21.011615546301623,
              -19.612076487844156,
              0,
              -18.49346564782118,
              18.305367922004248,
              13.715728959772909,
              0,
              -9.98799991607666,
              -0.009999999776482582,
              -9.996999740600586,
              1
            ],
            "_camera_str": "orthographic",
            "_dom_classes": [],
            "_gui_theme": null,
            "_ibtn_fullscreen": "IPY_MODEL_07324f62c49240ec8efdb97de57b316b",
            "_igui": null,
            "_iplayer": "IPY_MODEL_63a33f95bc98446eb9813bb2d7d5b75b",
            "_model_module": "nglview-js-widgets",
            "_model_module_version": "3.0.8",
            "_model_name": "NGLModel",
            "_ngl_color_dict": {},
            "_ngl_coordinate_resource": {},
            "_ngl_full_stage_parameters": {
              "impostor": true,
              "quality": "medium",
              "workerDefault": true,
              "sampleLevel": 0,
              "backgroundColor": "white",
              "rotateSpeed": 2,
              "zoomSpeed": 1.2,
              "panSpeed": 1,
              "clipNear": 0,
              "clipFar": 100,
              "clipDist": 0,
              "clipMode": "scene",
              "clipScale": "relative",
              "fogNear": 50,
              "fogFar": 100,
              "cameraFov": 40,
              "cameraEyeSep": 0.3,
              "cameraType": "orthographic",
              "lightColor": 14540253,
              "lightIntensity": 1,
              "ambientColor": 14540253,
              "ambientIntensity": 0.2,
              "hoverTimeout": 0,
              "tooltip": true,
              "mousePreset": "default"
            },
            "_ngl_msg_archive": [
              {
                "target": "Stage",
                "type": "call_method",
                "methodName": "loadFile",
                "reconstruc_color_scheme": false,
                "args": [
                  {
                    "type": "blob",
                    "data": "CRYST1   10.000   10.000   10.000  90.00  90.00  90.00 P 1\nMODEL     1\nATOM      1    N MOL     1       9.988   0.010   9.997  1.00  0.00           N  \nATOM      2    H MOL     1       0.842   1.450   0.893  1.00  0.00           H  \nATOM      3    H MOL     1       1.162   8.777   9.326  1.00  0.00           H  \nATOM      4    H MOL     1       8.906   0.936   8.728  1.00  0.00           H  \nATOM      5    H MOL     1       8.674   9.101   1.265  1.00  0.00           H  \nENDMDL\n",
                    "binary": false
                  }
                ],
                "kwargs": {
                  "name": "nglview.adaptor.ASETrajectory",
                  "defaultRepresentation": false,
                  "ext": "pdb"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "setSize",
                "reconstruc_color_scheme": false,
                "args": [
                  "500px",
                  "500px"
                ],
                "kwargs": {}
              },
              {
                "component_index": 0,
                "target": "compList",
                "type": "call_method",
                "methodName": "addRepresentation",
                "reconstruc_color_scheme": false,
                "args": [
                  "unitcell"
                ],
                "kwargs": {
                  "sele": "all"
                }
              },
              {
                "component_index": 0,
                "target": "compList",
                "type": "call_method",
                "methodName": "addRepresentation",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill"
                ],
                "kwargs": {
                  "sele": "all"
                }
              },
              {
                "target": "Stage",
                "type": "call_method",
                "methodName": "setParameters",
                "reconstruc_color_scheme": false,
                "args": [],
                "kwargs": {
                  "cameraType": "orthographic"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "setParameters",
                "reconstruc_color_scheme": false,
                "args": [
                  {
                    "clipDist": 0
                  }
                ],
                "kwargs": {}
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.5,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.5,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.49999999999999994,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.48,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.48,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.48,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.1,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.1,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.1,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.05,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.05,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.05,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.15,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.15,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.15,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.44,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.44,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.44,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.66,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.66,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.66,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.89,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.89,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.89,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.15,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.15,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.15,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.35,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.35,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.35,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.5,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.5,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.5,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.48,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.48,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 1.48,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.9,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.91,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              },
              {
                "target": "Widget",
                "type": "call_method",
                "methodName": "updateRepresentationsByName",
                "reconstruc_color_scheme": false,
                "args": [
                  "spacefill",
                  0
                ],
                "kwargs": {
                  "radiusType": "covalent",
                  "radiusScale": 0.91,
                  "colorScheme": "element",
                  "colorScale": "rainbow"
                }
              }
            ],
            "_ngl_original_stage_parameters": {
              "impostor": true,
              "quality": "medium",
              "workerDefault": true,
              "sampleLevel": 0,
              "backgroundColor": "white",
              "rotateSpeed": 2,
              "zoomSpeed": 1.2,
              "panSpeed": 1,
              "clipNear": 0,
              "clipFar": 100,
              "clipDist": 10,
              "clipMode": "scene",
              "clipScale": "relative",
              "fogNear": 50,
              "fogFar": 100,
              "cameraFov": 40,
              "cameraEyeSep": 0.3,
              "cameraType": "perspective",
              "lightColor": 14540253,
              "lightIntensity": 1,
              "ambientColor": 14540253,
              "ambientIntensity": 0.2,
              "hoverTimeout": 0,
              "tooltip": true,
              "mousePreset": "default"
            },
            "_ngl_repr_dict": {
              "0": {
                "0": {
                  "type": "unitcell",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "radiusSize": 0.049999997805194256,
                    "sphereDetail": 1,
                    "radialSegments": 10,
                    "disableImpostor": false,
                    "radiusType": "vdw",
                    "radiusData": {},
                    "radiusScale": 1,
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "",
                    "colorReverse": false,
                    "colorValue": "orange",
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                },
                "1": {
                  "type": "unitcell",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "radiusSize": 0.049999997805194256,
                    "sphereDetail": 1,
                    "radialSegments": 10,
                    "disableImpostor": false,
                    "radiusType": "vdw",
                    "radiusData": {},
                    "radiusScale": 1,
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "",
                    "colorReverse": false,
                    "colorValue": "orange",
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                },
                "2": {
                  "type": "spacefill",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "sphereDetail": 1,
                    "disableImpostor": false,
                    "radiusType": "covalent",
                    "radiusData": {},
                    "radiusSize": 1,
                    "radiusScale": 0.91,
                    "assembly": "default",
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "rainbow",
                    "colorReverse": false,
                    "colorValue": 9474192,
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                },
                "3": {
                  "type": "spacefill",
                  "params": {
                    "lazy": false,
                    "visible": true,
                    "quality": "medium",
                    "sphereDetail": 1,
                    "disableImpostor": false,
                    "radiusType": "covalent",
                    "radiusData": {},
                    "radiusSize": 1,
                    "radiusScale": 0.91,
                    "assembly": "default",
                    "defaultAssembly": "",
                    "clipNear": 0,
                    "clipRadius": 0,
                    "clipCenter": {
                      "x": 0,
                      "y": 0,
                      "z": 0
                    },
                    "flatShaded": false,
                    "opacity": 1,
                    "depthWrite": true,
                    "side": "double",
                    "wireframe": false,
                    "colorScheme": "element",
                    "colorScale": "rainbow",
                    "colorReverse": false,
                    "colorValue": 9474192,
                    "colorMode": "hcl",
                    "roughness": 0.4,
                    "metalness": 0,
                    "diffuse": 16777215,
                    "diffuseInterior": false,
                    "useInteriorColor": true,
                    "interiorColor": 2236962,
                    "interiorDarkening": 0,
                    "matrix": {
                      "elements": [
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1,
                        0,
                        0,
                        0,
                        0,
                        1
                      ]
                    },
                    "disablePicking": false,
                    "sele": "all"
                  }
                }
              },
              "1": {}
            },
            "_ngl_serialize": false,
            "_ngl_version": "2.0.0-dev.39",
            "_ngl_view_id": [
              "EA590E4C-33CE-4230-86CD-65DCE3785278"
            ],
            "_player_dict": {},
            "_scene_position": {},
            "_scene_rotation": {},
            "_synced_model_ids": [],
            "_synced_repr_model_ids": [],
            "_view_count": null,
            "_view_height": "",
            "_view_module": "nglview-js-widgets",
            "_view_module_version": "3.0.8",
            "_view_name": "NGLView",
            "_view_width": "",
            "background": "white",
            "frame": 18699,
            "gui_style": null,
            "layout": "IPY_MODEL_2b259bb2cd9043a6a016fdd4f68c902d",
            "max_frame": 18699,
            "n_components": 2,
            "picked": {
              "atom1": {
                "index": 0,
                "residueIndex": 0,
                "resname": "MOL",
                "x": 9.98799991607666,
                "y": 0.009999999776482582,
                "z": 9.996999740600586,
                "element": "N",
                "chainname": "A",
                "resno": 1,
                "serial": 1,
                "vdw": 1.55,
                "covalent": 0.71,
                "hetero": 0,
                "bfactor": 0,
                "altloc": "",
                "atomname": "N",
                "modelIndex": 0,
                "name": "[MOL]1:A.N"
              },
              "component": 0
            }
          }
        },
        "699a8050e045453291c43987fd8e7c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f245accbecbb46bdb89635c7f73eaeb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9ad173779214d28b8e41f86f5ae5994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fa251860a264a76b8fcbbe604b9f1c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "b4599d7d2ca040acb6f2766fb51dd280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1526029201349b589f342e5783fc96d",
              "IPY_MODEL_35bcf35338bc4b33896a6d663bedd189"
            ],
            "layout": "IPY_MODEL_de04b796a6734366a186f40036009bea"
          }
        },
        "35bcf35338bc4b33896a6d663bedd189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ce9ef48cdd4407d939ba258638a4151",
              "IPY_MODEL_57a4bd6ba6454a01b661b8b91d76be7f",
              "IPY_MODEL_b7d49dd491a04aad94c4de9ea1c665ff",
              "IPY_MODEL_4bb33f5cb18c4e19859aaf803ea7bcc3"
            ],
            "layout": "IPY_MODEL_528e5c482c0f48a697d845ebc1fe8691"
          }
        },
        "de04b796a6734366a186f40036009bea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce9ef48cdd4407d939ba258638a4151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "All",
              "N",
              "H"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Show",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_6cf7e7a469c64fac809744d34f534236",
            "style": "IPY_MODEL_03b537e3395e4241a10e473b3ba7cd27"
          }
        },
        "57a4bd6ba6454a01b661b8b91d76be7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              " ",
              "picking",
              "random",
              "uniform",
              "atomindex",
              "residueindex",
              "chainindex",
              "modelindex",
              "sstruc",
              "element",
              "resname",
              "bfactor",
              "hydrophobicity",
              "value",
              "volume",
              "occupancy"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Color scheme",
            "description_tooltip": null,
            "disabled": false,
            "index": 9,
            "layout": "IPY_MODEL_93b369f2408d48d1af62b64c274292f4",
            "style": "IPY_MODEL_5abf5dc620354b99a6a36128e404d099"
          }
        },
        "b7d49dd491a04aad94c4de9ea1c665ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Ball size",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ccab8246681d4684bda585c5b372a1a9",
            "max": 1.5,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.01,
            "style": "IPY_MODEL_9ea8bbe8cd354bfb89ec65a8673a675a",
            "value": 0.91
          }
        },
        "4bb33f5cb18c4e19859aaf803ea7bcc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_0d4c0d6613e34153ab1aca37e6ad4bb4",
            "max": 18699,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_4dc431bb637b4802b6a89b05b6e2a31c",
            "value": 18699
          }
        },
        "528e5c482c0f48a697d845ebc1fe8691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf7e7a469c64fac809744d34f534236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03b537e3395e4241a10e473b3ba7cd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93b369f2408d48d1af62b64c274292f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5abf5dc620354b99a6a36128e404d099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccab8246681d4684bda585c5b372a1a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ea8bbe8cd354bfb89ec65a8673a675a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "0d4c0d6613e34153ab1aca37e6ad4bb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc431bb637b4802b6a89b05b6e2a31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "2b259bb2cd9043a6a016fdd4f68c902d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07324f62c49240ec8efdb97de57b316b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "",
            "disabled": false,
            "icon": "compress",
            "layout": "IPY_MODEL_1e6c4927b0a64677a28eacaa85010e90",
            "style": "IPY_MODEL_19718e8a0e4041d5a08d2d58bb46584b",
            "tooltip": ""
          }
        },
        "63a33f95bc98446eb9813bb2d7d5b75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_994c35d93f0041ed99aff4e12a524337",
              "IPY_MODEL_a99c252d856a48d989d99e81742dacd5"
            ],
            "layout": "IPY_MODEL_42d1ea613ed644d49f89b1011640810f"
          }
        },
        "1e6c4927b0a64677a28eacaa85010e90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "34px"
          }
        },
        "19718e8a0e4041d5a08d2d58bb46584b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "42d1ea613ed644d49f89b1011640810f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coarsening Molecular environments"
      ],
      "metadata": {
        "id": "z8wYdH-gO7oJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First setup Nequip (E3NN), wandb, Allegro.\n",
        "\n",
        "# Confirm device is GPU\n",
        "\n",
        "Before you get started, make sure that in your menu bar Runtime --> Change runtime type is set to GPU"
      ],
      "metadata": {
        "id": "GCFxp5yhPC3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XynMFbIsYBUz",
        "outputId": "3797c1cf-2723-4e03-f1da-1bc4fb244317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Cloning into 'nequip'...\n",
            "remote: Enumerating objects: 218, done.\u001b[K\n",
            "remote: Counting objects: 100% (218/218), done.\u001b[K\n",
            "remote: Compressing objects: 100% (205/205), done.\u001b[K\n",
            "remote: Total 218 (delta 4), reused 90 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (218/218), 361.14 KiB | 4.51 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Processing ./nequip\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (1.26.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (4.66.6)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (6.0.2)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip==0.6.1) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (2.5.1+cu121)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (0.1.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase->nequip==0.6.1) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip==0.6.1) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip==0.6.1) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip==0.6.1) (3.0.2)\n",
            "Building wheels for collected packages: nequip\n",
            "  Building wheel for nequip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nequip: filename=nequip-0.6.1-py3-none-any.whl size=175386 sha256=430d5ad1b2d3e61efc694a895c8ff262ca5cc3623cd0c3f1516a3f5d927ba2fb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-11k9m083/wheels/11/64/44/9d30bacb0803dffa7821bb8685dbc60a0830cca339476e4e86\n",
            "Successfully built nequip\n",
            "Installing collected packages: nequip\n",
            "  Attempting uninstall: nequip\n",
            "    Found existing installation: nequip 0.6.1\n",
            "    Uninstalling nequip-0.6.1:\n",
            "      Successfully uninstalled nequip-0.6.1\n",
            "Successfully installed nequip-0.6.1\n",
            "Cloning into 'allegro'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 44 (delta 0), reused 24 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (44/44), 71.53 KiB | 1.40 MiB/s, done.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nequip>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from mir-allegro==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.66.6)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.5.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (0.1.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.0.2)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27432 sha256=54114043fcc948ea1f4cdefe8e08ac2c3f8df720b4ed82344f31170154d00962\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-irulo7v6/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: mir-allegro\n",
            "  Attempting uninstall: mir-allegro\n",
            "    Found existing installation: mir-allegro 0.2.0\n",
            "    Uninstalling mir-allegro-0.2.0:\n",
            "      Successfully uninstalled mir-allegro-0.2.0\n",
            "Successfully installed mir-allegro-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "\n",
        "# install nequip\n",
        "!rm -rf nequip\n",
        "!git clone --depth 1 \"https://github.com/mir-group/nequip.git\"\n",
        "!pip install nequip/\n",
        "\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "\n",
        "!rm -rf allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup PyTorch, check Nvidia GPUs/CUDA functioning."
      ],
      "metadata": {
        "id": "-sgXfyD6PbuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "print(\"-----------------------------\")\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "!which nvcc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZvdhYOaOp5I",
        "outputId": "277c3448-4aab-4a0b-dece-590c4d4a563d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "True\n",
            "1\n",
            "NVIDIA A100-SXM4-40GB\n",
            "-----------------------------\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Wed Dec 11 12:56:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              46W / 400W |      5MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Molecular Dynamics LAMMPS code.\n",
        "We will patch it with pair_allegro and build it with libtorch.\n",
        "\n",
        "We get Radial Function distribution of bond strength N-H using LAMMPS trajectory."
      ],
      "metadata": {
        "id": "xxDAm73fPpBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf lammps\n",
        "!git clone --depth 1 https://github.com/lammps/lammps.git\n",
        "# Stable Release (Simon )\n",
        "#!git clone -b stable_29Sep2021_update2 --depth 1 https://github.com/lammps/lammps.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJrTDEJJ8klU",
        "outputId": "e45c514a-d0df-4e2c-d99d-7be0798eec88"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lammps'...\n",
            "remote: Enumerating objects: 14161, done.\u001b[K\n",
            "remote: Counting objects: 100% (14161/14161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10275/10275), done.\u001b[K\n",
            "remote: Total 14161 (delta 4827), reused 7859 (delta 3650), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14161/14161), 129.17 MiB | 27.86 MiB/s, done.\n",
            "Resolving deltas: 100% (4827/4827), done.\n",
            "Updating files: 100% (13507/13507), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compile it with MKL Library."
      ],
      "metadata": {
        "id": "-RRkCs9iQACg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mkl-include"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBHpSXUc8wsa",
        "outputId": "03b62c85-7ddd-441a-e7b2-547bef74a7b6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mkl-include in /usr/local/lib/python3.10/dist-packages (2025.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install CMake for builing pair_allegro with libtorch and LAMMPS."
      ],
      "metadata": {
        "id": "PLNlAaznQFRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
        "!sh ./cmake-3.23.1-linux-x86_64.sh --prefix=/usr/local --exclude-subdir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyifJb3q81U1",
        "outputId": "335fa65f-ae03-4d2d-a52b-2cbf560882c3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-11 12:57:02--  https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1-linux-x86_64.sh\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241211%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241211T125702Z&X-Amz-Expires=300&X-Amz-Signature=6c1259161d9a6a76510d3642739ac9b6ffbaabee008a85359c4cfea76eb25a7e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-11 12:57:03--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/b57ef8e0-dc5d-4025-b6b0-7cdf73bbdecb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241211%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241211T125702Z&X-Amz-Expires=300&X-Amz-Signature=6c1259161d9a6a76510d3642739ac9b6ffbaabee008a85359c4cfea76eb25a7e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcmake-3.23.1-linux-x86_64.sh&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46004365 (44M) [application/octet-stream]\n",
            "Saving to: ‘cmake-3.23.1-linux-x86_64.sh.1’\n",
            "\n",
            "cmake-3.23.1-linux- 100%[===================>]  43.87M   169MB/s    in 0.3s    \n",
            "\n",
            "2024-12-11 12:57:03 (169 MB/s) - ‘cmake-3.23.1-linux-x86_64.sh.1’ saved [46004365/46004365]\n",
            "\n",
            "CMake Installer Version: 3.23.1, Copyright (c) Kitware\n",
            "This is a self-extracting archive.\n",
            "The archive will be extracted to: /usr/local\n",
            "\n",
            "Using target directory: /usr/local\n",
            "Extracting, please wait...\n",
            "\n",
            "Unpacking finished successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install libtorch. This library should match with NVCC, Nvidia toolchain present in your system."
      ],
      "metadata": {
        "id": "R6AVXNbKQTZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf libtorch\n",
        "!wget wget https://download.pytorch.org/libtorch/cu124/libtorch-cxx11-abi-shared-with-deps-2.5.1%2Bcu124.zip\n",
        "!unzip libtorch-cxx11-abi-shared-with-deps-2.5.1+cu124.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L0y1M-c9HNG",
        "outputId": "4e79fe36-cf65-4fb1-e0e4-2cebd7aead22"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/grid_sampler_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/group_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gru_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/gt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hamming_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hann_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardshrink_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardsigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardswish_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hardtanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/heaviside_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hinge_embedding_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogram_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/histogramdd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hsplit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hspmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hstack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/huber_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/hypot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/igamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/igammac_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/im2col_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/imag_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_add_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_fill_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_put_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/index_select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/infinitely_differentiable_gelu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/inner_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/instance_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/int_repr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/inverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_coalesced_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_complex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_conj_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_distributed_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_floating_point_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_inference_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_leaf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_nonzero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_pinned_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_same_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_set_to_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_signed_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/is_vulkan_available_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isclose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isfinite_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isin_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isinf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isnan_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isneginf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isposinf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/isreal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/istft_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/item.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/item_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kaiser_window_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kl_div_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kron_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/kthvalue_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/l1_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/layer_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lcm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ldexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/le.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/le_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/leaky_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lerp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/less.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_equal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/less_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lgamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_fresh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cholesky_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cond_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_cross_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_det_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_diagonal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eig_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvals_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_eigvalsh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_householder_product_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_inv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_factor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_ldl_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lstsq_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_factor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_lu_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matmul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_exp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_power_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_matrix_rank_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_multi_dot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_pinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_qr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_slogdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_solve_triangular_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_svdvals_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_tensorsolve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vander_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vecdot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linalg_vector_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/linspace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log10_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log1p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_normal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_sigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/log_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logaddexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logcumsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_and_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_not_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_or_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logical_xor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logspace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/logsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lshift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_mps_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lstm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/lu_unpack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mH_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mT_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/margin_ranking_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_fill_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/masked_select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matmul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_H_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_exp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/matrix_power_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool1d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool2d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_pool3d_with_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/max_unpool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/maximum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/median.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/median_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/meshgrid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/min.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/min_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/minimum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_add_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_convolution_transpose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_depthwise_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/miopen_rnn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mish_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_adaptive_avg_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_convolution_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_input_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_backward_weights_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_linear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv2d_weight_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_reorder_conv3d_weight_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mkldnn_rnn_layer_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mode_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/moveaxis_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/movedim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mps_convolution_transpose_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mse_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/msort_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mul_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multi_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multilabel_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multinomial_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/multiply_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/mvlgamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nan_to_num_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanmedian_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nanquantile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nansum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/narrow_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_channel_shuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_dropout_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_group_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_layer_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/native_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ne_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/negative_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nested_to_padded_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_empty_strided_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_full_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_ones_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/new_zeros_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nextafter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_nd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nll_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_numpy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nonzero_static_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_except_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/normal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/not_equal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/nuclear_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/numpy_T_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/one_hot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ones_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/or.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/or_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/orgqr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ormqr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/outer_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/output_nr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pad_sequence_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pairwise_distance_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pdist_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/permute_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pin_memory_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pinverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_shuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pixel_unshuffle_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_nll_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/poisson_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/polar_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/polygamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/positive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/pow_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/prelu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/prod_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/promote_types_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/put.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/put_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_axis_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_scales_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_per_channel_zero_points_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_scale_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/q_zero_point_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/qr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/qscheme_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_channel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_dynamic_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantize_per_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_batch_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_gru_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_lstm_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_max_pool3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_relu_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/quantized_rnn_tanh_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rad2deg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rand_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randint_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/random.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/random_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/randperm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/range.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/range_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/ravel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/real.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/real_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reciprocal_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/record_stream_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/refine_names_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reflection_pad3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu6_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/remainder_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rename_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/renorm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_interleave_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/repeat_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/replication_pad3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/requires_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/reshape_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_as_sparse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resize_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_conj_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/resolve_neg_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/result_type_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/retain_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/retains_grad_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rms_norm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_relu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_cell_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rnn_tanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/roll_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rot90_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/round.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/round_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/row_stack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rrelu_with_noise_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rshift_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsqrt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/rsub_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scalar_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scaled_dot_product_attention_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_add_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/scatter_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/searchsorted_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/segment_reduce_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/select_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/selu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/set.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_data_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/set_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sgn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sigmoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sign_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/signbit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/silu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sin_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sinh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/size.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_inverse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slice_scatter_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slogdet_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_forward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_dilated3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/slow_conv_transpose3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/smooth_l1_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/soft_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softplus_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/softshrink_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sort_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsc_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_bsr_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_compressed_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_coo_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csc_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_csr_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_mask_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_and_clear_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_resize_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sparse_sampled_addmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_airy_ai_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_j1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_bessel_y1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_u_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_v_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_chebyshev_polynomial_w_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_digamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_entr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erf_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfcx_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_erfinv_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_exp2_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_expm1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammainc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaincc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_gammaln_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_h_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_hermite_polynomial_he_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i0e_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_i1e_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_laguerre_polynomial_l_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_legendre_polynomial_p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log1p_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_ndtr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_log_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_logsumexp_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_i1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_modified_bessel_k1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_multigammaln_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_ndtri_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_polygamma_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_psi_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_round_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_scaled_modified_bessel_k1_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_sinc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_softmax_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_spherical_bessel_j0_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlog1py_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_xlogy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/special_zeta_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/split_with_sizes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sqrt_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/square.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/square_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/squeeze_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sspaddmm_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/std.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/std_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stft_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/stride_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sub_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/subtract_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sum_to_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/svd_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapaxes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/swapdims_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_for_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_constrain_range_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_numel_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_size_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_storage_offset_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/sym_stride_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/t.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/t_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/take.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_along_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/take_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tan_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tanh_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensor_split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tensordot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/thnn_conv2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/threshold_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tile_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_dense_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_mkldnn_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_padded_tensor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_bsr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_csr_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/to_sparse_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/topk_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trace_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/transpose_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapezoid_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trapz_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triangular_solve_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/tril_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triplet_margin_loss_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_indices_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/triu_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/true_divide_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/trunc_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/type_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unbind_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_dense_tensors_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unflatten_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unfold_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/uniform_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_consecutive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_consecutive_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unique_dim_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_chunk_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsafe_split_with_sizes_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/unsqueeze_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bicubic2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_bilinear2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_linear1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest1d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest2d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_nearest3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/upsample_trilinear3d_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/value_selecting_reduction_backward_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/values.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/values_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vander_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/var.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_mean_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/var_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vdot_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_complex_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_as_real_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_copy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/view_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vsplit_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/vstack_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/where.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/where_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_compositeexplicitautogradnonfunctional_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_meta.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/xlogy_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_compositeimplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/xor_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_cpu_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_cuda_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_meta_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zero_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_compositeexplicitautograd_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_compositeimplicitautogradnestedtensor_dispatch.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_like_ops.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_native.h  \n",
            "  inflating: libtorch/include/ATen/ops/zeros_ops.h  \n",
            "   creating: libtorch/include/ATen/hip/\n",
            "   creating: libtorch/include/ATen/hip/impl/\n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPAllocatorMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h  \n",
            "  inflating: libtorch/include/ATen/hip/impl/HIPStreamMasqueradingAsCUDA.h  \n",
            "   creating: libtorch/include/ATen/mps/\n",
            "  inflating: libtorch/include/ATen/mps/EmptyTensor.h  \n",
            "  inflating: libtorch/include/ATen/mps/IndexKernels.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSAllocator.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSAllocatorInterface.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSDevice.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSEvent.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSGeneratorImpl.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSGuardImpl.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSHooks.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSProfiler.h  \n",
            "  inflating: libtorch/include/ATen/mps/MPSStream.h  \n",
            "   creating: libtorch/include/ATen/miopen/\n",
            "  inflating: libtorch/include/ATen/miopen/Descriptors.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Exceptions.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Handle.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Types.h  \n",
            "  inflating: libtorch/include/ATen/miopen/Utils.h  \n",
            "  inflating: libtorch/include/ATen/miopen/miopen-wrapper.h  \n",
            "   creating: libtorch/include/ATen/detail/\n",
            "  inflating: libtorch/include/ATen/detail/AcceleratorHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/CUDAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/FunctionTraits.h  \n",
            "  inflating: libtorch/include/ATen/detail/HIPHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/IPUHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MAIAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MPSHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/MTIAHooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/PrivateUse1HooksInterface.h  \n",
            "  inflating: libtorch/include/ATen/detail/XPUHooksInterface.h  \n",
            "   creating: libtorch/include/ATen/native/\n",
            "  inflating: libtorch/include/ATen/native/Activation.h  \n",
            "  inflating: libtorch/include/ATen/native/AdaptivePooling.h  \n",
            "  inflating: libtorch/include/ATen/native/AmpKernels.h  \n",
            "  inflating: libtorch/include/ATen/native/BatchLinearAlgebra.h  \n",
            "  inflating: libtorch/include/ATen/native/BinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/BucketizationUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/CPUBlas.h  \n",
            "  inflating: libtorch/include/ATen/native/CPUFallback.h  \n",
            "  inflating: libtorch/include/ATen/native/CanUse32BitIndexMath.h  \n",
            "  inflating: libtorch/include/ATen/native/ComplexHelper.h  \n",
            "  inflating: libtorch/include/ATen/native/CompositeRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/CompositeRandomAccessorCommon.h  \n",
            "  inflating: libtorch/include/ATen/native/ConvUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/ConvolutionMM3d.h  \n",
            "  inflating: libtorch/include/ATen/native/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/Cross.h  \n",
            "  inflating: libtorch/include/ATen/native/DilatedConvolutionUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/DispatchStub.h  \n",
            "  inflating: libtorch/include/ATen/native/Distance.h  \n",
            "  inflating: libtorch/include/ATen/native/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/Distributions.h  \n",
            "  inflating: libtorch/include/ATen/native/EmbeddingBag.h  \n",
            "  inflating: libtorch/include/ATen/native/Fill.h  \n",
            "  inflating: libtorch/include/ATen/native/ForeachUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/FractionalMaxPooling.h  \n",
            "  inflating: libtorch/include/ATen/native/FunctionOfAMatrixUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedAdagrad.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedAdam.h  \n",
            "  inflating: libtorch/include/ATen/native/FusedSGD.h  \n",
            "  inflating: libtorch/include/ATen/native/GridSampler.h  \n",
            "  inflating: libtorch/include/ATen/native/GridSamplerUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Histogram.h  \n",
            "  inflating: libtorch/include/ATen/native/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/IndexingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Lerp.h  \n",
            "  inflating: libtorch/include/ATen/native/LinearAlgebra.h  \n",
            "  inflating: libtorch/include/ATen/native/LinearAlgebraUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/LossMulti.h  \n",
            "  inflating: libtorch/include/ATen/native/Math.h  \n",
            "  inflating: libtorch/include/ATen/native/MathBitFallThroughLists.h  \n",
            "  inflating: libtorch/include/ATen/native/MathBitsFallback.h  \n",
            "  inflating: libtorch/include/ATen/native/MaxPooling.h  \n",
            "  inflating: libtorch/include/ATen/native/NonEmptyUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/NonSymbolicBC.h  \n",
            "  inflating: libtorch/include/ATen/native/Normalization.h  \n",
            "  inflating: libtorch/include/ATen/native/Padding.h  \n",
            "  inflating: libtorch/include/ATen/native/PixelShuffle.h  \n",
            "  inflating: libtorch/include/ATen/native/PointwiseOps.h  \n",
            "  inflating: libtorch/include/ATen/native/Pool.h  \n",
            "  inflating: libtorch/include/ATen/native/Pow.h  \n",
            "  inflating: libtorch/include/ATen/native/RNN.h  \n",
            "  inflating: libtorch/include/ATen/native/RangeFactories.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceAllOps.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/ReduceOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/ReductionType.h  \n",
            "  inflating: libtorch/include/ATen/native/Repeat.h  \n",
            "  inflating: libtorch/include/ATen/native/Resize.h  \n",
            "  inflating: libtorch/include/ATen/native/ResizeCommon.h  \n",
            "  inflating: libtorch/include/ATen/native/ScatterGatherChecks.h  \n",
            "  inflating: libtorch/include/ATen/native/SegmentReduce.h  \n",
            "  inflating: libtorch/include/ATen/native/SharedReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/SobolEngineOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/Sorting.h  \n",
            "  inflating: libtorch/include/ATen/native/SortingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/SparseTensorUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/SpectralOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/StridedRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorAdvancedIndexing.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorAdvancedIndexingUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorCompare.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorConversions.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorDimApply.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorFactories.h  \n",
            " extracting: libtorch/include/ATen/native/TensorIterator.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorIteratorDynamicCasting.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorProperties.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorShape.h  \n",
            "  inflating: libtorch/include/ATen/native/TensorTransformations.h  \n",
            "  inflating: libtorch/include/ATen/native/TopKImpl.h  \n",
            "  inflating: libtorch/include/ATen/native/TransposeType.h  \n",
            "  inflating: libtorch/include/ATen/native/TriangularOpsUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/TypeProperties.h  \n",
            "  inflating: libtorch/include/ATen/native/UnaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/Unfold2d.h  \n",
            "  inflating: libtorch/include/ATen/native/Unfold3d.h  \n",
            "  inflating: libtorch/include/ATen/native/UnfoldBackward.h  \n",
            "  inflating: libtorch/include/ATen/native/UpSample.h  \n",
            "  inflating: libtorch/include/ATen/native/batch_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/group_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/im2col.h  \n",
            "  inflating: libtorch/include/ATen/native/im2col_shape_check.h  \n",
            "  inflating: libtorch/include/ATen/native/layer_norm.h  \n",
            "  inflating: libtorch/include/ATen/native/verbose_wrapper.h  \n",
            "  inflating: libtorch/include/ATen/native/vol2col.h  \n",
            "   creating: libtorch/include/ATen/native/cpu/\n",
            "  inflating: libtorch/include/ATen/native/cpu/AtomicAddFloat.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/CatKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/ChannelShuffleKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/CopyKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/DepthwiseConvKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/GridSamplerKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/IndexKernelUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Intrinsics.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/IsContiguous.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/LogAddExp.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Loops.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/MaxUnpoolKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/PixelShuffleKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/Reduce.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/ReduceUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SampledAddmmKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SerialStackImpl.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SoftmaxKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/SpmmReduceKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/StackKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/UpSampleKernelAVXAntialias.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/WeightNormKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/avx_mathfun.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/int_mm_kernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/mixed_data_type.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/moments_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cpu/zmath.h  \n",
            "   creating: libtorch/include/ATen/native/cuda/\n",
            "  inflating: libtorch/include/ATen/native/cuda/Activation.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/BinaryInternal.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CompositeRandomAccessor.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CuFFTPlanCache.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CuFFTUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/DistributionTemplates.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Distributions.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/GridSampler.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/LaunchUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MiscUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ReduceOps.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Resize.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/RowwiseScaledMM.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ScanKernels.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Sort.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortStable.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Sorting.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorModeKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorTopK.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/jit_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/thread_constants.h  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CUDAJitLoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/CUDALoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/DeviceSqrt.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/EmbeddingBackwardKernel.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ForeachFunctors.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ForeachMinMaxFunctors.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/GridSampler.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/JitLoops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/KernelUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Loops.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Math.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MemoryAccess.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/MultiTensorApply.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Normalization.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/PersistentSoftmax.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Pow.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Randperm.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/Reduce.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/ScanUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortUtils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortingCommon.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/SortingRadixSelect.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/TensorModeKernel.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/UniqueCub.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/UpSample.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/block_reduce.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_amsgrad_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adam_utils.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adamw_amsgrad_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/fused_adamw_impl.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/im2col.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/reduction_template.cuh  \n",
            "  inflating: libtorch/include/ATen/native/cuda/vol2col.cuh  \n",
            "   creating: libtorch/include/ATen/native/mps/\n",
            "  inflating: libtorch/include/ATen/native/mps/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphSequoiaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphSonomaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/MPSGraphVenturaOps.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/OperationUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/TensorFactory.h  \n",
            "  inflating: libtorch/include/ATen/native/mps/UnaryConstants.h  \n",
            "   creating: libtorch/include/ATen/native/nested/\n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorBinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorMath.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorTransformerFunctions.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorTransformerUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/nested/NestedTensorUtils.h  \n",
            "   creating: libtorch/include/ATen/native/quantized/\n",
            "  inflating: libtorch/include/ATen/native/quantized/AffineQuantizer.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/AffineQuantizerBase.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/ConvUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/Copy.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/FakeQuantAffine.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/IndexKernel.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/PackedParams.h  \n",
            "   creating: libtorch/include/ATen/native/quantized/cpu/\n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/BinaryOps.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/EmbeddingPackedParams.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/OnednnUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QnnpackUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QuantUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/QuantizedOps.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/RuyUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/XnnpackUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/conv_serialization.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/fbgemm_utils.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/init_qnnpack.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/qembeddingbag.h  \n",
            "  inflating: libtorch/include/ATen/native/quantized/cpu/qembeddingbag_prepack.h  \n",
            "   creating: libtorch/include/ATen/native/transformers/\n",
            "  inflating: libtorch/include/ATen/native/transformers/attention.h  \n",
            "  inflating: libtorch/include/ATen/native/transformers/sdp_utils_cpp.h  \n",
            "   creating: libtorch/include/ATen/native/utils/\n",
            "  inflating: libtorch/include/ATen/native/utils/Factory.h  \n",
            "  inflating: libtorch/include/ATen/native/utils/ParamUtils.h  \n",
            "  inflating: libtorch/include/ATen/native/utils/ParamsHash.h  \n",
            "   creating: libtorch/include/ATen/quantized/\n",
            "  inflating: libtorch/include/ATen/quantized/QTensorImpl.h  \n",
            "  inflating: libtorch/include/ATen/quantized/Quantizer.h  \n",
            "   creating: libtorch/include/ATen/xpu/\n",
            "  inflating: libtorch/include/ATen/xpu/CachingHostAllocator.h  \n",
            "  inflating: libtorch/include/ATen/xpu/PinnedMemoryAllocator.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUContext.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUDevice.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUEvent.h  \n",
            "  inflating: libtorch/include/ATen/xpu/XPUGeneratorImpl.h  \n",
            "   creating: libtorch/include/ATen/xpu/detail/\n",
            "  inflating: libtorch/include/ATen/xpu/detail/XPUHooks.h  \n",
            "   creating: libtorch/include/c10/\n",
            "   creating: libtorch/include/c10/xpu/\n",
            "  inflating: libtorch/include/c10/xpu/XPUCachingAllocator.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUDeviceProp.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUException.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUFunctions.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUMacros.h  \n",
            "  inflating: libtorch/include/c10/xpu/XPUStream.h  \n",
            "   creating: libtorch/include/c10/xpu/impl/\n",
            "  inflating: libtorch/include/c10/xpu/impl/XPUGuardImpl.h  \n",
            "   creating: libtorch/include/c10/macros/\n",
            "  inflating: libtorch/include/c10/macros/Export.h  \n",
            "  inflating: libtorch/include/c10/macros/Macros.h  \n",
            "  inflating: libtorch/include/c10/macros/cmake_macros.h  \n",
            "   creating: libtorch/include/c10/core/\n",
            "  inflating: libtorch/include/c10/core/Allocator.h  \n",
            "  inflating: libtorch/include/c10/core/AutogradState.h  \n",
            "  inflating: libtorch/include/c10/core/Backend.h  \n",
            "  inflating: libtorch/include/c10/core/CPUAllocator.h  \n",
            "  inflating: libtorch/include/c10/core/CachingDeviceAllocator.h  \n",
            "  inflating: libtorch/include/c10/core/CompileTimeFunctionPointer.h  \n",
            "  inflating: libtorch/include/c10/core/ConstantSymNodeImpl.h  \n",
            "  inflating: libtorch/include/c10/core/Contiguity.h  \n",
            "  inflating: libtorch/include/c10/core/CopyBytes.h  \n",
            "  inflating: libtorch/include/c10/core/DefaultDtype.h  \n",
            "  inflating: libtorch/include/c10/core/DefaultTensorOptions.h  \n",
            "  inflating: libtorch/include/c10/core/Device.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceArray.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceGuard.h  \n",
            "  inflating: libtorch/include/c10/core/DeviceType.h  \n",
            "  inflating: libtorch/include/c10/core/DispatchKey.h  \n",
            "  inflating: libtorch/include/c10/core/DispatchKeySet.h  \n",
            "  inflating: libtorch/include/c10/core/DynamicCast.h  \n",
            "  inflating: libtorch/include/c10/core/Event.h  \n",
            "  inflating: libtorch/include/c10/core/GeneratorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/GradMode.h  \n",
            "  inflating: libtorch/include/c10/core/InferenceMode.h  \n",
            "  inflating: libtorch/include/c10/core/Layout.h  \n",
            "  inflating: libtorch/include/c10/core/MemoryFormat.h  \n",
            "  inflating: libtorch/include/c10/core/OptionalRef.h  \n",
            "  inflating: libtorch/include/c10/core/PyHandleCache.h  \n",
            "  inflating: libtorch/include/c10/core/QEngine.h  \n",
            "  inflating: libtorch/include/c10/core/QScheme.h  \n",
            "  inflating: libtorch/include/c10/core/RefcountedDeleter.h  \n",
            "  inflating: libtorch/include/c10/core/SafePyObject.h  \n",
            "  inflating: libtorch/include/c10/core/Scalar.h  \n",
            "  inflating: libtorch/include/c10/core/ScalarType.h  \n",
            "  inflating: libtorch/include/c10/core/ScalarTypeToTypeMeta.h  \n",
            "  inflating: libtorch/include/c10/core/Storage.h  \n",
            "  inflating: libtorch/include/c10/core/StorageImpl.h  \n",
            "  inflating: libtorch/include/c10/core/Stream.h  \n",
            "  inflating: libtorch/include/c10/core/StreamGuard.h  \n",
            "  inflating: libtorch/include/c10/core/SymBool.h  \n",
            "  inflating: libtorch/include/c10/core/SymFloat.h  \n",
            "  inflating: libtorch/include/c10/core/SymInt.h  \n",
            "  inflating: libtorch/include/c10/core/SymIntArrayRef.h  \n",
            "  inflating: libtorch/include/c10/core/SymNodeImpl.h  \n",
            "  inflating: libtorch/include/c10/core/SymbolicShapeMeta.h  \n",
            "  inflating: libtorch/include/c10/core/TensorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/TensorOptions.h  \n",
            "  inflating: libtorch/include/c10/core/UndefinedTensorImpl.h  \n",
            "  inflating: libtorch/include/c10/core/WrapDimMinimal.h  \n",
            "  inflating: libtorch/include/c10/core/alignment.h  \n",
            "  inflating: libtorch/include/c10/core/thread_pool.h  \n",
            "   creating: libtorch/include/c10/core/impl/\n",
            "  inflating: libtorch/include/c10/core/impl/COW.h  \n",
            "  inflating: libtorch/include/c10/core/impl/COWDeleter.h  \n",
            "  inflating: libtorch/include/c10/core/impl/DeviceGuardImplInterface.h  \n",
            "  inflating: libtorch/include/c10/core/impl/FakeGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/core/impl/GPUTrace.h  \n",
            "  inflating: libtorch/include/c10/core/impl/HermeticPyObjectTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineDeviceGuard.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineEvent.h  \n",
            "  inflating: libtorch/include/c10/core/impl/InlineStreamGuard.h  \n",
            "  inflating: libtorch/include/c10/core/impl/LocalDispatchKeySet.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PyInterpreter.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PyObjectSlot.h  \n",
            "  inflating: libtorch/include/c10/core/impl/PythonDispatcherTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/SizesAndStrides.h  \n",
            "  inflating: libtorch/include/c10/core/impl/TorchDispatchModeTLS.h  \n",
            "  inflating: libtorch/include/c10/core/impl/VirtualGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/core/impl/alloc_cpu.h  \n",
            "   creating: libtorch/include/c10/util/\n",
            "  inflating: libtorch/include/c10/util/AbortHandler.h  \n",
            "  inflating: libtorch/include/c10/util/AlignOf.h  \n",
            "  inflating: libtorch/include/c10/util/ApproximateClock.h  \n",
            "  inflating: libtorch/include/c10/util/Array.h  \n",
            "  inflating: libtorch/include/c10/util/ArrayRef.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16-inl.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16-math.h  \n",
            "  inflating: libtorch/include/c10/util/BFloat16.h  \n",
            "  inflating: libtorch/include/c10/util/Backtrace.h  \n",
            "  inflating: libtorch/include/c10/util/Bitset.h  \n",
            "  inflating: libtorch/include/c10/util/C++17.h  \n",
            "  inflating: libtorch/include/c10/util/CallOnce.h  \n",
            "  inflating: libtorch/include/c10/util/ConstexprCrc.h  \n",
            "  inflating: libtorch/include/c10/util/DeadlockDetection.h  \n",
            "  inflating: libtorch/include/c10/util/Deprecated.h  \n",
            "  inflating: libtorch/include/c10/util/DimVector.h  \n",
            "  inflating: libtorch/include/c10/util/DynamicCounter.h  \n",
            "  inflating: libtorch/include/c10/util/Exception.h  \n",
            "  inflating: libtorch/include/c10/util/ExclusivelyOwned.h  \n",
            "  inflating: libtorch/include/c10/util/ExclusivelyOwnedTensorTraits.h  \n",
            "  inflating: libtorch/include/c10/util/FbcodeMaps.h  \n",
            "  inflating: libtorch/include/c10/util/Flags.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fn-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fn.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fnuz-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e4m3fnuz.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2fnuz-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_e5m2fnuz.h  \n",
            "  inflating: libtorch/include/c10/util/Float8_fnuz_cvt.h  \n",
            "  inflating: libtorch/include/c10/util/FunctionRef.h  \n",
            "  inflating: libtorch/include/c10/util/Gauge.h  \n",
            "  inflating: libtorch/include/c10/util/Half-inl.h  \n",
            "  inflating: libtorch/include/c10/util/Half.h  \n",
            "  inflating: libtorch/include/c10/util/IdWrapper.h  \n",
            "  inflating: libtorch/include/c10/util/Lazy.h  \n",
            "  inflating: libtorch/include/c10/util/LeftRight.h  \n",
            "  inflating: libtorch/include/c10/util/Load.h  \n",
            "  inflating: libtorch/include/c10/util/Logging.h  \n",
            "  inflating: libtorch/include/c10/util/MathConstants.h  \n",
            "  inflating: libtorch/include/c10/util/MaybeOwned.h  \n",
            "  inflating: libtorch/include/c10/util/Metaprogramming.h  \n",
            "  inflating: libtorch/include/c10/util/NetworkFlow.h  \n",
            "  inflating: libtorch/include/c10/util/Optional.h  \n",
            "  inflating: libtorch/include/c10/util/OptionalArrayRef.h  \n",
            "  inflating: libtorch/include/c10/util/ParallelGuard.h  \n",
            "  inflating: libtorch/include/c10/util/Registry.h  \n",
            "  inflating: libtorch/include/c10/util/ScopeExit.h  \n",
            "  inflating: libtorch/include/c10/util/SmallBuffer.h  \n",
            "  inflating: libtorch/include/c10/util/SmallVector.h  \n",
            "  inflating: libtorch/include/c10/util/StringUtil.h  \n",
            "  inflating: libtorch/include/c10/util/Synchronized.h  \n",
            "  inflating: libtorch/include/c10/util/ThreadLocal.h  \n",
            "  inflating: libtorch/include/c10/util/ThreadLocalDebugInfo.h  \n",
            "  inflating: libtorch/include/c10/util/Type.h  \n",
            "  inflating: libtorch/include/c10/util/TypeCast.h  \n",
            "  inflating: libtorch/include/c10/util/TypeIndex.h  \n",
            "  inflating: libtorch/include/c10/util/TypeList.h  \n",
            "  inflating: libtorch/include/c10/util/TypeSafeSignMath.h  \n",
            "  inflating: libtorch/include/c10/util/TypeTraits.h  \n",
            "  inflating: libtorch/include/c10/util/Unicode.h  \n",
            "  inflating: libtorch/include/c10/util/UniqueVoidPtr.h  \n",
            "  inflating: libtorch/include/c10/util/Unroll.h  \n",
            "  inflating: libtorch/include/c10/util/WaitCounter.h  \n",
            "  inflating: libtorch/include/c10/util/accumulate.h  \n",
            "  inflating: libtorch/include/c10/util/bit_cast.h  \n",
            "  inflating: libtorch/include/c10/util/bits.h  \n",
            "  inflating: libtorch/include/c10/util/complex.h  \n",
            "  inflating: libtorch/include/c10/util/complex_math.h  \n",
            "  inflating: libtorch/include/c10/util/complex_utils.h  \n",
            "  inflating: libtorch/include/c10/util/copysign.h  \n",
            "  inflating: libtorch/include/c10/util/env.h  \n",
            "  inflating: libtorch/include/c10/util/flat_hash_map.h  \n",
            "  inflating: libtorch/include/c10/util/floating_point_utils.h  \n",
            "  inflating: libtorch/include/c10/util/generic_math.h  \n",
            "  inflating: libtorch/include/c10/util/hash.h  \n",
            "  inflating: libtorch/include/c10/util/int128.h  \n",
            "  inflating: libtorch/include/c10/util/intrusive_ptr.h  \n",
            "  inflating: libtorch/include/c10/util/irange.h  \n",
            "  inflating: libtorch/include/c10/util/llvmMathExtras.h  \n",
            "  inflating: libtorch/include/c10/util/logging_is_google_glog.h  \n",
            "  inflating: libtorch/include/c10/util/logging_is_not_google_glog.h  \n",
            "  inflating: libtorch/include/c10/util/numa.h  \n",
            "  inflating: libtorch/include/c10/util/order_preserving_flat_hash_map.h  \n",
            "  inflating: libtorch/include/c10/util/overloaded.h  \n",
            "  inflating: libtorch/include/c10/util/python_stub.h  \n",
            "  inflating: libtorch/include/c10/util/qint32.h  \n",
            "  inflating: libtorch/include/c10/util/qint8.h  \n",
            "  inflating: libtorch/include/c10/util/quint2x4.h  \n",
            "  inflating: libtorch/include/c10/util/quint4x2.h  \n",
            "  inflating: libtorch/include/c10/util/quint8.h  \n",
            "  inflating: libtorch/include/c10/util/safe_numerics.h  \n",
            "  inflating: libtorch/include/c10/util/signal_handler.h  \n",
            "  inflating: libtorch/include/c10/util/sparse_bitset.h  \n",
            "  inflating: libtorch/include/c10/util/ssize.h  \n",
            "  inflating: libtorch/include/c10/util/static_tracepoint.h  \n",
            "  inflating: libtorch/include/c10/util/static_tracepoint_elfx86.h  \n",
            "  inflating: libtorch/include/c10/util/strides.h  \n",
            "  inflating: libtorch/include/c10/util/string_utils.h  \n",
            "  inflating: libtorch/include/c10/util/string_view.h  \n",
            "  inflating: libtorch/include/c10/util/strong_type.h  \n",
            "  inflating: libtorch/include/c10/util/tempfile.h  \n",
            "  inflating: libtorch/include/c10/util/thread_name.h  \n",
            "  inflating: libtorch/include/c10/util/typeid.h  \n",
            "  inflating: libtorch/include/c10/util/win32-headers.h  \n",
            "   creating: libtorch/include/c10/cuda/\n",
            "  inflating: libtorch/include/c10/cuda/CUDAAllocatorConfig.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDACachingAllocator.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDADeviceAssertionHost.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAException.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAFunctions.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAGuard.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMacros.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMathCompat.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAMiscFunctions.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAStream.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAAlgorithm.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDADeviceAssertion.h  \n",
            "  inflating: libtorch/include/c10/cuda/CUDAGraphsC10Utils.h  \n",
            "  inflating: libtorch/include/c10/cuda/driver_api.h  \n",
            "   creating: libtorch/include/c10/cuda/impl/\n",
            "  inflating: libtorch/include/c10/cuda/impl/CUDAGuardImpl.h  \n",
            "  inflating: libtorch/include/c10/cuda/impl/CUDATest.h  \n",
            "  inflating: libtorch/include/c10/cuda/impl/cuda_cmake_macros.h  \n",
            "   creating: libtorch/include/caffe2/\n",
            "   creating: libtorch/include/caffe2/serialize/\n",
            "  inflating: libtorch/include/caffe2/serialize/crc_alt.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/file_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/in_memory_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/inline_container.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/istream_adapter.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/read_adapter_interface.h  \n",
            "  inflating: libtorch/include/caffe2/serialize/versions.h  \n",
            "  inflating: libtorch/include/clog.h  \n",
            "  inflating: libtorch/include/cpuinfo.h  \n",
            "  inflating: libtorch/include/dnnl_config.h  \n",
            "  inflating: libtorch/include/dnnl_debug.h  \n",
            "  inflating: libtorch/include/dnnl.h  \n",
            "  inflating: libtorch/include/dnnl_ocl.h  \n",
            "  inflating: libtorch/include/dnnl_sycl.h  \n",
            "  inflating: libtorch/include/dnnl_sycl_types.h  \n",
            "  inflating: libtorch/include/dnnl_threadpool.h  \n",
            "  inflating: libtorch/include/dnnl_types.h  \n",
            "  inflating: libtorch/include/dnnl_version.h  \n",
            "  inflating: libtorch/include/experiments-config.h  \n",
            "  inflating: libtorch/include/fp16.h  \n",
            "  inflating: libtorch/include/fxdiv.h  \n",
            "   creating: libtorch/include/kineto/\n",
            "  inflating: libtorch/include/kineto/AbstractConfig.h  \n",
            "  inflating: libtorch/include/kineto/ActivityProfilerInterface.h  \n",
            "  inflating: libtorch/include/kineto/ActivityTraceInterface.h  \n",
            "  inflating: libtorch/include/kineto/ActivityType.h  \n",
            "  inflating: libtorch/include/kineto/Config.h  \n",
            "  inflating: libtorch/include/kineto/ClientInterface.h  \n",
            "  inflating: libtorch/include/kineto/GenericTraceActivity.h  \n",
            "  inflating: libtorch/include/kineto/IActivityProfiler.h  \n",
            "  inflating: libtorch/include/kineto/ILoggerObserver.h  \n",
            "  inflating: libtorch/include/kineto/ITraceActivity.h  \n",
            "  inflating: libtorch/include/kineto/LoggingAPI.h  \n",
            "  inflating: libtorch/include/kineto/TraceSpan.h  \n",
            "  inflating: libtorch/include/kineto/ThreadUtil.h  \n",
            "  inflating: libtorch/include/kineto/libkineto.h  \n",
            "  inflating: libtorch/include/kineto/time_since_epoch.h  \n",
            "  inflating: libtorch/include/kineto/output_base.h  \n",
            "  inflating: libtorch/include/libshm.h  \n",
            "  inflating: libtorch/include/nnpack.h  \n",
            "  inflating: libtorch/include/psimd.h  \n",
            "  inflating: libtorch/include/pthreadpool.h  \n",
            "   creating: libtorch/include/pybind11/\n",
            "  inflating: libtorch/include/pybind11/attr.h  \n",
            "  inflating: libtorch/include/pybind11/buffer_info.h  \n",
            "  inflating: libtorch/include/pybind11/cast.h  \n",
            "  inflating: libtorch/include/pybind11/chrono.h  \n",
            "  inflating: libtorch/include/pybind11/common.h  \n",
            "  inflating: libtorch/include/pybind11/complex.h  \n",
            "  inflating: libtorch/include/pybind11/eigen.h  \n",
            "  inflating: libtorch/include/pybind11/embed.h  \n",
            "  inflating: libtorch/include/pybind11/eval.h  \n",
            "  inflating: libtorch/include/pybind11/functional.h  \n",
            "  inflating: libtorch/include/pybind11/gil.h  \n",
            "  inflating: libtorch/include/pybind11/gil_safe_call_once.h  \n",
            "  inflating: libtorch/include/pybind11/iostream.h  \n",
            "  inflating: libtorch/include/pybind11/numpy.h  \n",
            "  inflating: libtorch/include/pybind11/operators.h  \n",
            "  inflating: libtorch/include/pybind11/options.h  \n",
            "  inflating: libtorch/include/pybind11/pybind11.h  \n",
            "  inflating: libtorch/include/pybind11/pytypes.h  \n",
            "  inflating: libtorch/include/pybind11/stl.h  \n",
            "  inflating: libtorch/include/pybind11/stl_bind.h  \n",
            "  inflating: libtorch/include/pybind11/type_caster_pyobject_ptr.h  \n",
            "  inflating: libtorch/include/pybind11/typing.h  \n",
            "   creating: libtorch/include/pybind11/detail/\n",
            "  inflating: libtorch/include/pybind11/detail/class.h  \n",
            "  inflating: libtorch/include/pybind11/detail/common.h  \n",
            "  inflating: libtorch/include/pybind11/detail/descr.h  \n",
            "  inflating: libtorch/include/pybind11/detail/init.h  \n",
            "  inflating: libtorch/include/pybind11/detail/internals.h  \n",
            "  inflating: libtorch/include/pybind11/detail/type_caster_base.h  \n",
            "  inflating: libtorch/include/pybind11/detail/typeid.h  \n",
            "  inflating: libtorch/include/pybind11/detail/value_and_holder.h  \n",
            "   creating: libtorch/include/pybind11/eigen/\n",
            "  inflating: libtorch/include/pybind11/eigen/common.h  \n",
            "  inflating: libtorch/include/pybind11/eigen/matrix.h  \n",
            "  inflating: libtorch/include/pybind11/eigen/tensor.h  \n",
            "  inflating: libtorch/include/qnnpack_func.h  \n",
            "   creating: libtorch/include/tensorpipe/\n",
            "  inflating: libtorch/include/tensorpipe/tensorpipe.h  \n",
            "  inflating: libtorch/include/tensorpipe/config.h  \n",
            "  inflating: libtorch/include/tensorpipe/tensorpipe_cuda.h  \n",
            "  inflating: libtorch/include/tensorpipe/config_cuda.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/\n",
            "  inflating: libtorch/include/tensorpipe/channel/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/channel/error.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/basic/\n",
            "  inflating: libtorch/include/tensorpipe/channel/basic/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/cma/\n",
            "  inflating: libtorch/include/tensorpipe/channel/cma/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/mpt/\n",
            "  inflating: libtorch/include/tensorpipe/channel/mpt/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/channel/xth/\n",
            "  inflating: libtorch/include/tensorpipe/channel/xth/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/common/\n",
            "  inflating: libtorch/include/tensorpipe/common/buffer.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/cpu_buffer.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/device.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/optional.h  \n",
            "  inflating: libtorch/include/tensorpipe/common/cuda_buffer.h  \n",
            "   creating: libtorch/include/tensorpipe/core/\n",
            "  inflating: libtorch/include/tensorpipe/core/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/listener.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/message.h  \n",
            "  inflating: libtorch/include/tensorpipe/core/pipe.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/\n",
            "   creating: libtorch/include/tensorpipe/transport/shm/\n",
            "  inflating: libtorch/include/tensorpipe/transport/shm/factory.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/uv/\n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/factory.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/uv/utility.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/context.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/error.h  \n",
            "   creating: libtorch/include/tensorpipe/transport/ibv/\n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/error.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/factory.h  \n",
            "  inflating: libtorch/include/tensorpipe/transport/ibv/utility.h  \n",
            "   creating: libtorch/include/THC/\n",
            "  inflating: libtorch/include/THC/THCAtomics.cuh  \n",
            " extracting: libtorch/include/THC/THCDeviceUtils.cuh  \n",
            "   creating: libtorch/include/torch/\n",
            "  inflating: libtorch/include/torch/custom_class.h  \n",
            "  inflating: libtorch/include/torch/custom_class_detail.h  \n",
            "  inflating: libtorch/include/torch/library.h  \n",
            "  inflating: libtorch/include/torch/script.h  \n",
            "  inflating: libtorch/include/torch/extension.h  \n",
            "   creating: libtorch/include/torch/csrc/\n",
            "  inflating: libtorch/include/torch/csrc/Export.h  \n",
            "  inflating: libtorch/include/torch/csrc/CudaIPCTypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/DataLoader.h  \n",
            "  inflating: libtorch/include/torch/csrc/Device.h  \n",
            "  inflating: libtorch/include/torch/csrc/Dtype.h  \n",
            "  inflating: libtorch/include/torch/csrc/DynamicTypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/Exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/Generator.h  \n",
            "  inflating: libtorch/include/torch/csrc/Layout.h  \n",
            "  inflating: libtorch/include/torch/csrc/MemoryFormat.h  \n",
            "  inflating: libtorch/include/torch/csrc/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/PyInterpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/QScheme.h  \n",
            "  inflating: libtorch/include/torch/csrc/Size.h  \n",
            "  inflating: libtorch/include/torch/csrc/Storage.h  \n",
            "  inflating: libtorch/include/torch/csrc/StorageMethods.h  \n",
            "  inflating: libtorch/include/torch/csrc/StorageSharing.h  \n",
            "  inflating: libtorch/include/torch/csrc/Stream.h  \n",
            "  inflating: libtorch/include/torch/csrc/THConcat.h  \n",
            "  inflating: libtorch/include/torch/csrc/THP.h  \n",
            "  inflating: libtorch/include/torch/csrc/TypeInfo.h  \n",
            "  inflating: libtorch/include/torch/csrc/Types.h  \n",
            "  inflating: libtorch/include/torch/csrc/copy_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/itt_wrapper.h  \n",
            "  inflating: libtorch/include/torch/csrc/python_dimname.h  \n",
            "  inflating: libtorch/include/torch/csrc/python_headers.h  \n",
            "  inflating: libtorch/include/torch/csrc/serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/\n",
            "   creating: libtorch/include/torch/csrc/api/include/\n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/all.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/arg.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/cuda.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/enum.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/expanding_array.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/fft.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/imethod.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/linalg.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/mps.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nested.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/ordered_dict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/python.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/sparse.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/special.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/torch.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/xpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/version.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader_options.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/example.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/iterator.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/worker_exception.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/dataloader/stateless.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/datasets/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/chunk.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/map.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/mnist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/shared.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/stateful.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/datasets/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/detail/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/data_shuttle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/queue.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/detail/sequencers.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/samplers/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/custom_batch_request.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/distributed.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/random.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/sequential.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/samplers/stream.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/data/transforms/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/base.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/collate.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/lambda.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/stack.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/data/transforms/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/detail/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/detail/TensorDataContainer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/detail/static.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/cloneable.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/pimpl-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/pimpl.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/functional/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/functional/vision.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/options/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/adaptive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/rnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformercoder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/transformerlayer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/options/vision.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/modules/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/activation.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/adaptive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/batchnorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/common.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/conv.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/embedding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/fold.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/instancenorm.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/loss.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/normalization.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/padding.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/pixelshuffle.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/rnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformercoder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/transformerlayer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/upsampling.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any_module_holder.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/any_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/functional.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/moduledict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/modulelist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/named_any.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/parameterdict.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/parameterlist.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/modules/container/sequential.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/parallel/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/parallel/data_parallel.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/nn/utils/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/clip_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/convert_parameters.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/nn/utils/rnn.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/optim/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adagrad.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adam.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/adamw.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/lbfgs.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/optimizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/rmsprop.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/serialize.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/sgd.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/reduce_on_plateau_scheduler.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/optim/schedulers/step_lr.h  \n",
            "   creating: libtorch/include/torch/csrc/api/include/torch/serialize/\n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/input-archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/output-archive.h  \n",
            "  inflating: libtorch/include/torch/csrc/api/include/torch/serialize/tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/onnx/\n",
            "  inflating: libtorch/include/torch/csrc/onnx/back_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/onnx/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/onnx/onnx.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/api.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/collection.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/combined_traceback.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/containers.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/data_flow.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/events.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/kineto_shim.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/perf-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/perf.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/util.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/orchestration/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/python_tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/orchestration/vulkan.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/standalone/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/execution_trace_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/itt_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/nvtx_observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/standalone/privateuse1_observer.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/stubs/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/stubs/base.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/unwind/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/action.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/communicate.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/dwarf_enums.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/dwarf_symbolize_enums.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/eh_frame_hdr.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/fast_symbolizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/fde.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/lexer.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/line_number_program.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/mem_file.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/range_table.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/sections.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwind.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwind_error.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/unwind/unwinder.h  \n",
            "   creating: libtorch/include/torch/csrc/profiler/python/\n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/combined_traceback.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/profiler/python/pybind.h  \n",
            "   creating: libtorch/include/torch/csrc/utils/\n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_numpy.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_new.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/byte_order.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/cpp_stacktraces.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/cuda_enabled.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/device_lazy_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/disable_torch_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/invalid_arguments.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/nested.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/numpy_stub.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/object_ptr.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/out_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pycfunction_helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pyobject_preservation.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_arg_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_dispatch.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_numbers.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_raii.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_scalars.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_strings.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_stub.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_symnode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_torch_function_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/python_tuples.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/pythoncapi_compat.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/schema_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/six.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/structseq.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_apply.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_dtypes.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_flatten.h  \n",
            " extracting: libtorch/include/torch/csrc/utils/tensor_layouts.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_memoryformats.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_qschemes.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/tensor_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/throughput_benchmark-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/throughput_benchmark.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/torch_dispatch_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/variadic.h  \n",
            "  inflating: libtorch/include/torch/csrc/utils/verbose.h  \n",
            "   creating: libtorch/include/torch/csrc/tensor/\n",
            "  inflating: libtorch/include/torch/csrc/tensor/python_tensor.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/\n",
            "   creating: libtorch/include/torch/csrc/lazy/backend/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_data.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_device.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/backend_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/backend/lowering_context.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/cache.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/config.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/debug_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/dynamic_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/hash.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_builder.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_dump_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ir_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/lazy_graph_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/metrics.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/multi_wait.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/permutation_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/shape.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/shape_inference.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/tensor_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/thread_pool.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/trie.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/unique.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/util.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/internal_ops/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/internal_ops/ltc_ops.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/core/ops/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ops/arithmetic_ir_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/core/ops/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/python/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/python/python_util.h  \n",
            "   creating: libtorch/include/torch/csrc/lazy/ts_backend/\n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/config.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/dynamic_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ir_builder.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/tensor_aten_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_autograd_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_backend_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_eager_fallback.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_lowering_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_node.h  \n",
            "  inflating: libtorch/include/torch/csrc/lazy/ts_backend/ts_node_lowering.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/FunctionsManual.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/InferenceMode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/VariableTypeUtils.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/anomaly_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/autograd_not_implemented_fallback.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/cpp_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/custom_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/edge.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/engine.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/forward_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/function_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/grad_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/graph_task.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/input_buffer.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/input_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/jit_decomp_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_kineto.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_legacy.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/profiler_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_anomaly_mode.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_cpp_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_engine.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_enum_tag.h  \n",
            " extracting: libtorch/include/torch/csrc/autograd/python_fft_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_function.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_legacy_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_linalg_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_nested_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_nn_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_saved_variable_hooks.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_sparse_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_special_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_torch_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/python_variable_indexing.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/record_function_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/saved_variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/saved_variable_hooks.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/symbolic.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/variable.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/variable_info.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/functions/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/accumulate_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/basic_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/functions/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/generated/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/python_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/python_return_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/VariableType.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/Functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/variable_factories.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/generated/ViewFuncs.h  \n",
            "   creating: libtorch/include/torch/csrc/autograd/utils/\n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/error_messages.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/grad_layout_contract.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/lambda_post_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/python_arg_parsing.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/warnings.h  \n",
            "  inflating: libtorch/include/torch/csrc/autograd/utils/wrap_outputs.h  \n",
            "   creating: libtorch/include/torch/csrc/xpu/\n",
            "  inflating: libtorch/include/torch/csrc/xpu/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/xpu/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/xpu/Stream.h  \n",
            "   creating: libtorch/include/torch/csrc/cuda/\n",
            "  inflating: libtorch/include/torch/csrc/cuda/CUDAPluggableAllocator.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Event.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/GdsFile.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Module.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/Stream.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/THCP.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/device_set.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/memory_snapshot.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/nccl.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/python_comm.h  \n",
            "  inflating: libtorch/include/torch/csrc/cuda/python_nccl.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/\n",
            "   creating: libtorch/include/torch/csrc/distributed/c10d/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/CUDASymmetricMemory-inl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TraceUtils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/c10d.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/debug.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/error.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/logging.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/python_comm_hook.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/socket.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Backend.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Backoff.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/CUDASymmetricMemory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/DMAConnectivity.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/FakeProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/FileStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Functional.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/GlooDeviceFactory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/GroupRegistry.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/HashStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/NCCLUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/NanCheck.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ParamCommsUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/PrefixStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupGloo.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupMPI.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupUCC.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/ProcessGroupWrapper.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/PyProcessGroup.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/RankLocal.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Store.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/SymmetricMemory.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TCPStore.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/TCPStoreBackend.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Types.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UCCTracing.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UCCUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/UnixSockUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Utils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/WinSockUtils.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/Work.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/comm.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/default_comm_hooks.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/intra_node_comm.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/logger.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/reducer.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/reducer_timer.hpp  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/c10d/sequence_num.hpp  \n",
            "   creating: libtorch/include/torch/csrc/distributed/rpc/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/agent_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/message.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/py_rref.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/python_rpc_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/request_callback_no_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc_agent.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rpc_command_base.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/rref_proto.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/script_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/tensorpipe_agent.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/tensorpipe_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/torchscript_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/unpickled_python_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/unpickled_python_remote_call.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/rpc/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/\n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/context/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/context/container.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/context/context.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/functions/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/functions/recvrpc_backward.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/functions/sendrpc_backward.h  \n",
            "   creating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/\n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/autograd_metadata.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_resp.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rref_backward_req.h  \n",
            "  inflating: libtorch/include/torch/csrc/distributed/autograd/rpc_messages/rref_backward_resp.h  \n",
            "   creating: libtorch/include/torch/csrc/dynamo/\n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cache_entry.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/compiled_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpp_shim.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpython_defs.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/cpython_includes.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/debug_macros.h  \n",
            " extracting: libtorch/include/torch/csrc/dynamo/eval_frame.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/extra_state.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/framelocals_mapping.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/guards.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/python_compiled_autograd.h  \n",
            "  inflating: libtorch/include/torch/csrc/dynamo/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/inductor_ops.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_runner/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner_cpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/model_container_runner_cuda.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runner/pybind.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_runtime/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/arrayref_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/device_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/model.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/model_container.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/scalar_to_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/thread_local.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_runtime/utils_cuda.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/mkldnn_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/oss_proxy_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/proxy_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/tensor_converter.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/utils.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/c/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/c/shim.h  \n",
            "   creating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/\n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h  \n",
            "  inflating: libtorch/include/torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/\n",
            "   creating: libtorch/include/torch/csrc/jit/api/\n",
            "  inflating: libtorch/include/torch/csrc/jit/api/compilation_unit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/function_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/method.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/api/object.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/serialization/\n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/callstack_debug_info_serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/source_range_serialization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/export.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/export_bytecode.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/flatbuffer_serializer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/flatbuffer_serializer_jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_export_helpers.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_read.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/import_source.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/mobile_bytecode_generated.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/onnx.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/pickle.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/pickler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/python_print.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/source_range_serialization_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/storage_context.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/type_name_uniquer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/serialization/unpickler.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/python/\n",
            "  inflating: libtorch/include/torch/csrc/jit/python/init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/module_python.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/pybind.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/pybind_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_arg_flatten.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_custom_class.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_dict.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_ivalue.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_sugared_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/python_tree_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/script_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/update_graph_executor_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/python/utf8_decoding_ignore.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/mobile/\n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/code.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/file_format.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/flatbuffer_loader.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/frame.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/function.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import_data.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/import_export_common.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/interpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/method.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/observer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/parse_bytecode.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/parse_operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/prim_ops_registery.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/profiler_edge.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/promoted_prim_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/quantization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/register_ops_common_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/mobile/upgrader_mobile.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/testing/\n",
            "  inflating: libtorch/include/torch/csrc/jit/testing/file_check.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/testing/hooks_for_testing.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/tensorexpr/\n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/block_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/bounds_inference.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/bounds_overlap.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cpp_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cpp_intrinsics.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cuda_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/cuda_random.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/eval.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/expr.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions_core.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/external_functions_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/fwd_decls.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/graph_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/half_support.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/hash_provider.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/intrinsic_symbols.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_cloner.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_mutator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_printer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_simplifier.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_verifier.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/ir_visitor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/kernel.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/llvm_codegen.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/llvm_jit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/loopnest.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/loopnest_randomization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/lowerings.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/mem_dependency_checker.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/reduction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/registerizer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/stmt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/tensorexpr_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/unique_name_manager.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/var_substitutor.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/tensorexpr/operators/\n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/conv2d.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/matmul.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/misc.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/norm.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/pointwise.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/quantization.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/reduction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/tensorexpr/operators/softmax.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/codegen/\n",
            "   creating: libtorch/include/torch/csrc/jit/codegen/cuda/\n",
            "  inflating: libtorch/include/torch/csrc/jit/codegen/cuda/interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/jit_log.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/jit_opt_limit.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/resource_guard.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/backends/\n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_debug_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_debug_info.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_detail.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_init.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_interface.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_preprocess.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/backends/backend_resolver.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/add_if_then_else.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/annotate_warns.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/autocast.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/bailout_graph.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/batch_mm.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/canonicalize.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/canonicalize_graph_fuser_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/check_strict_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/clear_profiling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/clear_undefinedness.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/common_subexpression_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/concat_opt.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/constant_pooling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/constant_propagation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/create_autodiff_subgraphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/create_functional_graphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/dead_code_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/decompose_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/device_type_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/dtype_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/eliminate_no_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/erase_number_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fixup_trace_scope_blocks.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fold_conv_bn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fold_linear_bn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/freeze_module.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_concat_linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_conv_add_relu_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_conv_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_graph_optimizations.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_linear_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_linear_transpose.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/frozen_ops_to_mkldnn.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fuse_linear.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/fuse_relu.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/graph_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/graph_rewrite_helper.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/guard_elimination.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/hoist_conv_packed_params.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_autodiff_subgraphs.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_fork_wait.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inline_forked_closures.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inliner.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/inplace_check.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/insert_guards.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/integer_value_refinement.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lift_closures.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/liveness.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/loop_unrolling.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_grad_of.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_graph.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/lower_tuples.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/metal_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/mkldnn_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/mobile_optimizer_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/normalize_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/onednn_graph_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/onnx.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/pass_manager.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_alias_sensitive.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_dict_idioms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_list_idioms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/peephole_non_tensor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/prepack_folding.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/refine_tuple_types.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_dropout.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_exceptions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_expands.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_inplace_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_mutation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/remove_redundant_profiles.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/replacement_of_old_operators.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/requires_grad_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/restore_mutation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/shape_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/specialize_autogradzero.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/subgraph_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_cache.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/symbolic_shape_runtime_fusion.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/tensorexpr_fuser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/update_differentiable_graph_requires_grad.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/value_refinement_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/variadic_ops.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/vulkan_rewrite.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/xnnpack_rewrite.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/quantization/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/dedup_module_uses.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/finalize.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/fusion_passes.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/helper.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/insert_observers.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/insert_quant_dequant.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/quantization_patterns.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/quantization_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/quantization/register_packed_params.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/passes/utils/\n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/check_alias_annotation.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/memory_dag.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/op_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/optimization_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/passes/utils/subgraph_utils.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/runtime/\n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/argument_spec.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/autodiff.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/calculate_necessary_args.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/custom_operator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/decomposition_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/decomposition_registry_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/exception_message.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_executor.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/graph_iterator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/instruction.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/interpreter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/jit_exception.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/jit_trace.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/logging.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/operator.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/operator_options.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/print_handler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/profiling_graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/profiling_record.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/register_ops_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/script_profile.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/serialized_shape_function_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/shape_function_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/simple_graph_executor_impl.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/slice_indices_adjust.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_script.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_shape_registry.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/symbolic_shape_registry_util.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/vararg_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/runtime/variable_tensor_list.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/ir/\n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/alias_analysis.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/attributes.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/graph_node_list.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/graph_utils.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/ir.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/ir_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/irparser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/named_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/node_hashing.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/scope.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/subgraph_matcher.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/ir/type_hashing.h  \n",
            "   creating: libtorch/include/torch/csrc/jit/frontend/\n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/source_range.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/lexer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/strtod.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parser_constants.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/function_schema_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parse_string_literal.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/schema_type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/error_report.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tree.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/builtin_functions.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/canonicalize_modified_loop.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/concrete_module_type.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/convert_to_ssa.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/edit_distance.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/exit_transforms.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/inline_loop_condition.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/ir_emitter.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/mini_environment.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/name_mangler.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/resolver.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/schema_matching.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/script_type_parser.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/source_ref.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/sugared_value.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tracer.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/tree_views.h  \n",
            "  inflating: libtorch/include/torch/csrc/jit/frontend/versioned_symbols.h  \n",
            "  inflating: libtorch/include/xnnpack.h  \n",
            "   creating: libtorch/share/\n",
            "   creating: libtorch/share/cmake/\n",
            "   creating: libtorch/share/cmake/ATen/\n",
            "  inflating: libtorch/share/cmake/ATen/ATenConfig.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Config.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUDAToolkit.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUSPARSELT.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindCUDSS.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/FindSYCLToolkit.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Targets.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Caffe2Targets-release.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/public/\n",
            "  inflating: libtorch/share/cmake/Caffe2/public/cuda.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/xpu.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/glog.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/gflags.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/mkl.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/mkldnn.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/protobuf.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/utils.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/public/LoadHIP.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/FindCUDA.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/FindCUDNN.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/CMakeInitializeConfigs.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindPackageHandleStandardArgs.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindPackageMessage.cmake  \n",
            "   creating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/\n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/make2cmake.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/parse_cubin.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/run_nvcc.cmake  \n",
            "  inflating: libtorch/share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake  \n",
            "   creating: libtorch/share/cmake/Tensorpipe/\n",
            "  inflating: libtorch/share/cmake/Tensorpipe/TensorpipeTargets.cmake  \n",
            "  inflating: libtorch/share/cmake/Tensorpipe/TensorpipeTargets-release.cmake  \n",
            "   creating: libtorch/share/cmake/Torch/\n",
            "  inflating: libtorch/share/cmake/Torch/TorchConfigVersion.cmake  \n",
            "  inflating: libtorch/share/cmake/Torch/TorchConfig.cmake  \n",
            " extracting: libtorch/build-version  \n",
            "  inflating: libtorch/build-hash     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch is not compatible with latest python 3.12+, make sure your python, libtorch and nvidia toolchain are in sync as per documentation.\n",
        "Install older Python if needed and make it your default python."
      ],
      "metadata": {
        "id": "EDKlllzLQntg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch;\n",
        "print(torch.utils.cmake_prefix_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaqxNAP8XG7p",
        "outputId": "5aaee67a-5ddc-4ae8-d71d-d7f3559fd03d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/share/cmake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pair_allegro\n",
        "While traversing particles present in receptive field of 4 A, we query LAMMPS for nearest neighbors. This way we get all neighbors, not just neighbors belonging to a particular graph."
      ],
      "metadata": {
        "id": "P2WHCNtYRH7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf pair_allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/pair_allegro.git\n",
        "# Latest is at -- now same as above. Mon Dec 9, 2024\n",
        "#!git clone --depth 1 https://github.com/mir-group/pair_allegro/tree/multicut"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoEfje93DXt3",
        "outputId": "f3a94f26-5d37-4b09-fd3c-594db21135e4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pair_allegro'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 28 (delta 0), reused 20 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (28/28), 195.32 KiB | 2.64 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAMMPS Patching to enable pair_allegro format."
      ],
      "metadata": {
        "id": "3dGN5mOgRqrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "!cd pair_allegro && bash patch_lammps.sh ../lammps/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaJUONWEDi--",
        "outputId": "5ade76b2-5f3e-4f3b-8830-60f7767d2ed4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying files...\n",
            "Updating CMakeLists.txt...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build LAMMPS with Libtorch, enabling Allegro integration."
      ],
      "metadata": {
        "id": "BY7VFzQ_SL7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "# Use Python 3.11 Libtorch\n",
        "#!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j$(nproc)\n",
        "# Use downloaded 12.2 CUDA libtorch\n",
        "!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=/content/libtorch -DMKL_INCLUDE_DIR=`python -c \"import sysconfig;from pathlib import Path;print(Path(sysconfig.get_paths()[\\\"include\\\"]).parent)\"` && make -j$(nproc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYwb5zHP9BYo",
        "outputId": "4195d598-b28f-4ecc-a906-f536730fe6a5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "-- Running check for auto-generated files from make-based build system\n",
            "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n",
            "-- Found MPI: TRUE (found version \"3.1\")  \n",
            "-- Looking for C++ include omp.h\n",
            "-- Looking for C++ include omp.h - found\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\") found components: CXX \n",
            "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\") \n",
            "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.37\") \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found GZIP: /usr/bin/gzip  \n",
            "-- Found FFMPEG: /usr/bin/ffmpeg  \n",
            "-- Looking for C++ include cmath\n",
            "-- Looking for C++ include cmath - found\n",
            "-- Generating style headers...\n",
            "-- Generating package headers...\n",
            "-- Generating lmpinstalledpkgs.h...\n",
            "-- Found Python3: /usr/local/bin/python (found version \"3.10.12\") found components: Interpreter \n",
            "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"11.0\")\n",
            "-- The following tools and libraries have been found and configured:\n",
            " * Git\n",
            " * MPI\n",
            " * OpenMP\n",
            " * JPEG\n",
            " * PNG\n",
            " * ZLIB\n",
            " * Python3\n",
            "\n",
            "-- <<< Build configuration >>>\n",
            "   LAMMPS Version:   20241119 a78aee5-modified\n",
            "   Operating System: Linux Ubuntu\" 22.04\n",
            "   CMake Version:    3.23.1\n",
            "   Build type:       RelWithDebInfo\n",
            "   Install path:     /root/.local\n",
            "   Generator:        Unix Makefiles using /usr/bin/gmake\n",
            "-- Enabled packages: <None>\n",
            "-- <<< Compilers and Flags: >>>\n",
            "-- C++ Compiler:     /usr/bin/c++\n",
            "      Type:          GNU\n",
            "      Version:       11.4.0\n",
            "      C++ Standard:  17\n",
            "      C++ Flags:     -O2 -g -DNDEBUG\n",
            "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=4;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n",
            "-- <<< Linker flags: >>>\n",
            "-- Executable name:  lmp\n",
            "-- Static library flags:    \n",
            "-- <<< MPI flags >>>\n",
            "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
            "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi\n",
            "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.2\") \n",
            "-- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.2.140\") \n",
            "-- Looking for C++ include pthread.h\n",
            "-- Looking for C++ include pthread.h - found\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Caffe2: CUDA detected: 12.2\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 12.2\n",
            "-- Found Python: /usr/local/bin/python (found version \"3.10.12\") found components: Interpreter \n",
            "\u001b[33mCMake Warning at /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:140 (message):\n",
            "  Failed to compute shorthash for libnvrtc.so\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/share/cmake-3.23/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
            "  The package name passed to `find_package_handle_standard_args` (nvtx3) does\n",
            "  not match the name of the calling package (Caffe2).  This can lead to\n",
            "  problems in calling code that expects `find_package` result variables\n",
            "  (e.g., `_FOUND`) to follow a certain pattern.\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:174 (find_package_handle_standard_args)\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could NOT find nvtx3 (missing: nvtx3_dir) \n",
            "\u001b[33mCMake Warning at /content/libtorch/share/cmake/Caffe2/public/cuda.cmake:180 (message):\n",
            "  Cannot find NVTX3, find old NVTX instead\n",
            "Call Stack (most recent call first):\n",
            "  /content/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /content/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:1095 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- USE_CUDNN is set to 0. Compiling without cuDNN support\n",
            "-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\n",
            "-- USE_CUDSS is set to 0. Compiling without cuDSS support\n",
            "-- USE_CUFILE is set to 0. Compiling without cuFile support\n",
            "-- Autodetected CUDA architecture(s):  8.0\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_80,code=sm_80\n",
            "-- Found Torch: /content/libtorch/lib/libtorch.so  \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/lammps/build\n",
            "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/format.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
            "[  3%] Built target input.h\n",
            "[  3%] Built target fmt_format.h\n",
            "[  3%] Built target lattice.h\n",
            "[  3%] Built target kspace.h\n",
            "[  3%] Built target info.h\n",
            "[  3%] Built target lammps.h\n",
            "[  3%] Built target library.h\n",
            "[  3%] Built target lmppython.h\n",
            "[  3%] Built target lmptype.h\n",
            "[  3%] Built target memory.h\n",
            "[  3%] Built target neighbor.h\n",
            "[  3%] Built target modify.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/platform.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/core.h\u001b[0m\n",
            "[  6%] Built target neigh_list.h\n",
            "[  6%] Built target pair.h\n",
            "[  6%] Built target output.h\n",
            "[  6%] Built target platform.h\n",
            "[  6%] Built target pointers.h\n",
            "[  6%] Built target region.h\n",
            "[  6%] Built target universe.h\n",
            "[  6%] Built target timer.h\n",
            "[  6%] Built target update.h\n",
            "[  6%] Built target utils.h\n",
            "[  6%] Built target variable.h\n",
            "[  6%] Built target fmt_core.h\n",
            "-- Git Directory: /content/lammps/.git\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/command.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/exceptions.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
            "[  8%] Built target atom.h\n",
            "[  8%] Built target angle.h\n",
            "[  8%] Built target citeme.h\n",
            "[  8%] Built target comm.h\n",
            "[  8%] Built target bond.h\n",
            "[  8%] Built target compute.h\n",
            "[  8%] Built target command.h\n",
            "[  8%] Built target dihedral.h\n",
            "[  8%] Built target domain.h\n",
            "[  8%] Built target exceptions.h\n",
            "[  8%] Built target error.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
            "-- Generating lmpgitversion.h...\n",
            "[  9%] Built target fix.h\n",
            "[  9%] Built target improper.h\n",
            "[  9%] Built target group.h\n",
            "[  9%] Built target force.h\n",
            "[  9%] Built target gitversion\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_write.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_allegro.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:535\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/fmt/base.h:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/fmt/format.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/pointers.h:33\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/atom_vec.h:17\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/lammps/src/atom_vec.cpp:14\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvirtual void LAMMPS_NS::AtomVec::write_data_restricted_to_general()\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/lammps/src/atom_vec.cpp:2272:21\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin_memcpy(void*, const void*, long unsigned int)\u001b[m\u001b[K’ specified bound between 18446744056529682432 and 18446744073709551592 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "   30 | \u001b[01;35m\u001b[K                                 __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
            "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_count_type.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_grid.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_write.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid_vtk.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_grid.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_bond_history.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_pair.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_langevin.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_atom.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_global.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_local.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_table.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid2d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid3d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/label_map.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_deprecated.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin_ghost.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi_old.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq_ghost.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_bin.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_nsq.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_trim.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_ghost_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi_old.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_allegro.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_molecular.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/platform.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_ellipsoid.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_id.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_image.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_mol.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint LAMMPS_NS::Variable::next(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/lammps/src/variable.cpp:820:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  820 |         \u001b[01;35m\u001b[Kfread(buf,1,64,fp)\u001b[m\u001b[K;\n",
            "      |         \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
            "[ 98%] Built target lammps\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
            "[100%] Built target lmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download extxyz format datasets and configuration files.\n",
        "Allegro runs with npz and extxyz formats. SPICE is in HDF5 format. ASE toolchain can read and write files in multiple formats. h5py, ase, ase.io, scipy.spatial are used for this.\n",
        "\n",
        "If running TorchMd-Net, please follow procedure outlined in five-et, by creating processed version of SPICE dataset.\n",
        "\n",
        "All scripts and dataset in extxyz format for DES from SPICE is at https://github.com/v365747/GraphCoarseningGNN"
      ],
      "metadata": {
        "id": "8pNLEQu_SY_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -rf Si_data\n",
        "!rm -rf DES_Config\n",
        "# download DES data of SPICE modal from Vinay's google drive.\n",
        "# Simon Batzner's data for Si-Si lattice\n",
        "!gdown --folder --id --no-cookies https://drive.google.com/drive/folders/1FEwF4i8IDHGmAIQ3RilA0jG9_lEX4Yk0?usp=sharing\n",
        "# DES data from SPICE <downloading it to Si_data.\n",
        "\n",
        "!gdown  --folder --id --no-cookies https://drive.google.com/drive/folders/179oeQ9zSlMp_7FKmO2i8rtcZqqPMJN9n?usp=sharing\n",
        "# Allegro modal SPICE DES Dataset config yaml files.\n",
        "!gdown --folder --id --no-cookies https://drive.google.com/drive/folders/1U57qI1v5x26TnH66XZ1hq9cvdaQEqYdR?usp=sharing"
      ],
      "metadata": {
        "id": "eYnhz_xH-P36"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Si_data\n",
        "!ls DES_Config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35kPXyy3-2SY",
        "outputId": "d3170bb4-8873-40c5-cd44-5b2502301819"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DES.xyz  sitraj.xyz\n",
            "DES_tutorial_nequip.yaml  DES_tutorial.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If not downloading files and using your own custom datasets, you can check:\n",
        "\n",
        "Edit /content/allegro/configs/tutorial.yaml removing optimizer_params (unused)*Comment out lines 94-100 of /content/allegro/configs/tutorial.yaml and default_dtype: float64 (line7) and change Line 14 ForceOutput to StressForceOutput*\n",
        "\n",
        "Don't activate conda as it will mess everything up!."
      ],
      "metadata": {
        "id": "7A4lfjdmA1hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!conda info --envs\n",
        "# conda, nglview is bad for allegro as it downgrades environment\n",
        "#!conda init\n",
        "#!conda deactivate"
      ],
      "metadata": {
        "id": "03iY1OAewWMD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copy downloaded configuration files for Allegro run"
      ],
      "metadata": {
        "id": "TUyxLfGaVpM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./results\n",
        "#!nequip-train allegro/configs/tutorial.yaml\n",
        "# Copy SPICE DES extxyz config file, to configs\n",
        "!cp DES_Config/DES_tutorial.yaml allegro/configs/"
      ],
      "metadata": {
        "id": "JL4h2O5Bqwzv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training\n",
        "\n",
        "We train for 50-10 epochs, and keep an eye at validation_f_mae (force components validation error normalized) and normalized validation and training errors for potential energies.\n",
        "\n",
        "First box data using Lattice (using extxyz)."
      ],
      "metadata": {
        "id": "yBl_DBucVxm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install extxyz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCF_4OJvqhaa",
        "outputId": "10ec624d-ddbc-42c7-c935-f29cbd8b2e42"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: extxyz in /usr/local/lib/python3.10/dist-packages (0.1.3)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from extxyz) (1.26.4)\n",
            "Requirement already satisfied: pyleri>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from extxyz) (1.4.3)\n",
            "Requirement already satisfied: ase>=3.17 in /usr/local/lib/python3.10/dist-packages (from extxyz) (3.23.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ase>=3.17->extxyz) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase>=3.17->extxyz) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase>=3.17->extxyz) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase>=3.17->extxyz) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ase.io import read, write, xyz\n",
        "from ase.io.extxyz import read_extxyz, write_extxyz\n",
        "#from extxyz import read #,iread, write, ExtXYZTrajectoryWriter\n",
        "import numpy as np\n",
        "from numpy import load\n",
        "from ase.visualize import view\n",
        "\n",
        "# define a Lattics (BOX for data)\n",
        "des_prod = np.array([10.0, 10.0, 10.0])\n",
        "example_atoms = read('./Si_data/DES.xyz', index=\":\")\n",
        "\n",
        "for frame in example_atoms:\n",
        "  frame.set_cell(des_prod)\n",
        "  #example_atoms[i].set_pbc([True, True, True])\n",
        "  frame.wrap()\n",
        "write_extxyz('./Si_data/DES_L.xyz', example_atoms)"
      ],
      "metadata": {
        "id": "p9MdfsGJoG2X"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple molecules Molecular Dynamics\n",
        "This step takes about 10 mins. We only run training for 100 epochs. To get most acurate run, Allegro/Nequip teams have tun training for 1 week on Single A100 GPU.\n",
        "\n",
        "Trained models are not publicly available."
      ],
      "metadata": {
        "id": "nYbFomRC2OKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Allegro\n",
        "!rm -rf results\n",
        "!nequip-train allegro/configs/DES_tutorial.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdlT2wo1-6C-",
        "outputId": "08bac799-7fc1-4ce1-f1a6-f7055b84acf0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-moose-454025935825185442\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241211_130200-Wyb81JOnRMwrj9EwpcOddGFcSwoq6Dea3y9gedAv0oA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDES\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-moose-454025935825185442/allegro-tutorial?apiKey=451c6a7cfc5462403c018b92eb96fd705ef82180\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-moose-454025935825185442/allegro-tutorial/runs/Wyb81JOnRMwrj9EwpcOddGFcSwoq6Dea3y9gedAv0oA?apiKey=451c6a7cfc5462403c018b92eb96fd705ef82180\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/_global_options.py:95: UserWarning: Do NOT manually set PYTORCH_JIT_USE_NNC_NOT_NVFUSER=0 unless you know exactly what you're doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_ase_dataset.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  datas = [torch.load(d) for d in datas]\n",
            "Loaded data: Batch(atomic_numbers=[239550, 1], batch=[239550], cell=[18700, 3, 3], edge_cell_shift=[766582, 3], edge_index=[2, 766582], forces=[239550, 3], pbc=[18700, 3], pos=[239550, 3], ptr=[18701], total_energy=[18700, 1])\n",
            "    processed data size: ~45.49 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(18700)...\n",
            "Replace string dataset_per_atom_total_energy_mean to -35.92002647258334\n",
            "Atomic outputs are scaled by: [H, C, N, O, F, P, S, Cl, Br, I: None], shifted by [H, C, N, O, F, P, S, Cl, Br, I: -35.920026].\n",
            "Replace string dataset_forces_rms to 0.02045545683041229\n",
            "Initially outputs are globally scaled by: 0.02045545683041229, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "Number of weights: 37928\n",
            "Number of trainable weights: 37928\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0     2     6.94e+05        0.973     6.94e+05        0.015       0.0202          244           16\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    2.134    0.002        0.915     5.92e+05     5.92e+05       0.0141       0.0196          197         14.3\n",
            "Wall time: 2.1349488670002756\n",
            "! Best model        0 591741.723\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10     6.59e+05         1.51     6.59e+05        0.019       0.0251          166         16.6\n",
            "      1    20     1.45e+06        0.767     1.45e+06       0.0134       0.0179          344         24.6\n",
            "      1    30     1.26e+05        0.629     1.26e+05       0.0114       0.0162         87.2         7.27\n",
            "      1    40     1.26e+05         1.21     1.26e+05       0.0179       0.0225         29.1         7.27\n",
            "      1    50     6.59e+05         1.01     6.59e+05       0.0165       0.0205          266         16.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1     2     6.94e+05        0.982     6.94e+05        0.015       0.0203          243           16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   11.164    0.002        0.976     2.79e+06     2.79e+06       0.0145       0.0206          268         23.5\n",
            "! Validation          1   11.164    0.002        0.923     5.91e+05     5.91e+05       0.0142       0.0197          197         14.2\n",
            "Wall time: 11.16497117200015\n",
            "! Best model        1 591308.610\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10     6.58e+05         1.11     6.58e+05       0.0173       0.0216          266         16.6\n",
            "      2    20     1.24e+06         1.66     1.24e+06       0.0204       0.0264          342         22.8\n",
            "      2    30     9.68e+03         1.05     9.68e+03       0.0159        0.021         22.1         2.01\n",
            "      2    40     5.39e+06         3.82     5.39e+06       0.0297         0.04          713         47.5\n",
            "      2    50     2.85e+06         3.22     2.85e+06       0.0297       0.0367          449         34.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2     2     6.92e+05         2.35     6.92e+05       0.0251       0.0314          243           16\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2   12.479    0.002         1.53     2.79e+06     2.79e+06       0.0194       0.0255          268         23.4\n",
            "! Validation          2   12.479    0.002         2.19      5.9e+05      5.9e+05       0.0243       0.0303          197         14.2\n",
            "Wall time: 12.47964516400043\n",
            "! Best model        2 590337.054\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10      9.8e+03          2.1     9.79e+03       0.0234       0.0296         22.3         2.02\n",
            "      3    20     2.85e+06         3.07     2.85e+06        0.026       0.0358          449         34.5\n",
            "      3    30     3.02e+05         5.34     3.02e+05       0.0362       0.0473           90         11.2\n",
            "      3    40     6.54e+05         9.38     6.54e+05       0.0558       0.0626          165         16.5\n",
            "      3    50     1.19e+06         14.5     1.19e+06       0.0635       0.0779          223         22.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3     2     6.91e+05         7.85     6.91e+05        0.046       0.0573          243         15.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3   13.779    0.002         5.72     2.79e+06     2.79e+06       0.0395       0.0497          268         23.4\n",
            "! Validation          3   13.779    0.002         7.39     5.89e+05     5.89e+05       0.0445       0.0557          197         14.2\n",
            "Wall time: 13.779660408000382\n",
            "! Best model        3 588880.673\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10     1.35e+06         31.1     1.35e+06       0.0939        0.114          190         23.8\n",
            "      4    20     8.03e+05         58.2     8.03e+05        0.129        0.156          385         18.3\n",
            "      4    30     1.23e+06         67.1     1.23e+06        0.146        0.168          340         22.7\n",
            "      4    40     2.31e+05          138     2.31e+05        0.185         0.24          128         9.83\n",
            "      4    50     4.51e+05          214      4.5e+05        0.249        0.299          192         13.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4     2     6.83e+05           80     6.83e+05        0.147        0.183          241         15.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4   15.073    0.002         72.4     2.79e+06     2.79e+06        0.135        0.177          267         23.4\n",
            "! Validation          4   15.073    0.002         74.7     5.83e+05     5.83e+05        0.141        0.177          196         14.2\n",
            "Wall time: 15.07325889799995\n",
            "! Best model        4 583239.770\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10      1.2e+06          664      1.2e+06        0.444        0.527          202         22.4\n",
            "      5    20     7.46e+05     2.58e+03     7.43e+05        0.895         1.04          212         17.6\n",
            "      5    30     1.28e+06     2.57e+03     1.27e+06        0.859         1.04          185         23.1\n",
            "      5    40     2.37e+05     1.18e+03     2.36e+05        0.553        0.704          149         9.93\n",
            "      5    50     9.37e+04     1.22e+03     9.25e+04         0.61        0.716          106         6.22\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5     2     6.59e+05          951     6.59e+05        0.506        0.631          237         15.5\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5   16.369    0.002     1.26e+03     2.78e+06     2.78e+06        0.551        0.724          265         23.2\n",
            "! Validation          5   16.369    0.002          868     5.65e+05     5.65e+05        0.478        0.604          193           14\n",
            "Wall time: 16.369548270999985\n",
            "! Best model        5 565449.565\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10     1.14e+06     3.33e+03     1.14e+06         0.96         1.18          196         21.8\n",
            "      6    20     1.18e+06     1.15e+04     1.16e+06         1.79         2.19          177         22.1\n",
            "      6    30     2.98e+05     1.13e+04     2.87e+05         1.71         2.17          142           11\n",
            "      6    40     6.77e+06      9.1e+03     6.76e+06         1.48         1.95          532         53.2\n",
            "      6    50     1.05e+06     6.16e+03     1.05e+06          1.1         1.61          272         20.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6     2     6.13e+05     5.09e+03     6.08e+05         1.17         1.46          227         14.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6   17.664    0.002     6.33e+03     2.76e+06     2.77e+06         1.24         1.64          259         22.9\n",
            "! Validation          6   17.664    0.002     4.55e+03     5.27e+05     5.31e+05         1.09         1.38          187         13.5\n",
            "Wall time: 17.66460583800017\n",
            "! Best model        6 531461.932\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10     6.36e+05     7.34e+03     6.29e+05         1.27         1.75          227         16.2\n",
            "      7    20      4.4e+05     1.21e+04     4.28e+05         1.68         2.25          228         13.4\n",
            "      7    30     4.72e+05     2.21e+04      4.5e+05          2.4         3.04          220         13.7\n",
            "      7    40     7.42e+04     1.29e+04     6.13e+04         2.03         2.33         20.3         5.06\n",
            "      7    50     3.77e+05     1.65e+04      3.6e+05         1.97         2.62          233         12.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7     2     5.57e+05     1.11e+04     5.46e+05         1.72         2.15          214           14\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7   18.952    0.002     1.26e+04     2.73e+06     2.75e+06         1.77         2.32          251         22.3\n",
            "! Validation          7   18.952    0.002     9.83e+03     4.81e+05      4.9e+05          1.6         2.03          179           13\n",
            "Wall time: 18.952887000000374\n",
            "! Best model        7 490389.512\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10     7.38e+04     1.13e+04     6.25e+04         1.68         2.17         40.9         5.11\n",
            "      8    20     6.24e+05     9.55e+03     6.14e+05         1.63            2          289           16\n",
            "      8    30     8.84e+05     1.51e+04     8.69e+05            2         2.51          191         19.1\n",
            "      8    40     9.05e+05     2.45e+04     8.81e+05         2.47          3.2          154         19.2\n",
            "      8    50     1.26e+06     1.49e+04     1.25e+06         1.73          2.5          297         22.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8     2     5.15e+05     1.47e+04        5e+05         1.97         2.48          205         13.4\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8   20.241    0.002     1.45e+04     2.72e+06     2.73e+06         1.87         2.49          247           22\n",
            "! Validation          8   20.241    0.002     1.36e+04     4.47e+05      4.6e+05         1.89         2.39          173         12.5\n",
            "Wall time: 20.24200719199962\n",
            "! Best model        8 460466.063\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10     5.75e+05      1.8e+04     5.57e+05         2.17         2.75          275         15.3\n",
            "      9    20     5.66e+06     4.47e+03     5.65e+06        0.915         1.37          729         48.6\n",
            "      9    30     3.32e+05     7.38e+04     2.58e+05         3.24         5.56          104         10.4\n",
            "      9    40     7.11e+05      2.8e+04     6.83e+05         2.65         3.43          355         16.9\n",
            "      9    50     8.35e+05     3.26e+04     8.02e+05         2.81         3.69          165         18.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9     2     4.83e+05     1.88e+04     4.64e+05         2.19         2.81          197         12.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9   21.535    0.002      2.1e+04      2.7e+06     2.73e+06         2.22         3.03          241         21.6\n",
            "! Validation          9   21.535    0.002     1.82e+04     4.21e+05     4.39e+05         2.14         2.77          169         12.2\n",
            "Wall time: 21.536029462999977\n",
            "! Best model        9 439269.923\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10      5.8e+06     1.06e+04     5.79e+06         1.37         2.11          738         49.2\n",
            "     10    20     3.42e+06     3.15e+04     3.39e+06         2.75         3.63          490         37.7\n",
            "     10    30     4.73e+05     2.75e+04     4.45e+05         2.72         3.39          164         13.6\n",
            "     10    40     8.19e+05     3.67e+04     7.82e+05         2.93         3.92          163         18.1\n",
            "     10    50     2.58e+05     6.12e+04     1.97e+05         3.42         5.06          127         9.08\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10     2     4.44e+05     2.74e+04     4.16e+05         2.63         3.39          186         12.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10   22.829    0.002     3.46e+04     2.68e+06     2.72e+06         2.86         3.88          233         21.1\n",
            "! Validation         10   22.829    0.002     2.76e+04     3.87e+05     4.15e+05         2.61          3.4          162         11.7\n",
            "Wall time: 22.830205997999656\n",
            "! Best model       10 414727.884\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10     9.11e+05     3.57e+04     8.75e+05         3.23         3.87          268         19.1\n",
            "     11    20     1.86e+06      1.9e+04     1.84e+06         2.29         2.82          444         27.7\n",
            "     11    30     4.11e+05     1.98e+04     3.91e+05         2.23         2.88          205         12.8\n",
            "     11    40      6.4e+07     1.03e+03      6.4e+07        0.511        0.657          655          164\n",
            "     11    50     8.78e+05     2.37e+04     8.54e+05         2.38         3.15          284         18.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11     2     4.38e+05     2.74e+04      4.1e+05          2.6         3.38          184           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11   24.128    0.002     2.92e+04     2.69e+06     2.72e+06         2.56         3.55          236         21.2\n",
            "! Validation         11   24.128    0.002     2.85e+04     3.83e+05     4.12e+05         2.61         3.45          161         11.6\n",
            "Wall time: 24.1290158769998\n",
            "! Best model       11 411632.705\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10     1.34e+05     7.35e+04     6.03e+04         4.23         5.54         80.4         5.02\n",
            "     12    20     3.77e+05     3.28e+04     3.45e+05          2.9          3.7          120           12\n",
            "     12    30     2.88e+05     4.69e+04     2.41e+05         3.32         4.43          191           10\n",
            "     12    40     7.37e+05     5.81e+04     6.79e+05         3.83         4.93          135         16.9\n",
            "     12    50        1e+06     2.32e+04      9.8e+05         2.75         3.12          223         20.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12     2     4.28e+05     2.98e+04     3.98e+05         2.72         3.53          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12   25.417    0.002     3.24e+04     2.68e+06     2.71e+06         2.78         3.77          233         21.1\n",
            "! Validation         12   25.417    0.002     3.13e+04     3.74e+05     4.06e+05         2.75         3.61          159         11.5\n",
            "Wall time: 25.418045981000432\n",
            "! Best model       12 405716.003\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10      1.8e+06     1.47e+04     1.78e+06            2         2.48          437         27.3\n",
            "     13    20     8.42e+05      3.2e+04      8.1e+05         2.64         3.66          166         18.4\n",
            "     13    30     5.42e+04     1.35e+04     4.07e+04         1.97         2.37         16.5         4.13\n",
            "     13    40     2.92e+05     3.96e+04     2.53e+05         2.93         4.07          195         10.3\n",
            "     13    50     3.36e+06     2.35e+04     3.34e+06         2.35         3.14          486         37.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13     2     4.27e+05     2.87e+04     3.98e+05         2.64         3.47          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13   26.703    0.002     2.95e+04     2.68e+06     2.71e+06         2.56         3.61          234         21.1\n",
            "! Validation         13   26.703    0.002      3.1e+04     3.75e+05     4.06e+05         2.69         3.59          159         11.5\n",
            "Wall time: 26.7038601080003\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10     4.87e+05      2.5e+04     4.62e+05         2.67         3.24          195         13.9\n",
            "     14    20     4.82e+04     1.68e+04     3.14e+04         2.28         2.65         61.6         3.62\n",
            "     14    30     2.84e+05     4.54e+04     2.39e+05          3.2         4.36          190         9.99\n",
            "     14    40     4.66e+05     5.32e+04     4.13e+05         3.29         4.72          184         13.1\n",
            "     14    50     3.58e+05     3.52e+04     3.23e+05         3.06         3.84          256         11.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14     2     4.25e+05     2.93e+04     3.95e+05         2.65          3.5          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14   27.996    0.002     3.09e+04     2.68e+06     2.71e+06         2.66         3.68          233           21\n",
            "! Validation         14   27.996    0.002     3.18e+04     3.73e+05     4.05e+05         2.72         3.64          158         11.5\n",
            "Wall time: 27.996263179999914\n",
            "! Best model       14 404556.778\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10     2.07e+07     2.86e+03     2.07e+07        0.741         1.09          466         93.1\n",
            "     15    20     7.02e+06     1.83e+04        7e+06         2.05         2.77          541         54.1\n",
            "     15    30     5.24e+04     2.25e+04     2.99e+04         2.39         3.07         28.3         3.53\n",
            "     15    40     4.09e+05     1.86e+04     3.91e+05         2.18         2.79          166         12.8\n",
            "     15    50     7.22e+05     6.46e+04     6.58e+05         3.95          5.2          133         16.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15     2     4.26e+05     2.87e+04     3.97e+05         2.63         3.47          181         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15   29.284    0.002     2.92e+04     2.68e+06     2.71e+06         2.63         3.58          233         21.1\n",
            "! Validation         15   29.284    0.002     3.13e+04     3.74e+05     4.05e+05         2.69         3.61          158         11.5\n",
            "Wall time: 29.28444771100021\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10     4.25e+05     2.14e+04     4.04e+05         2.33         2.99          169           13\n",
            "     16    20     4.75e+05     7.46e+04        4e+05         4.49         5.59          155         12.9\n",
            "     16    30     7.16e+05     5.65e+04     6.59e+05         3.81         4.86          166         16.6\n",
            "     16    40      5.6e+05     8.35e+04     4.76e+05          4.2         5.91          212         14.1\n",
            "     16    50     2.09e+07     3.69e+03     2.09e+07        0.833         1.24          467         93.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16     2     4.22e+05     2.99e+04     3.92e+05         2.67         3.54          180         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16   30.581    0.002     3.24e+04     2.67e+06      2.7e+06         2.75         3.77          231         20.9\n",
            "! Validation         16   30.581    0.002     3.26e+04      3.7e+05     4.03e+05         2.74         3.69          157         11.4\n",
            "Wall time: 30.58210453100037\n",
            "! Best model       16 403070.763\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10     4.44e+05     3.72e+04     4.07e+05          3.2         3.94          183         13.1\n",
            "     17    20     3.06e+05     7.45e+04     2.31e+05         4.16         5.58          167         9.83\n",
            "     17    30     7.66e+05     4.17e+04     7.24e+05         3.32         4.18          261         17.4\n",
            "     17    40      4.7e+05     4.05e+04     4.29e+05         3.21         4.11          241         13.4\n",
            "     17    50     6.22e+05     1.02e+05      5.2e+05         4.91         6.53          118         14.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17     2     4.11e+05     3.37e+04     3.77e+05         2.82         3.75          176         11.5\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17   31.975    0.002     4.08e+04     2.66e+06      2.7e+06         3.09         4.26          227         20.7\n",
            "! Validation         17   31.975    0.002     3.68e+04      3.6e+05     3.97e+05         2.89         3.92          155         11.3\n",
            "Wall time: 31.975548571000218\n",
            "! Best model       17 396821.680\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10     7.94e+05     3.73e+04     7.57e+05         2.97         3.95          160         17.8\n",
            "     18    20     3.29e+05     5.37e+04     2.75e+05         3.68         4.74          236         10.7\n",
            "     18    30     6.91e+05     7.04e+04      6.2e+05         4.01         5.43          161         16.1\n",
            "     18    40     8.57e+05     2.45e+04     8.32e+05         2.41          3.2          280         18.7\n",
            "     18    50     4.23e+05     1.27e+04      4.1e+05         1.76         2.31          170         13.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18     2     4.13e+05     3.26e+04      3.8e+05         2.73         3.69          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18   33.277    0.002     3.45e+04     2.66e+06      2.7e+06         2.74         3.92          229         20.8\n",
            "! Validation         18   33.277    0.002      3.6e+04     3.62e+05     3.98e+05         2.82         3.87          155         11.3\n",
            "Wall time: 33.277439065999715\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10      7.7e+04      1.7e+04        6e+04         2.08         2.67         55.1         5.01\n",
            "     19    20     6.91e+06     1.06e+04      6.9e+06          1.5         2.11          537         53.7\n",
            "     19    30     9.96e+05      1.7e+04     9.79e+05         2.24         2.67          405         20.2\n",
            "     19    40     5.84e+05     1.68e+04     5.67e+05         1.97         2.66          247         15.4\n",
            "     19    50     2.08e+07     3.14e+03     2.08e+07        0.754         1.15          467         93.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19     2     4.23e+05     2.91e+04     3.94e+05         2.57         3.49          180         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19   34.586    0.002     2.59e+04     2.67e+06      2.7e+06         2.38         3.38          234         21.1\n",
            "! Validation         19   34.586    0.002     3.22e+04     3.72e+05     4.04e+05         2.65         3.66          157         11.4\n",
            "Wall time: 34.58680934200038\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10      2.8e+05     4.54e+04     2.35e+05         3.13         4.36          188         9.91\n",
            "     20    20     4.44e+04     1.18e+04     3.26e+04         1.81         2.22         44.3         3.69\n",
            "     20    30     3.91e+05     3.13e+04     3.59e+05          2.7         3.62          123         12.3\n",
            "     20    40     7.92e+05     3.43e+04     7.57e+05         2.89         3.79          160         17.8\n",
            "     20    50     8.05e+05     2.38e+04     7.81e+05         2.24         3.16          271         18.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20     2     4.18e+05     3.11e+04     3.87e+05         2.64         3.61          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20   35.916    0.002     3.78e+04     2.66e+06     2.69e+06         2.84         4.07          227         20.6\n",
            "! Validation         20   35.916    0.002     3.45e+04     3.66e+05     4.01e+05         2.72         3.79          156         11.3\n",
            "Wall time: 35.91614643699995\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10     4.97e+05     2.91e+04     4.68e+05         2.68         3.49          252           14\n",
            "     21    20     6.39e+07          419     6.39e+07        0.302        0.419          654          164\n",
            "     21    30     2.78e+05     9.36e+04     1.85e+05         4.77         6.26          149         8.79\n",
            "     21    40     3.68e+05     2.63e+04     3.42e+05         2.62         3.32          120           12\n",
            "     21    50     4.38e+05     3.62e+04     4.02e+05         3.05         3.89          182           13\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21     2     4.13e+05     3.27e+04      3.8e+05         2.69          3.7          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21   37.251    0.002     3.96e+04     2.65e+06     2.69e+06          2.9         4.17          226         20.5\n",
            "! Validation         21   37.251    0.002     3.61e+04     3.62e+05     3.98e+05         2.78         3.88          155         11.3\n",
            "Wall time: 37.25179538199973\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10     4.26e+05     1.51e+04     4.11e+05          1.9         2.51          131         13.1\n",
            "     22    20     2.35e+05     6.57e+04      1.7e+05         3.71         5.24          118         8.42\n",
            "     22    30     6.91e+06     9.95e+03      6.9e+06         1.39         2.04          537         53.7\n",
            "     22    40     7.81e+04     1.69e+04     6.12e+04         2.05         2.66         55.7         5.06\n",
            "     22    50     3.34e+06     1.34e+04     3.33e+06         1.72         2.37          485         37.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22     2     4.25e+05     2.87e+04     3.97e+05         2.49         3.47          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22   38.543    0.002     2.61e+04     2.66e+06     2.69e+06         2.28         3.39          233           21\n",
            "! Validation         22   38.543    0.002      3.2e+04     3.73e+05     4.05e+05         2.57         3.65          157         11.4\n",
            "Wall time: 38.54356348100009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10     5.78e+05     6.83e+04      5.1e+05         4.18         5.34          307         14.6\n",
            "     23    20     6.89e+05     3.42e+04     6.55e+05         2.77         3.78          132         16.6\n",
            "     23    30     3.89e+04     1.25e+04     2.64e+04         1.86         2.29         26.6         3.32\n",
            "     23    40      4.7e+05      3.8e+04     4.32e+05         3.11         3.99          282         13.4\n",
            "     23    50     5.91e+05     1.03e+05     4.88e+05         4.72         6.56          114         14.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23     2     4.15e+05     3.15e+04     3.83e+05         2.61         3.63          178         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23   39.835    0.002     4.25e+04     2.65e+06     2.69e+06         2.99          4.3          224         20.4\n",
            "! Validation         23   39.835    0.002     3.48e+04     3.64e+05     3.99e+05         2.69          3.8          155         11.3\n",
            "Wall time: 39.83607317800033\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10     5.53e+04     7.03e+03     4.82e+04         1.42         1.72         53.9         4.49\n",
            "     24    20     4.04e+04     5.37e+03     3.51e+04         1.14          1.5         19.1         3.83\n",
            "     24    30     2.03e+05     1.05e+05     9.71e+04         4.91         6.64         89.2         6.37\n",
            "     24    40     6.39e+07          191     6.39e+07        0.174        0.283          654          163\n",
            "     24    50     8.09e+04      1.7e+04     6.39e+04         2.03         2.67         56.9         5.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24     2     4.16e+05     3.07e+04     3.85e+05         2.55         3.59          178         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24   41.126    0.002     3.24e+04     2.66e+06     2.69e+06         2.58          3.8          228         20.7\n",
            "! Validation         24   41.126    0.002     3.38e+04     3.65e+05     3.99e+05         2.63         3.75          155         11.3\n",
            "Wall time: 41.1264591449999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10     4.39e+05     3.23e+04     4.07e+05          2.8         3.67          183           13\n",
            "     25    20     3.26e+06      1.1e+04     3.25e+06         1.52         2.14          479         36.9\n",
            "     25    30     5.06e+05     8.62e+04     4.19e+05         4.35         6.01          199         13.2\n",
            "     25    40     2.78e+05     8.54e+03     2.69e+05         1.54         1.89          149         10.6\n",
            "     25    50     4.11e+05     2.44e+04     3.86e+05         2.34         3.19          127         12.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25     2     4.27e+05     2.76e+04     3.99e+05         2.38          3.4          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25   42.424    0.002     2.59e+04     2.66e+06     2.68e+06         2.22         3.38          232         20.9\n",
            "! Validation         25   42.424    0.002     3.05e+04     3.74e+05     4.05e+05         2.46         3.56          157         11.4\n",
            "Wall time: 42.42482168100014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10     5.67e+05     6.41e+04     5.03e+05         3.92         5.18          305         14.5\n",
            "     26    20      7.1e+06     1.72e+04     7.09e+06         1.82         2.68          545         54.5\n",
            "     26    30     3.05e+06     9.54e+03     3.04e+06         1.51            2          321         35.7\n",
            "     26    40      3.4e+06     1.95e+04     3.38e+06         2.06         2.86          489         37.6\n",
            "     26    50     7.43e+06     7.34e+03     7.42e+06         1.32         1.75          557         55.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26     2     4.17e+05     3.06e+04     3.86e+05          2.5         3.58          178         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26   43.712    0.002     4.11e+04     2.64e+06     2.68e+06         2.85         4.25          223         20.3\n",
            "! Validation         26   43.712    0.002     3.35e+04     3.65e+05     3.99e+05         2.57         3.74          155         11.3\n",
            "Wall time: 43.712460640000245\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10      6.6e+05      5.8e+04     6.02e+05         3.61         4.93          159         15.9\n",
            "     27    20     4.29e+05     5.22e+04     3.77e+05         3.18         4.67          176         12.6\n",
            "     27    30     4.13e+04     1.51e+04     2.61e+04         1.98         2.52         56.2         3.31\n",
            "     27    40     1.26e+06     8.13e+03     1.25e+06         1.37         1.84          298         22.9\n",
            "     27    50     9.46e+05     1.99e+04     9.26e+05         2.33         2.89          276         19.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27     2     4.14e+05     3.11e+04     3.83e+05         2.52         3.61          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27   44.997    0.002     3.74e+04     2.64e+06     2.68e+06         2.67         4.02          225         20.4\n",
            "! Validation         27   44.997    0.002     3.38e+04     3.63e+05     3.96e+05         2.58         3.75          154         11.2\n",
            "Wall time: 44.9974752750004\n",
            "! Best model       27 396496.656\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10     4.89e+04     1.05e+04     3.85e+04         1.59         2.09         32.1         4.01\n",
            "     28    20     3.28e+05     8.73e+04     2.41e+05         5.05         6.04          120           10\n",
            "     28    30     4.01e+05     1.36e+04     3.87e+05         1.78         2.38          204         12.7\n",
            "     28    40     2.76e+05      3.9e+04     2.37e+05         2.82         4.04          189         9.96\n",
            "     28    50     1.76e+06     7.48e+03     1.76e+06         1.36         1.77          434         27.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28     2     4.19e+05     2.93e+04     3.89e+05         2.41          3.5          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28   46.289    0.002     3.07e+04     2.64e+06     2.68e+06         2.38         3.67          226         20.5\n",
            "! Validation         28   46.289    0.002     3.19e+04     3.67e+05     3.99e+05         2.47         3.65          155         11.3\n",
            "Wall time: 46.289864450000096\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10     3.31e+06        1e+04      3.3e+06         1.42         2.05          483         37.2\n",
            "     29    20     5.76e+05      1.5e+04     5.62e+05         1.78          2.5          245         15.3\n",
            "     29    30      3.4e+06     1.89e+04     3.39e+06         1.99         2.81          489         37.6\n",
            "     29    40     2.27e+05     1.46e+04     2.12e+05         2.04         2.47          132         9.42\n",
            "     29    50     3.85e+04     3.73e+03     3.47e+04        0.933         1.25         19.1         3.81\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29     2     4.14e+05     3.04e+04     3.83e+05         2.45         3.56          177         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29   47.593    0.002     3.59e+04     2.65e+06     2.68e+06          2.6         3.93          223         20.3\n",
            "! Validation         29   47.593    0.002     3.28e+04     3.63e+05     3.95e+05         2.51          3.7          153         11.2\n",
            "Wall time: 47.59367121699961\n",
            "! Best model       29 395304.192\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10     2.97e+06     5.58e+03     2.97e+06         1.12         1.53          317         35.2\n",
            "     30    20     6.92e+06     7.99e+03     6.91e+06         1.16         1.83          538         53.8\n",
            "     30    30     6.01e+05     4.16e+04      5.6e+05         2.84         4.17          321         15.3\n",
            "     30    40     3.38e+06     1.34e+04     3.37e+06         1.64         2.36          488         37.5\n",
            "     30    50     2.09e+07     1.67e+03     2.09e+07        0.564        0.836          468         93.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30     2     4.19e+05     2.86e+04      3.9e+05         2.34         3.46          179         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30   48.920    0.002     3.02e+04     2.65e+06     2.68e+06         2.33         3.66          227         20.6\n",
            "! Validation         30   48.920    0.002      3.1e+04     3.67e+05     3.98e+05          2.4         3.59          154         11.3\n",
            "Wall time: 48.92073478600014\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10     7.25e+06     3.05e+03     7.24e+06         0.91         1.13          551         55.1\n",
            "     31    20     6.97e+05     4.11e+04     6.56e+05         2.77         4.15          166         16.6\n",
            "     31    30      2.8e+04     1.86e+03     2.61e+04        0.623        0.883         16.5          3.3\n",
            "     31    40     4.82e+05     1.24e+05     3.57e+05         4.85         7.21         97.8         12.2\n",
            "     31    50     3.33e+06     1.08e+04     3.32e+06         1.46         2.13          484         37.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31     2     4.27e+05     2.63e+04     4.01e+05         2.21         3.31          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31   50.206    0.002     2.74e+04     2.64e+06     2.67e+06          2.2         3.49          227         20.6\n",
            "! Validation         31   50.206    0.002     2.85e+04     3.74e+05     4.03e+05         2.27         3.45          156         11.4\n",
            "Wall time: 50.20706677399994\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10     9.77e+05     1.64e+04      9.6e+05            2         2.62          401           20\n",
            "     32    20     3.86e+05     1.67e+04     3.69e+05            2         2.65          124         12.4\n",
            "     32    30     7.06e+06     1.38e+04     7.05e+06         1.51          2.4          543         54.3\n",
            "     32    40     4.99e+04     7.37e+03     4.25e+04         1.41         1.76         50.6         4.22\n",
            "     32    50     6.14e+05      5.1e+04     5.63e+05         3.19         4.62          154         15.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32     2     4.22e+05     2.76e+04     3.95e+05         2.26          3.4          180         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32   51.491    0.002     3.62e+04     2.63e+06     2.67e+06         2.51         3.97          223         20.3\n",
            "! Validation         32   51.491    0.002        3e+04      3.7e+05        4e+05         2.31         3.53          154         11.3\n",
            "Wall time: 51.491725756999585\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10     3.59e+05     3.61e+04     3.23e+05         2.92         3.89          163         11.6\n",
            "     33    20     2.08e+07     1.08e+03     2.08e+07        0.471        0.671          466         93.3\n",
            "     33    30     1.71e+05     8.27e+04     8.81e+04         4.18         5.88           85         6.07\n",
            "     33    40     2.71e+05     6.39e+03     2.65e+05         1.32         1.63          147         10.5\n",
            "     33    50     5.85e+05     1.26e+04     5.72e+05         1.61         2.29          248         15.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33     2     4.27e+05      2.6e+04     4.01e+05         2.17          3.3          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33   52.772    0.002     2.81e+04     2.63e+06     2.66e+06         2.21         3.51          226         20.5\n",
            "! Validation         33   52.772    0.002     2.82e+04     3.74e+05     4.02e+05         2.22         3.43          155         11.4\n",
            "Wall time: 52.77253771600044\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10     9.47e+05     1.94e+04     9.28e+05         2.19         2.85          394         19.7\n",
            "     34    20     5.66e+05      1.1e+04     5.55e+05         1.53         2.14          122         15.2\n",
            "     34    30     4.56e+04     4.65e+03      4.1e+04         1.26          1.4         16.6         4.14\n",
            "     34    40     4.02e+05     2.93e+04     3.73e+05         2.39          3.5          125         12.5\n",
            "     34    50     5.01e+04     9.25e+03     4.09e+04          1.5         1.97         33.1         4.14\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34     2     4.24e+05     2.64e+04     3.97e+05         2.18         3.33          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34   54.055    0.002     3.31e+04     2.63e+06     2.66e+06          2.4         3.79          223         20.3\n",
            "! Validation         34   54.055    0.002     2.85e+04     3.71e+05        4e+05         2.23         3.45          154         11.3\n",
            "Wall time: 54.0553886079997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10     4.34e+04     9.52e+03     3.38e+04          1.6            2           64         3.76\n",
            "     35    20     5.59e+06      2.2e+03     5.59e+06        0.693        0.959          725         48.4\n",
            "     35    30     5.36e+05     4.11e+04     4.94e+05         2.85         4.15          302         14.4\n",
            "     35    40     2.65e+05     6.28e+03     2.58e+05         1.29         1.62          146         10.4\n",
            "     35    50     2.61e+05     1.97e+05     6.32e+04         7.21         9.09         51.4         5.14\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35     2     4.25e+05     2.61e+04     3.98e+05         2.14         3.31          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35   55.335    0.002     3.03e+04     2.63e+06     2.66e+06         2.27         3.65          224         20.4\n",
            "! Validation         35   55.335    0.002     2.82e+04     3.72e+05        4e+05         2.19         3.43          154         11.3\n",
            "Wall time: 55.33515204899959\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10     3.89e+05     1.07e+04     3.78e+05         1.51         2.12          164         12.6\n",
            "     36    20     1.19e+06     3.82e+03     1.18e+06        0.968         1.26          289         22.2\n",
            "     36    30     5.57e+06     2.32e+03     5.57e+06         0.73        0.985          724         48.3\n",
            "     36    40     4.14e+05     1.12e+05     3.02e+05          4.5         6.85           90         11.2\n",
            "     36    50      2.6e+05     4.11e+04     2.19e+05         2.85         4.14          182         9.58\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36     2     4.27e+05      2.5e+04     4.02e+05         2.08         3.23          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36   56.623    0.002     2.85e+04     2.63e+06     2.66e+06         2.21         3.55          224         20.4\n",
            "! Validation         36   56.623    0.002      2.7e+04     3.74e+05     4.01e+05         2.13         3.36          155         11.3\n",
            "Wall time: 56.62320051500046\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10     2.08e+07          957     2.08e+07        0.437        0.633          467         93.4\n",
            "     37    20     9.72e+05      1.6e+04     9.56e+05         1.93         2.58          400           20\n",
            "     37    30     7.51e+04     1.35e+04     6.16e+04         1.79         2.38         55.8         5.08\n",
            "     37    40     5.49e+04     4.96e+03     4.99e+04         1.14         1.44         54.8         4.57\n",
            "     37    50     5.59e+05     1.45e+04     5.44e+05          1.7         2.46          241         15.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37     2     4.26e+05     2.49e+04     4.01e+05         2.05         3.23          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37   57.907    0.002     3.03e+04     2.62e+06     2.65e+06         2.24         3.64          223         20.3\n",
            "! Validation         37   57.907    0.002      2.7e+04     3.73e+05        4e+05         2.11         3.35          154         11.3\n",
            "Wall time: 57.907718000000386\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10      4.7e+05     4.54e+04     4.25e+05         2.95         4.36          280         13.3\n",
            "     38    20     5.04e+04     8.05e+03     4.24e+04          1.4         1.83         33.7         4.21\n",
            "     38    30     3.24e+05     4.39e+04     2.81e+05         2.67         4.29          238         10.8\n",
            "     38    40     2.62e+04     1.89e+03     2.43e+04        0.633         0.89         15.9         3.19\n",
            "     38    50      4.3e+05     1.87e+04     4.11e+05         1.89          2.8          131         13.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38     2     4.29e+05     2.39e+04     4.05e+05         1.98         3.16          182           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38   59.201    0.002     2.85e+04     2.62e+06     2.65e+06         2.14         3.53          224         20.3\n",
            "! Validation         38   59.201    0.002     2.59e+04     3.76e+05     4.01e+05         2.04         3.29          155         11.3\n",
            "Wall time: 59.20139494900013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10     6.34e+07     3.07e+03     6.34e+07         0.97         1.13          651          163\n",
            "     39    20     2.21e+05     6.91e+04     1.52e+05         3.85         5.38          152         7.98\n",
            "     39    30     4.13e+05      1.4e+04     3.99e+05         1.78         2.42          168         12.9\n",
            "     39    40      6.9e+06     9.38e+03     6.89e+06         1.37         1.98          537         53.7\n",
            "     39    50     5.12e+05     2.72e+04     4.85e+05         2.27         3.37          299         14.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39     2     4.24e+05     2.42e+04     3.99e+05         1.99         3.18          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39   60.513    0.002     3.32e+04     2.62e+06     2.65e+06         2.35         3.81          220         20.2\n",
            "! Validation         39   60.513    0.002     2.61e+04     3.72e+05     3.98e+05         2.05          3.3          154         11.3\n",
            "Wall time: 60.51363118599966\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10     5.32e+05      1.3e+04     5.19e+05         1.62         2.33          265         14.7\n",
            "     40    20      4.7e+04     3.02e+03      4.4e+04        0.905         1.12         34.3         4.29\n",
            "     40    30     4.23e+05     7.58e+03     4.16e+05         1.34         1.78          171         13.2\n",
            "     40    40     1.89e+05     6.98e+04     1.19e+05         4.51         5.41         84.7         7.06\n",
            "     40    50     5.13e+05     1.01e+05     4.12e+05         3.72          6.5          158         13.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40     2     4.26e+05     2.28e+04     4.03e+05         1.91         3.09          181         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40   61.813    0.002     2.54e+04     2.61e+06     2.64e+06         2.07         3.35          224         20.4\n",
            "! Validation         40   61.813    0.002     2.47e+04     3.74e+05     3.99e+05         1.99         3.21          154         11.3\n",
            "Wall time: 61.8131399519998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10     2.08e+07          257     2.08e+07        0.231        0.328          467         93.4\n",
            "     41    20      5.4e+05     7.58e+03     5.32e+05          1.2         1.78          119         14.9\n",
            "     41    30     1.05e+06     8.14e+03     1.05e+06         1.31         1.85          230         20.9\n",
            "     41    40     8.39e+05     1.19e+04     8.27e+05         1.44         2.23          279         18.6\n",
            "     41    50     5.69e+05     3.21e+04     5.37e+05         2.45         3.67          150           15\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41     2     4.28e+05     2.16e+04     4.07e+05         1.84            3          182           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41   63.099    0.002      2.4e+04     2.61e+06     2.63e+06         2.05         3.27          223         20.3\n",
            "! Validation         41   63.099    0.002     2.35e+04     3.77e+05        4e+05         1.93         3.13          155         11.3\n",
            "Wall time: 63.09928024300007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10     5.05e+05     1.34e+05      3.7e+05          4.3          7.5          149         12.4\n",
            "     42    20     3.59e+05     1.53e+04     3.43e+05          1.9         2.53          120           12\n",
            "     42    30     8.87e+05     1.54e+04     8.72e+05         1.97         2.54          267         19.1\n",
            "     42    40     1.23e+05     9.94e+04     2.34e+04         4.52         6.45         43.8         3.13\n",
            "     42    50     2.09e+07          201     2.09e+07        0.216         0.29          468         93.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42     2     4.16e+05     2.35e+04     3.92e+05         1.93         3.13          178         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42   64.388    0.002     3.32e+04     2.59e+06     2.62e+06         2.43         3.82          217         19.9\n",
            "! Validation         42   64.388    0.002     2.54e+04     3.66e+05     3.92e+05         2.02         3.26          152         11.2\n",
            "Wall time: 64.38887087500007\n",
            "! Best model       42 391613.962\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10     5.55e+04     5.22e+03     5.03e+04         1.16         1.48         36.7         4.59\n",
            "     43    20     1.09e+06     6.66e+03     1.09e+06         1.07         1.67          235         21.3\n",
            "     43    30     4.21e+05     6.29e+04     3.58e+05         3.34         5.13         97.9         12.2\n",
            "     43    40     4.06e+05     2.68e+04     3.79e+05         2.07         3.35          176         12.6\n",
            "     43    50     1.17e+05     7.67e+04     4.06e+04         4.02         5.67         57.7         4.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43     2     4.26e+05     2.08e+04     4.05e+05          1.8         2.95          182           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43   65.679    0.002     2.27e+04     2.59e+06     2.61e+06         1.98         3.12          224         20.4\n",
            "! Validation         43   65.679    0.002     2.27e+04     3.75e+05     3.97e+05          1.9         3.08          154         11.3\n",
            "Wall time: 65.68005766599981\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10     7.36e+06     2.06e+03     7.36e+06        0.666        0.929          555         55.5\n",
            "     44    20     4.66e+04     3.86e+03     4.27e+04        0.976         1.27         50.7         4.23\n",
            "     44    30      2.6e+04     5.68e+03     2.04e+04         1.09         1.54         14.6         2.92\n",
            "     44    40     1.13e+06     3.29e+04      1.1e+06         2.11         3.71          279         21.5\n",
            "     44    50     2.67e+05     8.89e+04     1.78e+05         4.69          6.1          129         8.63\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44     2     4.19e+05     2.16e+04     3.98e+05         1.85         3.01          180         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44   66.971    0.002      3.5e+04     2.55e+06     2.58e+06         2.47         3.81          217         19.9\n",
            "! Validation         44   66.971    0.002     2.35e+04     3.69e+05     3.93e+05         1.95         3.13          153         11.2\n",
            "Wall time: 66.97177902800013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10      4.9e+05     1.51e+04     4.75e+05         1.78         2.52          254         14.1\n",
            "     45    20     2.83e+05     4.44e+04     2.38e+05          2.8         4.31          220         9.99\n",
            "     45    30     9.85e+05      1.1e+04     9.74e+05         1.66         2.14          222         20.2\n",
            "     45    40     5.41e+04     8.98e+03     4.51e+04         1.44         1.94         52.2         4.35\n",
            "     45    50      3.1e+05     1.57e+05     1.53e+05         5.26         8.09           64            8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45     2     4.17e+05     2.18e+04     3.95e+05          1.9         3.02          179         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45   68.258    0.002     5.26e+04      2.5e+06     2.55e+06         2.75          4.5          216         19.7\n",
            "! Validation         45   68.258    0.002     2.36e+04     3.67e+05     3.91e+05         1.99         3.14          152         11.2\n",
            "Wall time: 68.25901754300048\n",
            "! Best model       45 390620.975\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10     3.94e+05     6.26e+04     3.32e+05         3.29         5.12          153         11.8\n",
            "     46    20     1.11e+05     1.02e+05     9.03e+03         4.51         6.54         27.2         1.94\n",
            "     46    30     2.37e+04     1.55e+04     8.26e+03         1.88         2.55         9.29         1.86\n",
            "     46    40     6.62e+04     1.23e+04     5.39e+04         1.56         2.27           57         4.75\n",
            "     46    50     7.33e+06     3.64e+03     7.33e+06        0.857         1.23          554         55.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46     2     4.14e+05     2.29e+04     3.91e+05         1.95         3.09          178         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46   69.542    0.002     8.18e+04     2.45e+06     2.53e+06          3.1         5.46          212         19.4\n",
            "! Validation         46   69.542    0.002     2.46e+04     3.64e+05     3.88e+05         2.04          3.2          151         11.1\n",
            "Wall time: 69.54292585700023\n",
            "! Best model       46 388213.079\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10     5.98e+04     1.22e+04     4.76e+04         1.63         2.26         35.7         4.46\n",
            "     47    20     5.32e+05     2.81e+04     5.04e+05         2.27         3.43          145         14.5\n",
            "     47    30     7.64e+04     3.62e+04     4.02e+04         2.78         3.89         16.4          4.1\n",
            "     47    40     2.54e+05     6.49e+04     1.89e+05         3.34         5.21          196          8.9\n",
            "     47    50      4.1e+05     1.93e+04     3.91e+05         1.89         2.84          205         12.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47     2     4.14e+05     2.33e+04      3.9e+05         1.99         3.12          178         11.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47   70.833    0.002     1.01e+05     2.41e+06     2.51e+06         3.26         5.89          211         19.3\n",
            "! Validation         47   70.833    0.002      2.5e+04     3.63e+05     3.88e+05         2.09         3.23          151         11.1\n",
            "Wall time: 70.83315806900009\n",
            "! Best model       47 387541.387\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10        8e+05     2.35e+04     7.76e+05         2.13         3.14          270           18\n",
            "     48    20      6.7e+04     1.56e+04     5.14e+04         1.89         2.56         37.1         4.64\n",
            "     48    30     2.94e+05     1.06e+04     2.83e+05         1.29         2.11          152         10.9\n",
            "     48    40     2.59e+05     1.13e+05     1.45e+05         5.53         6.89           78          7.8\n",
            "     48    50     6.49e+04     3.61e+04     2.88e+04         2.79         3.89         13.9         3.47\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48     2     4.19e+05     2.35e+04     3.96e+05         2.04         3.13          180           12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48   72.142    0.002     1.17e+05     2.37e+06     2.49e+06         3.25         6.25          212         19.3\n",
            "! Validation         48   72.142    0.002     2.49e+04     3.65e+05      3.9e+05         2.11         3.22          152         11.2\n",
            "Wall time: 72.14273881900044\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10     7.64e+05     2.39e+04      7.4e+05         2.12         3.16          264         17.6\n",
            "     49    20     9.09e+05     1.14e+04     8.98e+05         1.55         2.18          271         19.4\n",
            "     49    30     3.98e+05     3.83e+04      3.6e+05         2.61            4          172         12.3\n",
            "     49    40     2.47e+05     4.85e+04     1.98e+05         2.97         4.51          200          9.1\n",
            "     49    50      9.5e+04     2.68e+04     6.82e+04          2.3         3.35         64.1         5.34\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49     2     4.18e+05     2.43e+04     3.93e+05         2.15         3.19          179         11.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49   73.453    0.002     1.13e+05     2.35e+06     2.46e+06         3.29         6.07          208         19.1\n",
            "! Validation         49   73.453    0.002     2.54e+04     3.63e+05     3.89e+05         2.18         3.26          151         11.2\n",
            "Wall time: 73.45402850899973\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10     1.35e+06     4.21e+05     9.27e+05         6.96         13.3          256         19.7\n",
            "     50    20     9.95e+04     9.02e+04     9.31e+03         4.42         6.14         9.87         1.97\n",
            "     50    30     9.21e+04     3.08e+04     6.12e+04         2.47         3.59         60.7         5.06\n",
            "     50    40     2.36e+05     5.11e+04     1.85e+05         3.15         4.62          194         8.81\n",
            "     50    50     5.83e+04     3.42e+04     2.41e+04         2.81         3.78         12.7         3.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50     2     4.08e+05     2.65e+04     3.81e+05          2.3         3.33          176         11.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50   74.746    0.002     1.33e+05     2.31e+06     2.44e+06         3.56          6.5          203         18.7\n",
            "! Validation         50   74.746    0.002     2.73e+04     3.55e+05     3.83e+05         2.31         3.38          149           11\n",
            "Wall time: 74.74669222699958\n",
            "! Best model       50 382787.452\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10      4.3e+05     4.03e+04     3.89e+05          3.3         4.11          268         12.8\n",
            "     51    20     9.93e+04     9.01e+04     9.23e+03         4.47         6.14         9.83         1.97\n",
            "     51    30     2.26e+05     6.23e+04     1.63e+05         3.47         5.11          182         8.26\n",
            "     51    40     3.51e+05     2.66e+04     3.25e+05         2.54         3.34          117         11.7\n",
            "     51    50     7.17e+05     1.66e+04     7.01e+05         1.85         2.64          257         17.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51     2     4.03e+05     2.86e+04     3.74e+05         2.41         3.46          174         11.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51   76.046    0.002     1.68e+05     2.25e+06     2.42e+06         3.71         7.16          201         18.5\n",
            "! Validation         51   76.046    0.002     2.91e+04     3.51e+05      3.8e+05         2.41         3.49          148         10.9\n",
            "Wall time: 76.04661132799993\n",
            "! Best model       51 379834.058\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10     5.37e+04     2.47e+04      2.9e+04         2.29         3.21         27.9         3.48\n",
            "     52    20     9.19e+04     7.39e+04      1.8e+04         4.04         5.56         13.7         2.74\n",
            "     52    30     4.22e+04     2.65e+04     1.57e+04         2.62         3.33         10.3         2.56\n",
            "     52    40     1.86e+05     7.45e+04     1.11e+05         4.32         5.58          102         6.82\n",
            "     52    50     1.35e+05     3.67e+04     9.82e+04         2.61         3.92          109         6.41\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52     2     3.94e+05     2.95e+04     3.64e+05         2.47         3.51          171         11.4\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52   77.347    0.002     1.43e+05     2.25e+06      2.4e+06         3.54         6.52          200         18.4\n",
            "! Validation         52   77.347    0.002     3.01e+04     3.45e+05     3.75e+05         2.47         3.55          146         10.8\n",
            "Wall time: 77.34739144899959\n",
            "! Best model       52 374885.744\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10     5.81e+04     4.01e+04      1.8e+04         3.37          4.1         46.6         2.74\n",
            "     53    20     4.31e+05     4.24e+04     3.89e+05         3.44         4.21          268         12.8\n",
            "     53    30     3.42e+05     3.53e+04     3.07e+05         2.98         3.84          113         11.3\n",
            "     53    40     2.42e+06     1.91e+05     2.23e+06          6.2         8.93          275         30.5\n",
            "     53    50     6.96e+05     2.46e+04     6.71e+05         2.31         3.21          302         16.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53     2     3.89e+05     3.07e+04     3.59e+05         2.54         3.58          170         11.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53   78.645    0.002     1.59e+05     2.22e+06     2.38e+06         3.64         6.87          199         18.3\n",
            "! Validation         53   78.645    0.002     3.12e+04     3.42e+05     3.73e+05         2.54         3.61          145         10.7\n",
            "Wall time: 78.64517704300033\n",
            "! Best model       53 372791.556\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10     1.73e+05     3.73e+04     1.35e+05         2.88         3.95          143         7.53\n",
            "     54    20     5.12e+05     6.06e+04     4.52e+05         3.47         5.03          220         13.7\n",
            "     54    30     5.46e+07     4.73e+06     4.99e+07         36.5         44.5          578          144\n",
            "     54    40     3.09e+05      4.1e+04     2.68e+05         3.27         4.14          106         10.6\n",
            "     54    50     2.19e+05     5.77e+04     1.62e+05         3.49         4.91          181         8.23\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54     2     3.81e+05     3.44e+04     3.46e+05         2.73          3.8          166         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54   79.946    0.002     2.05e+05     2.15e+06     2.35e+06         3.97         7.65          194         17.9\n",
            "! Validation         54   79.946    0.002     3.43e+04     3.34e+05     3.68e+05          2.7         3.79          144         10.6\n",
            "Wall time: 79.94651530400006\n",
            "! Best model       54 368247.009\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10     6.32e+05     3.22e+04        6e+05         2.08         3.67          127         15.8\n",
            "     55    20     6.21e+04     2.96e+04     3.25e+04         2.45         3.52         44.3         3.69\n",
            "     55    30     8.26e+04     6.03e+04     2.23e+04         3.66         5.02         15.3         3.05\n",
            "     55    40     7.23e+05     2.19e+04     7.01e+05         2.33         3.03          154         17.1\n",
            "     55    50     5.21e+04     3.18e+04     2.03e+04         2.65         3.65         23.3         2.91\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55     2     3.88e+05     3.43e+04     3.54e+05         2.74         3.79          168         11.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55   81.244    0.002     2.22e+05     2.12e+06     2.34e+06         3.64         7.52          199         18.2\n",
            "! Validation         55   81.244    0.002     3.41e+04     3.39e+05     3.73e+05         2.69         3.78          144         10.6\n",
            "Wall time: 81.24490354099999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10     4.54e+06     5.58e+05     3.98e+06           11         15.3          612         40.8\n",
            "     56    20     2.96e+05     3.79e+04     2.58e+05         2.89         3.98          146         10.4\n",
            "     56    30     3.64e+05     3.89e+04     3.25e+05         3.05         4.03          117         11.7\n",
            "     56    40     2.89e+05     3.35e+04     2.55e+05         2.69         3.75          145         10.3\n",
            "     56    50     7.62e+06     3.57e+04     7.59e+06         2.85         3.87          563         56.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56     2     3.89e+05     3.53e+04     3.53e+05         2.78         3.84          168         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56   82.528    0.002     2.16e+05     2.11e+06     2.33e+06         3.75         7.68          197         18.1\n",
            "! Validation         56   82.528    0.002     3.52e+04     3.39e+05     3.74e+05         2.73         3.84          144         10.7\n",
            "Wall time: 82.52879078299975\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10     5.31e+07     5.89e+06     4.72e+07         39.8         49.7          562          141\n",
            "     57    20     4.71e+06     4.14e+05      4.3e+06         7.54         13.2          424         42.4\n",
            "     57    30     7.72e+05     3.92e+04     7.33e+05         2.65         4.05          263         17.5\n",
            "     57    40     4.64e+05     5.82e+04     4.06e+05         3.72         4.93          274           13\n",
            "     57    50     3.84e+05     2.07e+04     3.63e+05         2.07         2.94          259         12.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57     2     3.95e+05     3.77e+04     3.57e+05         2.84         3.97          169         11.2\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57   83.831    0.002     2.35e+05     2.07e+06     2.31e+06         3.83         8.05          195         17.9\n",
            "! Validation         57   83.831    0.002     3.68e+04     3.42e+05     3.79e+05         2.78         3.93          145         10.7\n",
            "Wall time: 83.83146211199983\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10     3.67e+05     1.82e+04     3.49e+05         2.03         2.76          254         12.1\n",
            "     58    20     1.71e+05     3.83e+04     1.33e+05         2.81         4.01          142         7.46\n",
            "     58    30     3.77e+04     2.81e+04      9.6e+03         2.69         3.43         34.1            2\n",
            "     58    40     7.58e+04     2.52e+04     5.06e+04         2.45         3.25         78.2          4.6\n",
            "     58    50      2.1e+06     1.54e+05     1.95e+06         6.06         8.02          257         28.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58     2     3.84e+05     3.67e+04     3.47e+05         2.82         3.92          166           11\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58   85.148    0.002     1.97e+05     2.08e+06     2.28e+06         3.57         7.17          192         17.7\n",
            "! Validation         58   85.148    0.002     3.66e+04     3.36e+05     3.72e+05         2.77         3.91          144         10.6\n",
            "Wall time: 85.14828346200011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10     7.99e+05     2.52e+04     7.74e+05         2.48         3.25          360           18\n",
            "     59    20     5.34e+05     5.08e+04     4.83e+05         3.34         4.61          185         14.2\n",
            "     59    30     4.97e+04      3.6e+04     1.37e+04         2.83         3.88         19.1         2.39\n",
            "     59    40     4.23e+05     8.02e+04     3.43e+05          4.3         5.79          192           12\n",
            "     59    50     3.81e+05      1.9e+04     3.62e+05         2.17         2.82          222         12.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59     2     3.83e+05     3.91e+04     3.44e+05          2.9         4.04          165         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59   86.450    0.002     2.25e+05     2.03e+06     2.25e+06         3.84         7.81          190         17.5\n",
            "! Validation         59   86.450    0.002     3.85e+04     3.34e+05     3.72e+05         2.84         4.01          143         10.6\n",
            "Wall time: 86.4508596429996\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10     2.48e+05      3.9e+04     2.09e+05         3.02         4.04          131         9.34\n",
            "     60    20     8.19e+05     2.42e+04     7.95e+05         2.53         3.18          365         18.2\n",
            "     60    30     3.38e+04      1.8e+04     1.59e+04         1.93         2.74         12.9         2.58\n",
            "     60    40     1.39e+05     2.35e+04     1.16e+05         2.51         3.14         76.5         6.96\n",
            "     60    50     3.27e+04     2.67e+04     6.01e+03         2.54         3.34         26.9         1.59\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60     2     3.81e+05     4.01e+04     3.41e+05         2.95          4.1          164         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60   87.749    0.002     3.83e+05     1.87e+06     2.25e+06          3.9         8.83          188         17.2\n",
            "! Validation         60   87.749    0.002     3.91e+04     3.32e+05     3.71e+05         2.87         4.05          143         10.6\n",
            "Wall time: 87.74975457500022\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10     3.32e+05     1.42e+04     3.18e+05          1.8         2.44          242         11.5\n",
            "     61    20     3.13e+05     3.34e+04     2.79e+05         2.87         3.74          108         10.8\n",
            "     61    30      1.9e+05     1.15e+05     7.43e+04         5.09         6.95         83.7         5.58\n",
            "     61    40     2.46e+04     9.45e+03     1.52e+04         1.82         1.99         10.1         2.52\n",
            "     61    50     4.31e+04     2.87e+04     1.43e+04         2.57         3.47         19.6         2.45\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61     2     3.78e+05     3.78e+04     3.41e+05         2.89         3.98          163         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61   89.040    0.002     1.64e+05     2.08e+06     2.25e+06         3.51         6.97          192         17.7\n",
            "! Validation         61   89.040    0.002     3.74e+04     3.32e+05      3.7e+05         2.83         3.96          143         10.6\n",
            "Wall time: 89.04061115100012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10      3.5e+05     1.41e+04     3.36e+05         1.82         2.43          119         11.9\n",
            "     62    20     4.57e+04     3.62e+04     9.51e+03         3.03         3.89           16         1.99\n",
            "     62    30     2.32e+05      3.4e+04     1.98e+05         2.79         3.77          127          9.1\n",
            "     62    40     2.23e+04     1.13e+04     1.11e+04         1.97         2.17          8.6         2.15\n",
            "     62    50     3.76e+06     2.06e+06     1.69e+06         12.2         29.4          346         26.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62     2     3.77e+05     4.05e+04     3.36e+05            3         4.12          162         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62   90.326    0.002     2.39e+05     1.96e+06      2.2e+06         3.88         7.99          186         17.1\n",
            "! Validation         62   90.326    0.002     3.96e+04      3.3e+05     3.69e+05         2.92         4.07          143         10.5\n",
            "Wall time: 90.32691164599964\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10      5.2e+04     2.42e+04     2.78e+04          2.6         3.18         40.9         3.41\n",
            "     63    20     2.52e+05     2.34e+04     2.29e+05         2.29         3.13          137         9.79\n",
            "     63    30      6.5e+05     2.76e+04     6.22e+05         2.56          3.4          290         16.1\n",
            "     63    40     2.44e+04     1.42e+04     1.02e+04         1.68         2.43         10.3         2.07\n",
            "     63    50     1.97e+06     1.48e+05     1.83e+06         5.61         7.87          359         27.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63     2     3.79e+05     4.14e+04     3.38e+05         3.05         4.16          163         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63   91.619    0.002     2.83e+05     1.89e+06     2.17e+06         3.98         8.18          187         17.1\n",
            "! Validation         63   91.619    0.002     4.06e+04     3.31e+05     3.71e+05         2.96         4.12          143         10.6\n",
            "Wall time: 91.61990323099963\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10     3.76e+05        2e+04     3.56e+05         2.24          2.9          220         12.2\n",
            "     64    20     1.28e+06        1e+05     1.18e+06         4.15         6.47          355         22.2\n",
            "     64    30     2.22e+05     7.31e+04     1.49e+05         4.02         5.53          174         7.89\n",
            "     64    40     8.95e+04     3.51e+04     5.43e+04          2.6         3.83         57.2         4.77\n",
            "     64    50     1.77e+05     6.82e+04     1.09e+05         3.64         5.34          128         6.75\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64     2     3.87e+05     4.47e+04     3.43e+05         3.18         4.32          164         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              64   92.913    0.002     2.45e+05      1.9e+06     2.15e+06         4.24         8.11          186         17.1\n",
            "! Validation         64   92.913    0.002     4.36e+04     3.33e+05     3.77e+05         3.08         4.27          144         10.6\n",
            "Wall time: 92.9137153620004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65    10     1.81e+04     1.66e+04     1.51e+03         1.85         2.63         3.98        0.795\n",
            "     65    20      3.8e+06     2.28e+06     1.53e+06           13         30.9          329         25.3\n",
            "     65    30      7.8e+04      3.6e+04      4.2e+04         2.77         3.88         50.3         4.19\n",
            "     65    40     4.53e+04        4e+04     5.38e+03         3.07         4.09           12          1.5\n",
            "     65    50     7.19e+05     4.99e+04     6.69e+05         2.98         4.57          134         16.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65     2     3.77e+05      4.8e+04     3.29e+05         3.33         4.48          160         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              65   94.262    0.002     3.07e+05     1.81e+06     2.11e+06         4.34         8.89          177         16.3\n",
            "! Validation         65   94.262    0.002     4.68e+04     3.25e+05     3.72e+05         3.21         4.43          143         10.5\n",
            "Wall time: 94.26252295300037\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66    10     2.22e+05     4.61e+04     1.76e+05         3.28         4.39          120         8.59\n",
            "     66    20     4.58e+04     3.55e+04     1.03e+04         2.89         3.86         35.3         2.08\n",
            "     66    30     3.48e+05     3.03e+04     3.17e+05          2.9         3.56          242         11.5\n",
            "     66    40     5.59e+04     4.73e+04     8.63e+03         3.11         4.45         22.8          1.9\n",
            "     66    50     6.37e+05     3.48e+04     6.02e+05         2.87         3.82          286         15.9\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66     2     3.82e+05     4.86e+04     3.33e+05         3.38         4.51          161         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              66   95.554    0.002     2.73e+05     1.81e+06     2.08e+06         4.29         8.33          183         16.8\n",
            "! Validation         66   95.554    0.002     4.72e+04     3.27e+05     3.74e+05         3.24         4.45          143         10.6\n",
            "Wall time: 95.5550038829997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67    10     4.01e+05     4.22e+04     3.58e+05         3.47          4.2          257         12.2\n",
            "     67    20      4.2e+05     2.61e+04     3.93e+05         2.49         3.31          231         12.8\n",
            "     67    30     7.54e+05     7.25e+04     6.82e+05         3.84         5.51          253         16.9\n",
            "     67    40     8.11e+05     3.86e+04     7.73e+05         2.88         4.02          252           18\n",
            "     67    50     3.61e+05     1.13e+05     2.48e+05         5.14         6.87          122         10.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67     2        4e+05     5.39e+04     3.46e+05         3.55         4.75          165         11.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              67   96.886    0.002     3.12e+05     1.74e+06     2.05e+06         4.77         9.18          182         16.6\n",
            "! Validation         67   96.886    0.002     5.16e+04     3.34e+05     3.86e+05         3.38         4.65          144         10.7\n",
            "Wall time: 96.88671787600015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68    10     3.15e+06     9.39e+04     3.06e+06         4.84         6.27          358         35.8\n",
            "     68    20     1.78e+05      1.5e+05     2.82e+04         6.04         7.91         27.5         3.44\n",
            "     68    30     6.68e+05     8.23e+04     5.85e+05         4.27         5.87          203         15.6\n",
            "     68    40      2.1e+07     3.83e+04      2.1e+07         2.71            4          468         93.7\n",
            "     68    50     7.66e+06     1.34e+05     7.53e+06         5.59          7.5          561         56.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68     2     3.82e+05     5.63e+04     3.26e+05         3.66         4.85          159         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              68   98.206    0.002     3.27e+05     1.69e+06     2.01e+06         4.69         9.29          170         15.8\n",
            "! Validation         68   98.206    0.002     5.45e+04     3.22e+05     3.77e+05         3.49         4.78          142         10.5\n",
            "Wall time: 98.20661218100031\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69    10     7.57e+06     1.37e+05     7.43e+06         5.64         7.57          558         55.8\n",
            "     69    20     1.92e+05     7.64e+04     1.16e+05         3.74         5.65          132         6.96\n",
            "     69    30     1.45e+06      7.5e+05     6.98e+05         10.5         17.7          222         17.1\n",
            "     69    40     4.02e+05     9.02e+04     3.12e+05         4.49         6.14          240         11.4\n",
            "     69    50     1.24e+06     1.27e+05     1.12e+06         4.88          7.3          346         21.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69     2     3.85e+05     5.75e+04     3.28e+05          3.7          4.9          160         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              69   99.493    0.002     3.02e+05     1.66e+06     1.97e+06         4.68         8.82          175         16.1\n",
            "! Validation         69   99.493    0.002     5.55e+04     3.23e+05     3.79e+05         3.52         4.82          143         10.6\n",
            "Wall time: 99.49401326800034\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70    10     3.89e+05     3.76e+04     3.51e+05         3.28         3.97          121         12.1\n",
            "     70    20     5.96e+04     5.27e+04     6.91e+03          3.5         4.69         13.6          1.7\n",
            "     70    30     2.26e+06     1.18e+06     1.08e+06         11.7         22.2          276         21.2\n",
            "     70    40      3.6e+05     1.14e+05     2.46e+05         5.28          6.9          122         10.1\n",
            "     70    50     3.84e+06     1.23e+06     2.61e+06         16.8         22.7          495           33\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70     2     3.88e+05     5.97e+04     3.28e+05         3.77            5          160         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              70  100.787    0.002     4.09e+05     1.53e+06     1.94e+06         5.06          9.8          172         15.7\n",
            "! Validation         70  100.787    0.002     5.79e+04     3.23e+05     3.81e+05          3.6         4.92          143         10.6\n",
            "Wall time: 100.78806898499988\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71    10     6.52e+05     5.84e+04     5.94e+05         3.53         4.94          236         15.8\n",
            "     71    20     8.82e+04     4.62e+04      4.2e+04         3.48         4.39         62.9         4.19\n",
            "     71    30     2.48e+05     3.95e+04     2.09e+05          3.1         4.07          131         9.34\n",
            "     71    40      1.7e+05     6.41e+04     1.06e+05          3.8         5.18          146         6.66\n",
            "     71    50     6.19e+04      5.6e+04     5.92e+03         3.72         4.84         26.7         1.57\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71     2     3.86e+05     5.85e+04     3.28e+05         3.71         4.95          160         10.8\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              71  102.080    0.002      3.1e+05     1.61e+06     1.92e+06          4.8         9.13          174         15.9\n",
            "! Validation         71  102.080    0.002     5.72e+04     3.22e+05     3.79e+05         3.56          4.9          143         10.6\n",
            "Wall time: 102.08040614899983\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72    10     3.71e+07     8.43e+06     2.86e+07         41.3         59.4          438          109\n",
            "     72    20        1e+06     1.51e+05     8.53e+05         6.49         7.95          246         18.9\n",
            "     72    30     1.23e+05     6.99e+04     5.35e+04         4.24         5.41           71         4.73\n",
            "     72    40      3.8e+06     1.92e+06     1.88e+06         13.1         28.3          281         28.1\n",
            "     72    50     7.03e+05     5.05e+04     6.52e+05         3.43          4.6          231         16.5\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72     2     3.93e+05     6.37e+04     3.29e+05         3.89         5.16          161         10.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              72  103.369    0.002      3.7e+05      1.5e+06     1.87e+06         5.53         10.3          168         15.4\n",
            "! Validation         72  103.369    0.002     6.29e+04     3.22e+05     3.85e+05         3.73         5.13          143         10.6\n",
            "Wall time: 103.36933959999988\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73    10      3.1e+05     9.27e+04     2.17e+05         4.89         6.23         95.4         9.54\n",
            "     73    20     1.98e+05     6.18e+04     1.36e+05         3.75         5.08         83.1         7.56\n",
            "     73    30     6.24e+05     7.53e+04     5.49e+05         4.95         5.61          136         15.2\n",
            "     73    40     8.82e+05     1.11e+05     7.71e+05         5.32         6.81          234           18\n",
            "     73    50     1.14e+06     1.59e+05     9.82e+05         5.58         8.16          324         20.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73     2     3.83e+05     6.59e+04     3.17e+05         3.97         5.25          157         10.7\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              73  104.653    0.002     3.68e+05     1.47e+06     1.84e+06         5.42         9.96          163           15\n",
            "! Validation         73  104.653    0.002     6.58e+04     3.15e+05     3.81e+05         3.83         5.25          141         10.5\n",
            "Wall time: 104.6540249489999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74    10     7.69e+06      1.9e+05      7.5e+06         6.65         8.92          560           56\n",
            "     74    20     1.55e+06     6.61e+05     8.91e+05         11.7         16.6          251         19.3\n",
            "     74    30     1.83e+06     1.17e+06     6.67e+05         13.1         22.1          217         16.7\n",
            "     74    40     3.78e+06     1.53e+06     2.25e+06         17.8         25.3          461         30.7\n",
            "     74    50     1.16e+06     1.25e+05     1.03e+06         4.97         7.23          333         20.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74     2     3.77e+05     6.47e+04     3.12e+05         3.95          5.2          156         10.6\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              74  105.943    0.002     4.25e+05     1.42e+06     1.84e+06         5.48         10.6          165         15.1\n",
            "! Validation         74  105.943    0.002     6.49e+04     3.12e+05     3.77e+05         3.81         5.21          140         10.4\n",
            "Wall time: 105.94372061099966\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75    10     2.64e+05     5.64e+04     2.07e+05          3.8         4.86         93.1         9.31\n",
            "     75    20     2.58e+05     4.83e+04     2.09e+05         3.05          4.5          131         9.36\n",
            "     75    30     1.84e+05     1.13e+05     7.05e+04          5.3         6.88         43.5         5.43\n",
            "     75    40     1.11e+06     1.25e+05     9.81e+05         5.42         7.25          263         20.3\n",
            "     75    50     7.66e+04     4.13e+04     3.53e+04         3.18         4.16         65.3         3.84\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75     2     3.57e+05     5.83e+04     2.99e+05         3.75         4.94          152         10.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              75  107.236    0.002     3.42e+05     1.44e+06     1.78e+06         4.87         9.57          161         14.8\n",
            "! Validation         75  107.236    0.002     5.97e+04     3.05e+05     3.65e+05         3.65            5          139         10.3\n",
            "Wall time: 107.23666432600021\n",
            "! Best model       75 365175.553\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76    10     5.84e+05     5.61e+04     5.28e+05         3.71         4.85          267         14.9\n",
            "     76    20     1.68e+05     3.98e+04     1.28e+05          3.1         4.08          103         7.32\n",
            "     76    30     3.57e+05     3.62e+04     3.21e+05         3.11         3.89          116         11.6\n",
            "     76    40     5.73e+05     4.57e+04     5.27e+05          3.3         4.37          223         14.9\n",
            "     76    50     2.99e+05      9.2e+04     2.07e+05         4.88          6.2         93.1         9.31\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76     2     3.55e+05     5.73e+04     2.98e+05         3.74          4.9          152         10.3\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              76  108.534    0.002     4.12e+05     1.34e+06     1.76e+06          5.4         10.3          160         14.6\n",
            "! Validation         76  108.534    0.002     5.95e+04     3.05e+05     3.64e+05         3.65         4.98          139         10.3\n",
            "Wall time: 108.5347933539997\n",
            "! Best model       76 364123.448\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77    10     2.03e+05     1.52e+05     5.12e+04         5.18         7.97         55.5         4.63\n",
            "     77    20     1.69e+06     1.22e+06     4.68e+05         13.9         22.6          182           14\n",
            "     77    30     7.68e+05      2.3e+05     5.39e+05         7.46          9.8          135           15\n",
            "     77    40     5.14e+04     4.81e+04     3.28e+03         3.78         4.49         5.86         1.17\n",
            "     77    50      1.2e+05     5.03e+04     7.01e+04         3.38         4.59          119         5.42\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77     2     3.41e+05     5.85e+04     2.82e+05          3.8         4.95          147         9.99\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              77  109.834    0.002     4.37e+05     1.32e+06     1.75e+06         5.53         11.2          152           14\n",
            "! Validation         77  109.834    0.002      6.1e+04     2.96e+05     3.57e+05          3.7         5.05          137         10.2\n",
            "Wall time: 109.83486553399962\n",
            "! Best model       77 357060.570\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78    10      2.1e+05     5.31e+04     1.57e+05         3.39         4.72         89.2         8.11\n",
            "     78    20     1.16e+05     4.91e+04     6.64e+04         3.22         4.53          116         5.27\n",
            "     78    30     6.88e+05     5.33e+04     6.34e+05         3.54         4.72          228         16.3\n",
            "     78    40     7.25e+04     4.58e+04     2.67e+04         3.48         4.38         50.1         3.34\n",
            "     78    50     1.31e+06     6.47e+05     6.63e+05         10.7         16.5          216         16.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78     2     3.29e+05     5.65e+04     2.72e+05         3.71         4.86          144         9.76\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              78  111.130    0.002     3.94e+05     1.37e+06     1.76e+06         5.29         10.5          153         14.1\n",
            "! Validation         78  111.130    0.002     5.92e+04     2.91e+05      3.5e+05         3.62         4.97          135           10\n",
            "Wall time: 111.13099637699997\n",
            "! Best model       78 349794.789\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79    10     3.17e+05     2.79e+05     3.75e+04         6.43         10.8         51.5         3.96\n",
            "     79    20     7.99e+05     4.56e+04     7.53e+05         2.87         4.37          142         17.7\n",
            "     79    30      6.7e+05     8.76e+04     5.82e+05         4.41         6.06          203         15.6\n",
            "     79    40     1.85e+05     5.87e+04     1.26e+05         3.74         4.96         58.1         7.26\n",
            "     79    50     3.31e+05     2.72e+04     3.04e+05         2.81         3.37          113         11.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79     2     3.23e+05     5.28e+04      2.7e+05         3.56          4.7          144         9.71\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              79  112.422    0.002     4.29e+05     1.26e+06     1.69e+06         4.88         9.97          156         14.2\n",
            "! Validation         79  112.422    0.002     5.55e+04      2.9e+05     3.45e+05         3.49         4.81          135           10\n",
            "Wall time: 112.42251963699982\n",
            "! Best model       79 345391.483\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80    10     5.69e+04     5.47e+04     2.21e+03         3.69         4.78         7.69        0.961\n",
            "     80    20     4.96e+04     4.93e+04          299         3.52         4.54         6.02        0.354\n",
            "     80    30     5.47e+05     3.95e+04     5.08e+05         3.17         4.06          262         14.6\n",
            "     80    40      1.1e+06     4.72e+05      6.3e+05         9.49         14.1          211         16.2\n",
            "     80    50     5.48e+05     6.88e+04     4.79e+05         3.92         5.37          212         14.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80     2     3.19e+05     5.03e+04     2.69e+05         3.46         4.59          143         9.67\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              80  113.711    0.002     3.88e+05     1.27e+06     1.66e+06          4.8         9.78          154         14.1\n",
            "! Validation         80  113.711    0.002     5.31e+04     2.89e+05     3.42e+05          3.4         4.71          135         9.98\n",
            "Wall time: 113.7118435909997\n",
            "! Best model       80 342427.922\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81    10     1.27e+05      1.1e+05     1.74e+04         4.79         6.79         43.2          2.7\n",
            "     81    20     5.58e+05      3.7e+04     5.21e+05         2.97         3.93          295         14.8\n",
            "     81    30     1.14e+06     4.31e+05      7.1e+05         9.01         13.4          224         17.2\n",
            "     81    40     1.67e+06     8.67e+05     8.02e+05         11.9           19          165         18.3\n",
            "     81    50     2.65e+05      8.6e+04     1.79e+05         4.62            6          104         8.66\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81     2     3.03e+05     4.95e+04     2.53e+05         3.43         4.55          138         9.33\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              81  115.003    0.002     3.71e+05     1.28e+06     1.65e+06         4.81         9.82          149         13.8\n",
            "! Validation         81  115.003    0.002     5.23e+04     2.81e+05     3.34e+05         3.36         4.67          133         9.84\n",
            "Wall time: 115.0036269760003\n",
            "! Best model       81 333719.248\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82    10     1.16e+05     3.06e+04     8.58e+04         2.59         3.58         83.9         5.99\n",
            "     82    20     1.84e+06     1.02e+06     8.13e+05         12.8         20.7          166         18.4\n",
            "     82    30     1.17e+06     4.65e+05     7.04e+05         8.81         13.9          223         17.2\n",
            "     82    40     5.86e+04     2.96e+04      2.9e+04         2.63         3.52         59.2         3.48\n",
            "     82    50     5.77e+05     2.91e+04     5.48e+05         2.59         3.49          303         15.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82     2     2.95e+05     4.73e+04     2.48e+05         3.35         4.45          136         9.19\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              82  116.289    0.002     5.57e+05     1.13e+06     1.69e+06         4.77         11.7          147         13.4\n",
            "! Validation         82  116.289    0.002        5e+04     2.79e+05     3.29e+05         3.29         4.57          132         9.77\n",
            "Wall time: 116.28981567499977\n",
            "! Best model       82 328627.556\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83    10     5.11e+04     5.04e+04          779         3.57         4.59         7.99        0.571\n",
            "     83    20     6.46e+05      4.9e+04     5.97e+05         3.52         4.53          221         15.8\n",
            "     83    30     1.13e+05     2.76e+04      8.5e+04         2.48          3.4         83.5         5.96\n",
            "     83    40     1.22e+05     1.06e+05     1.62e+04         4.71         6.65         41.6          2.6\n",
            "     83    50     5.95e+05      3.3e+04     5.62e+05         2.88         3.72          307         15.3\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83     2     2.93e+05     4.76e+04     2.45e+05         3.34         4.46          135         9.13\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              83  117.573    0.002     2.92e+05     1.41e+06      1.7e+06         4.68         9.15          151         14.1\n",
            "! Validation         83  117.573    0.002     4.97e+04     2.77e+05     3.27e+05         3.26         4.56          132         9.74\n",
            "Wall time: 117.57404734600004\n",
            "! Best model       83 326960.798\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84    10     4.97e+04     4.97e+04         9.24         3.54         4.56         1.06       0.0622\n",
            "     84    20     1.39e+04     9.14e+03     4.72e+03          1.5         1.96         16.9         1.41\n",
            "     84    30     4.56e+05     7.29e+04     3.83e+05         4.41         5.52          127         12.7\n",
            "     84    40     9.06e+04     4.26e+04      4.8e+04         2.95         4.22         98.6         4.48\n",
            "     84    50     7.05e+04     4.67e+04     2.39e+04          3.3         4.42         37.9         3.16\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84     2      2.9e+05     4.89e+04     2.41e+05         3.37         4.52          134         9.06\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              84  118.864    0.002     4.57e+05     1.15e+06     1.61e+06         4.88         10.8          144         13.2\n",
            "! Validation         84  118.864    0.002     5.02e+04     2.75e+05     3.25e+05         3.27         4.58          131         9.71\n",
            "Wall time: 118.86487458099964\n",
            "! Best model       84 325411.581\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85    10     1.87e+05     1.47e+05     4.01e+04         6.18         7.84         61.4          4.1\n",
            "     85    20      3.9e+04     3.84e+04          590         3.12         4.01         3.97        0.497\n",
            "     85    30     1.03e+06     3.43e+05     6.82e+05         8.52           12          220         16.9\n",
            "     85    40     6.96e+05     8.11e+04     6.15e+05         4.16         5.83          209           16\n",
            "     85    50     3.69e+05     3.33e+05     3.55e+04         6.32         11.8         50.1         3.86\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85     2     2.84e+05     4.66e+04     2.37e+05         3.32         4.42          133         8.95\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              85  120.150    0.002     3.75e+05     1.24e+06     1.61e+06          4.8         10.1          150         13.8\n",
            "! Validation         85  120.150    0.002      4.8e+04     2.73e+05     3.21e+05         3.21         4.48          130         9.64\n",
            "Wall time: 120.15096725300009\n",
            "! Best model       85 321419.678\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86    10     6.15e+05     1.44e+04        6e+05         2.04         2.45          174         15.8\n",
            "     86    20     2.12e+05     3.76e+04     1.74e+05         3.05         3.97          119         8.53\n",
            "     86    30     3.74e+04     3.33e+04     4.12e+03          3.3         3.73         5.25         1.31\n",
            "     86    40     7.25e+05     1.03e+05     6.22e+05         4.73         6.56          210         16.1\n",
            "     86    50     1.31e+05     7.46e+04     5.61e+04         4.23         5.59           92         4.84\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86     2     2.84e+05      4.9e+04     2.35e+05         3.41         4.53          132         8.89\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              86  121.471    0.002     3.91e+05     1.17e+06     1.56e+06            5         10.1          144         13.2\n",
            "! Validation         86  121.471    0.002     5.02e+04     2.72e+05     3.22e+05         3.28         4.58          130          9.6\n",
            "Wall time: 121.47189163799976\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87    10     2.18e+05     2.27e+04     1.95e+05         2.46         3.08          190         9.04\n",
            "     87    20     2.29e+05     5.88e+04      1.7e+05         3.99         4.96         84.4         8.44\n",
            "     87    30     5.69e+04     3.75e+04     1.94e+04         3.23         3.96         42.7         2.85\n",
            "     87    40     4.17e+05     8.32e+04     3.34e+05         4.62          5.9          118         11.8\n",
            "     87    50     2.39e+05     8.71e+04     1.52e+05          4.5         6.04         95.7         7.97\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87     2     2.79e+05     4.86e+04      2.3e+05         3.38         4.51          131         8.78\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              87  122.783    0.002     4.57e+05      1.1e+06     1.56e+06         4.83         10.6          141         12.9\n",
            "! Validation         87  122.783    0.002     4.88e+04      2.7e+05     3.19e+05         3.22         4.52          129         9.56\n",
            "Wall time: 122.78334129599989\n",
            "! Best model       87 318721.083\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88    10     2.08e+04     2.07e+04         38.9         2.17         2.95         1.53        0.128\n",
            "     88    20     2.61e+05     3.81e+04     2.23e+05         2.71         3.99          135         9.66\n",
            "     88    30     7.98e+06     1.76e+05     7.81e+06         6.99         8.57          572         57.2\n",
            "     88    40     2.53e+05     5.98e+04     1.93e+05         3.82            5         98.8         8.98\n",
            "     88    50     2.12e+07      7.2e+04     2.11e+07         3.38         5.49          470           94\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88     2     2.74e+05     5.02e+04     2.23e+05         3.45         4.58          128         8.62\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              88  124.072    0.002     4.24e+05     1.15e+06     1.58e+06         4.86         11.1          138         12.8\n",
            "! Validation         88  124.072    0.002     4.96e+04     2.66e+05     3.16e+05         3.25         4.56          128         9.49\n",
            "Wall time: 124.0720938530003\n",
            "! Best model       88 315807.296\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89    10     7.11e+05     7.77e+04     6.33e+05         4.21          5.7          212         16.3\n",
            "     89    20     4.75e+05     3.62e+04     4.39e+05         3.31         3.89          122         13.6\n",
            "     89    30     1.22e+05     2.48e+04     9.76e+04         2.38         3.22         51.1         6.39\n",
            "     89    40      2.5e+04     2.28e+04     2.27e+03         2.73         3.09          3.9        0.975\n",
            "     89    50     2.38e+05     6.62e+04     1.72e+05         4.18         5.26         84.7         8.47\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89     2     2.67e+05     4.81e+04     2.18e+05         3.39         4.49          126         8.47\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              89  125.355    0.002     4.25e+05     1.17e+06      1.6e+06         4.95         10.6          143         13.2\n",
            "! Validation         89  125.355    0.002     4.78e+04     2.64e+05     3.12e+05          3.2         4.47          127         9.41\n",
            "Wall time: 125.35601676399983\n",
            "! Best model       89 311611.285\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90    10      2.4e+07     1.32e+07     1.07e+07         53.5         74.4          268           67\n",
            "     90    20     2.29e+05      3.2e+04     1.97e+05         2.59         3.66          163         9.07\n",
            "     90    30     2.57e+05     5.04e+04     2.07e+05         3.53         4.59          195          9.3\n",
            "     90    40      4.6e+04     2.17e+04     2.44e+04          2.3         3.01         54.3         3.19\n",
            "     90    50     1.02e+05     4.74e+04     5.41e+04         3.36         4.46         90.4         4.76\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90     2     2.63e+05     4.53e+04     2.17e+05         3.32         4.35          126         8.41\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              90  126.652    0.002     4.62e+05     1.08e+06     1.54e+06         4.72         10.5          144         13.1\n",
            "! Validation         90  126.652    0.002     4.54e+04     2.63e+05     3.09e+05         3.12         4.36          127         9.37\n",
            "Wall time: 126.65250553500027\n",
            "! Best model       90 308873.472\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91    10     7.87e+06     1.37e+05     7.74e+06         6.29         7.57          569         56.9\n",
            "     91    20     2.18e+05     2.76e+04      1.9e+05         2.44          3.4          161         8.92\n",
            "     91    30     6.06e+05     2.01e+05     4.05e+05         6.84         9.16          117           13\n",
            "     91    40     8.41e+05        5e+04     7.91e+05         3.13         4.57          146         18.2\n",
            "     91    50     2.25e+05     5.45e+04     1.71e+05         3.82         4.78         84.5         8.45\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91     2     2.66e+05     4.56e+04      2.2e+05         3.32         4.37          127         8.48\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              91  127.949    0.002     3.27e+05     1.16e+06     1.49e+06         4.48         9.73          141           13\n",
            "! Validation         91  127.949    0.002     4.51e+04     2.65e+05      3.1e+05          3.1         4.35          127          9.4\n",
            "Wall time: 127.94919338399995\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92    10     2.01e+05     3.08e+04      1.7e+05         2.52         3.59          152         8.43\n",
            "     92    20     1.49e+05     1.19e+05        3e+04          4.8         7.06         56.7         3.54\n",
            "     92    30     2.21e+07      1.1e+07     1.12e+07         54.4         67.8          273         68.3\n",
            "     92    40     7.32e+05     7.74e+04     6.55e+05         4.09         5.69          215         16.6\n",
            "     92    50     4.95e+05     4.87e+05     8.32e+03         6.95         14.3         24.2         1.87\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92     2     2.57e+05     4.65e+04     2.11e+05         3.35         4.41          123         8.25\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              92  129.233    0.002     3.92e+05     1.07e+06     1.46e+06         4.68         9.91          137         12.6\n",
            "! Validation         92  129.233    0.002     4.54e+04      2.6e+05     3.06e+05          3.1         4.36          126          9.3\n",
            "Wall time: 129.23386206800024\n",
            "! Best model       92 305687.511\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93    10     8.35e+05      4.8e+04     7.87e+05            3         4.48          145         18.1\n",
            "     93    20     6.21e+05     1.01e+04     6.11e+05          1.7         2.06          176           16\n",
            "     93    30     2.14e+07     1.07e+07     1.07e+07         54.6         66.8          268         67.1\n",
            "     93    40     8.15e+04     3.39e+04     4.76e+04         2.58         3.77         98.2         4.46\n",
            "     93    50     1.09e+05     5.45e+04     5.44e+04         3.58         4.78         90.6         4.77\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93     2      2.6e+05      4.6e+04     2.14e+05         3.36         4.39          125         8.32\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              93  130.526    0.002     3.79e+05     1.05e+06     1.43e+06         4.61         9.66          139         12.7\n",
            "! Validation         93  130.526    0.002     4.53e+04     2.61e+05     3.07e+05         3.11         4.35          126         9.31\n",
            "Wall time: 130.52676400000018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94    10     4.18e+05     3.96e+04     3.79e+05         3.21         4.07          189         12.6\n",
            "     94    20     7.35e+04     2.14e+04     5.21e+04         2.28         2.99         65.3         4.67\n",
            "     94    30     5.12e+04     4.79e+04     3.39e+03         3.47         4.47         20.3         1.19\n",
            "     94    40     3.68e+05     7.41e+04     2.94e+05          4.1         5.57          111         11.1\n",
            "     94    50      8.7e+05     4.63e+04     8.24e+05         2.92          4.4          149         18.6\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94     2     2.53e+05      4.7e+04     2.06e+05         3.41         4.43          122         8.12\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              94  131.816    0.002        4e+05     1.02e+06     1.42e+06          4.7         10.3          134         12.2\n",
            "! Validation         94  131.816    0.002     4.62e+04     2.57e+05     3.03e+05         3.15          4.4          125         9.22\n",
            "Wall time: 131.81677554199996\n",
            "! Best model       94 303304.024\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95    10     7.39e+05     8.27e+04     6.56e+05         4.22         5.88          215         16.6\n",
            "     95    20     3.19e+05     4.61e+04     2.73e+05         3.52         4.39          171         10.7\n",
            "     95    30      5.8e+06      5.6e+06        2e+05         19.4         48.4         91.5         9.15\n",
            "     95    40     1.82e+04     1.82e+04         53.8         2.11         2.76          1.8         0.15\n",
            "     95    50     5.85e+04     2.12e+04     3.72e+04         2.39         2.98         19.7         3.95\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95     2     2.51e+05     4.68e+04     2.04e+05         3.41         4.43          121         8.05\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              95  133.132    0.002     4.32e+05     1.04e+06     1.47e+06         4.88           11          133         12.2\n",
            "! Validation         95  133.132    0.002     4.62e+04     2.56e+05     3.02e+05         3.14          4.4          124         9.18\n",
            "Wall time: 133.1322017680004\n",
            "! Best model       95 301808.247\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96    10     7.93e+06     1.42e+05     7.79e+06         6.43         7.72          571         57.1\n",
            "     96    20     2.38e+05     4.46e+04     1.94e+05         3.13         4.32          189            9\n",
            "     96    30     4.25e+05     1.56e+04      4.1e+05         2.12         2.55          118         13.1\n",
            "     96    40     1.98e+05     4.36e+04     1.54e+05         3.39         4.27         80.4         8.04\n",
            "     96    50     3.98e+05     3.95e+05     3.64e+03         6.33         12.9           16         1.23\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96     2     2.46e+05     4.47e+04     2.01e+05          3.3         4.32          120         7.94\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              96  134.453    0.002     3.58e+05     1.03e+06     1.39e+06         4.34         9.41          136         12.4\n",
            "! Validation         96  134.453    0.002     4.34e+04     2.55e+05     2.98e+05         3.02         4.27          124         9.14\n",
            "Wall time: 134.4537815880003\n",
            "! Best model       96 297994.184\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97    10     1.95e+04     1.95e+04       0.0534         2.14         2.86       0.0567      0.00473\n",
            "     97    20     1.23e+05     1.09e+05     1.35e+04         4.65         6.75         38.1         2.38\n",
            "     97    30      2.4e+04     2.13e+04     2.71e+03         2.25         2.99         12.8         1.06\n",
            "     97    40      1.9e+07     9.32e+06     9.69e+06         53.2         62.5          255         63.7\n",
            "     97    50     1.05e+06     4.57e+05     5.89e+05         10.3         13.8          204         15.7\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97     2     2.42e+05     4.37e+04     1.98e+05         3.29         4.28          118         7.85\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              97  135.748    0.002     3.38e+05     1.02e+06     1.36e+06         4.56         9.21          134         12.3\n",
            "! Validation         97  135.748    0.002     4.33e+04     2.53e+05     2.96e+05         3.03         4.26          123         9.08\n",
            "Wall time: 135.74819162699987\n",
            "! Best model       97 295918.562\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98    10     4.06e+04     1.98e+04     2.08e+04         2.19         2.88         50.2         2.95\n",
            "     98    20     1.98e+05     7.54e+04     1.22e+05         4.11         5.62         85.9         7.16\n",
            "     98    30     3.58e+06     2.48e+06      1.1e+06         17.1         32.2          215         21.5\n",
            "     98    40      4.7e+05     2.37e+04     4.46e+05         2.53         3.15          246         13.7\n",
            "     98    50     1.01e+05     3.87e+04     6.19e+04         2.93         4.03         96.7         5.09\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98     2     2.41e+05     4.16e+04        2e+05         3.21         4.17          119         7.87\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              98  137.040    0.002     4.49e+05     9.68e+05     1.42e+06         4.66         10.6          139         12.4\n",
            "! Validation         98  137.040    0.002     4.14e+04     2.53e+05     2.94e+05         2.97         4.16          123         9.07\n",
            "Wall time: 137.04023738500018\n",
            "! Best model       98 294142.094\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99    10     1.71e+06     8.41e+05     8.69e+05         11.6         18.8          172         19.1\n",
            "     99    20     1.77e+05     1.49e+05     2.84e+04         6.28         7.88         51.7         3.45\n",
            "     99    30     1.56e+05     4.24e+04     1.14e+05         3.09         4.21         55.2          6.9\n",
            "     99    40     1.05e+06     5.12e+05     5.36e+05         10.7         14.6          195           15\n",
            "     99    50     4.41e+05     4.86e+04     3.92e+05         3.58         4.51          192         12.8\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99     2     2.43e+05     4.15e+04     2.01e+05          3.2         4.17          119          7.9\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              99  138.340    0.002     2.86e+05     1.12e+06     1.41e+06         4.54         9.54          137         12.8\n",
            "! Validation         99  138.340    0.002     4.13e+04     2.53e+05     2.94e+05         2.96         4.16          123         9.07\n",
            "Wall time: 138.34092589400007\n",
            "! Best model       99 294123.746\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100    10     2.06e+05     3.11e+04     1.75e+05         2.87         3.61          179         8.55\n",
            "    100    20     1.78e+05     4.86e+04     1.29e+05         3.47         4.51         73.6         7.36\n",
            "    100    30     1.89e+04     1.85e+04          469         2.29         2.78         3.55        0.443\n",
            "    100    40     1.47e+05     1.19e+05     2.86e+04         4.68         7.05         55.4         3.46\n",
            "    100    50     4.19e+05     1.06e+05     3.13e+05         4.76         6.67          114         11.4\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100     2     2.35e+05     4.39e+04     1.91e+05         3.27         4.29          115         7.63\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train             100  139.640    0.002     3.84e+05     9.43e+05     1.33e+06         4.81           10          126         11.6\n",
            "! Validation        100  139.640    0.002     4.36e+04     2.48e+05     2.91e+05         3.02         4.27          122         8.95\n",
            "Wall time: 139.64030861100036\n",
            "! Best model      100 291315.472\n",
            "! Stop training: max epochs\n",
            "Wall time: 139.6539176919996\n",
            "Cumulative wall time: 139.6539176919996\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mDES\u001b[0m at: \u001b[34mhttps://wandb.ai/anony-moose-454025935825185442/allegro-tutorial/runs/Wyb81JOnRMwrj9EwpcOddGFcSwoq6Dea3y9gedAv0oA\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241211_130200-Wyb81JOnRMwrj9EwpcOddGFcSwoq6Dea3y9gedAv0oA/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zvsbPHkkV3og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Potential file for MD simulations using LAMMPS"
      ],
      "metadata": {
        "id": "PPxZ3549XMFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!nequip-deploy build --train-dir results/silicon-tutorial/si si-deployed.pth\n",
        "!nequip-deploy build --train-dir results/DES-tutorial/DES as-deployed.pth\n",
        "!ls *pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnHEECs1CY4k",
        "outputId": "a74df093-5655-4a03-8ed6-305943a3e35a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "INFO:root:Loading best_model.pth from training session...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/deploy.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\n",
            "INFO:root:Compiled & optimized model.\n",
            "as-deployed.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check model on test dataset, that model did not see."
      ],
      "metadata": {
        "id": "cyUEWf0lXbPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!nequip-evaluate --train-dir results/silicon-tutorial/si --batch-size 5\n",
        "!nequip-evaluate --train-dir results/DES-tutorial/DES --batch-size 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nElT2PbgCgF3",
        "outputId": "9fe74a10-8b5a-4c72-c2cf-ce23ac7f93c7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/scripts/evaluate.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  trainer = torch.load(\n",
            "Using device: cuda\n",
            "Please note that _all_ machine learning models running on CUDA hardware are generally somewhat nondeterministic and that this can manifest in small, generally unimportant variation in the final test errors.\n",
            "Loading model... \n",
            "/usr/local/lib/python3.10/dist-packages/nequip/train/trainer.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_state_dict = torch.load(traindir + \"/\" + model_name, map_location=device)\n",
            "    loaded model\n",
            "Loading original dataset...\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/utils/torch_geometric/dataset.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
            "/usr/local/lib/python3.10/dist-packages/nequip/data/_dataset/_base_datasets.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, include_frames = torch.load(self.processed_paths[0])\n",
            "Loaded dataset specified in config.yaml.\n",
            "Using origial training dataset (18700 frames) minus training (50 frames) and validation frames (10 frames), yielding a test set size of 18640 frames.\n",
            "Starting...\n",
            "  0% 0/18640 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "  0% 5/18640 [00:01<1:16:08,  4.08it/s]\n",
            "  0% 10/18640 [00:01<48:25,  6.41it/s] \n",
            "  0% 15/18640 [00:03<1:26:12,  3.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  0% 50/18640 [00:03<16:12, 19.11it/s]  \n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  0% 85/18640 [00:04<07:59, 38.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 125/18640 [00:04<04:38, 66.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 165/18640 [00:04<03:07, 98.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 205/18640 [00:04<02:17, 134.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  1% 245/18640 [00:04<01:47, 170.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 285/18640 [00:04<01:29, 206.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 325/18640 [00:04<01:16, 238.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 365/18640 [00:04<01:08, 266.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 405/18640 [00:04<01:02, 289.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  2% 445/18640 [00:05<00:58, 309.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 485/18640 [00:05<00:56, 323.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 525/18640 [00:05<00:54, 334.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 565/18640 [00:05<00:52, 343.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 605/18640 [00:05<00:52, 346.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  3% 645/18640 [00:05<00:51, 351.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 685/18640 [00:05<00:50, 352.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 725/18640 [00:05<00:50, 355.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 765/18640 [00:05<00:50, 355.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  4% 805/18640 [00:06<00:50, 349.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 845/18640 [00:06<00:50, 349.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 885/18640 [00:06<00:50, 348.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 925/18640 [00:06<00:50, 351.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 965/18640 [00:06<00:50, 351.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  5% 1005/18640 [00:06<00:50, 350.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1045/18640 [00:06<00:50, 351.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1085/18640 [00:06<00:50, 349.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1120/18640 [00:06<00:50, 348.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1160/18640 [00:07<00:50, 349.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  6% 1200/18640 [00:07<00:49, 350.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1240/18640 [00:07<00:49, 351.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1280/18640 [00:07<00:49, 351.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1320/18640 [00:07<00:49, 352.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  7% 1360/18640 [00:07<00:48, 352.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1400/18640 [00:07<00:48, 353.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1440/18640 [00:07<00:48, 355.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1480/18640 [00:07<00:48, 355.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1520/18640 [00:08<00:47, 356.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  8% 1560/18640 [00:08<00:47, 358.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1600/18640 [00:08<00:47, 357.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1640/18640 [00:08<00:47, 357.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1680/18640 [00:08<00:47, 358.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1720/18640 [00:08<00:47, 354.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "  9% 1760/18640 [00:08<00:47, 353.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1800/18640 [00:08<00:47, 351.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1840/18640 [00:08<00:47, 352.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1880/18640 [00:09<00:47, 352.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 10% 1920/18640 [00:09<00:47, 353.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 1960/18640 [00:09<00:47, 354.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2000/18640 [00:09<00:46, 355.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2040/18640 [00:09<00:46, 355.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2080/18640 [00:09<00:46, 355.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 11% 2120/18640 [00:09<00:47, 350.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2160/18640 [00:09<00:47, 349.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2195/18640 [00:09<00:47, 349.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2235/18640 [00:10<00:46, 353.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2275/18640 [00:10<00:45, 355.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 12% 2315/18640 [00:10<00:45, 356.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2355/18640 [00:10<00:45, 359.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2395/18640 [00:10<00:45, 358.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2435/18640 [00:10<00:45, 353.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2475/18640 [00:10<00:45, 353.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 13% 2515/18640 [00:10<00:46, 349.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2550/18640 [00:10<00:46, 345.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2585/18640 [00:11<00:46, 346.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2620/18640 [00:11<00:46, 345.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2655/18640 [00:11<00:46, 340.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 14% 2695/18640 [00:11<00:46, 345.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2735/18640 [00:11<00:45, 349.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2770/18640 [00:11<00:45, 346.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2810/18640 [00:11<00:45, 348.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 15% 2850/18640 [00:11<00:45, 349.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2890/18640 [00:11<00:44, 352.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2930/18640 [00:12<00:44, 351.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 2970/18640 [00:12<00:44, 352.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 3010/18640 [00:12<00:44, 352.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 16% 3050/18640 [00:12<00:44, 353.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3090/18640 [00:12<00:43, 355.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3130/18640 [00:12<00:44, 350.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3170/18640 [00:12<00:43, 351.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3210/18640 [00:12<00:43, 352.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 17% 3250/18640 [00:12<00:43, 352.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3290/18640 [00:13<00:43, 350.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3330/18640 [00:13<00:43, 350.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3370/18640 [00:13<00:43, 352.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 18% 3410/18640 [00:13<00:42, 354.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3450/18640 [00:13<00:43, 351.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3490/18640 [00:13<00:43, 352.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3530/18640 [00:13<00:42, 353.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3570/18640 [00:13<00:42, 354.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 19% 3610/18640 [00:13<00:42, 354.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3650/18640 [00:14<00:42, 354.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3690/18640 [00:14<00:42, 355.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3730/18640 [00:14<00:41, 355.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3770/18640 [00:14<00:42, 353.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 20% 3810/18640 [00:14<00:42, 350.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3850/18640 [00:14<00:42, 351.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3890/18640 [00:14<00:41, 351.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3930/18640 [00:14<00:41, 352.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 21% 3970/18640 [00:15<00:41, 353.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4010/18640 [00:15<00:41, 353.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4050/18640 [00:15<00:41, 353.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4090/18640 [00:15<00:41, 352.14it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4130/18640 [00:15<00:41, 352.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 22% 4170/18640 [00:15<00:41, 349.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4210/18640 [00:15<00:41, 351.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4250/18640 [00:15<00:40, 352.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4290/18640 [00:15<00:40, 352.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4330/18640 [00:16<00:40, 350.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 23% 4370/18640 [00:16<00:40, 349.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4410/18640 [00:16<00:40, 352.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4450/18640 [00:16<00:40, 351.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4490/18640 [00:16<00:40, 353.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 24% 4530/18640 [00:16<00:39, 353.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4570/18640 [00:16<00:39, 353.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4610/18640 [00:16<00:39, 353.60it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4650/18640 [00:16<00:39, 353.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4690/18640 [00:17<00:39, 352.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 25% 4730/18640 [00:17<00:39, 353.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4770/18640 [00:17<00:39, 352.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4810/18640 [00:17<00:39, 353.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4850/18640 [00:17<00:38, 353.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4890/18640 [00:17<00:39, 351.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 26% 4930/18640 [00:17<00:38, 353.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 4970/18640 [00:17<00:38, 355.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5010/18640 [00:17<00:38, 355.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5050/18640 [00:18<00:38, 355.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 27% 5090/18640 [00:18<00:38, 356.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5130/18640 [00:18<00:37, 357.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5170/18640 [00:18<00:37, 357.86it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5210/18640 [00:18<00:37, 357.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5250/18640 [00:18<00:37, 354.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 28% 5290/18640 [00:18<00:37, 355.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5330/18640 [00:18<00:37, 357.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5370/18640 [00:18<00:37, 356.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5410/18640 [00:19<00:37, 356.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5450/18640 [00:19<00:36, 357.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 29% 5490/18640 [00:19<00:36, 357.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5530/18640 [00:19<00:36, 358.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5570/18640 [00:19<00:36, 356.91it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5610/18640 [00:19<00:36, 354.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 30% 5650/18640 [00:19<00:36, 354.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5690/18640 [00:19<00:37, 348.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5730/18640 [00:19<00:36, 351.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5770/18640 [00:20<00:36, 353.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5810/18640 [00:20<00:36, 354.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 31% 5850/18640 [00:20<00:35, 356.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5890/18640 [00:20<00:35, 358.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5930/18640 [00:20<00:35, 355.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 5970/18640 [00:20<00:35, 352.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 6010/18640 [00:20<00:35, 354.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 32% 6050/18640 [00:20<00:35, 355.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6090/18640 [00:20<00:35, 354.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6130/18640 [00:21<00:35, 355.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6170/18640 [00:21<00:35, 356.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 33% 6210/18640 [00:21<00:34, 357.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6250/18640 [00:21<00:34, 357.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6290/18640 [00:21<00:34, 353.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6330/18640 [00:21<00:35, 351.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6370/18640 [00:21<00:35, 350.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 34% 6410/18640 [00:21<00:34, 351.12it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6450/18640 [00:22<00:34, 352.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6490/18640 [00:22<00:34, 352.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6530/18640 [00:22<00:34, 354.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6570/18640 [00:22<00:33, 355.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 35% 6610/18640 [00:22<00:33, 357.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6650/18640 [00:22<00:33, 353.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6690/18640 [00:22<00:33, 355.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6730/18640 [00:22<00:33, 355.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 36% 6770/18640 [00:22<00:33, 353.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6810/18640 [00:23<00:33, 353.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6850/18640 [00:23<00:33, 354.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6890/18640 [00:23<00:33, 355.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6930/18640 [00:23<00:32, 356.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 37% 6970/18640 [00:23<00:32, 356.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7010/18640 [00:23<00:32, 356.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7050/18640 [00:23<00:32, 357.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7090/18640 [00:23<00:32, 358.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7130/18640 [00:23<00:32, 358.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 38% 7170/18640 [00:24<00:32, 357.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7210/18640 [00:24<00:31, 357.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7250/18640 [00:24<00:31, 358.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7290/18640 [00:24<00:31, 357.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 39% 7330/18640 [00:24<00:31, 357.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7370/18640 [00:24<00:31, 355.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7410/18640 [00:24<00:31, 356.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7450/18640 [00:24<00:31, 357.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7490/18640 [00:24<00:31, 357.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 40% 7530/18640 [00:25<00:30, 358.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7570/18640 [00:25<00:30, 358.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7610/18640 [00:25<00:30, 357.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7650/18640 [00:25<00:30, 357.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7690/18640 [00:25<00:30, 359.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 41% 7730/18640 [00:25<00:30, 357.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7770/18640 [00:25<00:30, 359.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7810/18640 [00:25<00:30, 360.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7850/18640 [00:25<00:30, 358.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 42% 7890/18640 [00:26<00:29, 359.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 7930/18640 [00:26<00:29, 359.99it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 7970/18640 [00:26<00:29, 359.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8010/18640 [00:26<00:29, 360.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8050/18640 [00:26<00:29, 361.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 43% 8090/18640 [00:26<00:29, 361.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8130/18640 [00:26<00:29, 361.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8170/18640 [00:26<00:29, 360.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8210/18640 [00:26<00:28, 359.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8250/18640 [00:27<00:28, 359.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 44% 8290/18640 [00:27<00:28, 359.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8330/18640 [00:27<00:28, 359.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8370/18640 [00:27<00:28, 359.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8410/18640 [00:27<00:28, 359.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 45% 8450/18640 [00:27<00:28, 358.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8490/18640 [00:27<00:28, 359.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8530/18640 [00:27<00:28, 358.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8570/18640 [00:27<00:27, 360.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8610/18640 [00:28<00:27, 359.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 46% 8650/18640 [00:28<00:27, 360.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8690/18640 [00:28<00:27, 360.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8730/18640 [00:28<00:27, 362.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8770/18640 [00:28<00:27, 362.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8810/18640 [00:28<00:27, 360.56it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 47% 8850/18640 [00:28<00:27, 360.89it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8890/18640 [00:28<00:27, 359.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8930/18640 [00:28<00:27, 357.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 8970/18640 [00:29<00:27, 358.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 48% 9010/18640 [00:29<00:27, 355.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9050/18640 [00:29<00:26, 355.72it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9090/18640 [00:29<00:26, 355.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9130/18640 [00:29<00:26, 355.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9170/18640 [00:29<00:26, 355.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 49% 9210/18640 [00:29<00:26, 356.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9250/18640 [00:29<00:26, 356.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9290/18640 [00:29<00:26, 354.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9330/18640 [00:30<00:26, 356.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9370/18640 [00:30<00:26, 355.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 50% 9410/18640 [00:30<00:25, 356.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9450/18640 [00:30<00:25, 355.61it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9490/18640 [00:30<00:25, 357.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9530/18640 [00:30<00:25, 357.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 51% 9570/18640 [00:30<00:25, 357.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9610/18640 [00:30<00:25, 356.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9650/18640 [00:30<00:25, 356.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9690/18640 [00:31<00:25, 354.43it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9730/18640 [00:31<00:25, 355.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 52% 9770/18640 [00:31<00:24, 356.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9810/18640 [00:31<00:24, 357.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9850/18640 [00:31<00:24, 357.96it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9890/18640 [00:31<00:24, 357.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9930/18640 [00:31<00:24, 357.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 53% 9970/18640 [00:31<00:24, 357.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10010/18640 [00:31<00:24, 357.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10050/18640 [00:32<00:24, 356.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10090/18640 [00:32<00:24, 354.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 54% 10130/18640 [00:32<00:24, 353.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10170/18640 [00:32<00:24, 352.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10210/18640 [00:32<00:23, 352.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10250/18640 [00:32<00:23, 352.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10290/18640 [00:32<00:23, 353.84it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 55% 10330/18640 [00:32<00:23, 354.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10370/18640 [00:32<00:23, 355.04it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10410/18640 [00:33<00:23, 355.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10450/18640 [00:33<00:23, 354.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10490/18640 [00:33<00:22, 356.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 56% 10530/18640 [00:33<00:22, 354.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10570/18640 [00:33<00:23, 350.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10610/18640 [00:33<00:22, 352.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10650/18640 [00:33<00:22, 350.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 57% 10690/18640 [00:33<00:22, 350.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10730/18640 [00:34<00:22, 350.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10770/18640 [00:34<00:22, 348.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10805/18640 [00:34<00:22, 349.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10845/18640 [00:34<00:22, 350.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 58% 10885/18640 [00:34<00:22, 348.87it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 10920/18640 [00:34<00:22, 348.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 10955/18640 [00:34<00:22, 348.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 10990/18640 [00:34<00:22, 346.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 11025/18640 [00:34<00:21, 346.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 59% 11065/18640 [00:34<00:21, 349.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11100/18640 [00:35<00:21, 346.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11140/18640 [00:35<00:21, 348.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11180/18640 [00:35<00:21, 350.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11215/18640 [00:35<00:21, 347.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 60% 11255/18640 [00:35<00:20, 351.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11295/18640 [00:35<00:20, 353.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11335/18640 [00:35<00:20, 355.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11375/18640 [00:35<00:20, 354.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11415/18640 [00:35<00:20, 356.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 61% 11455/18640 [00:36<00:20, 356.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11495/18640 [00:36<00:20, 354.70it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11535/18640 [00:36<00:19, 356.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11575/18640 [00:36<00:19, 356.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 62% 11615/18640 [00:36<00:19, 357.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11655/18640 [00:36<00:19, 356.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11695/18640 [00:36<00:19, 356.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11735/18640 [00:36<00:19, 355.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11775/18640 [00:36<00:19, 354.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 63% 11815/18640 [00:37<00:19, 354.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11855/18640 [00:37<00:19, 355.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11895/18640 [00:37<00:19, 354.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11935/18640 [00:37<00:18, 355.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 11975/18640 [00:37<00:18, 354.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 64% 12015/18640 [00:37<00:18, 355.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12055/18640 [00:37<00:18, 356.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12095/18640 [00:37<00:18, 355.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12135/18640 [00:37<00:18, 353.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 65% 12175/18640 [00:38<00:18, 353.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12215/18640 [00:38<00:18, 355.54it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12255/18640 [00:38<00:17, 356.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12295/18640 [00:38<00:17, 355.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12335/18640 [00:38<00:17, 354.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 66% 12375/18640 [00:38<00:17, 354.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12415/18640 [00:38<00:17, 355.80it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12455/18640 [00:38<00:17, 354.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12495/18640 [00:39<00:17, 354.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12535/18640 [00:39<00:17, 356.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 67% 12575/18640 [00:39<00:16, 358.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12615/18640 [00:39<00:16, 357.38it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12655/18640 [00:39<00:16, 358.08it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12695/18640 [00:39<00:16, 357.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 68% 12735/18640 [00:39<00:16, 356.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12775/18640 [00:39<00:16, 358.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12815/18640 [00:39<00:16, 358.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12855/18640 [00:40<00:16, 357.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12895/18640 [00:40<00:16, 356.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 69% 12935/18640 [00:40<00:15, 359.02it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 12975/18640 [00:40<00:15, 359.32it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13015/18640 [00:40<00:15, 357.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13055/18640 [00:40<00:15, 358.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13095/18640 [00:40<00:15, 357.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 70% 13135/18640 [00:40<00:15, 356.26it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13175/18640 [00:40<00:15, 354.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13215/18640 [00:41<00:15, 354.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13255/18640 [00:41<00:15, 354.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 71% 13295/18640 [00:41<00:15, 354.76it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13335/18640 [00:41<00:14, 355.27it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13375/18640 [00:41<00:14, 355.73it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13415/18640 [00:41<00:14, 356.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13455/18640 [00:41<00:14, 356.34it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 72% 13495/18640 [00:41<00:14, 355.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13535/18640 [00:41<00:14, 355.74it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13575/18640 [00:42<00:14, 355.39it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13615/18640 [00:42<00:14, 355.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13655/18640 [00:42<00:13, 356.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 73% 13695/18640 [00:42<00:13, 357.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13735/18640 [00:42<00:13, 355.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13775/18640 [00:42<00:13, 355.95it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13815/18640 [00:42<00:13, 355.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 74% 13855/18640 [00:42<00:13, 355.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13895/18640 [00:42<00:13, 355.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13935/18640 [00:43<00:13, 353.23it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 13975/18640 [00:43<00:13, 351.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 14015/18640 [00:43<00:13, 353.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 75% 14055/18640 [00:43<00:12, 354.29it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14095/18640 [00:43<00:12, 354.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14135/18640 [00:43<00:16, 271.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14175/18640 [00:43<00:15, 293.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14215/18640 [00:43<00:14, 310.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 76% 14255/18640 [00:44<00:13, 323.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14295/18640 [00:44<00:13, 333.11it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14335/18640 [00:44<00:12, 339.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14375/18640 [00:44<00:12, 346.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 77% 14415/18640 [00:44<00:12, 350.21it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14455/18640 [00:44<00:11, 352.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14495/18640 [00:44<00:11, 353.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14535/18640 [00:44<00:11, 352.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14575/18640 [00:44<00:11, 352.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 78% 14615/18640 [00:45<00:11, 354.52it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14655/18640 [00:45<00:11, 355.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14695/18640 [00:45<00:11, 357.13it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14735/18640 [00:45<00:10, 356.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14775/18640 [00:45<00:10, 354.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 79% 14815/18640 [00:45<00:10, 352.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14855/18640 [00:45<00:10, 356.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14895/18640 [00:45<00:10, 357.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14935/18640 [00:45<00:10, 356.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 80% 14975/18640 [00:46<00:10, 356.10it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15015/18640 [00:46<00:10, 357.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15055/18640 [00:46<00:10, 356.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15095/18640 [00:46<00:10, 351.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15135/18640 [00:46<00:10, 349.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 81% 15175/18640 [00:46<00:09, 349.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15210/18640 [00:46<00:09, 349.25it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15245/18640 [00:46<00:09, 347.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15280/18640 [00:46<00:09, 339.18it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15315/18640 [00:47<00:09, 337.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 82% 15355/18640 [00:47<00:09, 342.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15395/18640 [00:47<00:09, 346.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15435/18640 [00:47<00:09, 350.85it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15475/18640 [00:47<00:08, 354.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15515/18640 [00:47<00:08, 356.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 83% 15555/18640 [00:47<00:08, 358.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15595/18640 [00:47<00:08, 359.81it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15635/18640 [00:47<00:08, 360.31it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15675/18640 [00:48<00:08, 359.05it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 84% 15715/18640 [00:48<00:08, 357.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15755/18640 [00:48<00:08, 357.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15795/18640 [00:48<00:07, 358.06it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15835/18640 [00:48<00:07, 357.19it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15875/18640 [00:48<00:07, 356.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 85% 15915/18640 [00:48<00:07, 357.62it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 15955/18640 [00:48<00:07, 357.92it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 15995/18640 [00:48<00:07, 357.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16035/18640 [00:49<00:07, 355.45it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16075/18640 [00:49<00:07, 355.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 86% 16115/18640 [00:49<00:07, 356.41it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16155/18640 [00:49<00:06, 356.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16195/18640 [00:49<00:06, 357.71it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16235/18640 [00:49<00:06, 356.63it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 87% 16275/18640 [00:49<00:06, 356.01it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16315/18640 [00:49<00:06, 356.40it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16355/18640 [00:49<00:06, 357.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16395/18640 [00:50<00:06, 352.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16435/18640 [00:50<00:06, 354.69it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 88% 16475/18640 [00:50<00:06, 356.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16515/18640 [00:50<00:05, 355.24it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16555/18640 [00:50<00:05, 354.98it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16595/18640 [00:50<00:05, 354.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16635/18640 [00:50<00:05, 355.66it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 89% 16675/18640 [00:50<00:05, 355.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16715/18640 [00:50<00:05, 355.64it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16755/18640 [00:51<00:05, 355.58it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16795/18640 [00:51<00:05, 355.17it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 90% 16835/18640 [00:51<00:05, 355.53it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16875/18640 [00:51<00:04, 356.93it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16915/18640 [00:51<00:04, 357.75it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16955/18640 [00:51<00:04, 357.82it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 16995/18640 [00:51<00:04, 357.68it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 91% 17035/18640 [00:51<00:04, 358.03it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17075/18640 [00:51<00:04, 356.15it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17115/18640 [00:52<00:04, 355.36it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17155/18640 [00:52<00:04, 354.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17195/18640 [00:52<00:04, 355.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 92% 17235/18640 [00:52<00:03, 355.28it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17275/18640 [00:52<00:03, 356.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17315/18640 [00:52<00:03, 355.46it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17355/18640 [00:52<00:03, 356.20it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 93% 17395/18640 [00:52<00:03, 355.97it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17435/18640 [00:53<00:03, 355.83it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17475/18640 [00:53<00:03, 356.77it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17515/18640 [00:53<00:03, 357.55it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17555/18640 [00:53<00:03, 358.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 94% 17595/18640 [00:53<00:02, 358.37it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17635/18640 [00:53<00:02, 358.94it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17675/18640 [00:53<00:02, 358.90it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17715/18640 [00:53<00:02, 359.30it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17755/18640 [00:53<00:02, 358.88it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 95% 17795/18640 [00:54<00:02, 358.57it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17835/18640 [00:54<00:02, 358.00it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17875/18640 [00:54<00:02, 357.59it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17915/18640 [00:54<00:02, 358.65it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 96% 17955/18640 [00:54<00:01, 358.50it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 17995/18640 [00:54<00:01, 356.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18035/18640 [00:54<00:01, 358.09it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18075/18640 [00:54<00:01, 358.51it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18115/18640 [00:54<00:01, 358.22it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 97% 18155/18640 [00:55<00:01, 358.48it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18195/18640 [00:55<00:01, 359.16it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18235/18640 [00:55<00:01, 358.49it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18275/18640 [00:55<00:01, 360.07it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18315/18640 [00:55<00:00, 360.44it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 98% 18355/18640 [00:55<00:00, 360.33it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18395/18640 [00:55<00:00, 360.42it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18435/18640 [00:55<00:00, 360.35it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18475/18640 [00:55<00:00, 360.78it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            " 99% 18515/18640 [00:56<00:00, 359.67it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18555/18640 [00:56<00:00, 358.79it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18595/18640 [00:56<00:00, 358.47it/s]\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100% 18635/18640 [00:56<00:00, 358.65it/s]\n",
            "100% 18640/18640 [00:56<00:00, 330.71it/s]\n",
            "\n",
            "\n",
            "--- Final result: ---\n",
            "               f_mae =  4.031093           \n",
            "              f_rmse =  7.503781           \n",
            "               e_mae =  284.506191         \n",
            "             e/N_mae =  31.602408          \n",
            "               f_mae =  4.031093           \n",
            "              f_rmse =  7.503781           \n",
            "               e_mae =  284.506191         \n",
            "             e/N_mae =  31.602408          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time for Ground Truth using LAMMPS**"
      ],
      "metadata": {
        "id": "eOEE44cTCvSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup LAMMPS\n",
        "This is how we get RDF (Radial Function Distribution) for Molecular dynamics run, before and after coarsening."
      ],
      "metadata": {
        "id": "-KWEL5c4X8SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "example_atoms = read('./Si_data/DES_L.xyz', index=0)\n",
        "#example_atoms = load('./benchmark_data/aspirin_ccsd-train.npz')\n",
        "#write('./si.data', example_atoms, format='lammps-data')\n",
        "write('./des.data', example_atoms, format='lammps-data')"
      ],
      "metadata": {
        "id": "-YAmau_8D6bU"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual MD validation using LAMMPS that does not use ML\n",
        "Since we verified that Allegro was doing pretty well, we will stick to matching with Allegro and not run LAMMPS for coarsened molecules."
      ],
      "metadata": {
        "id": "kOIRt_rJYLmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lammps_input = \"\"\"\n",
        "units\tmetal\n",
        "atom_style atomic\n",
        "dimension 3\n",
        "\n",
        "# set newton on for pair_allegro (off for pair_nequip)\n",
        "newton on\n",
        "boundary p p p\n",
        "read_data ../des.data\n",
        "\n",
        "# if you want to run a larger system, simply replicate the system in space\n",
        "# replicate 3 3 3\n",
        "\n",
        "# allegro pair style\n",
        "pair_style\tallegro\n",
        "pair_coeff\t* * ../as-deployed.pth N H\n",
        "\n",
        "# N, H mass.\n",
        "mass 1 14.0067\n",
        "mass 2 1.00784\n",
        "\n",
        "velocity all create 300.0 1234567 loop geom\n",
        "\n",
        "neighbor 1.0 bin\n",
        "neigh_modify delay 5 every 1\n",
        "\n",
        "timestep 0.0001\n",
        "thermo 1\n",
        "\n",
        "# nose-hoover thermostat, 300K\n",
        "fix  1 all nvt temp 300 300 $(100*dt)\n",
        "\n",
        "# compute rdf and average after some equilibration\n",
        "comm_modify cutoff 7.0\n",
        "compute rdfall all rdf 1000 cutoff 5.0\n",
        "fix 2 all ave/time 1 2500 5000 c_rdfall[*] file des.rdf mode vector\n",
        "\n",
        "# run 5ps\n",
        "run 5000\n",
        "\"\"\"\n",
        "!rm -rf ./lammps_run\n",
        "!mkdir lammps_run\n",
        "with open(\"lammps_run/des_rdf.in\", \"w\") as f:\n",
        "    f.write(lammps_input)\n",
        "# DONE USING ALLEGRO FOR VALIDATION"
      ],
      "metadata": {
        "id": "5RQGnRCsC0gG"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running LAMMPS\n",
        "This step is not needed for our coarsening. We match RDF (Radial Distribution Function) output of binned atoms, which are binned wrt to their energies.\n",
        "\n",
        "MDAnalysis package can also be used for RDF analysis."
      ],
      "metadata": {
        "id": "QzVl5f-cYmTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NO NEED FOR LAMMPS\n",
        "!ALLEGRO_DEBUG=1; export ALLEGRO_DEBUG\n",
        "!cd lammps_run/ && ../lammps/build/lmp -in des_rdf.in"
      ],
      "metadata": {
        "id": "St_X2LxSDAKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63247ff-834e-458f-b959-6abeed22b86c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        30   19617.725     -122.86599      0             -112.72283      35178.268    \n",
            "        31   16171.612     -121.92492      0             -113.56354      43217.792    \n",
            "        32   13183.359     -121.0669       0             -114.25057      52376.04     \n",
            "        33   10837.824     -120.39427      0             -114.79068      61558.678    \n",
            "        34   9236.3199     -120.00748      0             -115.23193      70031.54     \n",
            "        35   8414.2805     -119.96989      0             -115.61937      77150.749    \n",
            "        36   8362.5221     -120.26249      0             -115.93872      82874.765    \n",
            "        37   9040.6842     -120.9775       0             -116.3031       86824.18     \n",
            "        38   10384.915     -122.08703      0             -116.7176       88980.633    \n",
            "        39   12312.109     -123.56938      0             -117.20352      89741.664    \n",
            "        40   14712.496     -125.43148      0             -117.82453      89021.926    \n",
            "        41   17433.169     -127.60829      0             -118.59464      87392.265    \n",
            "        42   20262.421     -130.03135      0             -119.55486      85447.068    \n",
            "        43   22922.394     -132.5694       0             -120.71759      83637.079    \n",
            "        44   25085.926     -135.08943      0             -122.11899      82138.092    \n",
            "        45   26451.319     -137.39783      0             -123.72142      80753.994    \n",
            "        46   26855.13      -139.36931      0             -125.48412      79085.883    \n",
            "        47   26338.963     -140.90574      0             -127.28743      75041.606    \n",
            "        48   25131.071     -142.04987      0             -129.05608      68484.121    \n",
            "        49   23533.297     -142.9567       0             -130.78903      58668.139    \n",
            "        50   21804.908     -143.67326      0             -132.39924      46632.329    \n",
            "        51   20103.949     -144.25549      0             -133.86093      33197.664    \n",
            "        52   18478.666     -144.7215       0             -135.16728      19137.451    \n",
            "        53   16895.624     -145.10718      0             -136.37146      5609.7212    \n",
            "        54   15285.334     -145.35076      0             -137.44762     -6483.6805    \n",
            "        55   13594.208     -145.41382      0             -138.38507     -16790.031    \n",
            "        56   11818.864     -145.27187      0             -139.16103     -24518.882    \n",
            "        57   10015.052     -145.00562      0             -139.82743     -29492.671    \n",
            "        58   8276.1179     -144.65684      0             -140.37775     -31891.608    \n",
            "        59   6697.6653     -144.27136      0             -140.8084      -32451.894    \n",
            "        60   5356.9874     -143.90989      0             -141.14011     -31817.424    \n",
            "        61   4304.2914     -143.62385      0             -141.39835     -30234.381    \n",
            "        62   3559.5137     -143.44816      0             -141.60775     -28791.054    \n",
            "        63   3115.4307     -143.37354      0             -141.76274     -27328.317    \n",
            "        64   2943.249      -143.43544      0             -141.91366     -26201.008    \n",
            "        65   2996.9473     -143.60047      0             -142.05093     -25530.226    \n",
            "        66   3219.4408     -143.87529      0             -142.21071     -25183.392    \n",
            "        67   3548.2455     -144.17463      0             -142.34004     -24710.787    \n",
            "        68   3920.926      -144.54753      0             -142.52025     -23991.173    \n",
            "        69   4281.7633     -144.90236      0             -142.68852     -22910.184    \n",
            "        70   4583.6261     -145.26692      0             -142.897       -20998.263    \n",
            "        71   4792.0517     -145.57512      0             -143.09744     -18214.667    \n",
            "        72   4887.6093     -145.81798      0             -143.29088     -14525.209    \n",
            "        73   4867.294      -146.03363      0             -143.51705     -10002.328    \n",
            "        74   4746.6392     -146.17217      0             -143.71797     -4846.2073    \n",
            "        75   4554.9549     -146.28118      0             -143.92608      484.22548    \n",
            "        76   4330.5591     -146.32843      0             -144.08935      5620.9201    \n",
            "        77   4113.9979     -146.40752      0             -144.28041      10022.012    \n",
            "        78   3936.651      -146.48003      0             -144.44463      13758.617    \n",
            "        79   3814.8937     -146.61386      0             -144.6414       16500.767    \n",
            "        80   3747.8102     -146.72689      0             -144.78912      18335.694    \n",
            "        81   3718.8764     -146.84337      0             -144.92056      19109.805    \n",
            "        82   3700.0522     -147.02626      0             -145.11318      19089.603    \n",
            "        83   3659.2112     -147.14375      0             -145.25179      18834.231    \n",
            "        84   3570.0934     -147.23241      0             -145.38653      18032.284    \n",
            "        85   3417.92       -147.2983       0             -145.5311       16929.53     \n",
            "        86   3203.8892     -147.35495      0             -145.69841      15867.478    \n",
            "        87   2944.7585     -147.33782      0             -145.81526      14691.331    \n",
            "        88   2668.8573     -147.29759      0             -145.91768      13282.828    \n",
            "        89   2408.6787     -147.28077      0             -146.03539      11736.078    \n",
            "        90   2191.8499     -147.23931      0             -146.10603      9888.8992    \n",
            "        91   2034.9434     -147.23134      0             -146.17919      7383.8845    \n",
            "        92   1939.6271     -147.27309      0             -146.27022      4898.7131    \n",
            "        93   1892.3259     -147.34478      0             -146.36637      2113.9934    \n",
            "        94   1868.0649     -147.38665      0             -146.42078     -800.58545    \n",
            "        95   1838.4059     -147.45006      0             -146.49953     -3712.2706    \n",
            "        96   1777.6189     -147.46728      0             -146.54818     -6238.781     \n",
            "        97   1670.1337     -147.47583      0             -146.61231     -8693.4153    \n",
            "        98   1514.7711     -147.44722      0             -146.66402     -10637.762    \n",
            "        99   1324.7914     -147.44413      0             -146.75916     -12311.556    \n",
            "       100   1124.6351     -147.34112      0             -146.75964     -13541.069    \n",
            "       101   943.78441     -147.29286      0             -146.80489     -14503.229    \n",
            "       102   810.31662     -147.265        0             -146.84603     -15465.875    \n",
            "       103   743.93218     -147.22575      0             -146.8411      -16287.362    \n",
            "       104   752.07245     -147.26737      0             -146.87852     -17172.836    \n",
            "       105   828.56668     -147.32296      0             -146.89456     -17919.895    \n",
            "       106   955.93861     -147.43247      0             -146.93821     -18732.387    \n",
            "       107   1109.3799     -147.54273      0             -146.96913     -19293.286    \n",
            "       108   1262.2415     -147.66688      0             -147.01425     -19405.109    \n",
            "       109   1392.3792     -147.74319      0             -147.02327     -19322.705    \n",
            "       110   1485.6351     -147.83588      0             -147.06775     -18782.937    \n",
            "       111   1537.3385     -147.92128      0             -147.12641     -17765.717    \n",
            "       112   1552.8657     -147.99704      0             -147.19415     -16478.545    \n",
            "       113   1542.8168     -148.01856      0             -147.22086     -14765.019    \n",
            "       114   1518.8811     -148.07819      0             -147.29287     -13072.398    \n",
            "       115   1490.1716     -148.07999      0             -147.30951     -11018.73     \n",
            "       116   1459.3516     -148.11467      0             -147.36013     -9109.3671    \n",
            "       117   1422.7577     -148.1306       0             -147.39498     -7167.1441    \n",
            "       118   1371.9376     -148.17319      0             -147.46384     -5191.5936    \n",
            "       119   1297.3898     -148.1618       0             -147.491       -3526.2379    \n",
            "       120   1191.8468     -148.17312      0             -147.55689     -1685.9227    \n",
            "       121   1052.9031     -148.13081      0             -147.58641      131.93776    \n",
            "       122   886.05164     -148.04276      0             -147.58463      1803.0831    \n",
            "       123   703.33613     -147.97992      0             -147.61627      3548.5726    \n",
            "       124   521.61864     -147.91859      0             -147.64889      5083.2503    \n",
            "       125   359.8737      -147.84008      0             -147.65401      6403.501     \n",
            "       126   235.15755     -147.78133      0             -147.65975      7534.3951    \n",
            "       127   160.07242     -147.74467      0             -147.66191      8121.7604    \n",
            "       128   140.65744     -147.7354       0             -147.66268      8421.8217    \n",
            "       129   175.56129     -147.77559      0             -147.68482      8512.3808    \n",
            "       130   256.77281     -147.8172       0             -147.68444      8158.2851    \n",
            "       131   370.78797     -147.88212      0             -147.69041      7485.3489    \n",
            "       132   500.91008     -147.95297      0             -147.69398      6386.7357    \n",
            "       133   630.6209      -148.04663      0             -147.72057      5242.5655    \n",
            "       134   745.27732     -148.11932      0             -147.73398      3933.2654    \n",
            "       135   835.21171     -148.16457      0             -147.73273      2548.3562    \n",
            "       136   896.03792     -148.2415       0             -147.77821      1072.4495    \n",
            "       137   927.80779     -148.25725      0             -147.77753     -638.11692    \n",
            "       138   934.52163     -148.30631      0             -147.82312     -2394.7451    \n",
            "       139   922.06091     -148.30795      0             -147.83121     -4110.9991    \n",
            "       140   896.06806     -148.3087       0             -147.8454      -5962.9833    \n",
            "       141   860.18854     -148.30163      0             -147.85688     -7876.7391    \n",
            "       142   815.96487     -148.31118      0             -147.88929     -9612.755     \n",
            "       143   762.67523     -148.31989      0             -147.92555     -11467.742    \n",
            "       144   698.41258     -148.30867      0             -147.94756     -12949.681    \n",
            "       145   621.88991     -148.29223      0             -147.97068     -14369.802    \n",
            "       146   533.89858     -148.23487      0             -147.95882     -15468.464    \n",
            "       147   437.97713     -148.18732      0             -147.96087     -16252.347    \n",
            "       148   340.50207     -148.18231      0             -148.00626     -16970.226    \n",
            "       149   249.62749     -148.13674      0             -148.00767     -17313.047    \n",
            "       150   174.32533     -148.10745      0             -148.01732     -17515.347    \n",
            "       151   122.75455     -148.08132      0             -148.01785     -17332.49     \n",
            "       152   100.2114      -148.05154      0             -147.99972     -16982.006    \n",
            "       153   108.36272     -148.0599       0             -148.00388     -16602.825    \n",
            "       154   144.71224     -148.10842      0             -148.0336      -16110.2      \n",
            "       155   202.94605     -148.10973      0             -148.0048      -15305.202    \n",
            "       156   274.26852     -148.15252      0             -148.01071     -14364.483    \n",
            "       157   348.75221     -148.20895      0             -148.02863     -13315.699    \n",
            "       158   416.9921      -148.24376      0             -148.02815     -11966.412    \n",
            "       159   472.03661     -148.29988      0             -148.05582     -10423.457    \n",
            "       160   509.91491     -148.28521      0             -148.02156     -8780.4733    \n",
            "       161   529.90483     -148.31279      0             -148.03881     -6922.9322    \n",
            "       162   534.12855     -148.33244      0             -148.05627     -5041.2017    \n",
            "       163   526.06418     -148.35643      0             -148.08443     -3187.1802    \n",
            "       164   509.88654     -148.35687      0             -148.09324     -1244.6536    \n",
            "       165   489.14501     -148.34847      0             -148.09556      662.3485     \n",
            "       166   465.99586     -148.36596      0             -148.12502      2300.4412    \n",
            "       167   440.91753     -148.35261      0             -148.12464      3851.8959    \n",
            "       168   413.04141     -148.36615      0             -148.15259      5243.6166    \n",
            "       169   381.28942     -148.33054      0             -148.1334       6555.7685    \n",
            "       170   344.83961     -148.34547      0             -148.16718      7588.6257    \n",
            "       171   304.05384     -148.32485      0             -148.16764      8503.3535    \n",
            "       172   261.26032     -148.28058      0             -148.1455       9236.5196    \n",
            "       173   219.93425     -148.29883      0             -148.18511      9646.8178    \n",
            "       174   184.53353     -148.28326      0             -148.18785      9953.1719    \n",
            "       175   159.54575     -148.27439      0             -148.1919       9993.0455    \n",
            "       176   148.1866      -148.25998      0             -148.18337      9760.7544    \n",
            "       177   151.97197     -148.27059      0             -148.19201      9260.058     \n",
            "       178   169.95517     -148.27389      0             -148.18601      8677.095     \n",
            "       179   198.7648      -148.30628      0             -148.20351      7663.8238    \n",
            "       180   233.25317     -148.29731      0             -148.17671      6623.5529    \n",
            "       181   267.37767     -148.32373      0             -148.18548      5548.2679    \n",
            "       182   295.11968     -148.355        0             -148.20241      4184.377     \n",
            "       183   312.19599     -148.35869      0             -148.19727      2937.0263    \n",
            "       184   316.64418     -148.39005      0             -148.22634      1586.3178    \n",
            "       185   308.85206     -148.37116      0             -148.21147      425.91945    \n",
            "       186   291.1428      -148.38687      0             -148.23634     -894.18449    \n",
            "       187   267.61221     -148.36833      0             -148.22997     -2068.3273    \n",
            "       188   242.84352     -148.34863      0             -148.22307     -3124.3296    \n",
            "       189   220.4439      -148.34139      0             -148.22741     -4089.7052    \n",
            "       190   202.991       -148.36285      0             -148.25789     -5039.1534    \n",
            "       191   191.58855     -148.34617      0             -148.24711     -5825.8859    \n",
            "       192   185.75881     -148.34918      0             -148.25313     -6235.3582    \n",
            "       193   183.89661     -148.35302      0             -148.25793     -6862.838     \n",
            "       194   183.72514     -148.34492      0             -148.24993     -7046.3648    \n",
            "       195   183.3111      -148.36353      0             -148.26875     -7182.4064    \n",
            "       196   181.82113     -148.36947      0             -148.27546     -7052.2916    \n",
            "       197   179.36489     -148.36497      0             -148.27223     -6664.7637    \n",
            "       198   176.83645     -148.3571       0             -148.26567     -6216.0169    \n",
            "       199   175.67497     -148.35562      0             -148.26479     -5363.4363    \n",
            "       200   177.25983     -148.36797      0             -148.27632     -4654.6924    \n",
            "       201   182.26213     -148.39408      0             -148.29984     -3710.7527    \n",
            "       202   190.21728     -148.37255      0             -148.2742      -2645.118     \n",
            "       203   199.56325     -148.41489      0             -148.31171     -1534.8036    \n",
            "       204   207.96723     -148.39367      0             -148.28615     -212.79449    \n",
            "       205   212.60171     -148.39662      0             -148.2867       921.25028    \n",
            "       206   210.89087     -148.41139      0             -148.30235      2239.2417    \n",
            "       207   201.40676     -148.41503      0             -148.31089      3422.9072    \n",
            "       208   184.12776     -148.39526      0             -148.30006      4655.5946    \n",
            "       209   160.55622     -148.38103      0             -148.29802      5901.9591    \n",
            "       210   133.63757     -148.373        0             -148.3039       6825.8888    \n",
            "       211   106.81318     -148.35771      0             -148.30248      7883.9849    \n",
            "       212   83.863419     -148.35443      0             -148.31107      8769.6182    \n",
            "       213   68.096248     -148.35226      0             -148.31705      9381.91      \n",
            "       214   61.356968     -148.33787      0             -148.30615      9848.7596    \n",
            "       215   64.032013     -148.3667       0             -148.3336       10107.174    \n",
            "       216   75.108272     -148.34532      0             -148.30648      10295.909    \n",
            "       217   92.318111     -148.3521       0             -148.30437      10106.476    \n",
            "       218   112.51442     -148.37447      0             -148.3163       9805.8278    \n",
            "       219   132.81385     -148.40176      0             -148.33309      9410.27      \n",
            "       220   150.9371      -148.38326      0             -148.30522      8794.9865    \n",
            "       221   165.15152     -148.38821      0             -148.30282      8156.2492    \n",
            "       222   174.73102     -148.40724      0             -148.3169       7172.9743    \n",
            "       223   179.82084     -148.4147       0             -148.32172      6208.8867    \n",
            "       224   181.12303     -148.39165      0             -148.298        5276.9844    \n",
            "       225   179.56361     -148.39955      0             -148.30671      4225.4401    \n",
            "       226   175.8664      -148.39883      0             -148.3079       3124.698     \n",
            "       227   170.11927     -148.43327      0             -148.34531      1886.771     \n",
            "       228   162.01392     -148.42525      0             -148.34148      700.27714    \n",
            "       229   150.95683     -148.43251      0             -148.35446     -472.53547    \n",
            "       230   136.44868     -148.40146      0             -148.33091     -1454.0497    \n",
            "       231   118.42427     -148.40104      0             -148.33981     -2383.5817    \n",
            "       232   97.431947     -148.39249      0             -148.34211     -3267.1789    \n",
            "       233   74.940793     -148.39065      0             -148.3519      -3912.6394    \n",
            "       234   53.040989     -148.36268      0             -148.33525     -4482.0162    \n",
            "       235   34.224246     -148.35781      0             -148.34012     -4850.6546    \n",
            "       236   20.916709     -148.36677      0             -148.35596     -5070.8709    \n",
            "       237   15.014962     -148.36491      0             -148.35715     -5171.7562    \n",
            "       238   17.603752     -148.35441      0             -148.34531     -5092.6222    \n",
            "       239   28.556821     -148.37073      0             -148.35597     -5041.7516    \n",
            "       240   46.570495     -148.38953      0             -148.36545     -4807.3434    \n",
            "       241   69.343673     -148.39121      0             -148.35536     -4395.4977    \n",
            "       242   93.978151     -148.3785       0             -148.32991     -3825.3501    \n",
            "       243   117.37704     -148.40866      0             -148.34797     -3076.7145    \n",
            "       244   137.11775     -148.42843      0             -148.35754     -2185.6239    \n",
            "       245   151.49667     -148.44879      0             -148.37046     -1365.536     \n",
            "       246   159.64506     -148.44452      0             -148.36198     -418.62578    \n",
            "       247   161.76192     -148.43246      0             -148.34882      570.35825    \n",
            "       248   158.7273      -148.45381      0             -148.37174      1572.5254    \n",
            "       249   151.83786     -148.46831      0             -148.38981      2625.9251    \n",
            "       250   142.44831     -148.41821      0             -148.34456      3624.9885    \n",
            "       251   131.66148     -148.41369      0             -148.34562      4559.3658    \n",
            "       252   119.97147     -148.4293       0             -148.36726      5382.827     \n",
            "       253   107.45367     -148.41311      0             -148.35755      6150.5971    \n",
            "       254   94.111056     -148.43134      0             -148.38268      6820.5565    \n",
            "       255   80.079723     -148.41028      0             -148.36888      7352.7613    \n",
            "       256   65.708859     -148.41749      0             -148.38351      7745.4535    \n",
            "       257   51.725387     -148.39524      0             -148.3685       8124.1936    \n",
            "       258   39.351995     -148.41086      0             -148.39051      8263.7199    \n",
            "       259   29.976482     -148.40551      0             -148.39001      8195.0693    \n",
            "       260   25.002846     -148.38357      0             -148.37064      8051.6422    \n",
            "       261   25.521805     -148.36813      0             -148.35494      7929.8469    \n",
            "       262   31.944587     -148.39122      0             -148.3747       7427.5107    \n",
            "       263   43.819528     -148.40545      0             -148.38279      6833.8191    \n",
            "       264   59.854466     -148.387        0             -148.35605      6296.1943    \n",
            "       265   78.060257     -148.40395      0             -148.36359      5396.4167    \n",
            "       266   96.050656     -148.41594      0             -148.36628      4498.2157    \n",
            "       267   111.64456     -148.42525      0             -148.36752      3624.7189    \n",
            "       268   123.02995     -148.4508       0             -148.38719      2621.89      \n",
            "       269   129.04374     -148.45044      0             -148.38372      1700.5458    \n",
            "       270   129.48207     -148.43648      0             -148.36953      666.8682     \n",
            "       271   125.01046     -148.44873      0             -148.38409     -225.42356    \n",
            "       272   116.9102      -148.44091      0             -148.38047     -1165.2651    \n",
            "       273   106.84163     -148.43561      0             -148.38037     -1986.4266    \n",
            "       274   96.395704     -148.42816      0             -148.37832     -2805.9004    \n",
            "       275   86.877739     -148.43787      0             -148.39295     -3489.9135    \n",
            "       276   78.91668      -148.42381      0             -148.38301     -4137.6895    \n",
            "       277   72.527287     -148.42406      0             -148.38657     -4727.6845    \n",
            "       278   67.448907     -148.40568      0             -148.37081     -5147.3992    \n",
            "       279   63.250611     -148.41633      0             -148.38362     -5455.1175    \n",
            "       280   59.523756     -148.41814      0             -148.38736     -5604.0799    \n",
            "       281   56.074715     -148.4131       0             -148.3841      -5643.8678    \n",
            "       282   53.151839     -148.41711      0             -148.38963     -5601.2812    \n",
            "       283   51.220452     -148.41741      0             -148.39092     -5498.478     \n",
            "       284   50.822264     -148.40003      0             -148.37376     -5148.3106    \n",
            "       285   52.429068     -148.40922      0             -148.38212     -4738.2873    \n",
            "       286   56.391192     -148.41909      0             -148.38993     -4220.4856    \n",
            "       287   62.594224     -148.42808      0             -148.39571     -3710.4169    \n",
            "       288   70.347694     -148.4166       0             -148.38023     -3097.3074    \n",
            "       289   78.488342     -148.44342      0             -148.40284     -2503.4555    \n",
            "       290   85.581349     -148.44065      0             -148.3964      -1829.5038    \n",
            "       291   90.355086     -148.41388      0             -148.36717     -1165.6671    \n",
            "       292   92.032662     -148.44667      0             -148.39909     -578.25067    \n",
            "       293   90.140619     -148.43354      0             -148.38694      177.94536    \n",
            "       294   84.858784     -148.43899      0             -148.39512      805.45195    \n",
            "       295   77.043232     -148.43286      0             -148.39302      1393.4311    \n",
            "       296   67.965399     -148.41919      0             -148.38405      1911.1955    \n",
            "       297   59.144375     -148.4173       0             -148.38672      2355.683     \n",
            "       298   51.961912     -148.41494      0             -148.38808      2877.321     \n",
            "       299   47.405294     -148.39962      0             -148.37511      3138.8474    \n",
            "       300   45.924478     -148.43311      0             -148.40936      3219.8773    \n",
            "       301   47.339422     -148.44304      0             -148.41856      3243.3351    \n",
            "       302   51.009629     -148.45048      0             -148.4241       3173.0377    \n",
            "       303   55.928355     -148.43545      0             -148.40654      3069.7883    \n",
            "       304   61.085186     -148.43382      0             -148.40224      2782.2039    \n",
            "       305   65.597277     -148.4347       0             -148.40078      2471.7622    \n",
            "       306   68.793026     -148.43854      0             -148.40297      2012.7572    \n",
            "       307   70.57298      -148.41383      0             -148.37734      1570.8887    \n",
            "       308   71.203133     -148.43659      0             -148.39977      974.15089    \n",
            "       309   71.01578      -148.4462       0             -148.40949      348.23321    \n",
            "       310   70.411596     -148.40606      0             -148.36966     -360.54316    \n",
            "       311   69.6828       -148.43514      0             -148.39911     -1071.9998    \n",
            "       312   68.912405     -148.41         0             -148.37437     -1790.6566    \n",
            "       313   67.838592     -148.46231      0             -148.42723     -2512.9368    \n",
            "       314   66.014179     -148.44245      0             -148.40832     -3271.524     \n",
            "       315   62.98939      -148.43895      0             -148.40638     -3949.3393    \n",
            "       316   58.385105     -148.41896      0             -148.38877     -4563.3172    \n",
            "       317   52.109246     -148.44077      0             -148.41383     -5017.6665    \n",
            "       318   44.460232     -148.42674      0             -148.40375     -5568.7968    \n",
            "       319   36.251761     -148.40046      0             -148.38171     -5862.5878    \n",
            "       320   28.677893     -148.41818      0             -148.40335     -6262.6754    \n",
            "       321   22.907995     -148.40718      0             -148.39533     -6567.1728    \n",
            "       322   19.92315      -148.41362      0             -148.40332     -6648.0679    \n",
            "       323   20.456676     -148.40212      0             -148.39155     -6613.4284    \n",
            "       324   24.758593     -148.39378      0             -148.38098     -6587.7469    \n",
            "       325   32.405835     -148.41909      0             -148.40233     -6392.3824    \n",
            "       326   42.449105     -148.42777      0             -148.40582     -6183.4444    \n",
            "       327   53.575969     -148.42007      0             -148.39237     -5809.0617    \n",
            "       328   64.343582     -148.45725      0             -148.42398     -5343.9105    \n",
            "       329   73.389969     -148.43097      0             -148.39303     -4794.8982    \n",
            "       330   79.836446     -148.43636      0             -148.39509     -4262.5169    \n",
            "       331   83.278442     -148.46194      0             -148.41888     -3641.7326    \n",
            "       332   83.636931     -148.45353      0             -148.41028     -2851.6114    \n",
            "       333   81.317474     -148.44407      0             -148.40203     -2049.3609    \n",
            "       334   77.015398     -148.43228      0             -148.39246     -1406.1301    \n",
            "       335   71.489692     -148.45317      0             -148.41621     -627.33495    \n",
            "       336   65.358023     -148.41654      0             -148.38274      67.612018    \n",
            "       337   58.976399     -148.44266      0             -148.41217      788.96304    \n",
            "       338   52.425477     -148.43756      0             -148.41046      1482.7982    \n",
            "       339   45.660812     -148.41864      0             -148.39503      2080.4385    \n",
            "       340   38.717871     -148.44869      0             -148.42867      2485.3084    \n",
            "       341   31.705362     -148.44335      0             -148.42696      2993.3732    \n",
            "       342   24.820365     -148.42914      0             -148.41631      3291.7895    \n",
            "       343   18.659104     -148.41175      0             -148.4021       3535.2619    \n",
            "       344   14.057356     -148.40205      0             -148.39478      3727.7081    \n",
            "       345   11.840318     -148.39152      0             -148.3854       3746.2798    \n",
            "       346   12.714266     -148.41454      0             -148.40796      3744.9431    \n",
            "       347   17.104646     -148.41114      0             -148.40229      3573.7924    \n",
            "       348   25.014953     -148.417        0             -148.40407      3221.7604    \n",
            "       349   35.847353     -148.41666      0             -148.39813      3023.3212    \n",
            "       350   48.452533     -148.43564      0             -148.41059      2494.1695    \n",
            "       351   61.392228     -148.43884      0             -148.4071       2022.6215    \n",
            "       352   73.230728     -148.4357       0             -148.39784      1383.2946    \n",
            "       353   82.576831     -148.4396       0             -148.39691      763.2404     \n",
            "       354   88.330802     -148.44617      0             -148.4005       77.860411    \n",
            "       355   90.128791     -148.44369      0             -148.39709     -531.85717    \n",
            "       356   88.217985     -148.47419      0             -148.42858     -1292.677     \n",
            "       357   83.203469     -148.46383      0             -148.42081     -1932.2139    \n",
            "       358   75.994683     -148.44804      0             -148.40875     -2599.4997    \n",
            "       359   67.691003     -148.44229      0             -148.40729     -3186.0715    \n",
            "       360   59.289362     -148.44627      0             -148.41561     -3667.4243    \n",
            "       361   51.485137     -148.42312      0             -148.3965      -4139.1857    \n",
            "       362   44.675254     -148.4496       0             -148.4265      -4628.0848    \n",
            "       363   38.962727     -148.43897      0             -148.41882     -5017.1539    \n",
            "       364   34.246236     -148.42057      0             -148.40286     -5215.7449    \n",
            "       365   30.471226     -148.44263      0             -148.42688     -5396.0052    \n",
            "       366   27.548152     -148.37851      0             -148.36427     -5300.0796    \n",
            "       367   25.465805     -148.4549       0             -148.44173     -5407.0927    \n",
            "       368   24.511266     -148.42637      0             -148.4137      -5058.7618    \n",
            "       369   25.117161     -148.43519      0             -148.42221     -4981.4697    \n",
            "       370   27.761289     -148.42668      0             -148.41233     -4573.8728    \n",
            "       371   32.647346     -148.41134      0             -148.39446     -4027.4463    \n",
            "       372   39.713363     -148.40915      0             -148.38861     -3497.5853    \n",
            "       373   48.523187     -148.41887      0             -148.39378     -2922.2017    \n",
            "       374   58.266597     -148.43883      0             -148.4087      -2339.7155    \n",
            "       375   67.739607     -148.43848      0             -148.40345     -1623.1529    \n",
            "       376   75.699784     -148.46045      0             -148.42131     -842.01383    \n",
            "       377   81.261111     -148.44216      0             -148.40015     -111.43136    \n",
            "       378   83.562477     -148.42894      0             -148.38574      641.39412    \n",
            "       379   82.277438     -148.4402       0             -148.39766      1312.3861    \n",
            "       380   77.84992      -148.46666      0             -148.42641      2082.0628    \n",
            "       381   71.075768     -148.45093      0             -148.41418      2720.1288    \n",
            "       382   63.013887     -148.43427      0             -148.40169      3447.0512    \n",
            "       383   54.92264      -148.44028      0             -148.41188      3929.2421    \n",
            "       384   47.867113     -148.44187      0             -148.41712      4426.0268    \n",
            "       385   42.629414     -148.44155      0             -148.41951      4773.3972    \n",
            "       386   39.619522     -148.45048      0             -148.42999      5073.0454    \n",
            "       387   38.759479     -148.4451       0             -148.42505      5208.993     \n",
            "       388   39.60013      -148.43024      0             -148.40977      5331.4246    \n",
            "       389   41.585093     -148.42957      0             -148.40807      5252.2764    \n",
            "       390   43.986119     -148.4317       0             -148.40896      5140.8479    \n",
            "       391   46.253859     -148.41717      0             -148.39325      4882.9188    \n",
            "       392   48.233284     -148.41609      0             -148.39115      4502.2964    \n",
            "       393   49.938903     -148.413        0             -148.38718      4050.6394    \n",
            "       394   51.529933     -148.42744      0             -148.4008       3542.9633    \n",
            "       395   53.231968     -148.43461      0             -148.40708      3042.1012    \n",
            "       396   55.26805      -148.40091      0             -148.37234      2474.8582    \n",
            "       397   57.654071     -148.44045      0             -148.41064      1831.0607    \n",
            "       398   60.08955      -148.45413      0             -148.42307      1119.0901    \n",
            "       399   62.157456     -148.43807      0             -148.40593      433.63641    \n",
            "       400   63.221891     -148.43214      0             -148.39945     -194.87873    \n",
            "       401   62.622077     -148.43405      0             -148.40167     -822.16062    \n",
            "       402   60.06751      -148.43633      0             -148.40527     -1317.7813    \n",
            "       403   55.603269     -148.41783      0             -148.38908     -1832.0984    \n",
            "       404   49.654778     -148.43388      0             -148.40821     -2295.9218    \n",
            "       405   43.086245     -148.42312      0             -148.40084     -2604.8141    \n",
            "       406   37.060804     -148.43388      0             -148.41472     -2956.6952    \n",
            "       407   32.657731     -148.40666      0             -148.38978     -3030.8249    \n",
            "       408   30.742201     -148.43267      0             -148.41678     -3037.6105    \n",
            "       409   31.882478     -148.41133      0             -148.39484     -2971.3244    \n",
            "       410   36.079939     -148.43532      0             -148.41666     -2886.4313    \n",
            "       411   42.787345     -148.4383       0             -148.41617     -2519.5918    \n",
            "       412   50.970015     -148.41463      0             -148.38827     -2063.8461    \n",
            "       413   59.333168     -148.42659      0             -148.39591     -1661.2503    \n",
            "       414   66.647895     -148.43715      0             -148.40269     -1153.2052    \n",
            "       415   71.873654     -148.43008      0             -148.39292     -491.11478    \n",
            "       416   74.295529     -148.43343      0             -148.39501      128.52732    \n",
            "       417   73.850426     -148.43676      0             -148.39858      892.50315    \n",
            "       418   70.936977     -148.45112      0             -148.41445      1625.7564    \n",
            "       419   66.151973     -148.45539      0             -148.42118      2508.5746    \n",
            "       420   60.33157      -148.44033      0             -148.40913      3368.9728    \n",
            "       421   54.223476     -148.43567      0             -148.40764      4154.4976    \n",
            "       422   48.356399     -148.43744      0             -148.41244      4890.3616    \n",
            "       423   42.976363     -148.43124      0             -148.40902      5535.07      \n",
            "       424   38.068283     -148.43528      0             -148.4156       6150.08      \n",
            "       425   33.552283     -148.43371      0             -148.41636      6603.7656    \n",
            "       426   29.300813     -148.38866      0             -148.37351      7161.2446    \n",
            "       427   25.377448     -148.411        0             -148.39788      7396.9905    \n",
            "       428   22.101233     -148.39673      0             -148.3853       7677.905     \n",
            "       429   20.167824     -148.40962      0             -148.39919      7719.8938    \n",
            "       430   20.249248     -148.42049      0             -148.41002      7795.294     \n",
            "       431   22.900251     -148.42318      0             -148.41134      7534.8236    \n",
            "       432   28.660649     -148.41665      0             -148.40183      7410.5907    \n",
            "       433   37.598414     -148.41274      0             -148.3933       6905.2464    \n",
            "       434   49.183741     -148.44926      0             -148.42383      6391.0739    \n",
            "       435   62.376305     -148.44947      0             -148.41721      5709.073     \n",
            "       436   75.712901     -148.45057      0             -148.41143      5097.4128    \n",
            "       437   87.506805     -148.45891      0             -148.41367      4375.7907    \n",
            "       438   96.189956     -148.45279      0             -148.40305      3356.5455    \n",
            "       439   100.62293     -148.44709      0             -148.39507      2598.4662    \n",
            "       440   100.1413      -148.46963      0             -148.41786      1768.4096    \n",
            "       441   94.84094      -148.44377      0             -148.39474      849.25708    \n",
            "       442   85.629738     -148.44537      0             -148.40109     -84.089827    \n",
            "       443   73.786728     -148.43395      0             -148.3958      -934.72402    \n",
            "       444   60.853979     -148.42805      0             -148.39658     -1652.6037    \n",
            "       445   48.408518     -148.41112      0             -148.38609     -2324.1325    \n",
            "       446   37.678133     -148.42502      0             -148.40554     -3040.2328    \n",
            "       447   29.441686     -148.40604      0             -148.39082     -3495.3896    \n",
            "       448   23.881827     -148.42017      0             -148.40783     -4010.6754    \n",
            "       449   20.898997     -148.38009      0             -148.36929     -4219.7881    \n",
            "       450   20.172688     -148.40976      0             -148.39933     -4464.604     \n",
            "       451   21.25528      -148.39592      0             -148.38493     -4519.6425    \n",
            "       452   23.944047     -148.39932      0             -148.38694     -4488.3262    \n",
            "       453   28.161741     -148.391        0             -148.37644     -4415.7366    \n",
            "       454   34.077966     -148.41205      0             -148.39443     -4166.1154    \n",
            "       455   41.957923     -148.42559      0             -148.4039      -3750.282     \n",
            "       456   51.931982     -148.44577      0             -148.41891     -3165.1163    \n",
            "       457   63.944791     -148.44433      0             -148.41127     -2730.8495    \n",
            "       458   77.457783     -148.41805      0             -148.378       -2013.3296    \n",
            "       459   91.418685     -148.43133      0             -148.38406     -1243.3491    \n",
            "       460   104.3503      -148.46728      0             -148.41332     -525.36358    \n",
            "       461   114.67397     -148.46195      0             -148.40266      226.44868    \n",
            "       462   120.91386     -148.43987      0             -148.37736      1076.1152    \n",
            "       463   121.97797     -148.44489      0             -148.38182      1822.542     \n",
            "       464   117.59485     -148.45923      0             -148.39843      2542.577     \n",
            "       465   108.33764     -148.43436      0             -148.37834      3276.26      \n",
            "       466   95.381864     -148.44672      0             -148.39741      3960.7927    \n",
            "       467   80.415355     -148.43694      0             -148.39537      4572.4915    \n",
            "       468   65.458761     -148.44057      0             -148.40673      5076.9836    \n",
            "       469   52.461324     -148.42026      0             -148.39314      5440.0095    \n",
            "       470   42.918498     -148.41707      0             -148.39488      5597.6755    \n",
            "       471   37.648033     -148.40228      0             -148.38282      5841.7552    \n",
            "       472   36.754482     -148.39886      0             -148.37986      5808.8185    \n",
            "       473   39.637299     -148.39321      0             -148.37271      5661.4744    \n",
            "       474   45.261456     -148.41473      0             -148.39133      5337.1551    \n",
            "       475   52.482542     -148.38657      0             -148.35943      5024.5283    \n",
            "       476   60.244386     -148.41305      0             -148.3819       4460.7003    \n",
            "       477   67.779554     -148.43018      0             -148.39513      3901.088     \n",
            "       478   74.857627     -148.43306      0             -148.39436      3196.8281    \n",
            "       479   81.556091     -148.43006      0             -148.38789      2442.1668    \n",
            "       480   88.045938     -148.44256      0             -148.39703      1580.0855    \n",
            "       481   94.634942     -148.4059       0             -148.35697      755.4333     \n",
            "       482   101.48552     -148.43219      0             -148.37972     -141.38239    \n",
            "       483   108.15867     -148.46091      0             -148.40499     -1163.385     \n",
            "       484   113.79066     -148.4597       0             -148.40086     -2095.2748    \n",
            "       485   117.45422     -148.44217      0             -148.38144     -2853.5344    \n",
            "       486   118.14855     -148.45379      0             -148.3927      -3714.6292    \n",
            "       487   115.20273     -148.4353       0             -148.37574     -4461.7662    \n",
            "       488   108.50794     -148.40331      0             -148.3472      -5137.4897    \n",
            "       489   98.673169     -148.40266      0             -148.35165     -5762.1546    \n",
            "       490   86.987174     -148.41087      0             -148.3659      -6212.3546    \n",
            "       491   75.230305     -148.41342      0             -148.37452     -6611.6947    \n",
            "       492   65.33235      -148.43184      0             -148.39806     -6842.1759    \n",
            "       493   59.103312     -148.41673      0             -148.38617     -6913.626     \n",
            "       494   57.801408     -148.39401      0             -148.36413     -6909.9894    \n",
            "       495   61.723361     -148.38002      0             -148.34811     -6771.6898    \n",
            "       496   70.066978     -148.37129      0             -148.33506     -6459.8884    \n",
            "       497   81.279398     -148.39736      0             -148.35533     -6184.4798    \n",
            "       498   93.340549     -148.42258      0             -148.37431     -5811.1883    \n",
            "       499   104.24539     -148.41512      0             -148.36122     -5126.7086    \n",
            "       500   112.3692      -148.4304       0             -148.3723      -4472.4192    \n",
            "       501   116.69307     -148.42112      0             -148.36078     -3706.7248    \n",
            "       502   117.01609     -148.40316      0             -148.34266     -2902.8584    \n",
            "       503   113.92016     -148.41954      0             -148.36064     -2090.3775    \n",
            "       504   108.50391     -148.40109      0             -148.34499     -1181.5651    \n",
            "       505   102.09013     -148.42698      0             -148.37419     -339.81751    \n",
            "       506   95.956728     -148.42128      0             -148.37166      484.66193    \n",
            "       507   90.84762      -148.41916      0             -148.37219      1367.5708    \n",
            "       508   86.996107     -148.39479      0             -148.3498       2082.6674    \n",
            "       509   84.030435     -148.40424      0             -148.3608       2705.2289    \n",
            "       510   81.194991     -148.38738      0             -148.3454       3244.6564    \n",
            "       511   77.982616     -148.3699       0             -148.32958      3755.7872    \n",
            "       512   74.293287     -148.40117      0             -148.36276      4061.293     \n",
            "       513   70.398285     -148.37716      0             -148.34076      4272.0866    \n",
            "       514   67.27236      -148.38385      0             -148.34906      4373.7627    \n",
            "       515   66.200663     -148.38673      0             -148.3525       4402.8126    \n",
            "       516   68.508121     -148.35676      0             -148.32134      4229.4569    \n",
            "       517   75.392601     -148.41661      0             -148.37763      3680.8556    \n",
            "       518   87.295749     -148.37055      0             -148.32541      3256.9627    \n",
            "       519   103.66201     -148.43053      0             -148.37693      2540.1411    \n",
            "       520   122.99897     -148.40513      0             -148.34154      1901.6855    \n",
            "       521   142.90002     -148.41171      0             -148.33783      985.52762    \n",
            "       522   160.48536     -148.42899      0             -148.34602      78.140324    \n",
            "       523   173.0683      -148.42788      0             -148.33839     -920.34716    \n",
            "       524   178.42537     -148.43857      0             -148.34632     -2085.1773    \n",
            "       525   175.50059     -148.41487      0             -148.32413     -3101.6436    \n",
            "       526   164.48929     -148.43418      0             -148.34914     -4074.9308    \n",
            "       527   146.80456     -148.4043       0             -148.3284      -5110.2505    \n",
            "       528   124.98278     -148.37432      0             -148.3097      -6020.1275    \n",
            "       529   101.92387     -148.37667      0             -148.32398     -6971.3309    \n",
            "       530   80.506881     -148.36418      0             -148.32256     -7735.143     \n",
            "       531   62.96443      -148.35569      0             -148.32313     -8569.0181    \n",
            "       532   50.620446     -148.36337      0             -148.3372      -9238.9371    \n",
            "       533   43.818595     -148.35661      0             -148.33395     -9814.1473    \n",
            "       534   41.963616     -148.3307       0             -148.309       -10243.244    \n",
            "       535   44.213617     -148.3481       0             -148.32524     -10453.886    \n",
            "       536   49.748591     -148.3757       0             -148.34998     -10571.764    \n",
            "       537   57.7153       -148.37465      0             -148.3448      -10427.818    \n",
            "       538   68.036609     -148.37135      0             -148.33617     -10282.262    \n",
            "       539   81.11858      -148.35087      0             -148.30892     -9867.9009    \n",
            "       540   97.669098     -148.36636      0             -148.31587     -9185.4239    \n",
            "       541   118.38613     -148.38981      0             -148.3286      -8504.2831    \n",
            "       542   143.353       -148.42836      0             -148.35424     -7790.4452    \n",
            "       543   171.64329     -148.40276      0             -148.31402     -6789.354     \n",
            "       544   201.18457     -148.4252       0             -148.32118     -5625.8937    \n",
            "       545   229.21574     -148.42193      0             -148.30342     -4610.2549    \n",
            "       546   252.07518     -148.44385      0             -148.31352     -3347.9591    \n",
            "       547   266.10693     -148.45766      0             -148.32007     -2160.4872    \n",
            "       548   268.89081     -148.40606      0             -148.26703     -891.49981    \n",
            "       549   259.17474     -148.4355       0             -148.3015       341.39475    \n",
            "       550   237.65407     -148.40814      0             -148.28527      1714.6117    \n",
            "       551   206.84412     -148.39838      0             -148.29143      3033.4746    \n",
            "       552   170.48675     -148.38059      0             -148.29245      4051.5793    \n",
            "       553   133.22647     -148.35212      0             -148.28323      5243.5324    \n",
            "       554   99.795065     -148.31262      0             -148.26102      6141.5688    \n",
            "       555   74.047707     -148.32529      0             -148.287        6711.0274    \n",
            "       556   58.503101     -148.27288      0             -148.24263      7312.1593    \n",
            "       557   54.108625     -148.30458      0             -148.2766       7436.1773    \n",
            "       558   60.091723     -148.31694      0             -148.28587      7720.5019    \n",
            "       559   74.343049     -148.32947      0             -148.29104      7657.72      \n",
            "       560   94.424038     -148.30564      0             -148.25682      7521.1014    \n",
            "       561   118.10302     -148.3151       0             -148.25403      7152.5794    \n",
            "       562   143.47581     -148.3626       0             -148.28842      6543.6123    \n",
            "       563   169.29504     -148.37323      0             -148.28569      5875.2277    \n",
            "       564   195.3675      -148.3622       0             -148.26118      5028.3296    \n",
            "       565   222.03103     -148.40573      0             -148.29093      4018.5212    \n",
            "       566   249.42597     -148.41171      0             -148.28274      2996.4663    \n",
            "       567   277.1376      -148.42432      0             -148.28103      1895.628     \n",
            "       568   303.68756     -148.41645      0             -148.25943      670.84751    \n",
            "       569   326.59817     -148.4458       0             -148.27694     -794.72077    \n",
            "       570   343.14469     -148.43441      0             -148.25699     -1912.6259    \n",
            "       571   350.12018     -148.42828      0             -148.24725     -3132.3652    \n",
            "       572   344.88099     -148.44054      0             -148.26223     -4338.6813    \n",
            "       573   326.83001     -148.40846      0             -148.23948     -5359.1686    \n",
            "       574   297.31588     -148.37858      0             -148.22485     -6205.2325    \n",
            "       575   259.46212     -148.37972      0             -148.24557     -6940.7459    \n",
            "       576   218.21663     -148.35132      0             -148.23849     -7434.5362    \n",
            "       577   179.58666     -148.32133      0             -148.22847     -7698.5881    \n",
            "       578   148.95021     -148.30944      0             -148.23243     -7901.8354    \n",
            "       579   130.52855     -148.28582      0             -148.21833     -8021.7002    \n",
            "       580   126.5098      -148.29662      0             -148.23121     -7900.8904    \n",
            "       581   136.5612      -148.29484      0             -148.22423     -7575.3276    \n",
            "       582   158.15857     -148.32036      0             -148.23859     -7019.3821    \n",
            "       583   187.26509     -148.32083      0             -148.22401     -6365.2354    \n",
            "       584   218.96045     -148.31972      0             -148.20651     -5460.5655    \n",
            "       585   248.70149     -148.33378      0             -148.20519     -4387.3294    \n",
            "       586   273.32217     -148.34146      0             -148.20014     -3090.1063    \n",
            "       587   291.39236     -148.38375      0             -148.23309     -1729.1928    \n",
            "       588   303.26326     -148.38325      0             -148.22645     -58.117933    \n",
            "       589   310.73168     -148.34056      0             -148.1799       1554.5012    \n",
            "       590   315.88378     -148.36614      0             -148.20282      3254.9417    \n",
            "       591   320.59975     -148.382        0             -148.21624      4871.0196    \n",
            "       592   325.75756     -148.37146      0             -148.20303      6529.8806    \n",
            "       593   330.79591     -148.34784      0             -148.17681      8085.1589    \n",
            "       594   334.20561     -148.38289      0             -148.2101       9461.281     \n",
            "       595   333.86153     -148.34426      0             -148.17164      10782.305    \n",
            "       596   327.49891     -148.36207      0             -148.19274      11890.313    \n",
            "       597   314.08015     -148.33373      0             -148.17134      12894.73     \n",
            "       598   294.53347     -148.30457      0             -148.15228      13686.746    \n",
            "       599   271.28548     -148.31796      0             -148.17769      14268.009    \n",
            "       600   248.59948     -148.30115      0             -148.17261      14530.302    \n",
            "       601   231.52867     -148.27812      0             -148.15841      14638.371    \n",
            "       602   224.50217     -148.27652      0             -148.16044      14354.437    \n",
            "       603   230.68547     -148.2841       0             -148.16483      14027.611    \n",
            "       604   250.75634     -148.27864      0             -148.14899      13047.317    \n",
            "       605   282.37251     -148.28351      0             -148.13752      12048.789    \n",
            "       606   320.64917     -148.29967      0             -148.13388      10676.109    \n",
            "       607   358.9535      -148.34295      0             -148.15736      9249.0274    \n",
            "       608   390.1771      -148.34142      0             -148.13968      7514.6601    \n",
            "       609   408.2825      -148.33234      0             -148.12124      5774.7982    \n",
            "       610   409.56731     -148.35187      0             -148.14011      3940.2426    \n",
            "       611   393.70964     -148.3362       0             -148.13263      2302.6979    \n",
            "       612   363.16418     -148.29539      0             -148.10762      580.14436    \n",
            "       613   322.82603     -148.28488      0             -148.11796     -1105.8826    \n",
            "       614   279.49275     -148.24505      0             -148.10054     -2685.1538    \n",
            "       615   239.91339     -148.24619      0             -148.12215     -4114.5092    \n",
            "       616   209.55574     -148.21434      0             -148.10599     -5451.0331    \n",
            "       617   191.68585     -148.20447      0             -148.10536     -6655.7799    \n",
            "       618   187.01884     -148.2145       0             -148.11781     -7660.9353    \n",
            "       619   193.78392     -148.18885      0             -148.08865     -8429.1645    \n",
            "       620   208.99094     -148.19659      0             -148.08853     -8914.4014    \n",
            "       621   229.54823     -148.24048      0             -148.12179     -9229.4091    \n",
            "       622   252.69372     -148.21186      0             -148.0812      -9134.7019    \n",
            "       623   277.25167     -148.23228      0             -148.08893     -8708.4814    \n",
            "       624   303.69141     -148.25314      0             -148.09612     -8121.2632    \n",
            "       625   333.44238     -148.26026      0             -148.08786     -7050.8907    \n",
            "       626   368.65449     -148.2695       0             -148.07889     -5845.4857    \n",
            "       627   410.34268     -148.29353      0             -148.08137     -4385.1167    \n",
            "       628   457.47496     -148.33615      0             -148.09962     -2757.9083    \n",
            "       629   506.75853     -148.32909      0             -148.06708     -986.62008    \n",
            "       630   552.11762     -148.3281       0             -148.04263      869.90412    \n",
            "       631   585.71857     -148.36368      0             -148.06084      2759.6755    \n",
            "       632   600.23554     -148.36842      0             -148.05807      4742.724     \n",
            "       633   589.71995     -148.32514      0             -148.02023      6913.7038    \n",
            "       634   551.31679     -148.32605      0             -148.04099      8942.3188    \n",
            "       635   487.45846     -148.27536      0             -148.02332      10864.437    \n",
            "       636   404.6282      -148.24486      0             -148.03565      12890.402    \n",
            "       637   313.11084     -148.20094      0             -148.03905      14647.776    \n",
            "       638   225.3385      -148.14848      0             -148.03197      16119.461    \n",
            "       639   153.35847     -148.11214      0             -148.03285      17358.392    \n",
            "       640   106.95977     -148.0619       0             -148.0066       18398.163    \n",
            "       641   92.065074     -148.0545       0             -148.0069       18943.433    \n",
            "       642   109.59115     -148.05535      0             -147.99869      19153.902    \n",
            "       643   155.81453     -148.08163      0             -148.00107      18979.883    \n",
            "       644   223.80463     -148.12537      0             -148.00965      18510.425    \n",
            "       645   304.83799     -148.15953      0             -148.00192      17627.98     \n",
            "       646   390.22551     -148.18911      0             -147.98735      16686.333    \n",
            "       647   472.97256     -148.25785      0             -148.0133       15471.512    \n",
            "       648   548.70914     -148.25607      0             -147.97237      13904.525    \n",
            "       649   615.76327     -148.30397      0             -147.9856       12295.241    \n",
            "       650   674.29073     -148.3373       0             -147.98867      10321.521    \n",
            "       651   725.34373     -148.3418       0             -147.96676      8181.6713    \n",
            "       652   769.17863     -148.3782       0             -147.9805       6020.9919    \n",
            "       653   804.45492     -148.37347      0             -147.95753      3587.9857    \n",
            "       654   827.31964     -148.39747      0             -147.96971      1191.807     \n",
            "       655   832.46997     -148.36713      0             -147.9367      -1076.2091    \n",
            "       656   815.08735     -148.34411      0             -147.92267     -3259.4976    \n",
            "       657   771.48372     -148.33865      0             -147.93977     -5448.0106    \n",
            "       658   701.17394     -148.28717      0             -147.92463     -7099.1527    \n",
            "       659   608.31091     -148.22112      0             -147.9066      -8739.017     \n",
            "       660   501.52623     -148.18322      0             -147.92391     -9951.319     \n",
            "       661   392.81422     -148.12028      0             -147.91718     -10898.495    \n",
            "       662   295.69596     -148.04872      0             -147.89583     -11568.744    \n",
            "       663   222.65896     -148.03956      0             -147.92443     -12198.65     \n",
            "       664   183.1219      -147.99483      0             -147.90015     -12481.938    \n",
            "       665   181.86311     -148.00964      0             -147.91561     -12670.274    \n",
            "       666   218.14198     -148.00859      0             -147.8958      -12677.243    \n",
            "       667   285.92871     -148.04886      0             -147.90102     -12563.961    \n",
            "       668   375.15979     -148.08394      0             -147.88997     -11979.716    \n",
            "       669   473.881       -148.13712      0             -147.8921      -11282.038    \n",
            "       670   570.78819     -148.19785      0             -147.90273     -10446.467    \n",
            "       671   657.33839     -148.22659      0             -147.88672     -9193.9152    \n",
            "       672   728.68631     -148.26631      0             -147.88955     -7730.8625    \n",
            "       673   783.98041     -148.25754      0             -147.85219     -6176.5184    \n",
            "       674   825.3845      -148.31026      0             -147.8835      -4476.7517    \n",
            "       675   856.75692     -148.30396      0             -147.86098     -2596.38      \n",
            "       676   881.4492      -148.32574      0             -147.87        -840.34706    \n",
            "       677   900.96496     -148.32513      0             -147.8593       743.25861    \n",
            "       678   913.86071     -148.31766      0             -147.84516      2307.2759    \n",
            "       679   916.2264      -148.3017       0             -147.82797      3716.7436    \n",
            "       680   902.59202     -148.28889      0             -147.82222      4991.8586    \n",
            "       681   867.97084     -148.27738      0             -147.8286       6085.8784    \n",
            "       682   810.25112     -148.26955      0             -147.85061      7089.8949    \n",
            "       683   731.11429     -148.2039       0             -147.82589      7917.3459    \n",
            "       684   636.60761     -148.1299       0             -147.80075      8488.9104    \n",
            "       685   536.56992     -148.06532      0             -147.78789      8924.5259    \n",
            "       686   443.42605     -148.01306      0             -147.78379      9034.2506    \n",
            "       687   369.59463     -147.99637      0             -147.80527      8861.4668    \n",
            "       688   325.0801      -147.95898      0             -147.7909       8249.6905    \n",
            "       689   315.40519     -147.95422      0             -147.79114      7494.474     \n",
            "       690   340.87665     -148.00108      0             -147.82484      6386.2517    \n",
            "       691   396.03764     -147.98439      0             -147.77962      5041.8098    \n",
            "       692   470.57455     -148.03827      0             -147.79496      3331.1219    \n",
            "       693   551.67809     -148.072        0             -147.78676      1559.7615    \n",
            "       694   626.76211     -148.12436      0             -147.8003      -291.84216    \n",
            "       695   685.49658     -148.13618      0             -147.78175     -2081.1981    \n",
            "       696   721.32064     -148.13939      0             -147.76644     -3921.1951    \n",
            "       697   732.93599     -148.16754      0             -147.78858     -5739.9482    \n",
            "       698   724.28662     -148.15474      0             -147.78025     -7400.3931    \n",
            "       699   703.33915     -148.14154      0             -147.77789     -8985.5084    \n",
            "       700   679.08996     -148.11972      0             -147.7686      -10504.825    \n",
            "       701   659.49332     -148.12279      0             -147.78181     -12003.846    \n",
            "       702   649.23821     -148.11757      0             -147.78188     -13512.046    \n",
            "       703   648.68513     -148.08892      0             -147.75352     -14670.477    \n",
            "       704   654.06358     -148.10691      0             -147.76873     -15954.277    \n",
            "       705   659.09051     -148.09279      0             -147.75202     -16928.053    \n",
            "       706   656.864       -148.11392      0             -147.77429     -17605.341    \n",
            "       707   642.45871     -148.07651      0             -147.74433     -17869.042    \n",
            "       708   614.06284     -148.07121      0             -147.75372     -18035.077    \n",
            "       709   574.51971     -148.04991      0             -147.75286     -17790.095    \n",
            "       710   530.92878     -148.04983      0             -147.77532     -17370.071    \n",
            "       711   492.75241     -147.98969      0             -147.73491     -16737.359    \n",
            "       712   469.51988     -147.99348      0             -147.75072     -15828.487    \n",
            "       713   467.88321     -148.00116      0             -147.75925     -14878.483    \n",
            "       714   489.47024     -148.00939      0             -147.75631     -13815.25     \n",
            "       715   530.15599     -148.02545      0             -147.75134     -12798.778    \n",
            "       716   580.91996     -148.04584      0             -147.74548     -11617.014    \n",
            "       717   629.4881      -148.08338      0             -147.75791     -10381.977    \n",
            "       718   663.43732     -148.07209      0             -147.72907     -8979.1357    \n",
            "       719   673.23212     -148.07668      0             -147.72859     -7447.8097    \n",
            "       720   654.11674     -148.10036      0             -147.76216     -5792.0882    \n",
            "       721   607.9584      -148.04891      0             -147.73457     -4132.8749    \n",
            "       722   542.0184      -147.98784      0             -147.7076      -2322.4101    \n",
            "       723   468.66001     -147.94367      0             -147.70135     -743.96711    \n",
            "       724   401.85364     -147.9318       0             -147.72402      933.44012    \n",
            "       725   353.36059     -147.90861      0             -147.72591      2301.5696    \n",
            "       726   331.18951     -147.92567      0             -147.75443      3487.068     \n",
            "       727   337.86305     -147.8844       0             -147.70971      4492.8301    \n",
            "       728   370.42275     -147.90475      0             -147.71323      5139.6183    \n",
            "       729   421.56737     -147.96718      0             -147.74922      5659.4312    \n",
            "       730   481.66425     -147.99148      0             -147.74244      5875.9217    \n",
            "       731   540.88409     -148.02286      0             -147.7432       6082.4773    \n",
            "       732   592.32508     -148.03148      0             -147.72522      6058.5706    \n",
            "       733   632.81375     -148.0418       0             -147.71461      5845.8033    \n",
            "       734   663.28379     -148.078        0             -147.73506      5588.2496    \n",
            "       735   687.88398     -148.08032      0             -147.72466      4948.1939    \n",
            "       736   712.42685     -148.09456      0             -147.72621      4190.3311    \n",
            "       737   742.08664     -148.10231      0             -147.71863      2981.9734    \n",
            "       738   779.31159     -148.11744      0             -147.7145       1777.0077    \n",
            "       739   822.56087     -148.16563      0             -147.74033      158.1154     \n",
            "       740   865.6431      -148.18997      0             -147.74239     -1608.8482    \n",
            "       741   899.25642     -148.2065       0             -147.74155     -3468.0389    \n",
            "       742   913.03131     -148.21055      0             -147.73848     -5366.8134    \n",
            "       743   897.95081     -148.18862      0             -147.72434     -7161.6417    \n",
            "       744   849.71792     -148.18317      0             -147.74383     -8871.1405    \n",
            "       745   768.96249     -148.12589      0             -147.72831     -10413.357    \n",
            "       746   662.7053      -148.06671      0             -147.72406     -11651.635    \n",
            "       747   543.73738     -148.0296       0             -147.74847     -12922.586    \n",
            "       748   427.48488     -147.96998      0             -147.74895     -13916.304    \n",
            "       749   328.90194     -147.91084      0             -147.74079     -14700.603    \n",
            "       750   260.12023     -147.85381      0             -147.71932     -15515.772    \n",
            "       751   228.40609     -147.8561       0             -147.738       -16281.508    \n",
            "       752   235.19091     -147.8526       0             -147.73099     -16948.463    \n",
            "       753   275.88159     -147.89328      0             -147.75064     -17521.764    \n",
            "       754   341.99989     -147.93145      0             -147.75463     -17849.333    \n",
            "       755   422.6312      -147.96096      0             -147.74245     -17857.092    \n",
            "       756   507.16263     -147.99876      0             -147.73654     -17827.809    \n",
            "       757   588.84459     -148.03599      0             -147.73154     -17366.195    \n",
            "       758   664.03258     -148.04684      0             -147.70351     -16579.817    \n",
            "       759   732.50634     -148.11737      0             -147.73864     -15576.508    \n",
            "       760   797.76377     -148.1597       0             -147.74723     -14292.399    \n",
            "       761   864.00916     -148.17392      0             -147.72719     -12866.73     \n",
            "       762   934.13387     -148.24993      0             -147.76695     -11426.915    \n",
            "       763   1007.5658     -148.26482      0             -147.74387     -9698.5731    \n",
            "       764   1078.9884     -148.30884      0             -147.75096     -7993.6692    \n",
            "       765   1139.1651     -148.31216      0             -147.72317     -6190.0758    \n",
            "       766   1176.7889     -148.34612      0             -147.73767     -4431.0521    \n",
            "       767   1180.7532     -148.3812       0             -147.77071     -2582.189     \n",
            "       768   1143.2887     -148.37303      0             -147.78191     -727.10355    \n",
            "       769   1062.4631     -148.33801      0             -147.78867      1180.8074    \n",
            "       770   943.11193     -148.25805      0             -147.77042      3226.2206    \n",
            "       771   797.33441     -148.17806      0             -147.76581      5135.8477    \n",
            "       772   641.51359     -148.10461      0             -147.77292      6834.7818    \n",
            "       773   493.54382     -148.02713      0             -147.77194      8425.194     \n",
            "       774   370.04393     -147.96919      0             -147.77786      9743.6961    \n",
            "       775   283.22431     -147.93135      0             -147.78492      10771.267    \n",
            "       776   239.26772     -147.90027      0             -147.77656      11450.035    \n",
            "       777   238.04413     -147.90114      0             -147.77806      11781.19     \n",
            "       778   273.6784      -147.94322      0             -147.80172      11894.733    \n",
            "       779   336.19792     -147.9657       0             -147.79187      11715.915    \n",
            "       780   414.15921     -148.02408      0             -147.80994      11259.483    \n",
            "       781   497.30345     -148.04433      0             -147.78721      10872.06     \n",
            "       782   577.52664     -148.09156      0             -147.79296      10203.588    \n",
            "       783   650.38892     -148.11878      0             -147.7825       9380.8427    \n",
            "       784   715.20956     -148.16326      0             -147.79346      8427.5712    \n",
            "       785   774.14314     -148.1976       0             -147.79733      7342.655     \n",
            "       786   829.81105     -148.2285       0             -147.79945      6022.3518    \n",
            "       787   883.31508     -148.27003      0             -147.81332      4495.9816    \n",
            "       788   933.20068     -148.31584      0             -147.83334      2998.1861    \n",
            "       789   974.70328     -148.31751      0             -147.81354      1364.2853    \n",
            "       790   1000.7145     -148.3297       0             -147.81229     -258.90365    \n",
            "       791   1002.7567     -148.33881      0             -147.82034     -1873.8126    \n",
            "       792   974.0314      -148.33644      0             -147.83282     -3273.7716    \n",
            "       793   911.5933      -148.28834      0             -147.81701     -4487.1048    \n",
            "       794   817.49283     -148.27447      0             -147.8518      -5505.1799    \n",
            "       795   699.68184     -148.21432      0             -147.85256     -6203.5952    \n",
            "       796   570.57764     -148.13826      0             -147.84324     -6584.2652    \n",
            "       797   444.52635     -148.09966      0             -147.86982     -6751.9809    \n",
            "       798   335.42382     -148.04168      0             -147.86825     -6912.033     \n",
            "       799   254.19425     -148.01365      0             -147.88222     -6746.7277    \n",
            "       800   206.50169     -147.98845      0             -147.88168     -6653.9051    \n",
            "       801   192.27009     -147.96461      0             -147.8652      -6219.6778    \n",
            "       802   206.1459      -147.99456      0             -147.88798     -5715.4442    \n",
            "       803   239.31572     -148.02566      0             -147.90192     -4998.9194    \n",
            "       804   281.56538     -148.03783      0             -147.89225     -4245.1097    \n",
            "       805   323.8981      -148.02757      0             -147.8601      -3093.2027    \n",
            "       806   360.79756     -148.07936      0             -147.89281     -1926.5884    \n",
            "       807   389.85261     -148.08804      0             -147.88647     -448.0534     \n",
            "       808   412.52375     -148.11977      0             -147.90648      1079.4604    \n",
            "       809   432.8807      -148.10775      0             -147.88393      2783.3511    \n",
            "       810   456.08886     -148.13363      0             -147.89782      4396.2981    \n",
            "       811   486.18356     -148.1612       0             -147.90982      6159.3952    \n",
            "       812   524.08023     -148.14579      0             -147.87482      7773.4219    \n",
            "       813   567.10027     -148.17919      0             -147.88598      9218.8976    \n",
            "       814   609.4525      -148.22866      0             -147.91355      10524.277    \n",
            "       815   643.18484     -148.23327      0             -147.90071      11691.615    \n",
            "       816   660.86544     -148.27594      0             -147.93425      12636.795    \n",
            "       817   657.26362     -148.26887      0             -147.92904      13671.422    \n",
            "       818   630.39031     -148.26979      0             -147.94385      14481.088    \n",
            "       819   582.94508     -148.24902      0             -147.94761      14966.148    \n",
            "       820   521.83702     -148.20952      0             -147.93971      15495.767    \n",
            "       821   456.14952     -148.18338      0             -147.94754      15575.617    \n",
            "       822   395.60347     -148.18238      0             -147.97784      15580.863    \n",
            "       823   348.53792     -148.12064      0             -147.94043      15155.756    \n",
            "       824   319.50717     -148.08952      0             -147.92433      14530.025    \n",
            "       825   309.21131     -148.1025       0             -147.94263      13477.311    \n",
            "       826   314.43474     -148.13259      0             -147.97002      12209.163    \n",
            "       827   329.14053     -148.11054      0             -147.94036      10954.342    \n",
            "       828   345.86664     -148.14475      0             -147.96593      9453.3798    \n",
            "       829   357.86436     -148.13343      0             -147.9484       7782.4185    \n",
            "       830   360.73264     -148.15515      0             -147.96864      6309.9048    \n",
            "       831   353.32027     -148.15456      0             -147.97188      4641.7624    \n",
            "       832   338.13224     -148.15833      0             -147.9835       3243.3413    \n",
            "       833   319.86471     -148.13569      0             -147.97031      1764.7437    \n",
            "       834   304.90206     -148.15303      0             -147.99538      387.99738    \n",
            "       835   299.29291     -148.16177      0             -148.00702     -1044.897     \n",
            "       836   306.97345     -148.13818      0             -147.97946     -2310.3075    \n",
            "       837   328.99947     -148.16479      0             -147.99468     -3403.3406    \n",
            "       838   363.09663     -148.17536      0             -147.98763     -4541.7274    \n",
            "       839   404.53746     -148.21463      0             -148.00547     -5528.2765    \n",
            "       840   447.09741     -148.19682      0             -147.96566     -6171.274     \n",
            "       841   484.09685     -148.24846      0             -147.99816     -6766.8259    \n",
            "       842   510.70262     -148.25972      0             -147.99566     -7020.4797    \n",
            "       843   524.73729     -148.28422      0             -148.01291     -7137.5455    \n",
            "       844   527.14801     -148.27555      0             -148.003       -6797.3371    \n",
            "       845   521.15881     -148.29744      0             -148.02798     -6306.8458    \n",
            "       846   511.47593     -148.30855      0             -148.0441      -5758.0034    \n",
            "       847   503.02222     -148.32095      0             -148.06086     -4843.2047    \n",
            "       848   499.08142     -148.30396      0             -148.04591     -3898.1864    \n",
            "       849   500.35381     -148.29822      0             -148.03952     -2948.4443    \n",
            "       850   504.73584     -148.32735      0             -148.06638     -1894.5735    \n",
            "       851   507.88562     -148.30955      0             -148.04695     -691.02836    \n",
            "       852   504.58758     -148.31919      0             -148.0583       463.38406    \n",
            "       853   489.78661     -148.30668      0             -148.05344      1716.2498    \n",
            "       854   460.81524     -148.31119      0             -148.07293      2902.142     \n",
            "       855   418.43378     -148.27788      0             -148.06153      4421.3879    \n",
            "       856   366.10409     -148.27148      0             -148.08219      5494.3413    \n",
            "       857   310.14069     -148.24268      0             -148.08233      6701.372     \n",
            "       858   258.34849     -148.23588      0             -148.1023       7886.7398    \n",
            "       859   218.37236     -148.19076      0             -148.07785      8887.298     \n",
            "       860   196.2599      -148.19998      0             -148.0985       9655.8038    \n",
            "       861   195.05613     -148.18183      0             -148.08098      10225.904    \n",
            "       862   214.25233     -148.18466      0             -148.07389      10715.245    \n",
            "       863   250.02565     -148.2026       0             -148.07333      10845.605    \n",
            "       864   296.38881     -148.25876      0             -148.10552      10878.607    \n",
            "       865   346.46997     -148.28359      0             -148.10445      10720.443    \n",
            "       866   393.73115     -148.27618      0             -148.07261      10389.715    \n",
            "       867   433.59405     -148.32922      0             -148.10503      9978.4985    \n",
            "       868   463.73935     -148.35743      0             -148.11766      9396.7757    \n",
            "       869   484.0364      -148.35025      0             -148.09998      8545.2636    \n",
            "       870   496.47398     -148.38942      0             -148.13273      7775.7003    \n",
            "       871   503.86909     -148.3909       0             -148.13038      6750.0392    \n",
            "       872   508.60547     -148.38923      0             -148.12626      5667.3419    \n",
            "       873   511.59296     -148.39912      0             -148.1346       4420.5416    \n",
            "       874   512.02727     -148.38011      0             -148.11537      3296.3031    \n",
            "       875   507.28926     -148.40437      0             -148.14208      1679.8023    \n",
            "       876   494.0693      -148.41375      0             -148.1583       464.21063    \n",
            "       877   469.62151     -148.38769      0             -148.14488     -797.56479    \n",
            "       878   432.1491      -148.37897      0             -148.15553     -2052.7088    \n",
            "       879   381.8293      -148.35804      0             -148.16062     -3169.3856    \n",
            "       880   321.91942     -148.34869      0             -148.18225     -4115.3452    \n",
            "       881   257.95537     -148.28308      0             -148.14971     -4872.0927    \n",
            "       882   196.57339     -148.26039      0             -148.15875     -5585.302     \n",
            "       883   144.48119     -148.23266      0             -148.15795     -6286.8763    \n",
            "       884   107.24725     -148.23341      0             -148.17796     -6750.85      \n",
            "       885   88.238776     -148.20312      0             -148.15749     -7249.6399    \n",
            "       886   87.913928     -148.21028      0             -148.16483     -7587.914     \n",
            "       887   104.09187     -148.23447      0             -148.18065     -7891.2059    \n",
            "       888   132.68513     -148.24652      0             -148.17792     -7951.8252    \n",
            "       889   168.27462     -148.26589      0             -148.17889     -8091.6181    \n",
            "       890   205.29948     -148.28606      0             -148.17991     -7902.2774    \n",
            "       891   239.73271     -148.30457      0             -148.18062     -7734.501     \n",
            "       892   269.24617     -148.31556      0             -148.17635     -7370.9928    \n",
            "       893   293.2488      -148.32694      0             -148.17532     -6778.0462    \n",
            "       894   312.76795     -148.3713       0             -148.20959     -6335.0432    \n",
            "       895   329.46363     -148.3419       0             -148.17155     -5624.9869    \n",
            "       896   344.85639     -148.36588      0             -148.18757     -4703.6855    \n",
            "       897   359.61439     -148.37985      0             -148.19392     -4125.9198    \n",
            "       898   372.94153     -148.38566      0             -148.19284     -3410.3803    \n",
            "       899   382.73246     -148.39763      0             -148.19974     -2631.1007    \n",
            "       900   385.94165     -148.41509      0             -148.21555     -1955.3129    \n",
            "       901   379.46717     -148.44407      0             -148.24787     -1510.0378    \n",
            "       902   361.06475     -148.40602      0             -148.21934     -692.99258    \n",
            "       903   330.11605     -148.35766      0             -148.18698     -114.95594    \n",
            "       904   288.26053     -148.34653      0             -148.19749      432.90234    \n",
            "       905   239.09284     -148.33623      0             -148.21261      818.36444    \n",
            "       906   187.57357     -148.33062      0             -148.23363      1246.3196    \n",
            "       907   139.52732     -148.29548      0             -148.22334      1451.5708    \n",
            "       908   99.995368     -148.27415      0             -148.22245      1575.329     \n",
            "       909   72.720242     -148.25113      0             -148.21353      1564.3993    \n",
            "       910   59.509358     -148.23942      0             -148.20866      1566.019     \n",
            "       911   60.007335     -148.25347      0             -148.22245      1208.3832    \n",
            "       912   71.974465     -148.26137      0             -148.22416      909.83607    \n",
            "       913   91.727059     -148.27918      0             -148.23176      317.9437     \n",
            "       914   115.09175     -148.29296      0             -148.23345     -321.1969     \n",
            "       915   138.60481     -148.29781      0             -148.22615     -934.35109    \n",
            "       916   159.61147     -148.32554      0             -148.24301     -1608.9319    \n",
            "       917   176.99593     -148.35196      0             -148.26045     -2329.1168    \n",
            "       918   190.88355     -148.32242      0             -148.22373     -2857.8693    \n",
            "       919   202.70637     -148.36178      0             -148.25698     -3689.2588    \n",
            "       920   214.4138      -148.3497       0             -148.23884     -4376.1582    \n",
            "       921   227.17313     -148.35682      0             -148.23936     -5081.2147    \n",
            "       922   241.41414     -148.37362      0             -148.2488      -5696.1632    \n",
            "       923   256.42938     -148.35754      0             -148.22495     -6455.9708    \n",
            "       924   270.44391     -148.4247       0             -148.28487     -7066.4197    \n",
            "       925   281.04078     -148.37664      0             -148.23133     -7504.7154    \n",
            "       926   285.50862     -148.3884       0             -148.24078     -8011.7297    \n",
            "       927   281.77336     -148.40091      0             -148.25522     -8308.4641    \n",
            "       928   269.27101     -148.38897      0             -148.24975     -8510.7734    \n",
            "       929   249.0519      -148.39084      0             -148.26207     -8565.1011    \n",
            "       930   223.66074     -148.39166      0             -148.27602     -8502.3875    \n",
            "       931   196.68072     -148.33817      0             -148.23647     -8370.7158    \n",
            "       932   171.82026     -148.32101      0             -148.23217     -8124.9623    \n",
            "       933   152.27211     -148.34457      0             -148.26584     -7792.8862    \n",
            "       934   140.11949     -148.32913      0             -148.25669     -7428.5151    \n",
            "       935   135.63375     -148.30381      0             -148.23369     -6790.2796    \n",
            "       936   137.51948     -148.34788      0             -148.27678     -6346.9061    \n",
            "       937   143.50736     -148.32498      0             -148.25078     -5675.2043    \n",
            "       938   150.5932      -148.3261       0             -148.24823     -5009.0738    \n",
            "       939   155.97151     -148.34321      0             -148.26257     -4333.013     \n",
            "       940   158.08276     -148.36276      0             -148.28102     -3491.1408    \n",
            "       941   156.65453     -148.3516       0             -148.27061     -2637.9322    \n",
            "       942   152.69682     -148.36783      0             -148.28888     -1890.4773    \n",
            "       943   148.20753     -148.34842      0             -148.27179     -988.06135    \n",
            "       944   145.86695     -148.3274       0             -148.25198     -92.149001    \n",
            "       945   147.96146     -148.35057      0             -148.27407      669.93366    \n",
            "       946   155.80634     -148.38374      0             -148.30318      1406.8891    \n",
            "       947   169.37684     -148.35415      0             -148.26657      2041.0569    \n",
            "       948   187.32514     -148.3378       0             -148.24094      2614.4358    \n",
            "       949   207.15295     -148.36799      0             -148.26089      3128.3215    \n",
            "       950   225.87538     -148.39352      0             -148.27673      3474.7759    \n",
            "       951   240.68128     -148.41694      0             -148.2925       3724.7618    \n",
            "       952   249.78902     -148.41339      0             -148.28424      3887.1492    \n",
            "       953   252.6737      -148.40093      0             -148.27029      3946.3827    \n",
            "       954   249.80564     -148.43565      0             -148.30649      3957.8354    \n",
            "       955   243.03713     -148.41204      0             -148.28638      3823.704     \n",
            "       956   234.58186     -148.42551      0             -148.30422      3582.9537    \n",
            "       957   226.65084     -148.41908      0             -148.3019       3230.071     \n",
            "       958   220.68952     -148.42484      0             -148.31074      2883.667     \n",
            "       959   217.08867     -148.38674      0             -148.27449      2346.9112    \n",
            "       960   215.02168     -148.41539      0             -148.30422      1618.4848    \n",
            "       961   213.00455     -148.36482      0             -148.25469      1017.4591    \n",
            "       962   208.92        -148.40447      0             -148.29645      212.70146    \n",
            "       963   200.82221     -148.39866      0             -148.29483     -477.80274    \n",
            "       964   187.9076      -148.41083      0             -148.31367     -1305.3864    \n",
            "       965   170.345       -148.3942       0             -148.30613     -1983.1931    \n",
            "       966   149.56618     -148.37964      0             -148.30231     -2693.3809    \n",
            "       967   128.01703     -148.35448      0             -148.28829     -3253.7281    \n",
            "       968   108.91817     -148.34872      0             -148.2924      -3735.6183    \n",
            "       969   95.184809     -148.36361      0             -148.3144      -4372.464     \n",
            "       970   88.813033     -148.36431      0             -148.31839     -4815.7594    \n",
            "       971   90.662027     -148.33692      0             -148.29004     -5119.2597    \n",
            "       972   100.2493      -148.35583      0             -148.30399     -5410.7633    \n",
            "       973   115.81278     -148.34155      0             -148.28167     -5647.407     \n",
            "       974   134.77223     -148.37502      0             -148.30533     -5877.659     \n",
            "       975   154.40034     -148.40799      0             -148.32815     -5886.3624    \n",
            "       976   172.54818     -148.41687      0             -148.32766     -5798.0828    \n",
            "       977   187.47697     -148.4136       0             -148.31667     -5592.9532    \n",
            "       978   198.49072     -148.43488      0             -148.33225     -5198.0819    \n",
            "       979   205.94552     -148.41065      0             -148.30417     -4813.1514    \n",
            "       980   210.73566     -148.4359       0             -148.32694     -4141.1319    \n",
            "       981   214.09954     -148.42636      0             -148.31566     -3508.3904    \n",
            "       982   216.88052     -148.41527      0             -148.30314     -2784.2133    \n",
            "       983   219.11764     -148.40422      0             -148.29093     -1978.672     \n",
            "       984   220.14611     -148.41726      0             -148.30343     -1321.263     \n",
            "       985   218.51765     -148.43954      0             -148.32655     -493.35414    \n",
            "       986   212.53675     -148.42819      0             -148.3183       254.47539    \n",
            "       987   200.8433      -148.40598      0             -148.30214      1192.0143    \n",
            "       988   182.79035     -148.42972      0             -148.33521      1954.15      \n",
            "       989   158.92438     -148.39794      0             -148.31577      2855.2551    \n",
            "       990   130.72714     -148.36951      0             -148.30192      3578.8489    \n",
            "       991   100.75649     -148.3612       0             -148.3091       4208.7233    \n",
            "       992   72.251966     -148.33909      0             -148.30173      4924.7041    \n",
            "       993   48.53861      -148.34935      0             -148.32426      5509.603     \n",
            "       994   32.409359     -148.35661      0             -148.33986      5942.7271    \n",
            "       995   25.464468     -148.33566      0             -148.32249      6224.9696    \n",
            "       996   27.93112      -148.34304      0             -148.3286       6532.3683    \n",
            "       997   38.677798     -148.33094      0             -148.31095      6532.4767    \n",
            "       998   55.464038     -148.3578       0             -148.32912      6464.9894    \n",
            "       999   75.606941     -148.36183      0             -148.32274      6372.4201    \n",
            "      1000   96.460548     -148.38017      0             -148.33029      6073.0104    \n",
            "      1001   115.73443     -148.3879       0             -148.32806      5846.7513    \n",
            "      1002   132.1158      -148.38429      0             -148.31598      5452.6038    \n",
            "      1003   145.26785     -148.38946      0             -148.31435      5002.1103    \n",
            "      1004   155.33542     -148.41269      0             -148.33237      4576.9073    \n",
            "      1005   163.16057     -148.41719      0             -148.33283      3975.4347    \n",
            "      1006   169.64132     -148.40841      0             -148.3207       3486.4724    \n",
            "      1007   175.10308     -148.4446       0             -148.35407      2898.4358    \n",
            "      1008   179.19916     -148.41311      0             -148.32045      2257.389     \n",
            "      1009   181.06109     -148.41851      0             -148.32489      1694.7117    \n",
            "      1010   179.36793     -148.42747      0             -148.33473      1119.8175    \n",
            "      1011   172.81188     -148.43358      0             -148.34423      597.21553    \n",
            "      1012   160.77673     -148.40264      0             -148.31951      141.70773    \n",
            "      1013   143.17951     -148.39376      0             -148.31973     -291.28485    \n",
            "      1014   121.06423     -148.40707      0             -148.34448     -547.8624     \n",
            "      1015   96.626725     -148.40498      0             -148.35502     -717.44627    \n",
            "      1016   72.48771      -148.38587      0             -148.34839     -718.66501    \n",
            "      1017   51.54607      -148.34752      0             -148.32087     -624.25693    \n",
            "      1018   36.360333     -148.33971      0             -148.32091     -488.78585    \n",
            "      1019   28.468308     -148.34562      0             -148.3309      -437.16333    \n",
            "      1020   28.284399     -148.32619      0             -148.31157     -90.407145    \n",
            "      1021   34.986941     -148.35827      0             -148.34018      95.863327    \n",
            "      1022   46.651969     -148.36962      0             -148.3455       561.55406    \n",
            "      1023   60.94857      -148.34565      0             -148.31414      1016.3531    \n",
            "      1024   75.760838     -148.35759      0             -148.31842      1499.3803    \n",
            "      1025   89.304701     -148.35855      0             -148.31238      2173.0806    \n",
            "      1026   100.39404     -148.36135      0             -148.30945      2772.1762    \n",
            "      1027   109.01494     -148.39194      0             -148.33558      3373.5279    \n",
            "      1028   115.92602     -148.37464      0             -148.3147       4144.5096    \n",
            "      1029   122.22643     -148.38587      0             -148.32267      4813.9936    \n",
            "      1030   129.1023      -148.41205      0             -148.3453       5337.7482    \n",
            "      1031   137.09843     -148.37885      0             -148.30796      5885.5886    \n",
            "      1032   146.05098     -148.43293      0             -148.35741      6501.393     \n",
            "      1033   155.22348     -148.41534      0             -148.33508      6920.2094    \n",
            "      1034   163.18745     -148.41394      0             -148.32956      7347.8656    \n",
            "      1035   168.18245     -148.42052      0             -148.33356      7491.9196    \n",
            "      1036   168.87582     -148.41488      0             -148.32756      7649.0429    \n",
            "      1037   164.54359     -148.44104      0             -148.35596      7691.5773    \n",
            "      1038   155.36235     -148.42837      0             -148.34804      7692.1108    \n",
            "      1039   142.68595     -148.42924      0             -148.35547      7680.7366    \n",
            "      1040   128.27394     -148.38878      0             -148.32246      7466.6215    \n",
            "      1041   114.16715     -148.39518      0             -148.33615      7033.6056    \n",
            "      1042   102.57974     -148.36892      0             -148.31588      6676.93      \n",
            "      1043   94.957383     -148.3774       0             -148.32831      6083.5092    \n",
            "      1044   91.739616     -148.38469      0             -148.33726      5474.0023    \n",
            "      1045   92.430339     -148.39702      0             -148.34923      4717.8827    \n",
            "      1046   95.637734     -148.38988      0             -148.34044      3909.2653    \n",
            "      1047   99.616851     -148.37706      0             -148.32556      3095.5264    \n",
            "      1048   102.73849     -148.39219      0             -148.33907      2270.4229    \n",
            "      1049   103.80716     -148.4025       0             -148.34883      1467.7987    \n",
            "      1050   102.47722     -148.37571      0             -148.32273      651.95442    \n",
            "      1051   99.429288     -148.3993       0             -148.34789     -164.55884    \n",
            "      1052   95.790792     -148.39227      0             -148.34274     -1051.1612    \n",
            "      1053   93.200875     -148.40337      0             -148.35518     -1722.7863    \n",
            "      1054   93.385368     -148.39329      0             -148.34501     -2264.9624    \n",
            "      1055   97.435068     -148.3967       0             -148.34632     -2923.9881    \n",
            "      1056   105.70593     -148.41594      0             -148.36128     -3330.0451    \n",
            "      1057   117.60649     -148.41538      0             -148.35457     -3802.4707    \n",
            "      1058   131.68728     -148.40701      0             -148.33892     -4269.9086    \n",
            "      1059   146.02187     -148.41285      0             -148.33735     -4309.7324    \n",
            "      1060   158.48416     -148.41896      0             -148.33702     -4451.4729    \n",
            "      1061   167.48854     -148.42233      0             -148.33573     -4490.1604    \n",
            "      1062   172.16056     -148.44724      0             -148.35822     -4342.018     \n",
            "      1063   172.42294     -148.4345       0             -148.34535     -4194.6518    \n",
            "      1064   168.94619     -148.43484      0             -148.34749     -3911.2257    \n",
            "      1065   163.05516     -148.4078       0             -148.32349     -3443.818     \n",
            "      1066   156.16136     -148.4236       0             -148.34286     -2927.7517    \n",
            "      1067   149.41533     -148.41767      0             -148.34042     -2447.3994    \n",
            "      1068   143.47489     -148.40428      0             -148.33009     -1756.2263    \n",
            "      1069   138.20477     -148.42391      0             -148.35245     -1159.0874    \n",
            "      1070   132.99486     -148.4154       0             -148.34663     -693.04224    \n",
            "      1071   126.66562     -148.40106      0             -148.33557     -50.467585    \n",
            "      1072   118.10562     -148.40663      0             -148.34556      623.52042    \n",
            "      1073   106.78713     -148.40665      0             -148.35144      1247.2087    \n",
            "      1074   92.954052     -148.36486      0             -148.3168       1702.6655    \n",
            "      1075   77.702648     -148.38114      0             -148.34096      2389.3028    \n",
            "      1076   62.74161      -148.37417      0             -148.34173      2808.0308    \n",
            "      1077   50.193801     -148.35514      0             -148.32919      3327.7211    \n",
            "      1078   42.352599     -148.3705       0             -148.3486       3660.4109    \n",
            "      1079   40.96757      -148.34541      0             -148.32423      3924.7429    \n",
            "      1080   46.745213     -148.36001      0             -148.33584      4077.9426    \n",
            "      1081   59.39208      -148.37156      0             -148.34085      4109.2593    \n",
            "      1082   77.479946     -148.39717      0             -148.3571       4082.9472    \n",
            "      1083   98.562199     -148.38365      0             -148.33269      3909.8087    \n",
            "      1084   120.01196     -148.42115      0             -148.3591       3614.511     \n",
            "      1085   139.46791     -148.41418      0             -148.34207      3320.4982    \n",
            "      1086   154.99115     -148.41913      0             -148.33899      2953.7119    \n",
            "      1087   165.4162      -148.4338       0             -148.34828      2396.143     \n",
            "      1088   170.70232     -148.41372      0             -148.32546      2006.8361    \n",
            "      1089   171.59616     -148.43008      0             -148.34136      1420.1681    \n",
            "      1090   169.28617     -148.43359      0             -148.34606      657.36287    \n",
            "      1091   165.05099     -148.43625      0             -148.35091      41.692164    \n",
            "      1092   159.81362     -148.44398      0             -148.36135     -556.57898    \n",
            "      1093   153.78384     -148.42853      0             -148.34902     -1181.095     \n",
            "      1094   146.54229     -148.426        0             -148.35023     -1792.5491    \n",
            "      1095   137.41648     -148.3936       0             -148.32255     -2463.832     \n",
            "      1096   125.49785     -148.41855      0             -148.35366     -3118.3764    \n",
            "      1097   110.28143     -148.38271      0             -148.32569     -3586.9286    \n",
            "      1098   91.968418     -148.39247      0             -148.34492     -4087.0373    \n",
            "      1099   71.609755     -148.38154      0             -148.34451     -4482.6578    \n",
            "      1100   51.142562     -148.3614       0             -148.33496     -4664.8599    \n",
            "      1101   32.72885      -148.34549      0             -148.32857     -4811.3436    \n",
            "      1102   18.946905     -148.38031      0             -148.37051     -5048.772     \n",
            "      1103   11.973035     -148.36561      0             -148.35942     -5192.1381    \n",
            "      1104   12.982291     -148.35946      0             -148.35275     -5196.2981    \n",
            "      1105   21.993393     -148.3498       0             -148.33843     -5087.9621    \n",
            "      1106   37.833421     -148.35134      0             -148.33178     -4932.2512    \n",
            "      1107   58.386233     -148.36894      0             -148.33875     -4816.2216    \n",
            "      1108   80.964884     -148.38533      0             -148.34346     -4515.2073    \n",
            "      1109   102.93813     -148.37769      0             -148.32446     -4067.8689    \n",
            "      1110   122.26552     -148.39155      0             -148.32833     -3925.4674    \n",
            "      1111   137.86074     -148.38643      0             -148.31515     -3377.2111    \n",
            "      1112   149.29976     -148.42141      0             -148.34421     -2817.4656    \n",
            "      1113   156.91738     -148.36777      0             -148.28664     -2299.144     \n",
            "      1114   161.78574     -148.43867      0             -148.35502     -1902.9611    \n",
            "      1115   165.07672     -148.43567      0             -148.35031     -1355.7748    \n",
            "      1116   167.56001     -148.42388      0             -148.33725     -812.7907     \n",
            "      1117   169.28261     -148.42823      0             -148.3407      -435.65042    \n",
            "      1118   169.8788      -148.40969      0             -148.32186      85.248423    \n",
            "      1119   168.35891     -148.44067      0             -148.35363      335.04905    \n",
            "      1120   163.43161     -148.40301      0             -148.31851      662.91188    \n",
            "      1121   154.3188      -148.409        0             -148.32921      912.15599    \n",
            "      1122   140.80327     -148.39673      0             -148.32393      1127.3916    \n",
            "      1123   123.49896     -148.39346      0             -148.3296       1145.1618    \n",
            "      1124   104.02627     -148.38571      0             -148.33193      1143.8873    \n",
            "      1125   84.590468     -148.39827      0             -148.35453      1093.1858    \n",
            "      1126   67.809502     -148.35012      0             -148.31506      1113.883     \n",
            "      1127   56.086543     -148.36228      0             -148.33328      809.07498    \n",
            "      1128   51.004083     -148.36351      0             -148.33714      436.81226    \n",
            "      1129   53.124538     -148.35492      0             -148.32746      20.423471    \n",
            "      1130   61.86488      -148.35061      0             -148.31862     -541.35647    \n",
            "      1131   75.61219      -148.34976      0             -148.31067     -1083.1497    \n",
            "      1132   91.933246     -148.38863      0             -148.3411      -1751.0929    \n",
            "      1133   108.46026     -148.37426      0             -148.31819     -2315.6097    \n",
            "      1134   123.35234     -148.3759       0             -148.31212     -2921.2818    \n",
            "      1135   135.12622     -148.37096      0             -148.3011      -3621.5335    \n",
            "      1136   143.44063     -148.42022      0             -148.34605     -4389.4244    \n",
            "      1137   148.97459     -148.39723      0             -148.32021     -4884.9426    \n",
            "      1138   153.04276     -148.4195       0             -148.34037     -5413.3308    \n",
            "      1139   157.23319     -148.39316      0             -148.31187     -5891.9769    \n",
            "      1140   162.81281     -148.42089      0             -148.33671     -6345.3771    \n",
            "      1141   170.31556     -148.40596      0             -148.3179      -6805.5341    \n",
            "      1142   179.40919     -148.41813      0             -148.32537     -7111.6827    \n",
            "      1143   188.99936     -148.42707      0             -148.32935     -7453.8808    \n",
            "      1144   197.32038     -148.3957       0             -148.29367     -7644.9719    \n",
            "      1145   202.50767     -148.41913      0             -148.31442     -7672.2991    \n",
            "      1146   203.33536     -148.41509      0             -148.30995     -7640.4252    \n",
            "      1147   199.17282     -148.41347      0             -148.31049     -7474.1094    \n",
            "      1148   190.35114     -148.39741      0             -148.29899     -7144.5515    \n",
            "      1149   178.27308     -148.4129       0             -148.32072     -6800.4747    \n",
            "      1150   164.98589     -148.38332      0             -148.29801     -6243.9875    \n",
            "      1151   152.69589     -148.37964      0             -148.30069     -5619.6949    \n",
            "      1152   143.36785     -148.41448      0             -148.34035     -4980.21      \n",
            "      1153   137.99808     -148.38714      0             -148.31579     -4196.9726    \n",
            "      1154   136.58052     -148.37596      0             -148.30534     -3366.2773    \n",
            "      1155   138.25195     -148.39675      0             -148.32527     -2707.3037    \n",
            "      1156   141.26945     -148.3805       0             -148.30746     -1874.5701    \n",
            "      1157   143.66976     -148.39056      0             -148.31628     -928.00768    \n",
            "      1158   143.94479     -148.38881      0             -148.31438     -117.86549    \n",
            "      1159   141.17346     -148.38784      0             -148.31485      886.3182     \n",
            "      1160   135.91121     -148.3965       0             -148.32623      1803.5995    \n",
            "      1161   129.64476     -148.36065      0             -148.29361      2778.4878    \n",
            "      1162   124.25012     -148.37448      0             -148.31024      3619.534     \n",
            "      1163   122.26702     -148.37068      0             -148.30747      4273.7636    \n",
            "      1164   125.67475     -148.36248      0             -148.2975       4979.3597    \n",
            "      1165   135.49776     -148.36122      0             -148.29117      5532.9247    \n",
            "      1166   151.60643     -148.37193      0             -148.29354      5913.0307    \n",
            "      1167   172.3941      -148.37499      0             -148.28585      6267.4047    \n",
            "      1168   195.31437     -148.40528      0             -148.30429      6553.9332    \n",
            "      1169   217.46021     -148.39529      0             -148.28285      6515.6164    \n",
            "      1170   235.89314     -148.42635      0             -148.30438      6553.6941    \n",
            "      1171   248.33493     -148.4329       0             -148.3045       6408.3038    \n",
            "      1172   253.82224     -148.43463      0             -148.3034       6149.3551    \n",
            "      1173   252.56798     -148.39598      0             -148.2654       5892.7589    \n",
            "      1174   246.02934     -148.42171      0             -148.2945       5400.5107    \n",
            "      1175   236.24652     -148.39434      0             -148.27219      4943.0763    \n",
            "      1176   225.23056     -148.38351      0             -148.26706      4391.5757    \n",
            "      1177   214.49255     -148.38495      0             -148.27405      3633.7165    \n",
            "      1178   204.51061     -148.36684      0             -148.2611       2962.6091    \n",
            "      1179   194.90023     -148.40183      0             -148.30105      2122.1455    \n",
            "      1180   184.57445     -148.40078      0             -148.30535      1434.2071    \n",
            "      1181   171.91829     -148.38317      0             -148.29428      632.06404    \n",
            "      1182   155.92959     -148.34984      0             -148.26922      24.408482    \n",
            "      1183   136.46574     -148.32774      0             -148.25718     -730.88562    \n",
            "      1184   114.62037     -148.33049      0             -148.27123     -1338.6179    \n",
            "      1185   92.672937     -148.32242      0             -148.2745      -1771.6015    \n",
            "      1186   73.499153     -148.28318      0             -148.24517     -2123.9204    \n",
            "      1187   60.686667     -148.31452      0             -148.28314     -2560.6094    \n",
            "      1188   57.203888     -148.29149      0             -148.26192     -2808.4409    \n",
            "      1189   64.687755     -148.30807      0             -148.27462     -3078.9919    \n",
            "      1190   83.301717     -148.31997      0             -148.2769      -3048.4505    \n",
            "      1191   111.52266     -148.32794      0             -148.27028     -2849.2506    \n",
            "      1192   146.31552     -148.36912      0             -148.29347     -2880.9146    \n",
            "      1193   183.50663     -148.37449      0             -148.27961     -2667.8996    \n",
            "      1194   218.98269     -148.40406      0             -148.29084     -2309.8611    \n",
            "      1195   249.31533     -148.35537      0             -148.22646     -1698.6272    \n",
            "      1196   272.00508     -148.41509      0             -148.27445     -1154.7706    \n",
            "      1197   286.26498     -148.41991      0             -148.2719      -490.27524    \n",
            "      1198   292.63526     -148.41462      0             -148.26331      363.93489    \n",
            "      1199   292.53839     -148.4188       0             -148.26755      1244.3079    \n",
            "      1200   287.85274     -148.40816      0             -148.25932      2037.9209    \n",
            "      1201   280.07325     -148.38727      0             -148.24246      2931.1694    \n",
            "      1202   269.93122     -148.40272      0             -148.26316      3728.8637    \n",
            "      1203   257.44483     -148.36562      0             -148.23251      4631.1959    \n",
            "      1204   241.87914     -148.38752      0             -148.26246      5351.4907    \n",
            "      1205   222.15534     -148.3741       0             -148.25924      5965.8882    \n",
            "      1206   197.6119      -148.3519       0             -148.24973      6614.1302    \n",
            "      1207   168.37627     -148.334        0             -148.24694      7149.3371    \n",
            "      1208   136.05818     -148.30534      0             -148.23499      7591.3676    \n",
            "      1209   103.39884     -148.30231      0             -148.24885      7872.5655    \n",
            "      1210   74.230963     -148.29786      0             -148.25948      8203.8992    \n",
            "      1211   52.801796     -148.25888      0             -148.23158      8349.201     \n",
            "      1212   42.82734      -148.26758      0             -148.24544      8292.2143    \n",
            "      1213   46.895213     -148.2806       0             -148.25635      7953.5143    \n",
            "      1214   65.572123     -148.27912      0             -148.24522      7751.9556    \n",
            "      1215   97.56049      -148.28142      0             -148.23098      7175.9395    \n",
            "      1216   139.48244     -148.2811       0             -148.20899      6466.3184    \n",
            "      1217   186.42274     -148.3233       0             -148.22691      5676.7146    \n",
            "      1218   233.25172     -148.37593      0             -148.25533      4762.9125    \n",
            "      1219   275.03675     -148.36919      0             -148.22698      3961.9565    \n",
            "      1220   308.24291     -148.38127      0             -148.22189      3060.58      \n",
            "      1221   330.91076     -148.40671      0             -148.23561      2150.0749    \n",
            "      1222   343.15567     -148.4006       0             -148.22317      1241.1569    \n",
            "      1223   346.63518     -148.41105      0             -148.23182      436.22512    \n",
            "      1224   344.06089     -148.41742      0             -148.23953     -447.4822     \n",
            "      1225   338.24059     -148.39205      0             -148.21716     -1273.7737    \n",
            "      1226   331.07403     -148.37651      0             -148.20533     -1910.5381    \n",
            "      1227   323.42165     -148.38602      0             -148.2188      -2715.2866    \n",
            "      1228   314.85187     -148.37402      0             -148.21123     -3285.5189    \n",
            "      1229   304.09379     -148.3638       0             -148.20657     -3756.339     \n",
            "      1230   289.61289     -148.34227      0             -148.19253     -4026.5176    \n",
            "      1231   270.16598     -148.34968      0             -148.20999     -4101.1663    \n",
            "      1232   245.65156     -148.31087      0             -148.18386     -4047.2651    \n",
            "      1233   217.53178     -148.32571      0             -148.21323     -3969.6199    \n",
            "      1234   188.8646      -148.28809      0             -148.19044     -3554.8301    \n",
            "      1235   163.64318     -148.27242      0             -148.18781     -3041.9392    \n",
            "      1236   146.03864     -148.27799      0             -148.20248     -2446.7349    \n",
            "      1237   139.43073     -148.25769      0             -148.1856      -1723.6707    \n",
            "      1238   145.62834     -148.26519      0             -148.18989     -922.94599    \n",
            "      1239   164.31154     -148.25154      0             -148.16659     -39.168517    \n",
            "      1240   193.00199     -148.24902      0             -148.14923      854.95908    \n",
            "      1241   227.69613     -148.30481      0             -148.18708      1843.7445    \n",
            "      1242   263.36076     -148.30616      0             -148.16999      2864.0188    \n",
            "      1243   295.27626     -148.34098      0             -148.18831      3960.1602    \n",
            "      1244   320.09445     -148.34157      0             -148.17607      5014.5566    \n",
            "      1245   336.05828     -148.34444      0             -148.17068      6132.6385    \n",
            "      1246   343.93949     -148.36867      0             -148.19084      7136.5192    \n",
            "      1247   346.21245     -148.36486      0             -148.18585      8210.5872    \n",
            "      1248   346.38213     -148.39214      0             -148.21305      9121.6227    \n",
            "      1249   347.95643     -148.35492      0             -148.17501      9898.5595    \n",
            "      1250   353.35796     -148.36768      0             -148.18498      10530.147    \n",
            "      1251   363.25027     -148.37484      0             -148.18703      10995.206    \n",
            "      1252   376.28665     -148.3599       0             -148.16535      11215.592    \n",
            "      1253   389.34084     -148.33899      0             -148.13769      11317.218    \n",
            "      1254   398.77971     -148.34096      0             -148.13477      11325.45     \n",
            "      1255   401.19326     -148.36358      0             -148.15615      10844.494    \n",
            "      1256   394.27719     -148.37724      0             -148.17339      10454.164    \n",
            "      1257   377.82375     -148.34127      0             -148.14592      9964.2128    \n",
            "      1258   353.91925     -148.3492       0             -148.16621      9183.2509    \n",
            "      1259   326.31212     -148.31296      0             -148.14425      8398.5066    \n",
            "      1260   299.64248     -148.29316      0             -148.13823      7465.8749    \n",
            "      1261   278.40768     -148.27242      0             -148.12847      6306.7522    \n",
            "      1262   265.48033     -148.30943      0             -148.17217      4997.0809    \n",
            "      1263   261.73477     -148.26555      0             -148.13023      3809.0903    \n",
            "      1264   265.77456     -148.2812       0             -148.14379      2314.2667    \n",
            "      1265   274.31492     -148.28159      0             -148.13976      832.4075     \n",
            "      1266   283.17147     -148.28034      0             -148.13393     -557.11435    \n",
            "      1267   288.31824     -148.27852      0             -148.12945     -2000.4229    \n",
            "      1268   287.12953     -148.26691      0             -148.11845     -3435.1242    \n",
            "      1269   279.48959     -148.265        0             -148.1205      -4765.9832    \n",
            "      1270   267.36304     -148.27319      0             -148.13495     -5793.244     \n",
            "      1271   254.6478      -148.24426      0             -148.11259     -6848.6984    \n",
            "      1272   246.44245     -148.24333      0             -148.11591     -7737.1097    \n",
            "      1273   247.3495      -148.24702      0             -148.11913     -8640.6164    \n",
            "      1274   260.57869     -148.22345      0             -148.08872     -9313.1577    \n",
            "      1275   286.99866     -148.25334      0             -148.10495     -9980.9855    \n",
            "      1276   324.64967     -148.2542       0             -148.08635     -10537.525    \n",
            "      1277   369.15568     -148.30036      0             -148.10949     -10828.84     \n",
            "      1278   414.45075     -148.32535      0             -148.11106     -11112.165    \n",
            "      1279   454.1626      -148.33416      0             -148.09934     -11154.776    \n",
            "      1280   483.10115     -148.32696      0             -148.07718     -10878.971    \n",
            "      1281   497.90306     -148.34721      0             -148.08978     -10553.126    \n",
            "      1282   498.10522     -148.37474      0             -148.1172      -10117.28     \n",
            "      1283   485.6154      -148.33821      0             -148.08712     -9327.5076    \n",
            "      1284   464.12038     -148.29982      0             -148.05985     -8663.6421    \n",
            "      1285   438.06644     -148.31295      0             -148.08645     -7789.7745    \n",
            "      1286   411.06119     -148.31776      0             -148.10523     -6881.5967    \n",
            "      1287   385.49963     -148.27482      0             -148.0755      -5941.8653    \n",
            "      1288   361.92205     -148.26137      0             -148.07424     -5021.3945    \n",
            "      1289   339.10044     -148.22027      0             -148.04494     -4065.7699    \n",
            "      1290   314.79618     -148.21751      0             -148.05475     -3177.448     \n",
            "      1291   286.6961      -148.19716      0             -148.04892     -2295.6858    \n",
            "      1292   253.86924     -148.18418      0             -148.05292     -1446.01      \n",
            "      1293   217.06237     -148.16421      0             -148.05198     -669.64396    \n",
            "      1294   179.09659     -148.16632      0             -148.07371      19.721893    \n",
            "      1295   144.86505     -148.14172      0             -148.06682      693.43117    \n",
            "      1296   120.5267      -148.1097       0             -148.04738      1120.6007    \n",
            "      1297   112.0344      -148.10408      0             -148.04615      1600.6196    \n",
            "      1298   124.01131     -148.13888      0             -148.07476      1643.851     \n",
            "      1299   158.52146     -148.16654      0             -148.08458      1727.8105    \n",
            "      1300   214.42817     -148.16684      0             -148.05597      1566.5809    \n",
            "      1301   287.12881     -148.18818      0             -148.03972      1229.9843    \n",
            "      1302   369.33113     -148.2453       0             -148.05434      799.10142    \n",
            "      1303   451.913       -148.25296      0             -148.0193       156.86253    \n",
            "      1304   525.62231     -148.31877      0             -148.047       -576.04898    \n",
            "      1305   583.0396      -148.33565      0             -148.0342      -1351.7088    \n",
            "      1306   619.42106     -148.35219      0             -148.03192     -2199.5636    \n",
            "      1307   633.35117     -148.35627      0             -148.0288      -3042.3523    \n",
            "      1308   627.07406     -148.34586      0             -148.02164     -3817.9363    \n",
            "      1309   605.17154     -148.34464      0             -148.03174     -4822.1445    \n",
            "      1310   573.48829     -148.30967      0             -148.01315     -5737.2696    \n",
            "      1311   537.64287     -148.29607      0             -148.01809     -6620.0956    \n",
            "      1312   501.20117     -148.24684      0             -147.9877      -7468.1619    \n",
            "      1313   465.36731     -148.23853      0             -147.99791     -8535.193     \n",
            "      1314   428.90351     -148.22855      0             -148.00679     -9325.6237    \n",
            "      1315   389.23807     -148.21195      0             -148.0107      -10137.806    \n",
            "      1316   343.80994     -148.15029      0             -147.97253     -10650.397    \n",
            "      1317   291.65932     -148.16719      0             -148.01639     -11223.627    \n",
            "      1318   234.44032     -148.11436      0             -147.99315     -11413.601    \n",
            "      1319   176.45024     -148.08418      0             -147.99295     -11599.619    \n",
            "      1320   124.46823     -148.0753       0             -148.01095     -11601.932    \n",
            "      1321   86.631977     -148.02766      0             -147.98286     -11392.895    \n",
            "      1322   70.693583     -148.04842      0             -148.01187     -11094.841    \n",
            "      1323   82.098507     -148.02609      0             -147.98364     -10784.984    \n",
            "      1324   122.41444     -148.05846      0             -147.99517     -10371.741    \n",
            "      1325   188.99251     -148.06907      0             -147.97135     -9867.7163    \n",
            "      1326   275.04353     -148.14008      0             -147.99788     -9359.8268    \n",
            "      1327   370.81986     -148.182        0             -147.99027     -8703.8975    \n",
            "      1328   465.41615     -148.21376      0             -147.97312     -7727.6882    \n",
            "      1329   549.41255     -148.27631      0             -147.99224     -6841.5887    \n",
            "      1330   615.69934     -148.27442      0             -147.95608     -5691.405     \n",
            "      1331   661.25865     -148.33198      0             -147.99009     -4495.5892    \n",
            "      1332   687.11932     -148.30352      0             -147.94825     -3080.0856    \n",
            "      1333   696.74202     -148.3216       0             -147.96135     -1719.0616    \n",
            "      1334   695.41693     -148.32874      0             -147.96918     -347.20061    \n",
            "      1335   687.761       -148.31018      0             -147.95458      992.40502    \n",
            "      1336   676.56465     -148.28236      0             -147.93255      2225.6225    \n",
            "      1337   662.04272     -148.2752       0             -147.93289      3206.4956    \n",
            "      1338   641.95663     -148.26776      0             -147.93584      4181.8678    \n",
            "      1339   612.83176     -148.2749       0             -147.95804      4900.0156    \n",
            "      1340   571.62002     -148.22532      0             -147.92977      5602.7915    \n",
            "      1341   517.02639     -148.1771       0             -147.90978      6088.6333    \n",
            "      1342   450.63749     -148.16923      0             -147.93623      6434.9822    \n",
            "      1343   377.19637     -148.11185      0             -147.91682      6671.9712    \n",
            "      1344   305.31999     -148.07393      0             -147.91607      6713.021     \n",
            "      1345   244.60001     -148.05664      0             -147.93017      6475.9794    \n",
            "      1346   204.14854     -148.02201      0             -147.91645      6250.4846    \n",
            "      1347   191.2814      -147.99127      0             -147.89237      5719.5514    \n",
            "      1348   209.1948      -148.0427       0             -147.93454      4840.8476    \n",
            "      1349   256.26643     -148.04231      0             -147.90981      3777.5848    \n",
            "      1350   326.40507     -148.09971      0             -147.93094      2463.5653    \n",
            "      1351   409.8962      -148.12754      0             -147.91561      1044.6171    \n",
            "      1352   495.34193     -148.1579       0             -147.90179     -429.45004    \n",
            "      1353   571.98004     -148.22572      0             -147.92999     -2036.9284    \n",
            "      1354   631.55722     -148.2316       0             -147.90506     -3657.3796    \n",
            "      1355   669.66063     -148.2353       0             -147.88906     -5317.8423    \n",
            "      1356   686.36704     -148.2666       0             -147.91172     -6785.318     \n",
            "      1357   685.85673     -148.2624       0             -147.90778     -8278.8691    \n",
            "      1358   674.56999     -148.2536       0             -147.90482     -9704.2058    \n",
            "      1359   659.62262     -148.2659       0             -147.92485     -11119.272    \n",
            "      1360   646.72981     -148.23812      0             -147.90373     -12381.166    \n",
            "      1361   638.87711     -148.2402       0             -147.90988     -13646.582    \n",
            "      1362   635.79828     -148.19143      0             -147.8627      -14699.191    \n",
            "      1363   634.18734     -148.20971      0             -147.88181     -15674.297    \n",
            "      1364   629.11135     -148.20853      0             -147.88326     -16370.105    \n",
            "      1365   616.05008     -148.21117      0             -147.89265     -16826.79     \n",
            "      1366   592.67018     -148.198        0             -147.89157     -17022.736    \n",
            "      1367   558.87021     -148.15789      0             -147.86893     -16716.318    \n",
            "      1368   518.23534     -148.16068      0             -147.89273     -16269.877    \n",
            "      1369   476.87335     -148.08819      0             -147.84163     -15472.533    \n",
            "      1370   441.97417     -148.11195      0             -147.88343     -14495.544    \n",
            "      1371   420.57352     -148.10116      0             -147.8837      -13243.273    \n",
            "      1372   417.07501     -148.09089      0             -147.87525     -11846.571    \n",
            "      1373   432.48387     -148.09798      0             -147.87437     -10237.134    \n",
            "      1374   463.67439     -148.11624      0             -147.8765      -8670.9651    \n",
            "      1375   504.09409     -148.14685      0             -147.88621     -6875.6647    \n",
            "      1376   544.94851     -148.17885      0             -147.89708     -4910.6864    \n",
            "      1377   577.1928      -148.1548       0             -147.85637     -2844.5068    \n",
            "      1378   593.68153     -148.16133      0             -147.85437     -556.90763    \n",
            "      1379   590.67999     -148.16908      0             -147.86368      1582.6091    \n",
            "      1380   569.53178     -148.14381      0             -147.84934      4070.0708    \n",
            "      1381   535.47549     -148.16023      0             -147.88336      6427.5063    \n",
            "      1382   496.69246     -148.14667      0             -147.88986      8664.7903    \n",
            "      1383   463.25592     -148.08367      0             -147.84415      10859.848    \n",
            "      1384   444.22806     -148.10381      0             -147.87412      12682.099    \n",
            "      1385   445.80502     -148.09021      0             -147.85972      14277.018    \n",
            "      1386   469.80007     -148.10286      0             -147.85995      15466.179    \n",
            "      1387   512.96847     -148.1096       0             -147.84438      16444.769    \n",
            "      1388   567.93762     -148.16275      0             -147.8691       17046.67     \n",
            "      1389   625.60671     -148.201        0             -147.87754      17290.11     \n",
            "      1390   676.59595     -148.2181       0             -147.86827      17393.474    \n",
            "      1391   713.30771     -148.20314      0             -147.83433      17266.937    \n",
            "      1392   731.6746      -148.24659      0             -147.86829      16736.41     \n",
            "      1393   732.14322     -148.23617      0             -147.85762      16102.981    \n",
            "      1394   719.1451      -148.2065       0             -147.83468      15222.278    \n",
            "      1395   699.02242     -148.1969       0             -147.83548      13953.197    \n",
            "      1396   678.38224     -148.19472      0             -147.84397      12562.685    \n",
            "      1397   661.24345     -148.19665      0             -147.85476      10960.17     \n",
            "      1398   648.21615     -148.18561      0             -147.85045      9235.3915    \n",
            "      1399   636.31267     -148.19993      0             -147.87093      7371.1425    \n",
            "      1400   619.45776     -148.1586       0             -147.83832      5452.7831    \n",
            "      1401   591.61498     -148.14706      0             -147.84117      3585.2848    \n",
            "      1402   547.75597     -148.09602      0             -147.81281      1800.5557    \n",
            "      1403   486.7402      -148.11272      0             -147.86105      130.20552    \n",
            "      1404   413.09922     -148.07436      0             -147.86077     -1249.5517    \n",
            "      1405   335.24056     -148.04224      0             -147.86891     -2374.7919    \n",
            "      1406   264.69009     -147.99095      0             -147.8541      -3318.6411    \n",
            "      1407   214.26852     -147.96775      0             -147.85696     -4080.5933    \n",
            "      1408   194.75942     -147.94204      0             -147.84134     -4772.4064    \n",
            "      1409   212.56392     -147.96274      0             -147.85284     -5125.0681    \n",
            "      1410   268.43979     -147.99656      0             -147.85777     -5363.14      \n",
            "      1411   357.03331     -148.0163       0             -147.8317      -5479.1824    \n",
            "      1412   467.71495     -148.08722      0             -147.84539     -5185.7941    \n",
            "      1413   586.88872     -148.14714      0             -147.84369     -4747.9517    \n",
            "      1414   700.3949      -148.20808      0             -147.84595     -4017.5711    \n",
            "      1415   796.59735     -148.24742      0             -147.83555     -2878.7908    \n",
            "      1416   867.51257     -148.31394      0             -147.8654      -1485.7877    \n",
            "      1417   910.04144     -148.32685      0             -147.85632      331.83978    \n",
            "      1418   925.65785     -148.35166      0             -147.87305      2155.7377    \n",
            "      1419   918.78743     -148.32696      0             -147.85191      4241.1084    \n",
            "      1420   895.11326     -148.29837      0             -147.83556      6343.3896    \n",
            "      1421   859.25983     -148.29001      0             -147.84574      8423.9786    \n",
            "      1422   813.94351     -148.29188      0             -147.87104      10293.224    \n",
            "      1423   759.43668     -148.24921      0             -147.85655      12001.841    \n",
            "      1424   694.30258     -148.24211      0             -147.88313      13776.807    \n",
            "      1425   617.4942      -148.19461      0             -147.87535      15272.369    \n",
            "      1426   529.30788     -148.16169      0             -147.88802      16428.971    \n",
            "      1427   432.79122     -148.06937      0             -147.8456       17605.717    \n",
            "      1428   335.00566     -148.03691      0             -147.86369      18290.225    \n",
            "      1429   246.02719     -148.01444      0             -147.88723      18998.495    \n",
            "      1430   177.33147     -147.96335      0             -147.87166      19240.239    \n",
            "      1431   139.75329     -147.95555      0             -147.8833       19165.278    \n",
            "      1432   141.71403     -147.94185      0             -147.86858      18795.357    \n",
            "      1433   186.73003     -147.97863      0             -147.88208      17966.179    \n",
            "      1434   272.18352     -148.01654      0             -147.87581      16730.684    \n",
            "      1435   389.60311     -148.07743      0             -147.87599      15258.949    \n",
            "      1436   525.81031     -148.15238      0             -147.88051      13258.637    \n",
            "      1437   664.614       -148.21964      0             -147.876        11218.894    \n",
            "      1438   789.75858     -148.28715      0             -147.87881      8888.4254    \n",
            "      1439   887.41401     -148.33731      0             -147.87848      6430.6826    \n",
            "      1440   947.9207      -148.37682      0             -147.88671      3944.5965    \n",
            "      1441   968.49937     -148.38165      0             -147.8809       1563.0581    \n",
            "      1442   951.95436     -148.37375      0             -147.88155     -692.6211     \n",
            "      1443   905.33403     -148.37129      0             -147.9032      -2971.5571    \n",
            "      1444   838.57871     -148.34037      0             -147.90679     -5211.1771    \n",
            "      1445   761.76452     -148.29723      0             -147.90337     -7234.4624    \n",
            "      1446   683.42311     -148.27077      0             -147.91742     -9140.982     \n",
            "      1447   609.09783     -148.19661      0             -147.88168     -10795.22     \n",
            "      1448   541.40664     -148.17229      0             -147.89236     -12308.966    \n",
            "      1449   479.94651     -148.17029      0             -147.92214     -13457.207    \n",
            "      1450   423.37716     -148.10596      0             -147.88706     -14352.224    \n",
            "      1451   371.22475     -148.11217      0             -147.92023     -14868.69     \n",
            "      1452   323.82577     -148.1024       0             -147.93497     -15104.274    \n",
            "      1453   283.79663     -148.05259      0             -147.90586     -14777.234    \n",
            "      1454   255.88132     -148.05109      0             -147.91879     -14244.007    \n",
            "      1455   245.27225     -148.06357      0             -147.93675     -13329.42     \n",
            "      1456   256.83045     -148.04313      0             -147.91034     -12244.507    \n",
            "      1457   293.47184     -148.04916      0             -147.89742     -10922.069    \n",
            "      1458   354.57518     -148.12526      0             -147.94193     -9536.0319    \n",
            "      1459   435.41427     -148.15603      0             -147.9309      -7943.2742    \n",
            "      1460   527.19101     -148.18649      0             -147.91391     -6028.3537    \n",
            "      1461   618.81625     -148.25476      0             -147.9348      -4174.2717    \n",
            "      1462   698.37299     -148.30658      0             -147.94549     -2188.0866    \n",
            "      1463   755.24231     -148.32122      0             -147.93072     -46.053304    \n",
            "      1464   782.50173     -148.34683      0             -147.94224      2203.9718    \n",
            "      1465   777.9963      -148.35009      0             -147.94783      4341.0629    \n",
            "      1466   744.56978     -148.33918      0             -147.9542       6607.2617    \n",
            "      1467   689.70363     -148.28804      0             -147.93144      8833.5531    \n",
            "      1468   624.18244     -148.27855      0             -147.95582      10657.763    \n",
            "      1469   558.99199     -148.26489      0             -147.97586      12268.75     \n",
            "      1470   502.91151     -148.20984      0             -147.94982      13687.846    \n",
            "      1471   461.43733     -148.20633      0             -147.96775      14686.892    \n",
            "      1472   436.13838     -148.18673      0             -147.96123      15385.459    \n",
            "      1473   424.83972     -148.21284      0             -147.99318      15661.762    \n",
            "      1474   422.74594     -148.18124      0             -147.96266      15652.277    \n",
            "      1475   424.53994     -148.16726      0             -147.94776      15339.302    \n",
            "      1476   425.34056     -148.19799      0             -147.97807      14714.602    \n",
            "      1477   422.61499     -148.20459      0             -147.98608      14075.558    \n",
            "      1478   417.1146      -148.19809      0             -147.98242      13063.71     \n",
            "      1479   411.50824     -148.18399      0             -147.97122      11854.662    \n",
            "      1480   409.66795     -148.21051      0             -147.99869      10478.987    \n",
            "      1481   415.28301     -148.18839      0             -147.97367      8891.4801    \n",
            "      1482   430.20651     -148.22147      0             -147.99904      6981.7891    \n",
            "      1483   453.44497     -148.24737      0             -148.01292      5081.4892    \n",
            "      1484   480.87023     -148.25011      0             -148.00148      2970.2269    \n",
            "      1485   506.23522     -148.29045      0             -148.02871      872.08451    \n",
            "      1486   522.48498     -148.27506      0             -148.00492     -1167.1966    \n",
            "      1487   523.64199     -148.28344      0             -148.01269     -3117.8197    \n",
            "      1488   506.55692     -148.26944      0             -148.00753     -4975.4056    \n",
            "      1489   471.70602     -148.25274      0             -148.00885     -6667.415     \n",
            "      1490   423.97674     -148.23859      0             -148.01938     -8213.4822    \n",
            "      1491   371.40014     -148.23206      0             -148.04003     -9418.3544    \n",
            "      1492   323.1771      -148.19866      0             -148.03156     -10589.96     \n",
            "      1493   288.12861     -148.17406      0             -148.02509     -11599.879    \n",
            "      1494   272.75195     -148.17141      0             -148.03038     -12395.679    \n",
            "      1495   279.51069     -148.18295      0             -148.03844     -12994.736    \n",
            "      1496   306.64473     -148.18907      0             -148.03052     -13523.708    \n",
            "      1497   348.58045     -148.20315      0             -148.02292     -13653.961    \n",
            "      1498   397.62341     -148.24246      0             -148.03687     -13645.474    \n",
            "      1499   445.49674     -148.27172      0             -148.04138     -13642.98     \n",
            "      1500   485.12529     -148.28577      0             -148.03494     -13078.869    \n",
            "      1501   511.94494     -148.32509      0             -148.06039     -12437.016    \n",
            "      1502   524.18677     -148.31643      0             -148.04541     -11594.872    \n",
            "      1503   523.2958      -148.31099      0             -148.04043     -10421.922    \n",
            "      1504   512.38969     -148.33978      0             -148.07486     -9195.6238    \n",
            "      1505   495.03969     -148.32193      0             -148.06597     -7841.981     \n",
            "      1506   474.70125     -148.33654      0             -148.0911      -6514.4543    \n",
            "      1507   453.21099     -148.30772      0             -148.07339     -5172.5213    \n",
            "      1508   430.44804     -148.31721      0             -148.09465     -3859.7302    \n",
            "      1509   404.82443     -148.2725       0             -148.06319     -2562.8281    \n",
            "      1510   374.4562      -148.28702      0             -148.09341     -1377.0813    \n",
            "      1511   337.87343     -148.25397      0             -148.07928     -263.06402    \n",
            "      1512   294.98898     -148.22254      0             -148.07002      868.66377    \n",
            "      1513   247.97929     -148.21953      0             -148.09132      1663.987     \n",
            "      1514   201.10402     -148.21232      0             -148.10834      2475.7829    \n",
            "      1515   159.99729     -148.20643      0             -148.12371      3290.4951    \n",
            "      1516   131.15448     -148.18306      0             -148.11525      3598.4298    \n",
            "      1517   120.3645      -148.15864      0             -148.0964       3878.5109    \n",
            "      1518   131.34313     -148.15268      0             -148.08477      3945.3714    \n",
            "      1519   164.82719     -148.17492      0             -148.0897       3673.2549    \n",
            "      1520   218.34402     -148.21079      0             -148.0979       3123.0489    \n",
            "      1521   286.19601     -148.22314      0             -148.07517      2264.935     \n",
            "      1522   360.32829     -148.26455      0             -148.07824      1434.0791    \n",
            "      1523   431.63991     -148.30429      0             -148.08112      203.26191    \n",
            "      1524   491.2353      -148.36783      0             -148.11384     -1134.3682    \n",
            "      1525   532.47807     -148.39381      0             -148.1185      -2422.9853    \n",
            "      1526   551.56796     -148.4          0             -148.11481     -3916.1837    \n",
            "      1527   547.96699     -148.38646      0             -148.10314     -5205.2753    \n",
            "      1528   524.52605     -148.38863      0             -148.11743     -6631.9364    \n",
            "      1529   486.21387     -148.38086      0             -148.12946     -7930.1845    \n",
            "      1530   439.02622     -148.36367      0             -148.13667     -9232.7346    \n",
            "      1531   388.77594     -148.31673      0             -148.11572     -10424.091    \n",
            "      1532   339.83283     -148.31022      0             -148.13451     -11468.837    \n",
            "      1533   294.76488     -148.3034       0             -148.15099     -12587.409    \n",
            "      1534   254.30339     -148.2545       0             -148.12301     -13271.317    \n",
            "      1535   217.72014     -148.27245      0             -148.15988     -14051.775    \n",
            "      1536   184.1127      -148.22694      0             -148.13174     -14616.986    \n",
            "      1537   153.24284     -148.21992      0             -148.14068     -14879.289    \n",
            "      1538   126.05532     -148.22664      0             -148.16147     -14934.493    \n",
            "      1539   105.21202     -148.20663      0             -148.15223     -14646.288    \n",
            "      1540   93.857468     -148.19111      0             -148.14258     -14216.825    \n",
            "      1541   95.752443     -148.19138      0             -148.14188     -13633.714    \n",
            "      1542   113.85468     -148.21459      0             -148.15572     -12761.001    \n",
            "      1543   149.10696     -148.20207      0             -148.12497     -11741.611    \n",
            "      1544   200.0784      -148.2358       0             -148.13235     -10543.995    \n",
            "      1545   262.42014     -148.32128      0             -148.1856      -9280.25      \n",
            "      1546   329.36922     -148.31687      0             -148.14658     -7892.681     \n",
            "      1547   393.20081     -148.34848      0             -148.14518     -6346.1717    \n",
            "      1548   445.93063     -148.39839      0             -148.16783     -4628.6697    \n",
            "      1549   481.02629     -148.42152      0             -148.17281     -2913.7945    \n",
            "      1550   494.80443     -148.39606      0             -148.14023     -937.70045    \n",
            "      1551   486.61475     -148.40463      0             -148.15303      999.37371    \n",
            "      1552   459.48212     -148.42871      0             -148.19114      2780.3237    \n",
            "      1553   418.46324     -148.40878      0             -148.19242      4632.3171    \n",
            "      1554   369.7456      -148.39339      0             -148.20222      6450.0517    \n",
            "      1555   319.79391     -148.3759       0             -148.21055      8069.3405    \n",
            "      1556   273.74218     -148.34071      0             -148.19918      9388.6103    \n",
            "      1557   234.81924     -148.31828      0             -148.19687      10489.419    \n",
            "      1558   204.0781      -148.29189      0             -148.18638      11425.879    \n",
            "      1559   180.96894     -148.29039      0             -148.19682      12129.058    \n",
            "      1560   164.2275      -148.27932      0             -148.1944       12560.724    \n",
            "      1561   152.37746     -148.27916      0             -148.20038      12759.18     \n",
            "      1562   144.61981     -148.24907      0             -148.1743       12674.515    \n",
            "      1563   141.33554     -148.27781      0             -148.20473      12469.163    \n",
            "      1564   143.84332     -148.27721      0             -148.20283      11921.445    \n",
            "      1565   153.81745     -148.27263      0             -148.1931       11185.53     \n",
            "      1566   172.65279     -148.30278      0             -148.21351      10239.288    \n",
            "      1567   200.78534     -148.30333      0             -148.19952      9134.7951    \n",
            "      1568   236.61915     -148.324        0             -148.20166      7746.2385    \n",
            "      1569   276.7728      -148.33656      0             -148.19345      6306.9607    \n",
            "      1570   316.30887     -148.38375      0             -148.2202       4578.3729    \n",
            "      1571   349.46664     -148.38864      0             -148.20795      2949.9993    \n",
            "      1572   371.47241     -148.37875      0             -148.18668      1348.5893    \n",
            "      1573   378.85404     -148.38658      0             -148.1907      -179.38044    \n",
            "      1574   370.05706     -148.38628      0             -148.19495     -1817.5733    \n",
            "      1575   346.47691     -148.41607      0             -148.23693     -3430.6263    \n",
            "      1576   311.81478     -148.39456      0             -148.23334     -4722.2483    \n",
            "      1577   271.89027     -148.36739      0             -148.22681     -5864.8583    \n",
            "      1578   232.89667     -148.33738      0             -148.21697     -6844.746     \n",
            "      1579   200.26543     -148.31651      0             -148.21297     -7858.9244    \n",
            "      1580   177.7836      -148.29468      0             -148.20276     -8430.5315    \n",
            "      1581   167.07795     -148.32343      0             -148.23705     -8969.017     \n",
            "      1582   167.57625     -148.28523      0             -148.19859     -9117.5418    \n",
            "      1583   176.70759     -148.30014      0             -148.20878     -9299.4287    \n",
            "      1584   190.96548     -148.31985      0             -148.22111     -9165.9949    \n",
            "      1585   206.68214     -148.32026      0             -148.21339     -8662.9781    \n",
            "      1586   220.67288     -148.3288       0             -148.2147      -8077.0834    \n",
            "      1587   231.11912     -148.37015      0             -148.25065     -7235.6071    \n",
            "      1588   237.77593     -148.36465      0             -148.24171     -6128.1683    \n",
            "      1589   241.4535      -148.35252      0             -148.22768     -4912.2938    \n",
            "      1590   243.08213     -148.36684      0             -148.24116     -3550.9582    \n",
            "      1591   243.96771     -148.37067      0             -148.24453     -2153.2171    \n",
            "      1592   244.8296      -148.36123      0             -148.23464     -581.96846    \n",
            "      1593   245.27466     -148.35084      0             -148.22403      1033.5777    \n",
            "      1594   244.24053     -148.36325      0             -148.23697      2507.5211    \n",
            "      1595   240.02922     -148.39476      0             -148.27065      3890.2882    \n",
            "      1596   230.99585     -148.36331      0             -148.24388      5254.7217    \n",
            "      1597   216.13767     -148.37251      0             -148.26076      6655.4899    \n",
            "      1598   195.73276     -148.36737      0             -148.26617      7880.1815    \n",
            "      1599   171.62214     -148.31857      0             -148.22983      9028.4343    \n",
            "      1600   146.75854     -148.29526      0             -148.21938      9934.2558    \n",
            "      1601   125.24694     -148.32524      0             -148.26048      10735.07     \n",
            "      1602   111.10685     -148.29793      0             -148.24049      11335.964    \n",
            "      1603   107.60043     -148.30974      0             -148.25411      11434.57     \n",
            "      1604   116.38005     -148.33136      0             -148.27118      11458.792    \n",
            "      1605   137.03732     -148.31614      0             -148.24529      11323.845    \n",
            "      1606   167.16294     -148.33144      0             -148.24501      10932.697    \n",
            "      1607   202.6108      -148.33943      0             -148.23467      10178.727    \n",
            "      1608   238.15988     -148.39867      0             -148.27554      9107.1046    \n",
            "      1609   268.53817     -148.40546      0             -148.26661      8168.5276    \n",
            "      1610   289.41582     -148.40724      0             -148.2576       7055.4464    \n",
            "      1611   298.04238     -148.40406      0             -148.24996      5761.997     \n",
            "      1612   293.73305     -148.43694      0             -148.28506      4512.2319    \n",
            "      1613   277.80885     -148.3947       0             -148.25106      3307.492     \n",
            "      1614   253.03464     -148.36937      0             -148.23855      2034.4659    \n",
            "      1615   223.31493     -148.39294      0             -148.27747      534.75638    \n",
            "      1616   192.38681     -148.36055      0             -148.26108     -595.61874    \n",
            "      1617   163.13361     -148.35295      0             -148.2686      -1716.6217    \n",
            "      1618   137.50503     -148.31242      0             -148.24132     -2715.1861    \n",
            "      1619   116.32403     -148.35085      0             -148.29071     -3638.9552    \n",
            "      1620   99.387915     -148.30958      0             -148.25819     -4302.8204    \n",
            "      1621   86.170082     -148.31988      0             -148.27533     -4868.5806    \n",
            "      1622   76.200486     -148.3256       0             -148.2862      -5227.4492    \n",
            "      1623   69.646069     -148.31196      0             -148.27595     -5347.6039    \n",
            "      1624   67.353219     -148.3053       0             -148.27048     -5216.3803    \n",
            "      1625   70.590684     -148.31924      0             -148.28274     -4817.2695    \n",
            "      1626   80.852845     -148.30763      0             -148.26583     -4279.7105    \n",
            "      1627   99.492865     -148.34734      0             -148.2959      -3695.8186    \n",
            "      1628   126.69077     -148.3381       0             -148.2726      -2786.5867    \n",
            "      1629   160.9876      -148.34665      0             -148.26341     -1744.0936    \n",
            "      1630   199.40346     -148.40099      0             -148.29789     -646.20145    \n",
            "      1631   237.72862     -148.41803      0             -148.29511      494.71446    \n",
            "      1632   271.03172     -148.43964      0             -148.2995       1894.722     \n",
            "      1633   294.43411     -148.42626      0             -148.27402      3247.5673    \n",
            "      1634   304.57596     -148.44977      0             -148.29229      4721.2438    \n",
            "      1635   299.6868      -148.43839      0             -148.28344      6142.9442    \n",
            "      1636   280.25265     -148.4327       0             -148.2878       7640.3557    \n",
            "      1637   249.06542     -148.41131      0             -148.28253      9042.8861    \n",
            "      1638   210.40502     -148.36654      0             -148.25775      10304.223    \n",
            "      1639   169.25581     -148.38489      0             -148.29737      11373.748    \n",
            "      1640   130.60696     -148.38046      0             -148.31293      12333.992    \n",
            "      1641   98.375789     -148.33887      0             -148.28801      13164.482    \n",
            "      1642   74.942208     -148.31392      0             -148.27517      13616.735    \n",
            "      1643   61.040079     -148.29993      0             -148.26837      14008.558    \n",
            "      1644   55.984098     -148.31792      0             -148.28897      14094.024    \n",
            "      1645   58.287844     -148.31335      0             -148.28321      13937.254    \n",
            "      1646   66.029773     -148.30369      0             -148.26955      13525.617    \n",
            "      1647   77.726867     -148.36611      0             -148.32593      12864.135    \n",
            "      1648   92.574749     -148.35869      0             -148.31082      12138.893    \n",
            "      1649   110.01992     -148.33942      0             -148.28253      11228.18     \n",
            "      1650   130.21792     -148.34624      0             -148.27891      10030.171    \n",
            "      1651   153.18856     -148.3885       0             -148.3093       8771.5031    \n",
            "      1652   178.60027     -148.37071      0             -148.27836      7338.164     \n",
            "      1653   205.26892     -148.40085      0             -148.29472      5871.2044    \n",
            "      1654   230.9798      -148.42057      0             -148.30114      4193.8128    \n",
            "      1655   253.05531     -148.41266      0             -148.28182      2478.7566    \n",
            "      1656   268.27432     -148.42037      0             -148.28166      852.23573    \n",
            "      1657   273.83486     -148.44113      0             -148.29954     -871.17193    \n",
            "      1658   268.43558     -148.45851      0             -148.31972     -2337.9762    \n",
            "      1659   252.0278      -148.42837      0             -148.29806     -3833.7363    \n",
            "      1660   226.18097     -148.4272       0             -148.31025     -5086.9482    \n",
            "      1661   194.18789     -148.40442      0             -148.30402     -6363.2922    \n",
            "      1662   160.23748     -148.39815      0             -148.3153      -7371.9053    \n",
            "      1663   128.77092     -148.37844      0             -148.31186     -8128.4515    \n",
            "      1664   103.62139     -148.34831      0             -148.29473     -8729.7509    \n",
            "      1665   87.485917     -148.34474      0             -148.2995      -9223.4616    \n",
            "      1666   81.40027      -148.3606       0             -148.31851     -9617.8283    \n",
            "      1667   84.707966     -148.35346      0             -148.30966     -9831.0045    \n",
            "      1668   95.388638     -148.34604      0             -148.29672     -9768.7613    \n",
            "      1669   110.83348     -148.337        0             -148.27969     -9587.3285    \n",
            "      1670   128.07467     -148.37299      0             -148.30677     -9202.1654    \n",
            "      1671   144.36228     -148.38573      0             -148.31108     -8610.0417    \n",
            "      1672   158.15543     -148.39416      0             -148.31239     -7815.0615    \n",
            "      1673   168.71991     -148.39076      0             -148.30353     -7028.7027    \n",
            "      1674   176.16148     -148.41185      0             -148.32077     -6011.6085    \n",
            "      1675   181.2837      -148.39184      0             -148.29811     -4901.0759    \n",
            "      1676   185.20781     -148.39178      0             -148.29602     -3790.4616    \n",
            "      1677   188.5787      -148.39795      0             -148.30044     -2723.4265    \n",
            "      1678   191.42753     -148.3913       0             -148.29232     -1595.1798    \n",
            "      1679   193.20099     -148.40265      0             -148.30276     -475.55943    \n",
            "      1680   192.67108     -148.41513      0             -148.31551      560.37065    \n",
            "      1681   188.62704     -148.39424      0             -148.29671      1529.0038    \n",
            "      1682   180.42288     -148.41279      0             -148.3195       2394.0344    \n",
            "      1683   168.00106     -148.39041      0             -148.30355      3216.4168    \n",
            "      1684   152.25388     -148.38308      0             -148.30436      3896.6349    \n",
            "      1685   135.14223     -148.37526      0             -148.30539      4328.1348    \n",
            "      1686   119.09594     -148.38288      0             -148.32131      4608.116     \n",
            "      1687   106.92629     -148.33804      0             -148.28275      4954.8772    \n",
            "      1688   101.19793     -148.40405      0             -148.35172      4880.3166    \n",
            "      1689   103.27315     -148.37209      0             -148.31869      4767.5217    \n",
            "      1690   113.2137      -148.37621      0             -148.31767      4465.6692    \n",
            "      1691   129.74302     -148.38521      0             -148.31812      3909.9714    \n",
            "      1692   150.3337      -148.36856      0             -148.29083      3277.3872    \n",
            "      1693   171.41776     -148.39238      0             -148.30375      2426.0411    \n",
            "      1694   189.38551     -148.41786      0             -148.31994      1700.1204    \n",
            "      1695   201.4572      -148.39494      0             -148.29078      748.519      \n",
            "      1696   205.71841     -148.42917      0             -148.3228      -254.53508    \n",
            "      1697   201.78928     -148.42851      0             -148.32418     -1301.5531    \n",
            "      1698   190.64219     -148.40838      0             -148.30981     -2221.7495    \n",
            "      1699   174.24668     -148.41142      0             -148.32133     -3250.4085    \n",
            "      1700   155.48516     -148.39326      0             -148.31287     -4169.2318    \n",
            "      1701   137.06911     -148.3833       0             -148.31243     -5107.4563    \n",
            "      1702   120.94347     -148.36258      0             -148.30005     -5936.411     \n",
            "      1703   108.25793     -148.3527       0             -148.29673     -6709.7918    \n",
            "      1704   99.233886     -148.38615      0             -148.33484     -7388.0655    \n",
            "      1705   93.388927     -148.36416      0             -148.31587     -7880.7846    \n",
            "      1706   89.559462     -148.3453       0             -148.299       -8351.1277    \n",
            "      1707   86.906397     -148.35288      0             -148.30794     -8660.5349    \n",
            "      1708   85.135137     -148.32155      0             -148.27753     -8649.5566    \n",
            "      1709   84.613875     -148.37577      0             -148.33202     -8712.1255    \n",
            "      1710   86.322605     -148.35713      0             -148.3125      -8474.7636    \n",
            "      1711   91.73094      -148.3543       0             -148.30687     -8164.6196    \n",
            "      1712   102.11503     -148.35554      0             -148.30274     -7698.8615    \n",
            "      1713   117.98359     -148.38083      0             -148.31983     -7083.6199    \n",
            "      1714   138.87878     -148.38108      0             -148.30928     -6352.6025    \n",
            "      1715   163.04863     -148.40089      0             -148.31659     -5590.9603    \n",
            "      1716   187.61855     -148.44427      0             -148.34726     -4746.4727    \n",
            "      1717   209.06751     -148.41621      0             -148.30812     -3641.6983    \n",
            "      1718   223.89562     -148.43119      0             -148.31543     -2649.8       \n",
            "      1719   229.18521     -148.4216       0             -148.3031      -1573.8308    \n",
            "      1720   223.70083     -148.44654      0             -148.33087     -493.7267     \n",
            "      1721   207.73808     -148.43341      0             -148.326        699.83724    \n",
            "      1722   183.14584     -148.40307      0             -148.30837      1824.8384    \n",
            "      1723   153.34811     -148.40873      0             -148.32944      2751.575     \n",
            "      1724   122.03089     -148.35886      0             -148.29576      3705.3402    \n",
            "      1725   92.9439       -148.35566      0             -148.30761      4524.6053    \n",
            "      1726   69.241244     -148.3455       0             -148.3097       5216.1822    \n",
            "      1727   52.836381     -148.31547      0             -148.28815      5766.8423    \n",
            "      1728   44.226506     -148.31946      0             -148.29659      6205.7348    \n",
            "      1729   42.985168     -148.32945      0             -148.30722      6456.0997    \n",
            "      1730   47.827688     -148.33552      0             -148.31079      6431.2603    \n",
            "      1731   57.117964     -148.34364      0             -148.31411      6290.8671    \n",
            "      1732   69.492306     -148.3263       0             -148.29037      6001.5043    \n",
            "      1733   84.023095     -148.36987      0             -148.32643      5442.9493    \n",
            "      1734   100.48019     -148.35738      0             -148.30543      4922.2068    \n",
            "      1735   118.95526     -148.36327      0             -148.30177      4208.7759    \n",
            "      1736   139.72771     -148.38903      0             -148.31678      3300.2016    \n",
            "      1737   162.58938     -148.39049      0             -148.30643      2201.0173    \n",
            "      1738   186.60383     -148.41258      0             -148.31609      1088.0962    \n",
            "      1739   210.08994     -148.44578      0             -148.33715     -128.44987    \n",
            "      1740   230.73809     -148.42615      0             -148.30685     -1362.1698    \n",
            "      1741   245.81344     -148.43761      0             -148.31052     -2753.5791    \n",
            "      1742   252.51681     -148.43185      0             -148.30129     -4097.0576    \n",
            "      1743   248.94519     -148.43793      0             -148.30921     -5266.0159    \n",
            "      1744   234.75038     -148.39827      0             -148.2769      -6436.5       \n",
            "      1745   210.87094     -148.4111       0             -148.30207     -7644.0899    \n",
            "      1746   179.66358     -148.4041       0             -148.31121     -8616.8045    \n",
            "      1747   144.85298     -148.38105      0             -148.30615     -9326.0787    \n",
            "      1748   110.58883     -148.36773      0             -148.31055     -10063.526    \n",
            "      1749   80.762164     -148.36151      0             -148.31975     -10677.366    \n",
            "      1750   58.73353      -148.33318      0             -148.30281     -11028.185    \n",
            "      1751   46.507818     -148.33391      0             -148.30986     -11320.852    \n",
            "      1752   44.523989     -148.34802      0             -148.325       -11435.885    \n",
            "      1753   51.73879      -148.34539      0             -148.31864     -11405.501    \n",
            "      1754   66.24301      -148.34687      0             -148.31262     -11067.434    \n",
            "      1755   85.543694     -148.32491      0             -148.28068     -10608.143    \n",
            "      1756   106.94249     -148.36415      0             -148.30886     -9992.1792    \n",
            "      1757   128.3106      -148.35793      0             -148.29159     -9082.0918    \n",
            "      1758   148.35069     -148.36194      0             -148.28524     -8234.9839    \n",
            "      1759   166.70707     -148.37026      0             -148.28407     -7170.8673    \n",
            "      1760   183.43156     -148.38646      0             -148.29162     -5938.6917    \n",
            "      1761   198.83302     -148.40476      0             -148.30195     -4631.4338    \n",
            "      1762   213.06146     -148.40433      0             -148.29417     -3250.2733    \n",
            "      1763   225.67866     -148.42905      0             -148.31236     -1779.5835    \n",
            "      1764   235.70092     -148.41596      0             -148.29409     -428.72392    \n",
            "      1765   241.72714     -148.43153      0             -148.30655      883.17067    \n",
            "      1766   241.98364     -148.4124       0             -148.28729      2264.1651    \n",
            "      1767   235.11254     -148.39632      0             -148.27476      3706.2556    \n",
            "      1768   220.68276     -148.41262      0             -148.29851      4837.1456    \n",
            "      1769   199.36718     -148.41246      0             -148.30938      6097.3658    \n",
            "      1770   173.35375     -148.37507      0             -148.28544      6980.2393    \n",
            "      1771   145.66593     -148.38851      0             -148.3132       7882.3644    \n",
            "      1772   119.84448     -148.36963      0             -148.30766      8664.7631    \n",
            "      1773   99.759261     -148.37709      0             -148.32551      9030.7239    \n",
            "      1774   88.354263     -148.35701      0             -148.31133      9335.8783    \n",
            "      1775   87.216714     -148.32503      0             -148.27994      9450.5088    \n",
            "      1776   96.318363     -148.35128      0             -148.30148      9354.9083    \n",
            "      1777   113.97799     -148.36728      0             -148.30835      9084.2984    \n",
            "      1778   136.98636     -148.37292      0             -148.3021       8430.8636    \n",
            "      1779   161.3612      -148.39878      0             -148.31535      7877.029     \n",
            "      1780   183.50009     -148.38503      0             -148.29016      7228.5468    \n",
            "      1781   200.62424     -148.37213      0             -148.2684       6217.8235    \n",
            "      1782   211.06424     -148.39489      0             -148.28576      5326.7499    \n",
            "      1783   214.29336     -148.42388      0             -148.31308      4259.7551    \n",
            "      1784   211.39643     -148.40765      0             -148.29834      3234.2286    \n",
            "      1785   204.55865     -148.37692      0             -148.27115      2169.218     \n",
            "      1786   195.94865     -148.39197      0             -148.29065      1128.8877    \n",
            "      1787   187.25036     -148.39679      0             -148.29997      134.30737    \n",
            "      1788   179.43151     -148.39273      0             -148.29996     -934.23872    \n",
            "      1789   172.66572     -148.39268      0             -148.30341     -1825.6678    \n",
            "      1790   166.33692     -148.36886      0             -148.28286     -2538.4699    \n",
            "      1791   159.29795     -148.38112      0             -148.29875     -3320.4435    \n",
            "      1792   150.96584     -148.3858       0             -148.30775     -3751.9932    \n",
            "      1793   141.18902     -148.36886      0             -148.29586     -4043.1459    \n",
            "      1794   130.85385     -148.36287      0             -148.29522     -4306.0528    \n",
            "      1795   121.67614     -148.34977      0             -148.28686     -4232.5002    \n",
            "      1796   115.82191     -148.34095      0             -148.28106     -3978.8884    \n",
            "      1797   115.80143     -148.3294       0             -148.26953     -3592.8609    \n",
            "      1798   123.17976     -148.35115      0             -148.28746     -3016.7193    \n",
            "      1799   138.18456     -148.3475       0             -148.27605     -2381.0332    \n",
            "      1800   159.40698     -148.37917      0             -148.29675     -1527.9425    \n",
            "      1801   183.96573     -148.37263      0             -148.27751     -607.24799    \n",
            "      1802   208.06982     -148.39689      0             -148.28931      478.8225     \n",
            "      1803   227.52862     -148.4116       0             -148.29396      1548.7044    \n",
            "      1804   238.61548     -148.40178      0             -148.27841      2838.1707    \n",
            "      1805   238.88439     -148.38696      0             -148.26345      4081.511     \n",
            "      1806   227.96026     -148.40482      0             -148.28696      5349.5894    \n",
            "      1807   207.3261      -148.39108      0             -148.28389      6598.0422    \n",
            "      1808   179.91743     -148.39145      0             -148.29842      7808.4305    \n",
            "      1809   149.87396     -148.35662      0             -148.27913      9109.4864    \n",
            "      1810   121.53167     -148.34287      0             -148.28003      10050.188    \n",
            "      1811   98.561867     -148.31499      0             -148.26403      10965.691    \n",
            "      1812   83.293472     -148.32791      0             -148.28485      11650.159    \n",
            "      1813   76.567704     -148.3108       0             -148.27122      12123.811    \n",
            "      1814   77.917319     -148.32649      0             -148.2862       12344.776    \n",
            "      1815   85.827082     -148.33316      0             -148.28878      12464.557    \n",
            "      1816   98.227771     -148.33599      0             -148.2852       12329.774    \n",
            "      1817   113.39013     -148.35472      0             -148.29609      12057.802    \n",
            "      1818   129.86997     -148.32468      0             -148.25753      11481.143    \n",
            "      1819   147.33755     -148.35945      0             -148.28327      10778.012    \n",
            "      1820   165.92349     -148.36246      0             -148.27667      9917.4298    \n",
            "      1821   185.84505     -148.3787       0             -148.28261      8791.3094    \n",
            "      1822   207.30584     -148.39271      0             -148.28552      7385.9486    \n",
            "      1823   229.71683     -148.3967       0             -148.27793      6019.9268    \n",
            "      1824   251.58586     -148.37984      0             -148.24976      4594.6259    \n",
            "      1825   270.25431     -148.41364      0             -148.27391      2963.8072    \n",
            "      1826   282.44707     -148.44305      0             -148.29701      1150.6048    \n",
            "      1827   285.29933     -148.4054       0             -148.25789     -448.49108    \n",
            "      1828   276.62024     -148.41071      0             -148.26768     -1980.7166    \n",
            "      1829   255.59038     -148.37337      0             -148.24122     -3489.7947    \n",
            "      1830   223.18682     -148.38179      0             -148.26639     -5003.6818    \n",
            "      1831   182.3636      -148.38136      0             -148.28707     -6255.7812    \n",
            "      1832   137.72424     -148.32724      0             -148.25603     -7263.4828    \n",
            "      1833   94.745142     -148.29019      0             -148.2412      -8262.5442    \n",
            "      1834   58.780795     -148.29994      0             -148.26954     -9028.8819    \n",
            "      1835   34.43297      -148.29598      0             -148.27818     -9627.4633    \n",
            "      1836   24.636549     -148.24564      0             -148.2329      -9984.8526    \n",
            "      1837   30.125244     -148.27182      0             -148.25625     -10167.148    \n",
            "      1838   49.626545     -148.27927      0             -148.25361     -10261.592    \n",
            "      1839   80.263276     -148.30058      0             -148.25908     -9958.1401    \n",
            "      1840   118.06233     -148.29387      0             -148.23283     -9389.2362    \n",
            "      1841   158.79567     -148.31287      0             -148.23077     -8794.4024    \n",
            "      1842   198.79392     -148.35871      0             -148.25593     -7924.7538    \n",
            "      1843   235.2679      -148.36806      0             -148.24641     -6906.4752    \n",
            "      1844   266.66257     -148.39853      0             -148.26066     -5685.8045    \n",
            "      1845   292.63664     -148.421        0             -148.26969     -4244.914     \n",
            "      1846   313.52757     -148.40858      0             -148.24648     -2667.8596    \n",
            "      1847   329.78369     -148.4309       0             -148.26039     -1339.515     \n",
            "      1848   341.50133     -148.41281      0             -148.23624      425.72292    \n",
            "      1849   347.82969     -148.42815      0             -148.24831      1914.9764    \n",
            "      1850   347.27872     -148.45321      0             -148.27365      3441.4801    \n",
            "      1851   338.30511     -148.41148      0             -148.23656      4946.8473    \n",
            "      1852   319.48064     -148.42305      0             -148.25786      6331.9844    \n",
            "      1853   290.23829     -148.38563      0             -148.23557      7628.2179    \n",
            "      1854   251.92829     -148.33174      0             -148.20148      8775.8706    \n",
            "      1855   207.54534     -148.33754      0             -148.23023      9803.4386    \n",
            "      1856   161.64121     -148.31884      0             -148.23526      10655.563    \n",
            "      1857   119.87375     -148.28015      0             -148.21817      11233.803    \n",
            "      1858   88.029265     -148.28465      0             -148.23914      11576.374    \n",
            "      1859   70.863402     -148.28188      0             -148.24524      11742.016    \n",
            "      1860   71.241089     -148.26981      0             -148.23298      11667.036    \n",
            "      1861   89.508264     -148.28418      0             -148.2379       11246.027    \n",
            "      1862   123.32292     -148.29961      0             -148.23585      10524.435    \n",
            "      1863   167.83132     -148.31903      0             -148.23225      9753.5605    \n",
            "      1864   217.03486     -148.33567      0             -148.22346      8905.2363    \n",
            "      1865   264.86047     -148.38809      0             -148.25114      7576.9851    \n",
            "      1866   305.89256     -148.37117      0             -148.21301      6343.9075    \n",
            "      1867   336.59442     -148.3843       0             -148.21027      5128.5313    \n",
            "      1868   355.5548      -148.43369      0             -148.24986      3469.5678    \n",
            "      1869   363.57056     -148.41071      0             -148.22273      2053.6737    \n",
            "      1870   362.9868      -148.40404      0             -148.21636      676.72148    \n",
            "      1871   356.87967     -148.42112      0             -148.2366      -834.21601    \n",
            "      1872   347.84171     -148.40091      0             -148.22107     -2250.7007    \n",
            "      1873   337.55849     -148.39592      0             -148.22139     -3631.8513    \n",
            "      1874   326.20014     -148.37273      0             -148.20408     -5017.6627    \n",
            "      1875   312.59493     -148.35936      0             -148.19773     -6249.3418    \n",
            "      1876   295.51845     -148.35579      0             -148.20299     -7219.3247    \n",
            "      1877   273.66397     -148.32869      0             -148.1872      -8297.6099    \n",
            "      1878   246.93367     -148.31765      0             -148.18998     -9001.8754    \n",
            "      1879   217.35831     -148.3102       0             -148.19782     -9524.7628    \n",
            "      1880   188.07788     -148.28388      0             -148.18664     -9821.2675    \n",
            "      1881   163.25961     -148.26361      0             -148.1792      -10030.73     \n",
            "      1882   147.46575     -148.29071      0             -148.21447     -9985.5607    \n",
            "      1883   144.30931     -148.26933      0             -148.19471     -9746.9655    \n",
            "      1884   155.55707     -148.28795      0             -148.20752     -9431.6515    \n",
            "      1885   180.49068     -148.27298      0             -148.17966     -8930.7369    \n",
            "      1886   215.86982     -148.29709      0             -148.18547     -8387.8792    \n",
            "      1887   256.34249     -148.30024      0             -148.1677      -7630.3518    \n",
            "      1888   295.70814     -148.34732      0             -148.19443     -6847.9598    \n",
            "      1889   328.08064     -148.35549      0             -148.18586     -5900.4708    \n",
            "      1890   348.67657     -148.3465       0             -148.16622     -4872.8558    \n",
            "      1891   355.31559     -148.36234      0             -148.17863     -3809.922     \n",
            "      1892   348.21281     -148.3391       0             -148.15906     -2589.3182    \n",
            "      1893   330.36699     -148.37194      0             -148.20112     -1502.1866    \n",
            "      1894   306.29296     -148.3263       0             -148.16793     -266.43824    \n",
            "      1895   280.67947     -148.34799      0             -148.20287      761.71282    \n",
            "      1896   257.99134     -148.27332      0             -148.13993      1763.2001    \n",
            "      1897   241.12163     -148.28798      0             -148.16331      2623.7118    \n",
            "      1898   231.03069     -148.27872      0             -148.15926      3127.4403    \n",
            "      1899   227.23598     -148.2739       0             -148.15641      3491.0067    \n",
            "      1900   227.81673     -148.27557      0             -148.15778      3772.7846    \n",
            "      1901   230.16616     -148.29193      0             -148.17292      3779.9135    \n",
            "      1902   232.33307     -148.28714      0             -148.16701      3587.7932    \n",
            "      1903   233.51964     -148.27249      0             -148.15175      3397.2597    \n",
            "      1904   234.12545     -148.28892      0             -148.16786      3053.901     \n",
            "      1905   235.76505     -148.27961      0             -148.15771      2337.3419    \n",
            "      1906   240.64416     -148.27659      0             -148.15217      1565.5466    \n",
            "      1907   250.79347     -148.29006      0             -148.16039      402.32019    \n",
            "      1908   267.50419     -148.26894      0             -148.13063     -738.28258    \n",
            "      1909   290.31843     -148.27469      0             -148.12459     -1975.536     \n",
            "      1910   316.78248     -148.32287      0             -148.15908     -3535.0059    \n",
            "      1911   342.96579     -148.32097      0             -148.14364     -4908.1646    \n",
            "      1912   363.65933     -148.32461      0             -148.13659     -6472.4327    \n",
            "      1913   373.594       -148.33109      0             -148.13793     -8107.1062    \n",
            "      1914   368.99574     -148.32343      0             -148.13264     -9625.6648    \n",
            "      1915   348.35421     -148.33092      0             -148.1508      -11112.888    \n",
            "      1916   313.06295     -148.28674      0             -148.12488     -12221.39     \n",
            "      1917   266.98224     -148.26571      0             -148.12767     -13396.152    \n",
            "      1918   216.02785     -148.2283       0             -148.1166      -14424.647    \n",
            "      1919   167.59727     -148.21032      0             -148.12366     -15334.422    \n",
            "      1920   128.93767     -148.19231      0             -148.12564     -15935.989    \n",
            "      1921   105.77171     -148.19403      0             -148.13934     -16485.137    \n",
            "      1922   101.17045     -148.15516      0             -148.10285     -16714.628    \n",
            "      1923   115.15327     -148.19735      0             -148.13781     -17021.052    \n",
            "      1924   145.23098     -148.19289      0             -148.1178      -16939.639    \n",
            "      1925   186.8776      -148.2374       0             -148.14078     -16742.043    \n",
            "      1926   234.33118     -148.2477       0             -148.12654     -16233.407    \n",
            "      1927   282.56983     -148.26964      0             -148.12354     -15432.435    \n",
            "      1928   327.72499     -148.27924      0             -148.10979     -14457.997    \n",
            "      1929   367.50961     -148.32349      0             -148.13347     -13165.037    \n",
            "      1930   401.90335     -148.30815      0             -148.10035     -11737.876    \n",
            "      1931   432.13545     -148.32779      0             -148.10436     -10133.307    \n",
            "      1932   459.60724     -148.33176      0             -148.09412     -8357.6296    \n",
            "      1933   485.01756     -148.32976      0             -148.07899     -6478.3938    \n",
            "      1934   507.24675     -148.34301      0             -148.08074     -4591.6736    \n",
            "      1935   523.5232      -148.35694      0             -148.08626     -2592.7655    \n",
            "      1936   529.59713     -148.3775       0             -148.10367     -604.16179    \n",
            "      1937   520.85045     -148.37321      0             -148.10391      1226.053     \n",
            "      1938   493.82598     -148.32623      0             -148.0709       3216.6377    \n",
            "      1939   447.44538     -148.32501      0             -148.09366      5132.522     \n",
            "      1940   383.6514      -148.25414      0             -148.05578      6928.1097    \n",
            "      1941   308.31621     -148.23862      0             -148.07921      8629.2577    \n",
            "      1942   230.00224     -148.19849      0             -148.07957      10066.912    \n",
            "      1943   159.19751     -148.16746      0             -148.08514      11392.515    \n",
            "      1944   106.10726     -148.116        0             -148.06114      12606.624    \n",
            "      1945   78.55171      -148.08796      0             -148.04735      13265.161    \n",
            "      1946   81.101225     -148.12605      0             -148.08412      13788.051    \n",
            "      1947   113.91928     -148.13913      0             -148.08023      13862.225    \n",
            "      1948   172.96406     -148.16868      0             -148.07925      13514.129    \n",
            "      1949   250.61178     -148.21564      0             -148.08607      13231.417    \n",
            "      1950   337.33263     -148.2333       0             -148.05888      12506.096    \n",
            "      1951   423.81187     -148.28022      0             -148.06109      11455.948    \n",
            "      1952   502.01779     -148.33602      0             -148.07646      10270.735    \n",
            "      1953   566.99247     -148.32816      0             -148.035        9065.7057    \n",
            "      1954   616.62735     -148.36304      0             -148.04422      7548.467     \n",
            "      1955   651.75921     -148.38511      0             -148.04812      5998.3443    \n",
            "      1956   674.88818     -148.38307      0             -148.03412      4246.3962    \n",
            "      1957   688.15518     -148.40725      0             -148.05145      2317.7744    \n",
            "      1958   692.88481     -148.3768       0             -148.01855      474.41339    \n",
            "      1959   688.69588     -148.39389      0             -148.03781     -1422.7605    \n",
            "      1960   673.37177     -148.39315      0             -148.04499     -3452.3291    \n",
            "      1961   644.14336     -148.36772      0             -148.03467     -5303.5734    \n",
            "      1962   598.73174     -148.33418      0             -148.02461     -6848.4155    \n",
            "      1963   536.94235     -148.31557      0             -148.03795     -8287.2568    \n",
            "      1964   461.81984     -148.24165      0             -148.00287     -9429.8774    \n",
            "      1965   379.72579     -148.17861      0             -147.98228     -10316.291    \n",
            "      1966   299.54338     -148.16676      0             -148.01189     -10923.29     \n",
            "      1967   231.37807     -148.13767      0             -148.01804     -11318.026    \n",
            "      1968   184.72405     -148.11661      0             -148.0211      -11620.318    \n",
            "      1969   166.76449     -148.09304      0             -148.00681     -11395.149    \n",
            "      1970   180.56668     -148.10463      0             -148.01127     -11162.914    \n",
            "      1971   224.63861     -148.12676      0             -148.01062     -10851.394    \n",
            "      1972   293.1474      -148.15261      0             -148.00104     -10229.821    \n",
            "      1973   376.88528     -148.20862      0             -148.01376     -9347.5496    \n",
            "      1974   465.03025     -148.25025      0             -148.00981     -8202.234     \n",
            "      1975   546.91285     -148.27053      0             -147.98775     -6921.3474    \n",
            "      1976   614.34732     -148.27727      0             -147.95963     -5286.5344    \n",
            "      1977   662.51665     -148.33535      0             -147.99281     -3433.5807    \n",
            "      1978   689.87119     -148.36299      0             -148.0063      -1474.2943    \n",
            "      1979   698.62875     -148.36138      0             -148.00016      659.95302    \n",
            "      1980   693.27727     -148.34953      0             -147.99108      2776.3425    \n",
            "      1981   678.73117     -148.3187       0             -147.96776      4860.2534    \n",
            "      1982   659.28084     -148.29953      0             -147.95865      6724.2707    \n",
            "      1983   637.12483     -148.28534      0             -147.95592      8556.9799    \n",
            "      1984   612.1926      -148.27579      0             -147.95927      10137.458    \n",
            "      1985   583.41717     -148.27431      0             -147.97266      11572.33     \n",
            "      1986   548.88907     -148.27967      0             -147.99588      12606.09     \n",
            "      1987   507.36419     -148.22458      0             -147.96225      13423.794    \n",
            "      1988   459.56982     -148.21486      0             -147.97725      14028.03     \n",
            "      1989   408.58101     -148.17497      0             -147.96371      14431.889    \n",
            "      1990   359.83534     -148.14346      0             -147.95741      14581.865    \n",
            "      1991   319.93107     -148.11305      0             -147.94763      14203.11     \n",
            "      1992   295.47201     -148.10218      0             -147.94941      13840.925    \n",
            "      1993   291.3843      -148.11073      0             -147.96007      12898.728    \n",
            "      1994   309.7474      -148.13425      0             -147.9741       11695.361    \n",
            "      1995   348.76703     -148.13824      0             -147.95791      10367.076    \n",
            "      1996   402.83198     -148.15709      0             -147.94881      8655.1496    \n",
            "      1997   463.38187     -148.17586      0             -147.93627      6819.9351    \n",
            "      1998   519.65554     -148.20649      0             -147.93781      4720.3532    \n",
            "      1999   561.63654     -148.23896      0             -147.94857      2741.9774    \n",
            "      2000   582.20662     -148.23682      0             -147.93579      677.34088    \n",
            "      2001   578.04373     -148.25962      0             -147.96075     -1138.5904    \n",
            "      2002   550.64566     -148.236        0             -147.9513      -2795.1342    \n",
            "      2003   505.84325     -148.20283      0             -147.94129     -4243.2944    \n",
            "      2004   452.99984     -148.19335      0             -147.95913     -5596.497     \n",
            "      2005   402.6549      -148.16302      0             -147.95483     -6699.6124    \n",
            "      2006   364.52568     -148.14963      0             -147.96115     -7620.7282    \n",
            "      2007   345.36404     -148.1284       0             -147.94984     -8253.0992    \n",
            "      2008   347.59184     -148.12374      0             -147.94403     -8713.0865    \n",
            "      2009   369.38511     -148.13791      0             -147.94693     -8910.6899    \n",
            "      2010   405.41624     -148.14984      0             -147.94022     -8644.49      \n",
            "      2011   447.93369     -148.19024      0             -147.95864     -8229.3294    \n",
            "      2012   488.87968     -148.1984       0             -147.94563     -7277.2799    \n",
            "      2013   522.42934     -148.21856      0             -147.94844     -6147.0423    \n",
            "      2014   545.50865     -148.22645      0             -147.9444      -4531.2024    \n",
            "      2015   557.87632     -148.2394       0             -147.95096     -2767.1171    \n",
            "      2016   562.05357     -148.21655      0             -147.92595     -514.65342    \n",
            "      2017   562.29405     -148.21006      0             -147.91933      1741.3772    \n",
            "      2018   562.55327     -148.19552      0             -147.90466      4107.7693    \n",
            "      2019   564.19102     -148.20363      0             -147.91192      6607.5586    \n",
            "      2020   565.69677     -148.21715      0             -147.92466      9019.0758    \n",
            "      2021   562.94997     -148.22223      0             -147.93116      11428.517    \n",
            "      2022   550.54998     -148.19842      0             -147.91377      13748.25     \n",
            "      2023   523.33873     -148.20487      0             -147.93428      15953.366    \n",
            "      2024   478.31139     -148.17168      0             -147.92437      18082.865    \n",
            "      2025   417.35681     -148.15467      0             -147.93888      19967.02     \n",
            "      2026   346.72956     -148.09951      0             -147.92023      21849.174    \n",
            "      2027   276.31981     -148.06254      0             -147.91967      23294.05     \n",
            "      2028   218.75124     -148.04324      0             -147.93013      24568.492    \n",
            "      2029   186.32256     -147.99301      0             -147.89667      25256.101    \n",
            "      2030   188.10348     -148.00258      0             -147.90532      25668.088    \n",
            "      2031   228.10271     -148.03717      0             -147.91923      25575.6      \n",
            "      2032   304.12542     -148.0723       0             -147.91506      25019.608    \n",
            "      2033   408.02653     -148.13782      0             -147.92685      24171.356    \n",
            "      2034   527.04052     -148.1794       0             -147.9069       22698.642    \n",
            "      2035   646.22547     -148.25358      0             -147.91945      20959.372    \n",
            "      2036   751.43631     -148.29435      0             -147.90582      19021.721    \n",
            "      2037   831.76234     -148.36578      0             -147.93573      16915.048    \n",
            "      2038   881.69737     -148.36624      0             -147.91037      14491.245    \n",
            "      2039   900.6449      -148.38344      0             -147.91777      11995.046    \n",
            "      2040   892.63062     -148.39434      0             -147.93281      9398.3757    \n",
            "      2041   864.77978     -148.36004      0             -147.91291      6823.5943    \n",
            "      2042   824.20673     -148.31714      0             -147.89099      4133.1193    \n",
            "      2043   776.45806     -148.29808      0             -147.89662      1461.5719    \n",
            "      2044   724.62855     -148.29293      0             -147.91826     -1196.9081    \n",
            "      2045   669.16815     -148.25081      0             -147.90483     -3692.3459    \n",
            "      2046   609.13394     -148.21684      0             -147.9019      -5814.7144    \n",
            "      2047   543.01784     -148.2062       0             -147.92543     -8039.2285    \n",
            "      2048   470.94657     -148.14621      0             -147.90271     -9765.687     \n",
            "      2049   395.94346     -148.11103      0             -147.90631     -11213.21     \n",
            "      2050   323.82377     -148.06412      0             -147.89669     -12263.213    \n",
            "      2051   262.53127     -148.03442      0             -147.89868     -12962.362    \n",
            "      2052   221.09142     -148.03153      0             -147.91722     -13478.185    \n",
            "      2053   207.8804      -148.03699      0             -147.92951     -13654.906    \n",
            "      2054   228.37335     -148.01952      0             -147.90144     -13715.889    \n",
            "      2055   283.77541     -148.05569      0             -147.90896     -13534.064    \n",
            "      2056   370.28799     -148.11749      0             -147.92603     -13225.5      \n",
            "      2057   479.23734     -148.17358      0             -147.9258      -12686.401    \n",
            "      2058   598.62914     -148.21627      0             -147.90675     -11915.023    \n",
            "      2059   714.59871     -148.27616      0             -147.90669     -10906.983    \n",
            "      2060   813.66998     -148.33689      0             -147.91619     -9740.7552    \n",
            "      2061   885.71196     -148.37499      0             -147.91704     -8184.3652    \n",
            "      2062   924.65962     -148.38897      0             -147.91089     -6552.0303    \n",
            "      2063   929.81438     -148.38085      0             -147.9001      -4854.6528    \n",
            "      2064   905.31959     -148.37606      0             -147.90797     -3131.489     \n",
            "      2065   858.3134      -148.37416      0             -147.93037     -1363.7844    \n",
            "      2066   797.40853     -148.34582      0             -147.93353      296.92449    \n",
            "      2067   730.64419     -148.29947      0             -147.9217       1754.3547    \n",
            "      2068   663.91545     -148.27955      0             -147.93628      3185.8524    \n",
            "      2069   600.37349     -148.24643      0             -147.93602      4158.28      \n",
            "      2070   540.82418     -148.20666      0             -147.92703      4976.8507    \n",
            "      2071   484.68743     -148.18243      0             -147.93183      5605.8022    \n",
            "      2072   430.69932     -148.17255      0             -147.94986      6067.2233    \n",
            "      2073   378.46514     -148.12715      0             -147.93147      6193.5171    \n",
            "      2074   329.12105     -148.12041      0             -147.95024      6181.2153    \n",
            "      2075   286.14233     -148.09166      0             -147.94371      5920.9969    \n",
            "      2076   254.16726     -148.05586      0             -147.92445      5581.8087    \n",
            "      2077   238.47377     -148.08506      0             -147.96176      4957.2817    \n",
            "      2078   243.58021     -148.08304      0             -147.9571       4065.5978    \n",
            "      2079   271.61542     -148.0962       0             -147.95577      2982.4947    \n",
            "      2080   321.42907     -148.1134       0             -147.94721      1830.132     \n",
            "      2081   388.1998      -148.13157      0             -147.93085      531.33225    \n",
            "      2082   463.80732     -148.18614      0             -147.94634     -1211.4339    \n",
            "      2083   538.14825     -148.21875      0             -147.94051     -2708.5671    \n",
            "      2084   600.61833     -148.26533      0             -147.95478     -4152.9273    \n",
            "      2085   642.33539     -148.296        0             -147.96389     -5605.4963    \n",
            "      2086   658.00619     -148.28824      0             -147.94802     -6905.1737    \n",
            "      2087   646.95916     -148.27382      0             -147.93931     -8115.9013    \n",
            "      2088   613.22543     -148.26339      0             -147.94632     -9159.6897    \n",
            "      2089   564.70916     -148.23449      0             -147.94251     -10086.084    \n",
            "      2090   511.37348     -148.2231       0             -147.95869     -10575.636    \n",
            "      2091   462.88021     -148.19711      0             -147.95778     -11216.627    \n",
            "      2092   426.39372     -148.17608      0             -147.95562     -11639.224    \n",
            "      2093   405.09503     -148.17849      0             -147.96904     -11842.535    \n",
            "      2094   397.91132     -148.17599      0             -147.97025     -11893.773    \n",
            "      2095   400.09273     -148.17599      0             -147.96913     -11826.978    \n",
            "      2096   404.9978      -148.15779      0             -147.94839     -11607.266    \n",
            "      2097   406.12863     -148.17941      0             -147.96943     -11090.104    \n",
            "      2098   399.03067     -148.16854      0             -147.96223     -10402.096    \n",
            "      2099   382.2555      -148.17072      0             -147.97308     -9431.8597    \n",
            "      2100   357.83777     -148.15387      0             -147.96885     -8260.706     \n",
            "      2101   330.75597     -148.1466       0             -147.97559     -7052.5053    \n",
            "      2102   307.15115     -148.12809      0             -147.96928     -5750.9605    \n",
            "      2103   292.52649     -148.14797      0             -147.99672     -4269.2889    \n",
            "      2104   289.85958     -148.11011      0             -147.96024     -2858.8527    \n",
            "      2105   298.94956     -148.11767      0             -147.96311     -1528.3883    \n",
            "      2106   316.11421     -148.16352      0             -148.00007     -265.20135    \n",
            "      2107   335.52664     -148.14363      0             -147.97014      1052.6215    \n",
            "      2108   350.54486     -148.14502      0             -147.96377      2317.7436    \n",
            "      2109   355.89562     -148.16996      0             -147.98595      3587.5026    \n",
            "      2110   349.22298     -148.17944      0             -147.99888      4652.9225    \n",
            "      2111   331.62026     -148.17705      0             -148.00559      5695.0663    \n",
            "      2112   307.67458     -148.14452      0             -147.98544      6710.2146    \n",
            "      2113   284.68196     -148.14131      0             -147.99412      7455.3503    \n",
            "      2114   270.5854      -148.14399      0             -148.00408      8030.382     \n",
            "      2115   271.77042     -148.17368      0             -148.03316      8301.9301    \n",
            "      2116   291.88551     -148.14233      0             -147.99142      8450.7869    \n",
            "      2117   330.73701     -148.16703      0             -147.99603      8278.761     \n",
            "      2118   384.2241      -148.18513      0             -147.98647      7771.5143    \n",
            "      2119   444.84847     -148.23033      0             -148.00033      7038.9771    \n",
            "      2120   503.63825     -148.27448      0             -148.01408      6073.82      \n",
            "      2121   552.12131     -148.28553      0             -148.00007      4888.5012    \n",
            "      2122   583.9249      -148.34437      0             -148.04246      3647.301     \n",
            "      2123   595.90365     -148.31818      0             -148.01007      2294.0019    \n",
            "      2124   588.58347     -148.31296      0             -148.00864      832.59022    \n",
            "      2125   565.35253     -148.28415      0             -147.99184     -630.22151    \n",
            "      2126   531.60343     -148.28488      0             -148.01002     -2163.4344    \n",
            "      2127   493.51769     -148.2605       0             -148.00533     -3855.7838    \n",
            "      2128   456.3449      -148.25097      0             -148.01502     -5452.5946    \n",
            "      2129   423.12907     -148.24758      0             -148.0288      -7158.7979    \n",
            "      2130   394.39883     -148.21447      0             -148.01055     -8763.8227    \n",
            "      2131   368.65751     -148.20699      0             -148.01638     -10343.911    \n",
            "      2132   343.16823     -148.21675      0             -148.03932     -11832.819    \n",
            "      2133   315.64973     -148.19419      0             -148.03099     -13048.299    \n",
            "      2134   285.28992     -148.19316      0             -148.04565     -14214.187    \n",
            "      2135   252.98784     -148.14448      0             -148.01367     -14987.811    \n",
            "      2136   222.08743     -148.14517      0             -148.03034     -15664.15     \n",
            "      2137   197.93052     -148.14411      0             -148.04177     -16017.701    \n",
            "      2138   186.49463     -148.12761      0             -148.03118     -16201.01     \n",
            "      2139   193.08026     -148.17654      0             -148.07671     -16327.827    \n",
            "      2140   220.982       -148.13529      0             -148.02103     -16091.46     \n",
            "      2141   270.46976     -148.17334      0             -148.0335      -15783.321    \n",
            "      2142   338.12561     -148.19731      0             -148.02249     -15286.133    \n",
            "      2143   417.65692     -148.24215      0             -148.0262      -14689.156    \n",
            "      2144   500.57292     -148.31284      0             -148.05402     -13867.059    \n",
            "      2145   577.12404     -148.34789      0             -148.04949     -12895.655    \n",
            "      2146   638.91743     -148.37313      0             -148.04278     -11720.486    \n",
            "      2147   679.8311      -148.40488      0             -148.05338     -10349.795    \n",
            "      2148   696.87446     -148.39404      0             -148.03373     -8806.7289    \n",
            "      2149   690.65687     -148.41956      0             -148.06246     -7115.0859    \n",
            "      2150   664.76031     -148.40522      0             -148.06151     -5346.0906    \n",
            "      2151   624.63232     -148.40525      0             -148.08229     -3597.4593    \n",
            "      2152   576.61053     -148.37416      0             -148.07603     -1821.9433    \n",
            "      2153   526.06286     -148.33399      0             -148.06199     -61.152123    \n",
            "      2154   476.58175     -148.34451      0             -148.09809      1344.8036    \n",
            "      2155   429.88893     -148.2973       0             -148.07503      2904.8177    \n",
            "      2156   385.89821     -148.2792       0             -148.07967      4321.879     \n",
            "      2157   343.74246     -148.25413      0             -148.0764       5369.5195    \n",
            "      2158   302.87056     -148.248        0             -148.0914       6356.6139    \n",
            "      2159   263.46427     -148.21269      0             -148.07647      7212.1493    \n",
            "      2160   227.49395     -148.24322      0             -148.12559      7932.668     \n",
            "      2161   198.19627     -148.1976       0             -148.09513      8543.3773    \n",
            "      2162   179.86455     -148.17284      0             -148.07984      8885.2801    \n",
            "      2163   176.90792     -148.19695      0             -148.10548      8979.6755    \n",
            "      2164   192.21024     -148.18002      0             -148.08064      9132.6672    \n",
            "      2165   226.79173     -148.22414      0             -148.10688      8841.6551    \n",
            "      2166   278.56379     -148.26622      0             -148.12219      8362.0343    \n",
            "      2167   342.3315      -148.28666      0             -148.10966      7801.2972    \n",
            "      2168   410.67721     -148.31733      0             -148.10499      6886.3464    \n",
            "      2169   474.73516     -148.35627      0             -148.11082      5963.6015    \n",
            "      2170   526.10008     -148.36335      0             -148.09134      4980.7071    \n",
            "      2171   558.53657     -148.40907      0             -148.12028      4030.1254    \n",
            "      2172   568.95562     -148.42353      0             -148.12935      3048.6252    \n",
            "      2173   557.77512     -148.41981      0             -148.13141      2220.0672    \n",
            "      2174   528.63003     -148.41156      0             -148.13824      1365.9989    \n",
            "      2175   487.83982     -148.40583      0             -148.1536       679.68709    \n",
            "      2176   442.29146     -148.3409       0             -148.11222     -13.921581    \n",
            "      2177   397.87548     -148.31682      0             -148.1111      -584.90317    \n",
            "      2178   358.62982     -148.32142      0             -148.136       -1145.0921    \n",
            "      2179   326.01629     -148.30122      0             -148.13266     -1519.9873    \n",
            "      2180   299.14834     -148.29869      0             -148.14401     -1828.5915    \n",
            "      2181   275.44015     -148.29496      0             -148.15254     -1864.6696    \n",
            "      2182   252.07912     -148.27501      0             -148.14468     -1779.0038    \n",
            "      2183   227.19028     -148.27195      0             -148.15448     -1540.4568    \n",
            "      2184   200.53486     -148.25189      0             -148.1482      -1128.9763    \n",
            "      2185   173.60504     -148.23392      0             -148.14416     -510.31433    \n",
            "      2186   149.87874     -148.18136      0             -148.10387      206.66925    \n",
            "      2187   133.45239     -148.21697      0             -148.14797      1159.7462    \n",
            "      2188   128.05671     -148.21537      0             -148.14916      2066.8975    \n",
            "      2189   135.92671     -148.22576      0             -148.15548      3035.0996    \n",
            "      2190   156.81846     -148.23733      0             -148.15625      4105.4917    \n",
            "      2191   187.98231     -148.25939      0             -148.1622       4998.2509    \n",
            "      2192   224.57777     -148.27422      0             -148.15811      5936.7669    \n",
            "      2193   260.87555     -148.28818      0             -148.1533       7028.4031    \n",
            "      2194   291.39758     -148.29631      0             -148.14564      7984.8571    \n",
            "      2195   312.21788     -148.32913      0             -148.1677       8926.7948    \n",
            "      2196   321.62111     -148.32723      0             -148.16094      9781.3613    \n",
            "      2197   320.2835      -148.35791      0             -148.19231      10629.529    \n",
            "      2198   311.40009     -148.32389      0             -148.16288      11331.566    \n",
            "      2199   299.36686     -148.3095       0             -148.15472      11955.704    \n",
            "      2200   288.67803     -148.31975      0             -148.17049      12270.225    \n",
            "      2201   282.73406     -148.33724      0             -148.19106      12479.863    \n",
            "      2202   283.04352     -148.33617      0             -148.18983      12400.228    \n",
            "      2203   288.74788     -148.33679      0             -148.18749      12175.623    \n",
            "      2204   297.19579     -148.35265      0             -148.19899      11693.242    \n",
            "      2205   304.76037     -148.31009      0             -148.15252      11144.727    \n",
            "      2206   307.56563     -148.35108      0             -148.19206      10308.483    \n",
            "      2207   302.68774     -148.33987      0             -148.18337      9413.8458    \n",
            "      2208   289.16655     -148.32741      0             -148.1779       8289.5998    \n",
            "      2209   267.8184      -148.31332      0             -148.17485      7093.4479    \n",
            "      2210   241.19492     -148.30002      0             -148.17532      5941.7451    \n",
            "      2211   213.1693      -148.29632      0             -148.1861       4658.9097    \n",
            "      2212   187.72674     -148.30023      0             -148.20316      3328.9752    \n",
            "      2213   168.37125     -148.27755      0             -148.1905       1922.9711    \n",
            "      2214   157.12074     -148.27512      0             -148.19388      611.74083    \n",
            "      2215   154.25931     -148.27594      0             -148.19618     -694.15912    \n",
            "      2216   158.33795     -148.27961      0             -148.19774     -1993.5006    \n",
            "      2217   166.65911     -148.30219      0             -148.21602     -3211.2955    \n",
            "      2218   176.49524     -148.26791      0             -148.17666     -4258.7565    \n",
            "      2219   185.53061     -148.29967      0             -148.20375     -5388.6018    \n",
            "      2220   192.24237     -148.28649      0             -148.18709     -6106.8732    \n",
            "      2221   197.004       -148.31076      0             -148.2089      -6584.6329    \n",
            "      2222   201.49004     -148.28672      0             -148.18254     -6956.4347    \n",
            "      2223   208.16124     -148.30219      0             -148.19456     -7254.8696    \n",
            "      2224   219.67442     -148.31382      0             -148.20024     -7200.935     \n",
            "      2225   237.95813     -148.31298      0             -148.18994     -7129.0539    \n",
            "      2226   263.53352     -148.32286      0             -148.1866      -6773.9598    \n",
            "      2227   295.0942      -148.36884      0             -148.21627     -6418.2691    \n",
            "      2228   329.38565     -148.38446      0             -148.21415     -5800.1628    \n",
            "      2229   362.00289     -148.39683      0             -148.20966     -5223.4729    \n",
            "      2230   388.19763     -148.41251      0             -148.21179     -4422.9703    \n",
            "      2231   403.57167     -148.40878      0             -148.20012     -3399.2193    \n",
            "      2232   405.32033     -148.41442      0             -148.20485     -2373.2351    \n",
            "      2233   392.71988     -148.40376      0             -148.20071     -1252.4292    \n",
            "      2234   367.25252     -148.40538      0             -148.21549      60.103321    \n",
            "      2235   332.29146     -148.40123      0             -148.22942      1339.2941    \n",
            "      2236   292.60334     -148.38367      0             -148.23239      2465.3987    \n",
            "      2237   252.98123     -148.35706      0             -148.22626      3687.0168    \n",
            "      2238   217.60109     -148.33245      0             -148.21994      4815.7313    \n",
            "      2239   189.3042      -148.33492      0             -148.23704      5903.9454    \n",
            "      2240   169.1143      -148.33015      0             -148.24271      6786.2459    \n",
            "      2241   156.61438     -148.29723      0             -148.21626      7564.4673    \n",
            "      2242   150.37832     -148.31335      0             -148.2356       8173.9644    \n",
            "      2243   148.3364      -148.31069      0             -148.234        8723.6425    \n",
            "      2244   149.02975     -148.31281      0             -148.23576      9125.7358    \n",
            "      2245   152.04299     -148.31369      0             -148.23508      9394.0093    \n",
            "      2246   157.61044     -148.31164      0             -148.23015      9572.5995    \n",
            "      2247   166.90979     -148.32866      0             -148.24236      9492.8505    \n",
            "      2248   181.6855      -148.32         0             -148.22607      9376.8246    \n",
            "      2249   203.31563     -148.32959      0             -148.22447      9068.4421    \n",
            "      2250   231.95634     -148.37247      0             -148.25254      8605.0208    \n",
            "      2251   266.222       -148.37601      0             -148.23836      7956.5764    \n",
            "      2252   303.17903     -148.40148      0             -148.24472      7126.8654    \n",
            "      2253   338.44269     -148.4123       0             -148.23731      6470.0425    \n",
            "      2254   367.05244     -148.41311      0             -148.22333      5643.9836    \n",
            "      2255   384.86026     -148.43284      0             -148.23385      4684.5407    \n",
            "      2256   388.8524      -148.4251       0             -148.22405      3591.6329    \n",
            "      2257   378.04316     -148.43614      0             -148.24068      2724.7071    \n",
            "      2258   353.86664     -148.42446      0             -148.24149      1847.015     \n",
            "      2259   319.44587     -148.4127       0             -148.24754      890.87056    \n",
            "      2260   279.35386     -148.39547      0             -148.25103      143.31276    \n",
            "      2261   238.49874     -148.38567      0             -148.26235     -661.81415    \n",
            "      2262   201.09895     -148.36712      0             -148.26314     -1262.1083    \n",
            "      2263   169.96131     -148.34216      0             -148.25429     -1877.0588    \n",
            "      2264   146.04214     -148.34605      0             -148.27054     -2670.5373    \n",
            "      2265   128.78919     -148.31267      0             -148.24608     -3150.1846    \n",
            "      2266   116.6694      -148.3075       0             -148.24718     -3494.9969    \n",
            "      2267   107.82845     -148.31149      0             -148.25574     -3827.3074    \n",
            "      2268   100.67687     -148.31682      0             -148.26477     -4135.3541    \n",
            "      2269   94.672243     -148.32812      0             -148.27917     -4158.8088    \n",
            "      2270   90.361846     -148.30272      0             -148.256       -4130.8183    \n",
            "      2271   89.335328     -148.28348      0             -148.23729     -3989.5107    \n",
            "      2272   93.666424     -148.29706      0             -148.24863     -3782.1342    \n",
            "      2273   105.05347     -148.33033      0             -148.27601     -3550.8725    \n",
            "      2274   124.19391     -148.36805      0             -148.30383     -3427.9947    \n",
            "      2275   150.44814     -148.34013      0             -148.26234     -3093.3812    \n",
            "      2276   181.57176     -148.37378      0             -148.2799      -2708.0905    \n",
            "      2277   214.18156     -148.37845      0             -148.26771     -2407.8811    \n",
            "      2278   244.13971     -148.38484      0             -148.25861     -2021.2036    \n",
            "      2279   267.3973      -148.41908      0             -148.28083     -1798.101     \n",
            "      2280   281.15908     -148.41225      0             -148.26688     -1383.7048    \n",
            "      2281   284.08811     -148.4043       0             -148.25742     -950.01251    \n",
            "      2282   276.51833     -148.41202      0             -148.26905     -749.23665    \n",
            "      2283   260.25601     -148.39715      0             -148.26259     -381.80152    \n",
            "      2284   238.37463     -148.40826      0             -148.28501     -181.65439    \n",
            "      2285   214.42468     -148.39142      0             -148.28056      17.729575    \n",
            "      2286   191.29311     -148.3588       0             -148.25989      181.71442    \n",
            "      2287   170.98893     -148.38173      0             -148.29332     -5.2777684    \n",
            "      2288   154.45219     -148.33515      0             -148.25529      45.497582    \n",
            "      2289   141.4647      -148.34162      0             -148.26847     -324.88277    \n",
            "      2290   130.86321     -148.32385      0             -148.25619     -570.44163    \n",
            "      2291   121.13646     -148.3218       0             -148.25917     -1002.9469    \n",
            "      2292   110.91169     -148.33474      0             -148.27739     -1388.8601    \n",
            "      2293   99.774042     -148.32277      0             -148.27118     -1949.7911    \n",
            "      2294   87.971189     -148.32825      0             -148.28277     -2509.3219    \n",
            "      2295   76.761134     -148.32814      0             -148.28845     -3027.6122    \n",
            "      2296   68.227729     -148.28802      0             -148.25275     -3627.4716    \n",
            "      2297   64.202331     -148.31809      0             -148.28489     -4307.0907    \n",
            "      2298   66.362129     -148.30784      0             -148.27352     -5023.3369    \n",
            "      2299   75.493618     -148.3343       0             -148.29527     -5723.0492    \n",
            "      2300   91.273341     -148.33008      0             -148.28289     -6362.957     \n",
            "      2301   112.07423     -148.32236      0             -148.26441     -7084.0591    \n",
            "      2302   135.18717     -148.36512      0             -148.29522     -7784.194     \n",
            "      2303   157.75361     -148.38091      0             -148.29934     -8380.1966    \n",
            "      2304   177.027       -148.37727      0             -148.28574     -8891.8425    \n",
            "      2305   190.95958     -148.35679      0             -148.25806     -9243.1882    \n",
            "      2306   198.79999     -148.41054      0             -148.30775     -9675.2418    \n",
            "      2307   200.84632     -148.37981      0             -148.27596     -9763.2055    \n",
            "      2308   198.80055     -148.37786      0             -148.27507     -9802.4617    \n",
            "      2309   194.95649     -148.37948      0             -148.27868     -9824.5542    \n",
            "      2310   191.50432     -148.3714       0             -148.27238     -9683.5018    \n",
            "      2311   190.2792      -148.37443      0             -148.27605     -9560.7494    \n",
            "      2312   191.91352     -148.39993      0             -148.30071     -9263.6609    \n",
            "      2313   195.94443     -148.39846      0             -148.29715     -8822.98      \n",
            "      2314   200.80128     -148.38406      0             -148.28024     -8337.6269    \n",
            "      2315   204.17181     -148.37954      0             -148.27398     -7806.679     \n",
            "      2316   203.9044      -148.37608      0             -148.27065     -7145.0425    \n",
            "      2317   198.30153     -148.39396      0             -148.29143     -6466.592     \n",
            "      2318   186.7361      -148.39971      0             -148.30316     -5627.9942    \n",
            "      2319   170.19634     -148.39585      0             -148.30785     -4711.0196    \n",
            "      2320   150.61981     -148.34706      0             -148.26918     -3619.4806    \n",
            "      2321   130.76834     -148.36722      0             -148.29961     -2854.1848    \n",
            "      2322   113.68155     -148.32547      0             -148.2667      -1793.4223    \n",
            "      2323   101.74583     -148.32935      0             -148.27674     -907.28857    \n",
            "      2324   96.44764      -148.32872      0             -148.27885      16.564143    \n",
            "      2325   97.905016     -148.33008      0             -148.27946      819.95295    \n",
            "      2326   105.07975     -148.35263      0             -148.2983       1629.5837    \n",
            "      2327   116.00851     -148.34521      0             -148.28523      2443.4397    \n",
            "      2328   128.49139     -148.36609      0             -148.29965      2944.8194    \n",
            "      2329   140.43711     -148.38203      0             -148.30941      3460.7548    \n",
            "      2330   150.70339     -148.37429      0             -148.29638      4095.6493    \n",
            "      2331   159.15153     -148.37432      0             -148.29203      4465.3266    \n",
            "      2332   166.73892     -148.38744      0             -148.30123      4890.13      \n",
            "      2333   174.92675     -148.38289      0             -148.29245      5204.0088    \n",
            "      2334   185.18811     -148.41164      0             -148.31589      5276.0515    \n",
            "      2335   198.48678     -148.3781       0             -148.27547      5290.644     \n",
            "      2336   214.97125     -148.38945      0             -148.2783       5132.6134    \n",
            "      2337   233.48783     -148.41207      0             -148.29134      4930.6529    \n",
            "      2338   251.68839     -148.43058      0             -148.30045      4485.8331    \n",
            "      2339   266.72933     -148.43794      0             -148.30003      4090.7313    \n",
            "      2340   275.76962     -148.41425      0             -148.27166      3537.6803    \n",
            "      2341   276.49787     -148.44613      0             -148.30316      2875.4998    \n",
            "      2342   267.98832     -148.43314      0             -148.29458      2222.4851    \n",
            "      2343   250.7839      -148.41697      0             -148.2873       1679.5681    \n",
            "      2344   226.76963     -148.41988      0             -148.30263      1089.9669    \n",
            "      2345   199.06252     -148.40624      0             -148.30332      263.12435    \n",
            "      2346   170.92018     -148.38481      0             -148.29644     -294.35589    \n",
            "      2347   145.46399     -148.35044      0             -148.27523     -972.49342    \n",
            "      2348   125.00483     -148.35773      0             -148.29309     -1636.2468    \n",
            "      2349   110.64615     -148.34842      0             -148.29121     -2226.4181    \n",
            "      2350   102.32571     -148.32715      0             -148.27424     -2805.0687    \n",
            "      2351   98.924804     -148.34373      0             -148.29258     -3335.5928    \n",
            "      2352   98.636957     -148.36635      0             -148.31535     -3874.2307    \n",
            "      2353   100.17558     -148.3313       0             -148.2795      -4316.1939    \n",
            "      2354   102.68945     -148.35927      0             -148.30618     -4586.5925    \n",
            "      2355   106.06971     -148.33609      0             -148.28124     -4802.8215    \n",
            "      2356   111.10214     -148.34552      0             -148.28807     -4872.9915    \n",
            "      2357   119.10059     -148.36116      0             -148.29958     -4783.5413    \n",
            "      2358   131.49523     -148.3813       0             -148.31331     -4638.9838    \n",
            "      2359   149.04595     -148.38452      0             -148.30746     -4485.7833    \n",
            "      2360   171.54708     -148.38969      0             -148.30099     -4301.0936    \n",
            "      2361   197.68021     -148.38578      0             -148.28357     -3978.9077    \n",
            "      2362   224.94371     -148.44401      0             -148.32771     -3771.0846    \n",
            "      2363   249.97707     -148.42218      0             -148.29294     -3332.213     \n",
            "      2364   269.2869      -148.40684      0             -148.2676      -2851.1047    \n",
            "      2365   280.04897     -148.41976      0             -148.27497     -2284.5829    \n",
            "      2366   280.72813     -148.43411      0             -148.28897     -1722.956     \n",
            "      2367   270.852       -148.44519      0             -148.30515     -1141.9567    \n",
            "      2368   251.45889     -148.43705      0             -148.30703     -496.17821    \n",
            "      2369   224.98428     -148.42628      0             -148.30996      186.27163    \n",
            "      2370   194.63791     -148.40163      0             -148.30099      809.0467     \n",
            "      2371   163.69157     -148.37937      0             -148.29473      1401.81      \n",
            "      2372   135.03338     -148.38031      0             -148.31049      1899.8564    \n",
            "      2373   110.72151     -148.36049      0             -148.30324      2273.3051    \n",
            "      2374   91.556355     -148.34696      0             -148.29962      2734.7341    \n",
            "      2375   77.420003     -148.34796      0             -148.30793      2947.4025    \n",
            "      2376   67.579593     -148.33195      0             -148.29701      3146.1012    \n",
            "      2377   60.902279     -148.3195       0             -148.28801      3326.8083    \n",
            "      2378   56.667127     -148.33064      0             -148.30134      3388.9271    \n",
            "      2379   54.66135      -148.31957      0             -148.29131      3301.7961    \n",
            "      2380   55.239085     -148.31839      0             -148.28983      3298.7801    \n",
            "      2381   59.399738     -148.33195      0             -148.30124      3043.7833    \n",
            "      2382   68.251407     -148.31213      0             -148.27684      2875.9734    \n",
            "      2383   82.63838      -148.33921      0             -148.29648      2709.9943    \n",
            "      2384   103.00861     -148.34251      0             -148.28925      2287.5369    \n",
            "      2385   128.71889     -148.36773      0             -148.30118      1886.7801    \n",
            "      2386   157.79681     -148.36694      0             -148.28535      1481.0014    \n",
            "      2387   187.42193     -148.40129      0             -148.30438      963.75095    \n",
            "      2388   214.44963     -148.38775      0             -148.27687      510.98248    \n",
            "      2389   235.61372     -148.41508      0             -148.29326      58.781476    \n",
            "      2390   248.11852     -148.40963      0             -148.28134     -332.21598    \n",
            "      2391   250.84729     -148.4359       0             -148.3062      -601.49179    \n",
            "      2392   244.01664     -148.4461       0             -148.31993     -950.45233    \n",
            "      2393   229.17645     -148.43623      0             -148.31774     -1078.3042    \n",
            "      2394   209.15352     -148.40907      0             -148.30093     -1066.5521    \n",
            "      2395   187.14305     -148.39739      0             -148.30063     -1138.8626    \n",
            "      2396   166.19288     -148.41762      0             -148.33169     -1188.2793    \n",
            "      2397   148.40687     -148.35553      0             -148.2788      -1001.4139    \n",
            "      2398   134.76543     -148.35007      0             -148.28039     -704.89915    \n",
            "      2399   125.06379     -148.35067      0             -148.286       -493.84613    \n",
            "      2400   118.00205     -148.36145      0             -148.30044     -176.9493     \n",
            "      2401   111.98515     -148.35823      0             -148.30033      376.20132    \n",
            "      2402   105.69717     -148.35456      0             -148.29991      776.08636    \n",
            "      2403   98.524744     -148.34866      0             -148.29772      1435.9569    \n",
            "      2404   90.756442     -148.34289      0             -148.29597      2115.392     \n",
            "      2405   83.654062     -148.33147      0             -148.28822      2940.8015    \n",
            "      2406   79.274285     -148.33314      0             -148.29215      3625.8638    \n",
            "      2407   79.812698     -148.30207      0             -148.2608       4584.8557    \n",
            "      2408   86.874258     -148.31004      0             -148.26513      5487.7196    \n",
            "      2409   101.10449     -148.33713      0             -148.28486      6295.7637    \n",
            "      2410   121.9186      -148.3444       0             -148.28136      6917.4269    \n",
            "      2411   147.22891     -148.35971      0             -148.28359      7704.8637    \n",
            "      2412   174.0439      -148.36022      0             -148.27023      8516.2486    \n",
            "      2413   199.10861     -148.38809      0             -148.28514      9068.4398    \n",
            "      2414   219.48845     -148.40176      0             -148.28828      9496.4762    \n",
            "      2415   233.5523      -148.40572      0             -148.28496      9994.9471    \n",
            "      2416   240.69775     -148.39457      0             -148.27012      10412.177    \n",
            "      2417   241.96241     -148.40578      0             -148.28067      10625.974    \n",
            "      2418   239.29508     -148.42239      0             -148.29867      10741.455    \n",
            "      2419   234.90322     -148.40552      0             -148.28406      10814.609    \n",
            "      2420   231.17357     -148.42207      0             -148.30255      10672.482    \n",
            "      2421   229.42695     -148.42434      0             -148.30572      10443.862    \n",
            "      2422   229.76927     -148.39766      0             -148.27886      9965.6547    \n",
            "      2423   231.10278     -148.39127      0             -148.27178      9477.3529    \n",
            "      2424   231.56186     -148.39102      0             -148.27129      8896.9749    \n",
            "      2425   228.97867     -148.40589      0             -148.2875       8079.7836    \n",
            "      2426   221.66469     -148.3792       0             -148.26459      7307.5352    \n",
            "      2427   209.05209     -148.36622      0             -148.25813      6472.7858    \n",
            "      2428   191.58425     -148.37917      0             -148.28011      5415.2944    \n",
            "      2429   171.14015     -148.35305      0             -148.26456      4536.2704    \n",
            "      2430   150.68422     -148.34334      0             -148.26543      3584.0734    \n",
            "      2431   133.46115     -148.3571       0             -148.2881       2543.9863    \n",
            "      2432   122.14616     -148.32776      0             -148.2646       1493.8609    \n",
            "      2433   118.45276     -148.33841      0             -148.27717      496.46816    \n",
            "      2434   122.80396     -148.331        0             -148.2675      -523.77474    \n",
            "      2435   134.17634     -148.35848      0             -148.2891      -1498.4744    \n",
            "      2436   150.29325     -148.34725      0             -148.26954     -2372.3083    \n",
            "      2437   168.51008     -148.36588      0             -148.27876     -3302.8602    \n",
            "      2438   186.50623     -148.35849      0             -148.26206     -4036.3166    \n",
            "      2439   202.48648     -148.37553      0             -148.27083     -4680.567     \n",
            "      2440   215.39337     -148.38922      0             -148.27785     -5258.054     \n",
            "      2441   225.47255     -148.39839      0             -148.28181     -5612.5087    \n",
            "      2442   234.11193     -148.37432      0             -148.25328     -5875.0427    \n",
            "      2443   242.8133      -148.37825      0             -148.25271     -5991.6169    \n",
            "      2444   252.95078     -148.40457      0             -148.27378     -6138.2606    \n",
            "      2445   265.21334     -148.39842      0             -148.26129     -6038.6181    \n",
            "      2446   279.02754     -148.42846      0             -148.28419     -6023.9256    \n",
            "      2447   292.65575     -148.40177      0             -148.25045     -5772.5464    \n",
            "      2448   303.28167     -148.42485      0             -148.26804     -5512.6411    \n",
            "      2449   308.14245     -148.39158      0             -148.23226     -5161.4431    \n",
            "      2450   304.75877     -148.41508      0             -148.25751     -4724.4723    \n",
            "      2451   291.5755      -148.42471      0             -148.27396     -4166.5593    \n",
            "      2452   268.81038     -148.38084      0             -148.24185     -3512.4856    \n",
            "      2453   238.09372     -148.39581      0             -148.2727      -2958.7204    \n",
            "      2454   202.53422     -148.36968      0             -148.26496     -2333.4763    \n",
            "      2455   166.0464      -148.33979      0             -148.25394     -1689.9749    \n",
            "      2456   132.64316     -148.32543      0             -148.25685     -972.19334    \n",
            "      2457   105.84476     -148.2775       0             -148.22277     -398.31027    \n",
            "      2458   88.017068     -148.31515      0             -148.26964      19.122596    \n",
            "      2459   79.99645      -148.30775      0             -148.26639      450.04623    \n",
            "      2460   81.122015     -148.28767      0             -148.24573      772.16544    \n",
            "      2461   89.622281     -148.29657      0             -148.25023      1104.6044    \n",
            "      2462   103.20841     -148.29835      0             -148.24499      1249.769     \n",
            "      2463   119.61765     -148.33336      0             -148.27151      1354.6285    \n",
            "      2464   137.28318     -148.31492      0             -148.24394      1581.9684    \n",
            "      2465   155.24681     -148.33296      0             -148.25269      1459.9887    \n",
            "      2466   173.64451     -148.33966      0             -148.24988      1465.2457    \n",
            "      2467   193.23688     -148.36781      0             -148.2679       1440.4023    \n",
            "      2468   214.96309     -148.37748      0             -148.26634      1160.413     \n",
            "      2469   239.26759     -148.35311      0             -148.2294       1006.2706    \n",
            "      2470   265.60338     -148.36627      0             -148.22894      633.93109    \n",
            "      2471   292.38556     -148.4057       0             -148.25453      91.140132    \n",
            "      2472   316.84557     -148.41016      0             -148.24633     -432.43365    \n",
            "      2473   335.63357     -148.40605      0             -148.23251     -911.86479    \n",
            "      2474   345.41031     -148.41212      0             -148.23352     -1399.7329    \n",
            "      2475   343.40378     -148.408        0             -148.23044     -1914.4245    \n",
            "      2476   328.37776     -148.3994       0             -148.22961     -2406.3692    \n",
            "      2477   301.2189      -148.3925       0             -148.23676     -2689.8272    \n",
            "      2478   264.53982     -148.37392      0             -148.23714     -3053.7136    \n",
            "      2479   222.4772      -148.31804      0             -148.20301     -3442.1879    \n",
            "      2480   179.98315     -148.33852      0             -148.24547     -3729.5733    \n",
            "      2481   141.5174      -148.32342      0             -148.25025     -3864.2596    \n",
            "      2482   110.62555     -148.30726      0             -148.25006     -4072.2487    \n",
            "      2483   89.244928     -148.27915      0             -148.23301     -4084.4481    \n",
            "      2484   77.415518     -148.27308      0             -148.23305     -4271.867     \n",
            "      2485   73.582325     -148.25911      0             -148.22107     -4149.1799    \n",
            "      2486   75.499085     -148.27986      0             -148.24082     -4098.6967    \n",
            "      2487   80.684714     -148.25708      0             -148.21536     -3990.8056    \n",
            "      2488   87.462137     -148.29898      0             -148.25376     -3703.6869    \n",
            "      2489   95.216062     -148.28331      0             -148.23408     -3306.9177    \n",
            "      2490   104.78598     -148.28409      0             -148.22991     -3043.5326    \n",
            "      2491   118.09494     -148.30301      0             -148.24195     -2586.3491    \n",
            "      2492   136.8747      -148.30272      0             -148.23195     -2136.5167    \n",
            "      2493   162.6529      -148.3112       0             -148.22711     -1631.8557    \n",
            "      2494   195.71471     -148.33087      0             -148.22967     -1100.6484    \n",
            "      2495   234.52566     -148.34043      0             -148.21917     -565.5037     \n",
            "      2496   275.80777     -148.36793      0             -148.22533     -111.04406    \n",
            "      2497   314.9296      -148.38621      0             -148.22338      312.1854     \n",
            "      2498   347.24515     -148.40146      0             -148.22192      807.55402    \n",
            "      2499   368.72382     -148.40096      0             -148.21032      1213.2104    \n",
            "      2500   376.69013     -148.42587      0             -148.23111      1580.1208    \n",
            "      2501   370.77341     -148.41184      0             -148.22013      2053.803     \n",
            "      2502   352.98665     -148.39198      0             -148.20947      2390.2501    \n",
            "      2503   326.88016     -148.37342      0             -148.20441      2579.7202    \n",
            "      2504   296.76707     -148.36578      0             -148.21234      2751.4465    \n",
            "      2505   267.16868     -148.33836      0             -148.20022      2831.9347    \n",
            "      2506   241.48999     -148.32349      0             -148.19863      2815.0594    \n",
            "      2507   221.50841     -148.32685      0             -148.21232      2455.7129    \n",
            "      2508   207.298       -148.26588      0             -148.1587       2242.7437    \n",
            "      2509   197.76105     -148.31224      0             -148.20999      1589.4349    \n",
            "      2510   190.68675     -148.30121      0             -148.20262      1050.4853    \n",
            "      2511   183.90549     -148.2731       0             -148.17801      450.82899    \n",
            "      2512   176.39261     -148.29776      0             -148.20656     -352.85371    \n",
            "      2513   168.03643     -148.28921      0             -148.20233     -1095.5849    \n",
            "      2514   160.25908     -148.27464      0             -148.19178     -1883.2565    \n",
            "      2515   155.42163     -148.26151      0             -148.18115     -2683.3034    \n",
            "      2516   156.29664     -148.2919       0             -148.21109     -3619.185     \n",
            "      2517   165.38727     -148.26727      0             -148.18175     -4531.6932    \n",
            "      2518   183.87663     -148.27213      0             -148.17706     -5567.5643    \n",
            "      2519   211.39533     -148.30439      0             -148.19509     -6659.5777    \n",
            "      2520   245.86565     -148.33023      0             -148.2031      -7555.58      \n",
            "      2521   283.646       -148.32655      0             -148.1799      -8458.1139    \n",
            "      2522   320.20427     -148.379        0             -148.21344     -9484.4094    \n",
            "      2523   351.38162     -148.35029      0             -148.16862     -10305.666    \n",
            "      2524   373.98474     -148.34381      0             -148.15045     -10893.463    \n",
            "      2525   386.3292      -148.3841       0             -148.18436     -11716.816    \n",
            "      2526   388.62503     -148.4051       0             -148.20416     -11993.798    \n",
            "      2527   383.15521     -148.37259      0             -148.17448     -12364.11     \n",
            "      2528   373.17431     -148.34922      0             -148.15627     -12466.194    \n",
            "      2529   362.36795     -148.34245      0             -148.15509     -12384.839    \n",
            "      2530   353.89536     -148.35773      0             -148.17475     -12513.902    \n",
            "      2531   349.33876     -148.35246      0             -148.17183     -12343.3      \n",
            "      2532   348.59183     -148.32282      0             -148.14259     -11853.843    \n",
            "      2533   349.86851     -148.34619      0             -148.16529     -11423.924    \n",
            "      2534   350.07313     -148.33345      0             -148.15245     -10967.891    \n",
            "      2535   345.9561      -148.33018      0             -148.15131     -10276.305    \n",
            "      2536   334.98847     -148.32856      0             -148.15536     -9270.2248    \n",
            "      2537   316.09517     -148.30192      0             -148.13848     -8305.1275    \n",
            "      2538   289.82538     -148.33904      0             -148.18919     -7192.714     \n",
            "      2539   259.00026     -148.29989      0             -148.16598     -6014.0309    \n",
            "      2540   227.77641     -148.26178      0             -148.14401     -4690.6395    \n",
            "      2541   200.94115     -148.25444      0             -148.15054     -3467.7852    \n",
            "      2542   182.80528     -148.22871      0             -148.13419     -2031.0363    \n",
            "      2543   176.34443     -148.2243       0             -148.13312     -749.68825    \n",
            "      2544   182.68392     -148.24691      0             -148.15246      526.18255    \n",
            "      2545   200.716       -148.24879      0             -148.14501      1649.7071    \n",
            "      2546   227.37195     -148.27124      0             -148.15368      2826.7766    \n",
            "      2547   258.60887     -148.29696      0             -148.16325      3806.9978    \n",
            "      2548   290.1655      -148.28932      0             -148.13929      4936.7168    \n",
            "      2549   318.5353      -148.30836      0             -148.14367      5835.001     \n",
            "      2550   342.03477     -148.33504      0             -148.15819      6682.4018    \n",
            "      2551   360.42052     -148.31378      0             -148.12743      7358.3611    \n",
            "      2552   375.44925     -148.31679      0             -148.12267      7997.2467    \n",
            "      2553   389.76066     -148.31288      0             -148.11136      8485.594     \n",
            "      2554   405.74434     -148.32447      0             -148.11468      8685.6141    \n",
            "      2555   424.67933     -148.35087      0             -148.13129      8757.7664    \n",
            "      2556   445.89894     -148.37217      0             -148.14162      8649.2489    \n",
            "      2557   466.7131      -148.3581       0             -148.11679      8505.2601    \n",
            "      2558   482.7176      -148.35271      0             -148.10313      8092.2852    \n",
            "      2559   488.75503     -148.35494      0             -148.10223      7645.1926    \n",
            "      2560   480.49244     -148.37186      0             -148.12343      6989.5908    \n",
            "      2561   455.3964      -148.3338       0             -148.09834      6443.881     \n",
            "      2562   413.49389     -148.27692      0             -148.06313      5930.6441    \n",
            "      2563   358.20294     -148.28882      0             -148.10362      5220.1264    \n",
            "      2564   295.32279     -148.25649      0             -148.1038       4667.7096    \n",
            "      2565   232.1731      -148.24226      0             -148.12222      4055.42      \n",
            "      2566   176.18737     -148.1881       0             -148.097        3523.7797    \n",
            "      2567   133.30685     -148.18654      0             -148.11761      2907.5562    \n",
            "      2568   106.97864     -148.15853      0             -148.10321      2393.7895    \n",
            "      2569   97.629873     -148.1479       0             -148.09743      1890.5894    \n",
            "      2570   103.12202     -148.15701      0             -148.10369      1294.9142    \n",
            "      2571   119.34913     -148.18153      0             -148.11982      966.7348     \n",
            "      2572   141.81959     -148.16669      0             -148.09336      585.03418    \n",
            "      2573   167.16955     -148.18238      0             -148.09595      548.11246    \n",
            "      2574   193.87779     -148.17673      0             -148.07649      584.77054    \n",
            "      2575   222.3931      -148.2244       0             -148.10942      624.69762    \n",
            "      2576   254.61199     -148.24241      0             -148.11077      959.13324    \n",
            "      2577   292.91338     -148.23581      0             -148.08436      1390.2799    \n",
            "      2578   339.35258     -148.27204      0             -148.09658      1930.0712    \n",
            "      2579   393.83495     -148.27046      0             -148.06684      2543.7748    \n",
            "      2580   453.68067     -148.30617      0             -148.0716       3208.9405    \n",
            "      2581   513.59296     -148.35057      0             -148.08502      3880.2647    \n",
            "      2582   566.47461     -148.36378      0             -148.07089      4544.6319    \n",
            "      2583   604.52452     -148.40797      0             -148.09541      5344.0731    \n",
            "      2584   621.12403     -148.37968      0             -148.05854      6297.0933    \n",
            "      2585   612.52363     -148.40102      0             -148.08432      7102.6378    \n",
            "      2586   578.51239     -148.38673      0             -148.08761      8009.4529    \n",
            "      2587   522.58247     -148.34139      0             -148.07119      9009.2661    \n",
            "      2588   451.86959     -148.30075      0             -148.06711      9687.3459    \n",
            "      2589   375.18169     -148.26037      0             -148.06638      10385.223    \n",
            "      2590   301.4299      -148.21742      0             -148.06157      10911.461    \n",
            "      2591   238.34773     -148.18077      0             -148.05754      11158.818    \n",
            "      2592   191.11757     -148.13793      0             -148.03912      11411.342    \n",
            "      2593   161.88247     -148.14983      0             -148.06613      11150.168    \n",
            "      2594   150.16536     -148.15141      0             -148.07377      10981.293    \n",
            "      2595   153.07036     -148.1301       0             -148.05096      10482.885    \n",
            "      2596   166.9711      -148.10856      0             -148.02223      9999.3459    \n",
            "      2597   188.63872     -148.1398       0             -148.04226      9172.7305    \n",
            "      2598   215.37759     -148.15169      0             -148.04033      8426.7425    \n",
            "      2599   246.13343     -148.16685      0             -148.03958      7395.5686    \n",
            "      2600   281.1933      -148.19606      0             -148.05067      6260.6916    \n",
            "      2601   321.71317     -148.20794      0             -148.0416       5161.254     \n",
            "      2602   369.09953     -148.22878      0             -148.03794      3766.4112    \n",
            "      2603   423.52229     -148.26967      0             -148.05069      2301.6942    \n",
            "      2604   483.01948     -148.29966      0             -148.04992      882.56245    \n",
            "      2605   543.83669     -148.31955      0             -148.03837     -544.41203    \n",
            "      2606   600.36779     -148.34567      0             -148.03525     -2089.7912    \n",
            "      2607   645.92772     -148.37468      0             -148.04071     -3527.9634    \n",
            "      2608   674.40211     -148.35115      0             -148.00246     -4768.9587    \n",
            "      2609   681.46736     -148.38722      0             -148.03487     -6070.416     \n",
            "      2610   665.6247      -148.37228      0             -148.02813     -6846.647     \n",
            "      2611   628.58911     -148.33806      0             -148.01305     -7592.8567    \n",
            "      2612   575.72863     -148.31395      0             -148.01628     -8126.5682    \n",
            "      2613   514.56669     -148.29016      0             -148.02411     -8332.5327    \n",
            "      2614   453.49393     -148.24829      0             -148.01382     -8323.9625    \n",
            "      2615   400.51734     -148.20685      0             -147.99977     -8223.2649    \n",
            "      2616   361.23085     -148.19838      0             -148.0116      -7860.046     \n",
            "      2617   338.04213     -148.21791      0             -148.04313     -7495.4563    \n",
            "      2618   330.29553     -148.19408      0             -148.02331     -6776.6459    \n",
            "      2619   334.66855     -148.18735      0             -148.01431     -6010.9542    \n",
            "      2620   345.79979     -148.18542      0             -148.00663     -4857.3147    \n",
            "      2621   358.17027     -148.19163      0             -148.00644     -3717.3467    \n",
            "      2622   367.7918      -148.19361      0             -148.00344     -2408.1394    \n",
            "      2623   372.70665     -148.19855      0             -148.00585     -841.83294    \n",
            "      2624   373.76653     -148.2241       0             -148.03084      920.0429     \n",
            "      2625   374.2045      -148.19962      0             -148.00614      2566.5794    \n",
            "      2626   378.59529     -148.21486      0             -148.01911      4361.9683    \n",
            "      2627   391.56023     -148.20385      0             -148.0014       6251.681     \n",
            "      2628   416.06883     -148.19233      0             -147.97721      7912.7819    \n",
            "      2629   452.43589     -148.2297       0             -147.99577      9578.9427    \n",
            "      2630   498.00343     -148.26328      0             -148.00579      11071.916    \n",
            "      2631   547.23709     -148.28726      0             -148.00432      12359.703    \n",
            "      2632   593.00742     -148.28842      0             -147.98181      13560.347    \n",
            "      2633   628.3858      -148.29825      0             -147.97334      14605.88     \n",
            "      2634   648.30156     -148.33096      0             -147.99576      15422.882    \n",
            "      2635   650.23569     -148.31123      0             -147.97503      16163.648    \n",
            "      2636   635.99407     -148.31674      0             -147.98791      16535.113    \n",
            "      2637   610.73589     -148.28865      0             -147.97288      16763.254    \n",
            "      2638   581.20177     -148.29246      0             -147.99195      16868.14     \n",
            "      2639   554.22242     -148.26397      0             -147.97741      16599.136    \n",
            "      2640   534.9008      -148.26467      0             -147.9881       16097.057    \n",
            "      2641   525.18373     -148.2673       0             -147.99576      15252.497    \n",
            "      2642   523.32923     -148.2199       0             -147.94932      14325.568    \n",
            "      2643   524.24285     -148.22663      0             -147.95557      12999.262    \n",
            "      2644   521.37595     -148.23715      0             -147.96758      11678.919    \n",
            "      2645   508.47717     -148.25287      0             -147.98997      10074.374    \n",
            "      2646   481.59357     -148.2356       0             -147.9866       8661.2092    \n",
            "      2647   440.84304     -148.17213      0             -147.9442       7117.4692    \n",
            "      2648   389.72034     -148.17928      0             -147.97777      5653.9008    \n",
            "      2649   334.98169     -148.14203      0             -147.96883      4105.2025    \n",
            "      2650   285.65747     -148.11546      0             -147.96777      2623.9946    \n",
            "      2651   250.17696     -148.12044      0             -147.99109      1126.7232    \n",
            "      2652   234.40396     -148.0766       0             -147.9554      -331.19302    \n",
            "      2653   240.53123     -148.09162      0             -147.96725     -1849.9873    \n",
            "      2654   266.93901     -148.09867      0             -147.96065     -3089.6488    \n",
            "      2655   308.47638     -148.11747      0             -147.95798     -4480.6785    \n",
            "      2656   357.86977     -148.15645      0             -147.97142     -5679.5296    \n",
            "      2657   407.71947     -148.19356      0             -147.98275     -6845.04      \n",
            "      2658   452.73949     -148.22614      0             -147.99206     -7578.3072    \n",
            "      2659   490.35943     -148.22336      0             -147.96982     -8201.3961    \n",
            "      2660   520.9321      -148.22366      0             -147.95431     -8578.0559    \n",
            "      2661   547.26254     -148.21993      0             -147.93697     -8826.2681    \n",
            "      2662   573.32419     -148.2499       0             -147.95347     -8911.3386    \n",
            "      2663   602.583       -148.26732      0             -147.95576     -8789.3837    \n",
            "      2664   636.2599      -148.27585      0             -147.94688     -8558.2974    \n",
            "      2665   672.41483     -148.32277      0             -147.97511     -8393.6969    \n",
            "      2666   706.26158     -148.32446      0             -147.9593      -7923.5525    \n",
            "      2667   730.71545     -148.35276      0             -147.97495     -7485.0942    \n",
            "      2668   738.12477     -148.36395      0             -147.98231     -6863.6607    \n",
            "      2669   722.77207     -148.31221      0             -147.93851     -6259.2148    \n",
            "      2670   681.39328     -148.30153      0             -147.94922     -5433.0841    \n",
            "      2671   614.71935     -148.26754      0             -147.9497      -4640.2502    \n",
            "      2672   528.32084     -148.22331      0             -147.95015     -3828.2449    \n",
            "      2673   430.87015     -148.18201      0             -147.95923     -3037.4463    \n",
            "      2674   333.16052     -148.11556      0             -147.94331     -2251.3133    \n",
            "      2675   245.98582     -148.11083      0             -147.98364     -1533.414     \n",
            "      2676   178.47178     -148.00569      0             -147.91341     -1109.6858    \n",
            "      2677   136.76764     -148.01136      0             -147.94065     -620.25385    \n",
            "      2678   123.24464     -148.03326      0             -147.96954     -694.60812    \n",
            "      2679   136.60516     -148.04197      0             -147.97134     -646.01453    \n",
            "      2680   172.3263      -148.01919      0             -147.93009     -847.21957    \n",
            "      2681   224.0941      -148.04991      0             -147.93404     -1253.3565    \n",
            "      2682   285.41548     -148.089        0             -147.94143     -1642.7819    \n",
            "      2683   350.12645     -148.12366      0             -147.94263     -2334.7339    \n",
            "      2684   413.62378     -148.17655      0             -147.96269     -2894.6842    \n",
            "      2685   473.63063     -148.19083      0             -147.94594     -3555.6921    \n",
            "      2686   529.29166     -148.21201      0             -147.93835     -4316.7266    \n",
            "      2687   581.50957     -148.2359       0             -147.93523     -5301.8449    \n",
            "      2688   631.39576     -148.29342      0             -147.96696     -6287.7461    \n",
            "      2689   678.85818     -148.29278      0             -147.94178     -7366.861     \n",
            "      2690   722.30294     -148.32926      0             -147.95581     -8514.6263    \n",
            "      2691   757.94677     -148.33677      0             -147.94488     -9625.5929    \n",
            "      2692   780.49062     -148.35415      0             -147.95061     -10744.41     \n",
            "      2693   784.41923     -148.33343      0             -147.92785     -11872.905    \n",
            "      2694   765.60647     -148.36267      0             -147.96682     -12908.421    \n",
            "      2695   721.85798     -148.31112      0             -147.93789     -13642.566    \n",
            "      2696   654.44952     -148.26725      0             -147.92888     -14126.621    \n",
            "      2697   568.8076      -148.25684      0             -147.96274     -14627.329    \n",
            "      2698   473.46825     -148.18566      0             -147.94086     -14886.213    \n",
            "      2699   378.91308     -148.1449       0             -147.94898     -14913.686    \n",
            "      2700   295.64685     -148.10907      0             -147.95621     -14843.699    \n",
            "      2701   232.66432     -148.09146      0             -147.97117     -14658.088    \n",
            "      2702   195.41158     -148.07493      0             -147.9739      -14309.649    \n",
            "      2703   185.14668     -148.05176      0             -147.95603     -13905.716    \n",
            "      2704   198.97168     -148.07514      0             -147.97226     -13395.634    \n",
            "      2705   230.76387     -148.10453      0             -147.98522     -12770.71     \n",
            "      2706   273.25376     -148.09681      0             -147.95553     -11842.302    \n",
            "      2707   319.26287     -148.12287      0             -147.9578      -10860.088    \n",
            "      2708   363.3991      -148.16938      0             -147.98148     -9772.1611    \n",
            "      2709   402.89656     -148.18353      0             -147.97522     -8439.625     \n",
            "      2710   438.30665     -148.16721      0             -147.94059     -6835.5112    \n",
            "      2711   472.31142     -148.21065      0             -147.96645     -5269.0698    \n",
            "      2712   508.22529     -148.22264      0             -147.95987     -3566.8575    \n",
            "      2713   548.66922     -148.22516      0             -147.94147     -1906.7516    \n",
            "      2714   593.9881      -148.24751      0             -147.94039     -331.62091    \n",
            "      2715   641.28372     -148.31684      0             -147.98527      1319.2581    \n",
            "      2716   684.9229      -148.29574      0             -147.9416       2924.4454    \n",
            "      2717   717.53873     -148.33114      0             -147.96014      4428.5099    \n",
            "      2718   731.88782     -148.32604      0             -147.94762      5813.4084    \n",
            "      2719   723.16916     -148.32596      0             -147.95206      7080.2639    \n",
            "      2720   689.85462     -148.33852      0             -147.98183      8366.1049    \n",
            "      2721   635.0028      -148.30771      0             -147.97939      9404.8004    \n",
            "      2722   565.6899      -148.27138      0             -147.9789       10293.318    \n",
            "      2723   491.23063     -148.26365      0             -148.00966      10859.584    \n",
            "      2724   422.10323     -148.18622      0             -147.96798      11487.221    \n",
            "      2725   367.20294     -148.16548      0             -147.97562      11736.351    \n",
            "      2726   332.16199     -148.14091      0             -147.96917      11638.138    \n",
            "      2727   318.35993     -148.1513       0             -147.9867       11229.575    \n",
            "      2728   323.11535     -148.14283      0             -147.97577      10537.488    \n",
            "      2729   340.64044     -148.15502      0             -147.9789       9756.5542    \n",
            "      2730   363.65235     -148.16151      0             -147.97349      8554.4341    \n",
            "      2731   385.25223     -148.19044      0             -147.99125      7334.7932    \n",
            "      2732   400.56868     -148.19267      0             -147.98556      6098.2416    \n",
            "      2733   407.98591     -148.19959      0             -147.98864      4684.7824    \n",
            "      2734   408.98102     -148.19901      0             -147.98756      3075.3803    \n",
            "      2735   407.54122     -148.22135      0             -148.01064      1703.2213    \n",
            "      2736   408.58186     -148.18623      0             -147.97498      112.19564    \n",
            "      2737   416.09729     -148.19077      0             -147.97563     -1590.2864    \n",
            "      2738   432.18293     -148.18587      0             -147.96241     -3213.3501    \n",
            "      2739   456.07974     -148.23355      0             -147.99774     -4968.9355    \n",
            "      2740   484.17514     -148.23903      0             -147.98869     -6597.3227    \n",
            "      2741   510.99173     -148.25753      0             -147.99332     -8325.1414    \n",
            "      2742   530.77482     -148.267        0             -147.99256     -9828.1049    \n",
            "      2743   539.20301     -148.26965      0             -147.99086     -11071.031    \n",
            "      2744   534.29835     -148.25225      0             -147.97599     -12101.702    \n",
            "      2745   517.58557     -148.2599       0             -147.99229     -12947.909    \n",
            "      2746   493.11537     -148.26532      0             -148.01036     -13597.003    \n",
            "      2747   466.75082     -148.27297      0             -148.03165     -14084.552    \n",
            "      2748   444.94595     -148.26766      0             -148.0376      -14124.369    \n",
            "      2749   433.03199     -148.24463      0             -148.02073     -13997.978    \n",
            "      2750   433.64209     -148.2434       0             -148.01919     -13829.709    \n",
            "      2751   446.00015     -148.24338      0             -148.01278     -13401.919    \n",
            "      2752   466.40044     -148.2487       0             -148.00756     -12683.282    \n",
            "      2753   488.79809     -148.26543      0             -148.0127      -12017.848    \n",
            "      2754   506.2082      -148.29961      0             -148.03788     -10963.508    \n",
            "      2755   512.37113     -148.30189      0             -148.03697     -9807.2492    \n",
            "      2756   502.80772     -148.30553      0             -148.04556     -8436.2892    \n",
            "      2757   476.21993     -148.26535      0             -148.01913     -6853.9117    \n",
            "      2758   435.13118     -148.23763      0             -148.01265     -5169.1143    \n",
            "      2759   384.71927     -148.23409      0             -148.03518     -3418.7185    \n",
            "      2760   331.92847     -148.1976       0             -148.02598     -1668.7119    \n",
            "      2761   284.10007     -148.17744      0             -148.03055      41.343305    \n",
            "      2762   247.56397     -148.17009      0             -148.04208      1644.5307    \n",
            "      2763   226.45715     -148.13549      0             -148.01841      3190.2082    \n",
            "      2764   222.15239     -148.13982      0             -148.02496      4639.2761    \n",
            "      2765   233.51221     -148.17277      0             -148.05203      5824.5798    \n",
            "      2766   257.06074     -148.17353      0             -148.04062      6741.1175    \n",
            "      2767   288.18613     -148.20854      0             -148.05953      7514.4261    \n",
            "      2768   322.36711     -148.21309      0             -148.04642      8268.1817    \n",
            "      2769   356.11522     -148.22445      0             -148.04032      8691.5918    \n",
            "      2770   387.40471     -148.24817      0             -148.04786      9110.8447    \n",
            "      2771   416.09064     -148.24817      0             -148.03303      9087.3253    \n",
            "      2772   443.5784      -148.26274      0             -148.0334       9069.8666    \n",
            "      2773   471.54941     -148.291        0             -148.04719      8687.7627    \n",
            "      2774   501.22653     -148.27537      0             -148.01622      8151.576     \n",
            "      2775   532.36997     -148.304        0             -148.02874      7373.9952    \n",
            "      2776   562.66433     -148.33378      0             -148.04286      6619.5662    \n",
            "      2777   587.6511      -148.33325      0             -148.02941      5467.6434    \n",
            "      2778   601.6261      -148.35732      0             -148.04626      4344.7007    \n",
            "      2779   599.16791     -148.3768       0             -148.067        3149.4321    \n",
            "      2780   576.18935     -148.34653      0             -148.04862      1907.0916    \n",
            "      2781   531.58535     -148.34134      0             -148.06649      832.68432    \n",
            "      2782   468.0713      -148.3192       0             -148.07718     -160.90457    \n",
            "      2783   391.15957     -148.25945      0             -148.0572      -865.53514    \n",
            "      2784   309.19744     -148.24244      0             -148.08257     -1694.4847    \n",
            "      2785   232.02568     -148.20615      0             -148.08618     -2229.9879    \n",
            "      2786   168.45078     -148.16492      0             -148.07783     -2731.6102    \n",
            "      2787   125.2662      -148.14614      0             -148.08137     -3119.5977    \n",
            "      2788   105.98812     -148.11214      0             -148.05734     -3315.5989    \n",
            "      2789   110.28358     -148.14799      0             -148.09097     -3401.7035    \n",
            "      2790   134.43359     -148.14082      0             -148.07131     -3313.1375    \n",
            "      2791   172.84904     -148.17441      0             -148.08504     -3088.3748    \n",
            "      2792   219.07556     -148.1847       0             -148.07143     -2528.3509    \n",
            "      2793   267.28215     -148.23405      0             -148.09586     -1877.6766    \n",
            "      2794   313.97785     -148.25244      0             -148.0901      -911.17235    \n",
            "      2795   357.41008     -148.28489      0             -148.1001       207.37096    \n",
            "      2796   397.5489      -148.32015      0             -148.1146       1601.0142    \n",
            "      2797   435.25392     -148.30909      0             -148.08404      3032.1101    \n",
            "      2798   471.64439     -148.31603      0             -148.07218      4605.4924    \n",
            "      2799   506.54062     -148.34552      0             -148.08362      6238.6689    \n",
            "      2800   537.80237     -148.40402      0             -148.12595      7750.4113    \n",
            "      2801   561.46183     -148.40083      0             -148.11053      9316.2453    \n",
            "      2802   572.3723      -148.38315      0             -148.08721      10896.812    \n",
            "      2803   565.44943     -148.39672      0             -148.10436      12375.378    \n",
            "      2804   537.49617     -148.38366      0             -148.10575      13754.547    \n",
            "      2805   488.68227     -148.36117      0             -148.1085       14973.715    \n",
            "      2806   422.43161     -148.32835      0             -148.10994      16194.719    \n",
            "      2807   345.34111     -148.29074      0             -148.11218      17147.274    \n",
            "      2808   266.29711     -148.2637       0             -148.12601      17919.895    \n",
            "      2809   194.98286     -148.21344      0             -148.11262      18445.555    \n",
            "      2810   139.85149     -148.19247      0             -148.12016      18640.048    \n",
            "      2811   106.66771     -148.16873      0             -148.11358      18487.098    \n",
            "      2812   97.536004     -148.15439      0             -148.10396      17962.521    \n",
            "      2813   111.02454     -148.16507      0             -148.10767      17334.562    \n",
            "      2814   142.72358     -148.18112      0             -148.10733      16266.628    \n",
            "      2815   186.32104     -148.21549      0             -148.11916      15150.892    \n",
            "      2816   234.63394     -148.2483       0             -148.12699      13695.407    \n",
            "      2817   281.64135     -148.23221      0             -148.08659      12093.267    \n",
            "      2818   323.3756      -148.28139      0             -148.1142       10285.716    \n",
            "      2819   357.43695     -148.31433      0             -148.12952      8486.6967    \n",
            "      2820   383.92238     -148.31669      0             -148.11819      6467.6024    \n",
            "      2821   404.16026     -148.33092      0             -148.12195      4452.4351    \n",
            "      2822   419.68995     -148.35407      0             -148.13708      2478.5263    \n",
            "      2823   431.62525     -148.35924      0             -148.13607      366.63099    \n",
            "      2824   439.93821     -148.35355      0             -148.12608     -1571.6521    \n",
            "      2825   443.41524     -148.35454      0             -148.12528     -3513.086     \n",
            "      2826   439.75756     -148.3777       0             -148.15033     -5564.2384    \n",
            "      2827   426.65524     -148.35237      0             -148.13177     -7196.565     \n",
            "      2828   402.6634      -148.36357      0             -148.15538     -8727.9954    \n",
            "      2829   367.99149     -148.37178      0             -148.18152     -9970.013     \n",
            "      2830   324.97638     -148.31607      0             -148.14804     -10942.043    \n",
            "      2831   277.73545     -148.30072      0             -148.15712     -11704.322    \n",
            "      2832   231.69777     -148.28488      0             -148.16509     -12149.676    \n",
            "      2833   192.47954     -148.24556      0             -148.14604     -12451.557    \n",
            "      2834   165.04922     -148.25158      0             -148.16624     -12393.414    \n",
            "      2835   152.60689     -148.24596      0             -148.16706     -12258.375    \n",
            "      2836   155.96383     -148.20976      0             -148.12912     -11886.411    \n",
            "      2837   173.32187     -148.29112      0             -148.20151     -11427.219    \n",
            "      2838   200.62966     -148.24471      0             -148.14098     -10746.806    \n",
            "      2839   232.7225      -148.255        0             -148.13468     -9790.4576    \n",
            "      2840   263.95288     -148.30239      0             -148.16592     -8774.1205    \n",
            "      2841   289.36375     -148.30223      0             -148.15262     -7691.1808    \n",
            "      2842   305.57299     -148.31233      0             -148.15434     -6225.2944    \n",
            "      2843   311.47022     -148.3309       0             -148.16986     -4786.1753    \n",
            "      2844   308.29796     -148.31216      0             -148.15276     -3240.7985    \n",
            "      2845   298.83101     -148.31582      0             -148.16131     -1712.5763    \n",
            "      2846   286.7655      -148.31023      0             -148.16196     -181.96087    \n",
            "      2847   275.81266     -148.32253      0             -148.17992      1316.7458    \n",
            "      2848   268.664       -148.3097       0             -148.17079      2655.7499    \n",
            "      2849   266.41767     -148.29755      0             -148.1598       3864.5054    \n",
            "      2850   268.70464     -148.3064       0             -148.16747      5056.7725    \n",
            "      2851   273.96397     -148.3163       0             -148.17465      5898.1134    \n",
            "      2852   279.69837     -148.33039      0             -148.18577      6587.0756    \n",
            "      2853   283.45362     -148.32828      0             -148.18172      7194.125     \n",
            "      2854   283.66703     -148.3501       0             -148.20344      7519.2688    \n",
            "      2855   280.43134     -148.35562      0             -148.21063      7804.81      \n",
            "      2856   274.67708     -148.32869      0             -148.18667      7832.0061    \n",
            "      2857   268.78531     -148.34986      0             -148.21088      7594.968     \n",
            "      2858   265.62418     -148.34495      0             -148.20761      7234.8951    \n",
            "      2859   267.23203     -148.3286       0             -148.19043      6845.5447    \n",
            "      2860   274.58664     -148.30011      0             -148.15813      6154.0205    \n",
            "      2861   286.93163     -148.34047      0             -148.19212      5304.4373    \n",
            "      2862   301.65665     -148.3285       0             -148.17253      4295.6243    \n",
            "      2863   315.0642      -148.34136      0             -148.17846      3082.8691    \n",
            "      2864   323.01248     -148.36969      0             -148.20268      2043.4417    \n",
            "      2865   321.57851     -148.37884      0             -148.21257      882.88204    \n",
            "      2866   308.60596     -148.3447       0             -148.18514     -297.89844    \n",
            "      2867   284.35668     -148.34105      0             -148.19403     -1350.5797    \n",
            "      2868   251.36597     -148.33174      0             -148.20177     -2334.4676    \n",
            "      2869   213.91977     -148.30161      0             -148.191       -3406.6843    \n",
            "      2870   177.3542      -148.28967      0             -148.19797     -4189.897     \n",
            "      2871   147.03243     -148.28499      0             -148.20897     -4999.5854    \n",
            "      2872   127.18677     -148.2681       0             -148.20234     -5680.5404    \n",
            "      2873   119.95754     -148.26908      0             -148.20706     -6210.8778    \n",
            "      2874   125.27852     -148.2788       0             -148.21403     -6882.3201    \n",
            "      2875   140.84591     -148.28985      0             -148.21702     -7139.8308    \n",
            "      2876   163.07685     -148.28654      0             -148.20222     -7373.9383    \n",
            "      2877   188.34648     -148.31421      0             -148.21683     -7462.9601    \n",
            "      2878   213.2173      -148.31662      0             -148.20638     -7382.2464    \n",
            "      2879   235.39772     -148.30994      0             -148.18823     -7022.4338    \n",
            "      2880   254.38139     -148.34153      0             -148.21        -6675.1396    \n",
            "      2881   271.05573     -148.35183      0             -148.21169     -6158.757     \n",
            "      2882   286.56937     -148.36768      0             -148.21952     -5458.1851    \n",
            "      2883   302.10882     -148.37673      0             -148.22053     -4626.8962    \n",
            "      2884   318.03853     -148.38184      0             -148.2174      -3838.7867    \n",
            "      2885   333.31181     -148.37297      0             -148.20064     -2942.4429    \n",
            "      2886   345.48147     -148.4012       0             -148.22258     -2061.6853    \n",
            "      2887   351.19516     -148.38347      0             -148.20189     -1185.544     \n",
            "      2888   347.1347      -148.37303      0             -148.19355     -139.40206    \n",
            "      2889   330.95075     -148.38755      0             -148.21643      695.15854    \n",
            "      2890   302.14462     -148.39764      0             -148.24142      1559.4106    \n",
            "      2891   262.30078     -148.36248      0             -148.22686      2430.975     \n",
            "      2892   215.10084     -148.35069      0             -148.23947      3201.8917    \n",
            "      2893   165.90657     -148.29984      0             -148.21406      3778.4456    \n",
            "      2894   120.55689     -148.30647      0             -148.24414      4400.535     \n",
            "      2895   84.558201     -148.27496      0             -148.23124      4656.7474    \n",
            "      2896   61.99015      -148.27712      0             -148.24506      4841.4378    \n",
            "      2897   54.932745     -148.25084      0             -148.22243      4741.4862    \n",
            "      2898   63.230684     -148.27645      0             -148.24375      4651.5061    \n",
            "      2899   84.864904     -148.2722       0             -148.22832      4306.4038    \n",
            "      2900   116.09493     -148.29973      0             -148.2397       3550.7335    \n",
            "      2901   152.30954     -148.30568      0             -148.22693      2796.4588    \n",
            "      2902   189.41186     -148.2941       0             -148.19617      1987.872     \n",
            "      2903   223.90122     -148.35071      0             -148.23495      904.77524    \n",
            "      2904   253.5508      -148.37216      0             -148.24106     -238.20877    \n",
            "      2905   277.53275     -148.35776      0             -148.21426     -1301.3362    \n",
            "      2906   295.77037     -148.36267      0             -148.20974     -2491.4878    \n",
            "      2907   309.06326     -148.39786      0             -148.23806     -3912.0771    \n",
            "      2908   318.09844     -148.38093      0             -148.21646     -5195.8104    \n",
            "      2909   322.844       -148.40901      0             -148.24209     -6671.1755    \n",
            "      2910   322.84829     -148.38366      0             -148.21673     -7903.6191    \n",
            "      2911   317.00209     -148.4227       0             -148.25879     -9285.1811    \n",
            "      2912   303.71541     -148.39885      0             -148.24182     -10588.045    \n",
            "      2913   282.23006     -148.38413      0             -148.2382      -11720.123    \n",
            "      2914   252.52746     -148.38012      0             -148.24955     -12689.896    \n",
            "      2915   215.59384     -148.32428      0             -148.21281     -13410.848    \n",
            "      2916   174.23387     -148.31456      0             -148.22448     -14101.991    \n",
            "      2917   132.22247     -148.31724      0             -148.24887     -14478.631    \n",
            "      2918   93.993383     -148.28209      0             -148.23349     -14743.494    \n",
            "      2919   64.151522     -148.27355      0             -148.24039     -14904.965    \n",
            "      2920   46.396202     -148.28072      0             -148.25673     -14794.327    \n",
            "      2921   42.862106     -148.28316      0             -148.261       -14500.852    \n",
            "      2922   53.784773     -148.27827      0             -148.25047     -14191.725    \n",
            "      2923   77.372772     -148.31342      0             -148.27341     -13619.721    \n",
            "      2924   110.07008     -148.29344      0             -148.23653     -12896.974    \n",
            "      2925   147.63862     -148.33768      0             -148.26134     -11971.047    \n",
            "      2926   185.63027     -148.32474      0             -148.22876     -10789.353    \n",
            "      2927   219.77126     -148.36468      0             -148.25105     -9567.0032    \n",
            "      2928   247.26413     -148.3885       0             -148.26066     -8239.2692    \n",
            "      2929   266.79614     -148.39259      0             -148.25464     -6816.7578    \n",
            "      2930   278.47871     -148.39297      0             -148.24899     -5131.3783    \n",
            "      2931   283.37111     -148.39652      0             -148.25        -3336.0676    \n",
            "      2932   283.38148     -148.40292      0             -148.2564      -1769.3738    \n",
            "      2933   280.214       -148.38066      0             -148.23577     -110.99464    \n",
            "      2934   274.85736     -148.38427      0             -148.24215      1396.7974    \n",
            "      2935   267.67002     -148.43238      0             -148.29398      2830.2888    \n",
            "      2936   258.17641     -148.38932      0             -148.25583      4332.3703    \n",
            "      2937   245.44323     -148.3843       0             -148.2574       5653.5305    \n",
            "      2938   228.78884     -148.3803       0             -148.262        6708.1526    \n",
            "      2939   208.25644     -148.3778       0             -148.27012      7804.9546    \n",
            "      2940   184.76983     -148.33151      0             -148.23598      8646.2406    \n",
            "      2941   160.5845      -148.32941      0             -148.24638      9343.1892    \n",
            "      2942   138.64139     -148.3307       0             -148.25902      9750.3131    \n",
            "      2943   122.17155     -148.32244      0             -148.25927      9960.6356    \n",
            "      2944   114.21118     -148.32612      0             -148.26707      10008.992    \n",
            "      2945   116.63437     -148.33373      0             -148.27342      9875.6995    \n",
            "      2946   129.47588     -148.32407      0             -148.25713      9441.493     \n",
            "      2947   150.94984     -148.36161      0             -148.28356      8878.6141    \n",
            "      2948   177.55076     -148.32492      0             -148.23312      8116.4982    \n",
            "      2949   204.8411      -148.36193      0             -148.25602      7315.6959    \n",
            "      2950   228.37465     -148.35683      0             -148.23875      6398.6435    \n",
            "      2951   244.59413     -148.38811      0             -148.26164      5221.9428    \n",
            "      2952   251.29095     -148.40105      0             -148.27112      4022.0933    \n",
            "      2953   248.19305     -148.35929      0             -148.23096      3019.3911    \n",
            "      2954   237.31321     -148.39373      0             -148.27103      1829.2175    \n",
            "      2955   221.64718     -148.3684       0             -148.2538       755.95977    \n",
            "      2956   204.40857     -148.38301      0             -148.27732     -367.89717    \n",
            "      2957   188.95067     -148.35916      0             -148.26146     -1264.0187    \n",
            "      2958   177.62684     -148.36238      0             -148.27054     -2235.9362    \n",
            "      2959   171.44648     -148.33038      0             -148.24174     -3056.0369    \n",
            "      2960   170.0891      -148.35238      0             -148.26444     -3740.5192    \n",
            "      2961   172.09793     -148.36163      0             -148.27264     -4248.9475    \n",
            "      2962   175.78201     -148.34265      0             -148.25177     -4673.5443    \n",
            "      2963   179.63015     -148.37107      0             -148.27819     -4901.8189    \n",
            "      2964   182.81569     -148.3576       0             -148.26307     -4924.7819    \n",
            "      2965   185.54644     -148.38164      0             -148.28571     -4750.6075    \n",
            "      2966   188.66302     -148.34135      0             -148.2438      -4316.3057    \n",
            "      2967   193.63952     -148.35799      0             -148.25787     -3922.0214    \n",
            "      2968   201.69198     -148.37287      0             -148.26859     -3130.8535    \n",
            "      2969   213.15578     -148.40921      0             -148.299       -2353.6193    \n",
            "      2970   227.52069     -148.40344      0             -148.2858      -1495.3803    \n",
            "      2971   242.84972     -148.39498      0             -148.26942     -464.96411    \n",
            "      2972   256.22427     -148.38271      0             -148.25023      626.10929    \n",
            "      2973   264.40665     -148.39509      0             -148.25838      1700.9383    \n",
            "      2974   264.50454     -148.40361      0             -148.26685      2781.7077    \n",
            "      2975   254.88093     -148.40875      0             -148.27697      4040.8278    \n",
            "      2976   235.44589     -148.36264      0             -148.2409       5138.5011    \n",
            "      2977   208.02826     -148.36883      0             -148.26128      6207.5636    \n",
            "      2978   175.82185     -148.36667      0             -148.27576      7168.9411    \n",
            "      2979   143.2813      -148.3385       0             -148.26442      8228.2036    \n",
            "      2980   115.13399     -148.3441       0             -148.28457      9016.927     \n",
            "      2981   95.263261     -148.33356      0             -148.28431      9509.485     \n",
            "      2982   86.137837     -148.32091      0             -148.27637      9845.7482    \n",
            "      2983   88.686366     -148.33064      0             -148.28479      10093.276    \n",
            "      2984   102.07883     -148.33765      0             -148.28487      10099.272    \n",
            "      2985   123.82406     -148.35171      0             -148.28769      9797.8725    \n",
            "      2986   150.61532     -148.36821      0             -148.29034      9306.5784    \n",
            "      2987   178.99046     -148.37003      0             -148.27748      8748.7308    \n",
            "      2988   205.73581     -148.37893      0             -148.27256      7891.9858    \n",
            "      2989   228.35366     -148.37875      0             -148.26068      6960.1935    \n",
            "      2990   245.65538     -148.39804      0             -148.27103      5962.7065    \n",
            "      2991   257.48207     -148.41046      0             -148.27733      4723.5863    \n",
            "      2992   264.19046     -148.39686      0             -148.26026      3456.0874    \n",
            "      2993   266.47483     -148.40731      0             -148.26953      2129.9529    \n",
            "      2994   264.80058     -148.40863      0             -148.27172      725.15918    \n",
            "      2995   259.203       -148.3915       0             -148.25748     -641.92757    \n",
            "      2996   249.14419     -148.40371      0             -148.27489     -2087.4815    \n",
            "      2997   233.81793     -148.40186      0             -148.28097     -3440.3812    \n",
            "      2998   212.76389     -148.40958      0             -148.29957     -4646.7862    \n",
            "      2999   186.17084     -148.36811      0             -148.27186     -5728.875     \n",
            "      3000   154.7978      -148.35883      0             -148.2788      -6679.6449    \n",
            "      3001   120.726       -148.35574      0             -148.29332     -7448.6434    \n",
            "      3002   87.268877     -148.34161      0             -148.29649     -7857.7232    \n",
            "      3003   58.368917     -148.3253       0             -148.29512     -8141.6638    \n",
            "      3004   37.998098     -148.2605       0             -148.24086     -8123.4814    \n",
            "      3005   29.30379      -148.28072      0             -148.26557     -8105.2286    \n",
            "      3006   34.187997     -148.306        0             -148.28832     -7846.2936    \n",
            "      3007   52.841072     -148.29661      0             -148.26929     -7199.3292    \n",
            "      3008   83.530259     -148.33184      0             -148.28865     -6662.2034    \n",
            "      3009   122.83093     -148.31722      0             -148.25371     -5801.2054    \n",
            "      3010   166.16921     -148.35924      0             -148.27332     -4687.561     \n",
            "      3011   208.53309     -148.39857      0             -148.29075     -3606.1145    \n",
            "      3012   245.09885     -148.4119       0             -148.28517     -2171.0169    \n",
            "      3013   272.52111     -148.43392      0             -148.29302     -722.11396    \n",
            "      3014   289.02372     -148.41397      0             -148.26453      873.89243    \n",
            "      3015   294.09959     -148.44046      0             -148.2884       2553.5444    \n",
            "      3016   289.19445     -148.39513      0             -148.2456       4309.3189    \n",
            "      3017   276.67103     -148.39874      0             -148.25569      6043.4789    \n",
            "      3018   258.97753     -148.41472      0             -148.28082      7693.218     \n",
            "      3019   238.25312     -148.38988      0             -148.2667       9141.9797    \n",
            "      3020   216.06585     -148.38769      0             -148.27598      10510.368    \n",
            "      3021   193.01544     -148.41259      0             -148.31279      11773.571    \n",
            "      3022   169.27492     -148.35485      0             -148.26733      12926.966    \n",
            "      3023   145.13725     -148.3402       0             -148.26515      13810.02     \n",
            "      3024   120.91638     -148.33364      0             -148.27113      14387.738    \n",
            "      3025   97.93         -148.33451      0             -148.28388      14729.35     \n",
            "      3026   78.06418      -148.33225      0             -148.29189      15017.386    \n",
            "      3027   63.940369     -148.29927      0             -148.26621      15060.35     \n",
            "      3028   58.362911     -148.31772      0             -148.28754      14738.786    \n",
            "      3029   63.277832     -148.30632      0             -148.2736       14391.339    \n",
            "      3030   79.559839     -148.30727      0             -148.26613      13559.23     \n",
            "      3031   106.33391     -148.30608      0             -148.25111      12588.178    \n",
            "      3032   140.98476     -148.35986      0             -148.28697      11503.127    \n",
            "      3033   179.42914     -148.36391      0             -148.27114      10184.129    \n",
            "      3034   216.70129     -148.38809      0             -148.27604      8713.0595    \n",
            "      3035   248.04755     -148.37925      0             -148.251        7224.6248    \n",
            "      3036   269.6015      -148.40889      0             -148.26949      5651.0213    \n",
            "      3037   279.20415     -148.42375      0             -148.27939      3998.7215    \n",
            "      3038   276.80342     -148.41201      0             -148.26889      2463.2718    \n",
            "      3039   264.10576     -148.40825      0             -148.27169      915.06487    \n",
            "      3040   244.49008     -148.38832      0             -148.2619      -509.70219    \n",
            "      3041   221.85033     -148.36997      0             -148.25526     -1881.6242    \n",
            "      3042   199.96266     -148.38229      0             -148.2789      -3325.4817    \n",
            "      3043   181.80077     -148.38315      0             -148.28916     -4343.577     \n",
            "      3044   168.69483     -148.35028      0             -148.26306     -5456.6833    \n",
            "      3045   160.89502     -148.33595      0             -148.25276     -6340.0162    \n",
            "      3046   157.49424     -148.37805      0             -148.29662     -6992.5521    \n",
            "      3047   156.83653     -148.34824      0             -148.26715     -7467.9735    \n",
            "      3048   157.46222     -148.31965      0             -148.23824     -7784.5587    \n",
            "      3049   158.62919     -148.35612      0             -148.2741      -7915.4895    \n",
            "      3050   160.47908     -148.36248      0             -148.2795      -7898.0324    \n",
            "      3051   163.53643     -148.33783      0             -148.25327     -7474.3395    \n",
            "      3052   168.95105     -148.37644      0             -148.28909     -7066.35      \n",
            "      3053   177.91887     -148.39111      0             -148.29912     -6508.4991    \n",
            "      3054   190.75285     -148.3827       0             -148.28407     -5834.2883    \n",
            "      3055   206.74841     -148.38718      0             -148.28029     -5179.8634    \n",
            "      3056   224.15563     -148.36505      0             -148.24915     -4218.425     \n",
            "      3057   240.44357     -148.41871      0             -148.29439     -3320.7564    \n",
            "      3058   252.47004     -148.38543      0             -148.25489     -2407.5164    \n",
            "      3059   257.36094     -148.41904      0             -148.28597     -1416.9239    \n",
            "      3060   253.3672      -148.39939      0             -148.26839     -466.7477     \n",
            "      3061   240.10498     -148.37872      0             -148.25458      513.47596    \n",
            "      3062   218.97648     -148.37292      0             -148.2597       1458.8831    \n",
            "      3063   193.03494     -148.36844      0             -148.26863      2168.6647    \n",
            "      3064   166.03732     -148.33675      0             -148.25091      2919.2861    \n",
            "      3065   142.30353     -148.31965      0             -148.24607      3490.2208    \n",
            "      3066   125.77799     -148.30802      0             -148.24299      3871.8761    \n",
            "      3067   119.05473     -148.32219      0             -148.26064      3943.5357    \n",
            "      3068   123.03021     -148.30161      0             -148.238        3900.7956    \n",
            "      3069   136.82931     -148.32805      0             -148.2573       3687.5571    \n",
            "      3070   157.96128     -148.34419      0             -148.26252      3273.1598    \n",
            "      3071   183.14645     -148.34194      0             -148.24725      2688.7473    \n",
            "      3072   208.69132     -148.34645      0             -148.23854      2189.5808    \n",
            "      3073   231.19186     -148.35196      0             -148.23242      1284.9624    \n",
            "      3074   248.2256      -148.37148      0             -148.24314      279.73898    \n",
            "      3075   258.65363     -148.38532      0             -148.25159     -907.02482    \n",
            "      3076   262.55581     -148.39212      0             -148.25637     -1928.6065    \n",
            "      3077   260.90745     -148.39631      0             -148.26141     -3160.1451    \n",
            "      3078   255.37802     -148.36525      0             -148.23321     -4324.7611    \n",
            "      3079   247.40836     -148.38237      0             -148.25445     -5677.2953    \n",
            "      3080   237.82154     -148.34887      0             -148.2259      -6835.1764    \n",
            "      3081   226.62044     -148.37823      0             -148.26105     -8053.8997    \n",
            "      3082   213.09594     -148.38065      0             -148.27047     -9166.1754    \n",
            "      3083   196.52984     -148.35199      0             -148.25037     -10260.562    \n",
            "      3084   176.29334     -148.36378      0             -148.27263     -11123.308    \n",
            "      3085   152.48734     -148.33179      0             -148.25295     -11759.058    \n",
            "      3086   126.41291     -148.32742      0             -148.26206     -12308.157    \n",
            "      3087   100.55526     -148.29106      0             -148.23907     -12557.862    \n",
            "      3088   78.485354     -148.29348      0             -148.2529      -12734.058    \n",
            "      3089   64.004996     -148.28698      0             -148.25388     -12547.416    \n",
            "      3090   60.544435     -148.2544       0             -148.2231      -12434.633    \n",
            "      3091   70.226522     -148.28202      0             -148.24571     -12109.324    \n",
            "      3092   93.354639     -148.28265      0             -148.23438     -11515.123    \n",
            "      3093   128.2632      -148.30008      0             -148.23376     -10861.421    \n",
            "      3094   171.35309     -148.33769      0             -148.24909     -9900.0441    \n",
            "      3095   217.58042     -148.37361      0             -148.26111     -8903.0869    \n",
            "      3096   261.17125     -148.37886      0             -148.24382     -7679.9546    \n",
            "      3097   296.93121     -148.39512      0             -148.2416      -6419.2863    \n",
            "      3098   321.09272     -148.40109      0             -148.23507     -5007.5354    \n",
            "      3099   331.75106     -148.42805      0             -148.25652     -3394.9252    \n",
            "      3100   329.03327     -148.40464      0             -148.23452     -1826.418     \n",
            "      3101   314.92552     -148.43319      0             -148.27036     -132.1999     \n",
            "      3102   292.3635      -148.41102      0             -148.25986      1458.9983    \n",
            "      3103   264.73276     -148.36841      0             -148.23153      3112.0267    \n",
            "      3104   235.00508     -148.36732      0             -148.24582      4611.9209    \n",
            "      3105   205.16409     -148.34444      0             -148.23836      6090.1551    \n",
            "      3106   176.01152     -148.34336      0             -148.25235      7247.4604    \n",
            "      3107   147.62674     -148.29313      0             -148.2168       8385.1031    \n",
            "      3108   120.24291     -148.26631      0             -148.20414      9196.4039    \n",
            "      3109   94.123893     -148.26914      0             -148.22047      9899.329     \n",
            "      3110   70.219332     -148.26294      0             -148.22664      10423.223    \n",
            "      3111   50.972119     -148.25818      0             -148.23182      10674.509    \n",
            "      3112   39.130536     -148.24439      0             -148.22416      10831.543    \n",
            "      3113   37.854432     -148.24005      0             -148.22048      10669.52     \n",
            "      3114   49.908412     -148.26215      0             -148.23635      10237.886    \n",
            "      3115   76.482122     -148.28304      0             -148.2435       9625.6509    \n",
            "      3116   116.85685     -148.30719      0             -148.24677      8835.6637    \n",
            "      3117   167.91308     -148.31588      0             -148.22906      7779.7343    \n",
            "      3118   224.46769     -148.35856      0             -148.24251      6472.9814    \n",
            "      3119   280.16401     -148.38009      0             -148.23524      5164.2773    \n",
            "      3120   328.51643     -148.39661      0             -148.22676      3538.6539    \n",
            "      3121   363.79683     -148.40851      0             -148.22042      1847.5662    \n",
            "      3122   382.54822     -148.44876      0             -148.25096      186.26481    \n",
            "      3123   383.82241     -148.41835      0             -148.2199      -1306.1048    \n",
            "      3124   369.14475     -148.40604      0             -148.21518     -3034.9728    \n",
            "      3125   342.20536     -148.38337      0             -148.20644     -4490.377     \n",
            "      3126   307.91541     -148.39736      0             -148.23816     -6018.4877    \n",
            "      3127   271.44596     -148.35132      0             -148.21097     -7364.7773    \n",
            "      3128   237.02273     -148.33953      0             -148.21698     -8649.8336    \n",
            "      3129   207.55227     -148.3103       0             -148.20299     -9734.3767    \n",
            "      3130   184.33721     -148.32922      0             -148.23391     -10785.53     \n",
            "      3131   167.23289     -148.31075      0             -148.22428     -11572.903    \n",
            "      3132   155.13854     -148.29336      0             -148.21315     -12181.474    \n",
            "      3133   146.89063     -148.28089      0             -148.20495     -12427.686    \n",
            "      3134   141.92481     -148.28828      0             -148.2149      -12555.595    \n",
            "      3135   140.48367     -148.28806      0             -148.21543     -12506.485    \n",
            "      3136   143.87517     -148.29921      0             -148.22482     -12235.062    \n",
            "      3137   153.79003     -148.28229      0             -148.20278     -11623.374    \n",
            "      3138   171.76272     -148.30663      0             -148.21783     -10824.299    \n",
            "      3139   198.7064      -148.30947      0             -148.20673     -9982.5421    \n",
            "      3140   233.75421     -148.32334      0             -148.20248     -8864.6702    \n",
            "      3141   274.26584     -148.36219      0             -148.22039     -7580.7737    \n",
            "      3142   316.15339     -148.38858      0             -148.22511     -6276.722     \n",
            "      3143   354.13141     -148.3987       0             -148.2156      -4904.0826    \n",
            "      3144   383.17726     -148.40378      0             -148.20566     -3333.4869    \n",
            "      3145   399.18014     -148.39432      0             -148.18792     -1791.9695    \n",
            "      3146   399.52494     -148.40623      0             -148.19966     -147.25475    \n",
            "      3147   384.1791      -148.41599      0             -148.21735      1368.3031    \n",
            "      3148   355.96963     -148.41846      0             -148.23441      2904.6583    \n",
            "      3149   319.35044     -148.35721      0             -148.19209      4439.5146    \n",
            "      3150   280.14125     -148.33405      0             -148.18921      5820.1063    \n",
            "      3151   244.72057     -148.32355      0             -148.19702      6799.5007    \n",
            "      3152   218.28958     -148.28383      0             -148.17096      7732.9659    \n",
            "      3153   204.07952     -148.30389      0             -148.19838      8414.6191    \n",
            "      3154   203.27299     -148.30206      0             -148.19696      8912.2791    \n",
            "      3155   214.6726      -148.30675      0             -148.19576      9048.6381    \n",
            "      3156   235.13983     -148.28798      0             -148.1664       8901.916     \n",
            "      3157   260.53588     -148.31559      0             -148.18089      8594.3601    \n",
            "      3158   286.59435     -148.31912      0             -148.17094      8080.4322    \n",
            "      3159   309.96812     -148.34354      0             -148.18328      7222.3944    \n",
            "      3160   328.75458     -148.32126      0             -148.15128      6507.9397    \n",
            "      3161   342.13723     -148.3718       0             -148.1949       5568.1145    \n",
            "      3162   350.74909     -148.34734      0             -148.16599      4452.7136    \n",
            "      3163   356.22938     -148.35461      0             -148.17042      3107.2444    \n",
            "      3164   359.80119     -148.33586      0             -148.14983      1742.0484    \n",
            "      3165   362.19911     -148.34468      0             -148.15741      544.91683    \n",
            "      3166   362.9385      -148.35067      0             -148.16302     -955.36408    \n",
            "      3167   360.48001     -148.33428      0             -148.1479      -2258.6596    \n",
            "      3168   352.8179      -148.37099      0             -148.18857     -3506.6393    \n",
            "      3169   338.10133     -148.29659      0             -148.12177     -4513.5712    \n",
            "      3170   315.6357      -148.28985      0             -148.12666     -5484.9664    \n",
            "      3171   286.19302     -148.30335      0             -148.15538     -6212.9322    \n",
            "      3172   252.56361     -148.29785      0             -148.16727     -6721.1898    \n",
            "      3173   219.6176      -148.27424      0             -148.16069     -6884.7997    \n",
            "      3174   193.39656     -148.27768      0             -148.17769     -6977.2899    \n",
            "      3175   179.28913     -148.22831      0             -148.13561     -6675.832     \n",
            "      3176   181.5187      -148.23975      0             -148.1459      -6180.2631    \n",
            "      3177   201.92777     -148.26798      0             -148.16358     -5587.4919    \n",
            "      3178   239.21751     -148.28887      0             -148.16519     -4732.6185    \n",
            "      3179   289.14275     -148.32105      0             -148.17155     -3597.1974    \n",
            "      3180   344.88901     -148.34491      0             -148.16659     -2301.1391    \n",
            "      3181   398.05943     -148.3499       0             -148.14409     -707.2292     \n",
            "      3182   440.86639     -148.35967      0             -148.13172      1025.4109    \n",
            "      3183   467.48889     -148.3895       0             -148.14779      2871.4245    \n",
            "      3184   474.59611     -148.39149      0             -148.14611      5079.3515    \n",
            "      3185   462.56671     -148.37766      0             -148.1385       7111.676     \n",
            "      3186   434.21066     -148.35429      0             -148.12979      9302.0016    \n",
            "      3187   394.47806     -148.35562      0             -148.15166      11447.561    \n",
            "      3188   349.30555     -148.32002      0             -148.13941      13533.372    \n",
            "      3189   303.68537     -148.29396      0             -148.13694      15502.433    \n",
            "      3190   261.10388     -148.25104      0             -148.11603      17176.731    \n",
            "      3191   223.4868      -148.2446       0             -148.12905      18570.338    \n",
            "      3192   190.91916     -148.22286      0             -148.12415      19827.476    \n",
            "      3193   162.8389      -148.20498      0             -148.12078      20770.842    \n",
            "      3194   139.42602     -148.19293      0             -148.12085      21509.984    \n",
            "      3195   121.68834     -148.19765      0             -148.13474      21992.998    \n",
            "      3196   111.80484     -148.1954       0             -148.13759      21951.236    \n",
            "      3197   112.89841     -148.18687      0             -148.1285       21847.188    \n",
            "      3198   128.38387     -148.19294      0             -148.12656      21246.156    \n",
            "      3199   161.12238     -148.23109      0             -148.14779      20290.04     \n",
            "      3200   211.88799     -148.23599      0             -148.12644      19113.141    \n",
            "      3201   278.44954     -148.25629      0             -148.11232      17606.685    \n",
            "      3202   355.44375     -148.30225      0             -148.11847      15579.555    \n",
            "      3203   434.79713     -148.37445      0             -148.14965      13489.158    \n",
            "      3204   507.05653     -148.39798      0             -148.13581      11150.894    \n",
            "      3205   562.82919     -148.37591      0             -148.08491      8747.217     \n",
            "      3206   594.62027     -148.41548      0             -148.10804      6229.1359    \n",
            "      3207   598.09868     -148.41938      0             -148.11014      3727.6987    \n",
            "      3208   572.94858     -148.3877       0             -148.09147      1148.6414    \n",
            "      3209   523.35995     -148.35476      0             -148.08417     -956.94032    \n",
            "      3210   456.65622     -148.33982      0             -148.10371     -3274.4405    \n",
            "      3211   381.68558     -148.30385      0             -148.10651     -5341.5782    \n",
            "      3212   307.52608     -148.25862      0             -148.09961     -7191.8415    \n",
            "      3213   241.83509     -148.21146      0             -148.08643     -8935.0058    \n",
            "      3214   189.80772     -148.17594      0             -148.0778      -10287.085    \n",
            "      3215   154.1461      -148.1775       0             -148.0978      -11627.21     \n",
            "      3216   134.86374     -148.17087      0             -148.10114     -12670.562    \n",
            "      3217   130.14717     -148.14325      0             -148.07596     -13299.16     \n",
            "      3218   137.49629     -148.1671       0             -148.09601     -13756.581    \n",
            "      3219   154.464       -148.16641      0             -148.08654     -13763.572    \n",
            "      3220   179.68968     -148.19492      0             -148.10201     -13491.465    \n",
            "      3221   212.87392     -148.20587      0             -148.09581     -12994.052    \n",
            "      3222   254.14678     -148.23466      0             -148.10325     -12187.06     \n",
            "      3223   303.72393     -148.27408      0             -148.11705     -11058.006    \n",
            "      3224   361.25003     -148.29285      0             -148.10607     -9721.4498    \n",
            "      3225   424.90268     -148.31895      0             -148.09926     -8266.397     \n",
            "      3226   490.84261     -148.33982      0             -148.08604     -6616.1798    \n",
            "      3227   553.43329     -148.35897      0             -148.07282     -4911.977     \n",
            "      3228   605.72646     -148.41517      0             -148.10198     -3296.991     \n",
            "      3229   640.75372     -148.40046      0             -148.06916     -1450.0682    \n",
            "      3230   652.84461     -148.37913      0             -148.04158      380.73582    \n",
            "      3231   639.00806     -148.39507      0             -148.06467      2177.7492    \n",
            "      3232   599.85079     -148.3821       0             -148.07195      3909.2539    \n",
            "      3233   539.98504     -148.35206      0             -148.07286      5560.8061    \n",
            "      3234   467.09726     -148.29599      0             -148.05448      7195.6313    \n",
            "      3235   390.69254     -148.27023      0             -148.06823      8430.5736    \n",
            "      3236   321.01817     -148.21329      0             -148.04731      9515.0674    \n",
            "      3237   266.82342     -148.20612      0             -148.06816      10275.883    \n",
            "      3238   234.04211     -148.18222      0             -148.06121      10744.562    \n",
            "      3239   225.19941     -148.18942      0             -148.07298      10755.738    \n",
            "      3240   238.91708     -148.18542      0             -148.06189      10596.873    \n",
            "      3241   270.82712     -148.19038      0             -148.05035      10135.745    \n",
            "      3242   314.58542     -148.24615      0             -148.0835       9281.0229    \n",
            "      3243   362.73819     -148.23649      0             -148.04894      8489.9497    \n",
            "      3244   409.12108     -148.24521      0             -148.03368      7455.2258    \n",
            "      3245   449.62325     -148.27887      0             -148.04639      6091.4816    \n",
            "      3246   482.33643     -148.3006       0             -148.05121      4922.7054    \n",
            "      3247   507.43515     -148.31469      0             -148.05233      3441.5284    \n",
            "      3248   526.55919     -148.32526      0             -148.053        1916.1675    \n",
            "      3249   542.25774     -148.31962      0             -148.03925      454.2645     \n",
            "      3250   555.9703      -148.32669      0             -148.03923     -1129.6322    \n",
            "      3251   567.36426     -148.3483       0             -148.05495     -2822.1873    \n",
            "      3252   574.40985     -148.31702      0             -148.02003     -4340.2622    \n",
            "      3253   573.90881     -148.31896      0             -148.02223     -5840.843     \n",
            "      3254   562.30724     -148.32437      0             -148.03364     -7187.6016    \n",
            "      3255   537.12291     -148.29832      0             -148.02061     -8235.0277    \n",
            "      3256   498.13164     -148.30601      0             -148.04845     -9200.2989    \n",
            "      3257   448.13922     -148.24349      0             -148.01179     -9728.0642    \n",
            "      3258   393.33732     -148.23385      0             -148.03048     -10250.821    \n",
            "      3259   341.9196      -148.2226       0             -148.04581     -10404.106    \n",
            "      3260   302.1381      -148.17838      0             -148.02216     -10397.084    \n",
            "      3261   280.895       -148.15954      0             -148.0143      -10197.893    \n",
            "      3262   282.02745     -148.17617      0             -148.03035     -9753.1733    \n",
            "      3263   305.33932     -148.18596      0             -148.02809     -9324.9072    \n",
            "      3264   346.39961     -148.2084       0             -148.0293      -8623.4353    \n",
            "      3265   397.35844     -148.2371       0             -148.03165     -7859.1833    \n",
            "      3266   448.65372     -148.25094      0             -148.01897     -6775.786     \n",
            "      3267   491.35351     -148.26783      0             -148.01378     -5677.7741    \n",
            "      3268   518.8258      -148.27527      0             -148.00702     -4331.3822    \n",
            "      3269   527.51338     -148.2898       0             -148.01705     -2902.8706    \n",
            "      3270   518.20552     -148.2899       0             -148.02197     -1343.6072    \n",
            "      3271   495.16624     -148.28566      0             -148.02964      257.0783     \n",
            "      3272   464.38103     -148.28482      0             -148.04471      1694.4776    \n",
            "      3273   432.16393     -148.22742      0             -148.00398      3288.5094    \n",
            "      3274   403.18971     -148.21991      0             -148.01144      4598.9933    \n",
            "      3275   379.99165     -148.20624      0             -148.00977      5767.9547    \n",
            "      3276   362.47132     -148.20431      0             -148.0169       6844.9817    \n",
            "      3277   348.60137     -148.18587      0             -148.00563      7644.0968    \n",
            "      3278   335.32401     -148.18194      0             -148.00856      8303.1808    \n",
            "      3279   320.30413     -148.15963      0             -147.99402      8742.0732    \n",
            "      3280   303.28834     -148.17185      0             -148.01504      9062.2302    \n",
            "      3281   285.672       -148.13869      0             -147.99099      9248.8829    \n",
            "      3282   271.24432     -148.12952      0             -147.98927      8981.3945    \n",
            "      3283   265.29076     -148.14441      0             -148.00724      8615.3564    \n",
            "      3284   272.83974     -148.14087      0             -147.9998       8071.3722    \n",
            "      3285   297.26537     -148.17676      0             -148.02307      7091.9742    \n",
            "      3286   339.28942     -148.18476      0             -148.00934      5808.3642    \n",
            "      3287   396.18287     -148.21612      0             -148.01127      4275.2288    \n",
            "      3288   461.70141     -148.25665      0             -148.01793      2442.4634    \n",
            "      3289   527.04616     -148.27105      0             -147.99855      617.29214    \n",
            "      3290   582.47738     -148.29918      0             -147.99802     -1426.596     \n",
            "      3291   619.08542     -148.31018      0             -147.99009     -3442.1235    \n",
            "      3292   630.50127     -148.33984      0             -148.01385     -5557.8773    \n",
            "      3293   614.11007     -148.3288       0             -148.01128     -7684.9002    \n",
            "      3294   571.86059     -148.29019      0             -147.99451     -9607.6495    \n",
            "      3295   509.21673     -148.26112      0             -147.99783     -11491.766    \n",
            "      3296   434.44253     -148.21241      0             -147.98779     -13352.051    \n",
            "      3297   357.08116     -148.17658      0             -147.99195     -14933.353    \n",
            "      3298   285.43769     -148.14911      0             -148.00153     -16477.569    \n",
            "      3299   225.71136     -148.11989      0             -148.00319     -17968.101    \n",
            "      3300   181.63455     -148.12388      0             -148.02997     -19258.802    \n",
            "      3301   153.76429     -148.0849       0             -148.0054      -20241.02     \n",
            "      3302   140.52737     -148.0701       0             -147.99744     -21147.345    \n",
            "      3303   139.32408     -148.03796      0             -147.96592     -21899.412    \n",
            "      3304   147.00559     -148.08282      0             -148.00681     -22233.678    \n",
            "      3305   161.65574     -148.06803      0             -147.98444     -22293.896    \n",
            "      3306   182.76827     -148.06129      0             -147.96679     -22094.321    \n",
            "      3307   211.65709     -148.11378      0             -148.00434     -21677.362    \n",
            "      3308   250.83662     -148.10825      0             -147.97856     -20776.168    \n",
            "      3309   302.37316     -148.16067      0             -148.00433     -19833.093    \n",
            "      3310   367.06253     -148.17161      0             -147.98183     -18746.554    \n",
            "      3311   443.30014     -148.21469      0             -147.98548     -17365.208    \n",
            "      3312   526.666       -148.26351      0             -147.9912      -15899.522    \n",
            "      3313   609.81763     -148.31416      0             -147.99886     -14250.295    \n",
            "      3314   683.66801     -148.32479      0             -147.97131     -12400.941    \n",
            "      3315   738.12192     -148.36992      0             -147.98828     -10452.358    \n",
            "      3316   764.80136     -148.3887       0             -147.99326     -8435.7328    \n",
            "      3317   758.99374     -148.36483      0             -147.9724      -6248.5037    \n",
            "      3318   719.60814     -148.36458      0             -147.99251     -4004.436     \n",
            "      3319   650.95945     -148.33345      0             -147.99688     -1783.4733    \n",
            "      3320   561.67492     -148.27801      0             -147.9876       388.27021    \n",
            "      3321   462.7958      -148.2236       0             -147.98431      2439.3326    \n",
            "      3322   366.20231     -148.14737      0             -147.95802      4385.8548    \n",
            "      3323   282.65925     -148.13509      0             -147.98895      6013.34      \n",
            "      3324   220.04901     -148.10201      0             -147.98823      7474.1106    \n",
            "      3325   182.48408     -148.08634      0             -147.99199      8616.1717    \n",
            "      3326   170.06216     -148.06705      0             -147.97912      9517.9002    \n",
            "      3327   179.81345     -148.08009      0             -147.98712      10162.719    \n",
            "      3328   206.95916     -148.1025       0             -147.9955       10472.663    \n",
            "      3329   245.90281     -148.10516      0             -147.97802      10639.675    \n",
            "      3330   291.8888      -148.12353      0             -147.97261      10607.651    \n",
            "      3331   342.01118     -148.16889      0             -147.99206      10294.105    \n",
            "      3332   395.3048      -148.18792      0             -147.98353      9910.495     \n",
            "      3333   452.17578     -148.21248      0             -147.97869      9196.3731    \n",
            "      3334   513.55713     -148.25677      0             -147.99124      8207.8294    \n",
            "      3335   579.11305     -148.30753      0             -148.0081       7011.1188    \n",
            "      3336   646.5625      -148.32253      0             -147.98823      5804.6898    \n",
            "      3337   710.87977     -148.37378      0             -148.00623      4315.5676    \n",
            "      3338   765.07869     -148.38787      0             -147.99229      2778.7744    \n",
            "      3339   801.11474     -148.42145      0             -148.00724      1238.2997    \n",
            "      3340   811.4372      -148.41538      0             -147.99583     -354.73738    \n",
            "      3341   791.48829     -148.4167       0             -148.00747     -1788.7299    \n",
            "      3342   741.04146     -148.37216      0             -147.98901     -3036.355     \n",
            "      3343   665.11794     -148.30723      0             -147.96334     -4181.26      \n",
            "      3344   572.50336     -148.27841      0             -147.98241     -5101.4323    \n",
            "      3345   475.05762     -148.24419      0             -147.99856     -5867.9019    \n",
            "      3346   385.24923     -148.17126      0             -147.97207     -6417.2255    \n",
            "      3347   313.45797     -148.14055      0             -147.97848     -6902.8918    \n",
            "      3348   266.30699     -148.12529      0             -147.9876      -7185.0818    \n",
            "      3349   245.79006     -148.12936      0             -148.00227     -7359.8835    \n",
            "      3350   249.43404     -148.09454      0             -147.96557     -7290.2143    \n",
            "      3351   271.3311      -148.11067      0             -147.97038     -7197.4316    \n",
            "      3352   303.89785     -148.13526      0             -147.97814     -6632.2964    \n",
            "      3353   340.09912     -148.1395       0             -147.96366     -6049.4287    \n",
            "      3354   374.619       -148.16602      0             -147.97232     -5175.9677    \n",
            "      3355   405.00436     -148.18826      0             -147.97885     -3992.4697    \n",
            "      3356   432.07684     -148.21087      0             -147.98747     -2775.2028    \n",
            "      3357   458.355       -148.21325      0             -147.97626     -1363.2227    \n",
            "      3358   486.65065     -148.2379       0             -147.98628      198.70647    \n",
            "      3359   518.8736      -148.26502      0             -147.99674      1882.3308    \n",
            "      3360   554.65481     -148.27814      0             -147.99136      3455.7795    \n",
            "      3361   590.73967     -148.29346      0             -147.98802      5016.7852    \n",
            "      3362   621.81255     -148.29938      0             -147.97788      6553.7834    \n",
            "      3363   641.51474     -148.35517      0             -148.02348      7948.5508    \n",
            "      3364   644.25102     -148.34186      0             -148.00875      9354.0406    \n",
            "      3365   627.0391      -148.32178      0             -147.99757      10577.749    \n",
            "      3366   589.89285     -148.30011      0             -147.99511      11659.861    \n",
            "      3367   536.50031     -148.27716      0             -147.99977      12635.275    \n",
            "      3368   474.17198     -148.25331      0             -148.00814      13444.671    \n",
            "      3369   412.06215     -148.24426      0             -148.0312       13886.843    \n",
            "      3370   359.06533     -148.19001      0             -148.00436      14171.768    \n",
            "      3371   322.52599     -148.16408      0             -147.99732      14004.004    \n",
            "      3372   306.37847     -148.15387      0             -147.99546      13668.962    \n",
            "      3373   310.50437     -148.17091      0             -148.01037      12715.036    \n",
            "      3374   331.15295     -148.17238      0             -148.00116      11625.508    \n",
            "      3375   361.66755     -148.20779      0             -148.02079      10348.63     \n",
            "      3376   393.89563     -148.20479      0             -148.00113      8741.5601    \n",
            "      3377   420.26256     -148.23884      0             -148.02155      6908.9205    \n",
            "      3378   435.3215      -148.24382      0             -148.01874      5159.051     \n",
            "      3379   436.31079     -148.21132      0             -147.98573      3353.7643    \n",
            "      3380   423.79383     -148.23053      0             -148.01141      1445.9315    \n",
            "      3381   401.1177      -148.22733      0             -148.01993     -469.02617    \n",
            "      3382   373.23809     -148.19971      0             -148.00673     -2294.8499    \n",
            "      3383   345.48032     -148.19392      0             -148.01529     -4196.6478    \n",
            "      3384   322.53135     -148.17998      0             -148.01322     -5838.6127    \n",
            "      3385   307.37375     -148.16493      0             -148.006       -7451.9418    \n",
            "      3386   300.54014     -148.17297      0             -148.01758     -9011.7968    \n",
            "      3387   300.37503     -148.17382      0             -148.01851     -10323.247    \n",
            "      3388   304.21239     -148.15006      0             -147.99277     -11391.346    \n",
            "      3389   308.99756     -148.18769      0             -148.02792     -12112.947    \n",
            "      3390   312.44896     -148.18843      0             -148.02688     -12644.992    \n",
            "      3391   314.24946     -148.1827       0             -148.02022     -12775.949    \n",
            "      3392   315.61821     -148.16903      0             -148.00585     -12500.823    \n",
            "      3393   319.27398     -148.19932      0             -148.03424     -12015.207    \n",
            "      3394   329.1429      -148.17787      0             -148.00769     -11256.026    \n",
            "      3395   348.40122     -148.21537      0             -148.03523     -10076.414    \n",
            "      3396   378.7105      -148.23328      0             -148.03747     -8861.1969    \n",
            "      3397   418.993       -148.25242      0             -148.03579     -7417.2383    \n",
            "      3398   465.13323     -148.27556      0             -148.03506     -5672.7917    \n",
            "      3399   510.63296     -148.26943      0             -148.00541     -3749.6562    \n",
            "      3400   547.55738     -148.32459      0             -148.04148     -1866.033     \n",
            "      3401   568.09549     -148.33297      0             -148.03924      341.90536    \n",
            "      3402   566.54739     -148.33802      0             -148.04509      2452.6517    \n",
            "      3403   540.49527     -148.30558      0             -148.02612      4792.5386    \n",
            "      3404   491.86681     -148.2971       0             -148.04278      6957.5514    \n",
            "      3405   426.4307      -148.25152      0             -148.03104      9284.8358    \n",
            "      3406   353.07923     -148.23877      0             -148.05622      11498.922    \n",
            "      3407   281.79792     -148.18548      0             -148.03978      13599.018    \n",
            "      3408   222.05982     -148.14849      0             -148.03368      15423.028    \n",
            "      3409   181.15886     -148.12714      0             -148.03347      16924.296    \n",
            "      3410   162.71606     -148.16146      0             -148.07733      17995.533    \n",
            "      3411   166.54807     -148.129        0             -148.04289      18955.544    \n",
            "      3412   189.16278     -148.14683      0             -148.04903      19556.327    \n",
            "      3413   225.27796     -148.18086      0             -148.06439      19972.807    \n",
            "      3414   269.15513     -148.1739       0             -148.03474      20032.505    \n",
            "      3415   315.71965     -148.19933      0             -148.03609      19880.984    \n",
            "      3416   361.58685     -148.24749      0             -148.06053      19310.315    \n",
            "      3417   405.57648     -148.23836      0             -148.02866      18731.033    \n",
            "      3418   448.32354     -148.27597      0             -148.04417      17697.435    \n",
            "      3419   491.21394     -148.31494      0             -148.06097      16454.92     \n",
            "      3420   534.71941     -148.34294      0             -148.06647      15007.244    \n",
            "      3421   577.52215     -148.37917      0             -148.08057      13286.923    \n",
            "      3422   616.2938      -148.39211      0             -148.07346      11465.941    \n",
            "      3423   646.08431     -148.38754      0             -148.05348      9580.3085    \n",
            "      3424   660.78296     -148.42025      0             -148.0786       7566.1158    \n",
            "      3425   654.92836     -148.41312      0             -148.07449      5532.4882    \n",
            "      3426   625.65214     -148.38999      0             -148.0665       3673.2447    \n",
            "      3427   573.53089     -148.36306      0             -148.06652      1900.201     \n",
            "      3428   502.52729     -148.33859      0             -148.07876      214.45904    \n",
            "      3429   420.29322     -148.27262      0             -148.05531     -1281.0236    \n",
            "      3430   336.63359     -148.2456       0             -148.07154     -2514.8745    \n",
            "      3431   261.34076     -148.19924      0             -148.06412     -3631.6515    \n",
            "      3432   203.05405     -148.19228      0             -148.08729     -4716.4012    \n",
            "      3433   167.20438     -148.14909      0             -148.06264     -5575.7237    \n",
            "      3434   155.5826      -148.17314      0             -148.0927      -6285.0138    \n",
            "      3435   166.21573     -148.14992      0             -148.06397     -6872.9428    \n",
            "      3436   194.65598     -148.17769      0             -148.07705     -7208.2122    \n",
            "      3437   234.76349     -148.19809      0             -148.07671     -7575.9869    \n",
            "      3438   280.11546     -148.23182      0             -148.08699     -7547.8146    \n",
            "      3439   325.99952     -148.24966      0             -148.08111     -7210.8136    \n",
            "      3440   369.29782     -148.26301      0             -148.07207     -6824.6904    \n",
            "      3441   408.71363     -148.30891      0             -148.09759     -6225.2599    \n",
            "      3442   445.07568     -148.29452      0             -148.0644      -5527.0519    \n",
            "      3443   479.56062     -148.31892      0             -148.07097     -4609.9843    \n",
            "      3444   512.71789     -148.35758      0             -148.09248     -3720.6806    \n",
            "      3445   544.09298     -148.3673       0             -148.08598     -2770.9872    \n",
            "      3446   571.29805     -148.39167      0             -148.09629     -1789.2125    \n",
            "      3447   590.55676     -148.4003       0             -148.09496     -895.11073    \n",
            "      3448   597.70689     -148.4086       0             -148.09957     -7.1481647    \n",
            "      3449   588.80335     -148.40662      0             -148.10218      804.22441    \n",
            "      3450   561.41846     -148.41173      0             -148.12146      1647.4107    \n",
            "      3451   515.83887     -148.39391      0             -148.1272       2337.3099    \n",
            "      3452   455.1251      -148.33871      0             -148.10339      2948.5151    \n",
            "      3453   385.07184     -148.31066      0             -148.11156      3500.061     \n",
            "      3454   313.15245     -148.27555      0             -148.11364      3879.8457    \n",
            "      3455   247.05215     -148.24177      0             -148.11403      4002.6868    \n",
            "      3456   193.70663     -148.22264      0             -148.12249      3982.0046    \n",
            "      3457   158.04294     -148.18949      0             -148.10778      3715.9202    \n",
            "      3458   142.08781     -148.20357      0             -148.13011      3220.6075    \n",
            "      3459   144.96666     -148.18125      0             -148.1063       2575.0442    \n",
            "      3460   163.17703     -148.22509      0             -148.14072      1757.166     \n",
            "      3461   191.50322     -148.22804      0             -148.12902      852.74831    \n",
            "      3462   224.14074     -148.22097      0             -148.10509     -221.11394    \n",
            "      3463   255.79175     -148.27068      0             -148.13843     -1431.1201    \n",
            "      3464   282.69254     -148.28181      0             -148.13565     -2736.7218    \n",
            "      3465   302.94129     -148.28679      0             -148.13016     -3847.2526    \n",
            "      3466   316.69485     -148.30272      0             -148.13897     -5030.683     \n",
            "      3467   325.62474     -148.30101      0             -148.13265     -6275.937     \n",
            "      3468   332.06056     -148.29301      0             -148.12132     -7374.5927    \n",
            "      3469   338.23717     -148.27723      0             -148.10234     -8530.1964    \n",
            "      3470   345.44776     -148.33757      0             -148.15896     -9630.4239    \n",
            "      3471   353.60046     -148.33735      0             -148.15453     -10731.942    \n",
            "      3472   361.26894     -148.31187      0             -148.12508     -11413.188    \n",
            "      3473   365.88665     -148.34395      0             -148.15477     -12204.398    \n",
            "      3474   364.65082     -148.33188      0             -148.14335     -12801.615    \n",
            "      3475   355.58487     -148.28645      0             -148.1026      -13034.043    \n",
            "      3476   337.87817     -148.31046      0             -148.13577     -13143.264    \n",
            "      3477   312.98779     -148.32231      0             -148.16049     -13241.027    \n",
            "      3478   283.95304     -148.31147      0             -148.16466     -12999.944    \n",
            "      3479   255.14917     -148.28766      0             -148.15574     -12591.027    \n",
            "      3480   231.10946     -148.27218      0             -148.15269     -11980.983    \n",
            "      3481   215.45136     -148.28498      0             -148.17359     -11198.469    \n",
            "      3482   210.21927     -148.26828      0             -148.15958     -10331.252    \n",
            "      3483   215.11323     -148.27254      0             -148.16132     -9383.3683    \n",
            "      3484   227.64175     -148.27959      0             -148.16189     -8329.5584    \n",
            "      3485   243.61556     -148.29074      0             -148.16478     -7215.5894    \n",
            "      3486   258.02691     -148.27837      0             -148.14496     -5911.4361    \n",
            "      3487   266.29323     -148.30476      0             -148.16708     -4712.0466    \n",
            "      3488   265.5996      -148.28898      0             -148.15166     -3287.6105    \n",
            "      3489   255.48596     -148.27781      0             -148.14572     -1918.0396    \n",
            "      3490   237.89565     -148.27484      0             -148.15184     -439.94093    \n",
            "      3491   216.51911     -148.29447      0             -148.18253      932.02775    \n",
            "      3492   195.90757     -148.2451       0             -148.14381      2481.2139    \n",
            "      3493   180.59132     -148.27914      0             -148.18577      3639.204     \n",
            "      3494   174.03599     -148.26408      0             -148.17409      4826.5646    \n",
            "      3495   177.62983     -148.28008      0             -148.18824      5823.9682    \n",
            "      3496   190.46594     -148.26429      0             -148.16582      6626.0355    \n",
            "      3497   209.89121     -148.28926      0             -148.18073      7225.1502    \n",
            "      3498   232.07408     -148.28849      0             -148.1685       7749.0189    \n",
            "      3499   253.32485     -148.31689      0             -148.18592      8069.431     \n",
            "      3500   270.80229     -148.32136      0             -148.18134      8114.9979    \n",
            "      3501   283.34896     -148.32989      0             -148.18339      8212.8891    \n",
            "      3502   291.79306     -148.32431      0             -148.17344      8027.0629    \n",
            "      3503   297.75261     -148.33267      0             -148.17872      7666.6352    \n",
            "      3504   303.58195     -148.34605      0             -148.18908      7164.8029    \n",
            "      3505   311.65183     -148.32784      0             -148.16671      6414.6547    \n",
            "      3506   322.89208     -148.32269      0             -148.15574      5634.6575    \n",
            "      3507   336.63753     -148.35377      0             -148.17972      4610.9115    \n",
            "      3508   350.60714     -148.35557      0             -148.1743       3291.9979    \n",
            "      3509   361.12793     -148.4014       0             -148.21468      2078.4234    \n",
            "      3510   364.30689     -148.36815      0             -148.17979      748.60442    \n",
            "      3511   357.05467     -148.35678      0             -148.17217     -663.7859     \n",
            "      3512   337.87337     -148.38844      0             -148.21374     -2003.4633    \n",
            "      3513   307.5214      -148.35333      0             -148.19433     -3236.6779    \n",
            "      3514   268.88966     -148.33838      0             -148.19935     -4545.5179    \n",
            "      3515   226.75185     -148.30182      0             -148.18458     -5624.7221    \n",
            "      3516   186.69681     -148.27367      0             -148.17714     -6566.7795    \n",
            "      3517   154.051       -148.26802      0             -148.18837     -7503.7048    \n",
            "      3518   132.89559     -148.2493       0             -148.18059     -8400.0132    \n",
            "      3519   125.39162     -148.23639      0             -148.17156     -9201.084     \n",
            "      3520   131.36779     -148.28311      0             -148.21518     -9935.1128    \n",
            "      3521   148.81333     -148.27108      0             -148.19413     -10376.879    \n",
            "      3522   174.48135     -148.32025      0             -148.23004     -10857.687    \n",
            "      3523   204.47942     -148.30521      0             -148.19948     -11020.255    \n",
            "      3524   235.29709     -148.33629      0             -148.21463     -11026.322    \n",
            "      3525   264.47532     -148.34828      0             -148.21154     -10969.9      \n",
            "      3526   291.00115     -148.36972      0             -148.21926     -10691.415    \n",
            "      3527   314.95605     -148.35547      0             -148.19263     -10109.741    \n",
            "      3528   337.15788     -148.36177      0             -148.18744     -9304.5151    \n",
            "      3529   358.53767     -148.3751       0             -148.18972     -8621.1171    \n",
            "      3530   379.26244     -148.39621      0             -148.20011     -7853.2706    \n",
            "      3531   398.46926     -148.41554      0             -148.20952     -7026.0494    \n",
            "      3532   414.19217     -148.41628      0             -148.20213     -5861.1341    \n",
            "      3533   423.48439     -148.41229      0             -148.19333     -4875.4207    \n",
            "      3534   423.18278     -148.45809      0             -148.23929     -3843.5581    \n",
            "      3535   410.8268      -148.42101      0             -148.2086      -2796.6661    \n",
            "      3536   385.04661     -148.41424      0             -148.21515     -1606.9154    \n",
            "      3537   346.54505     -148.35891      0             -148.17973     -516.29972    \n",
            "      3538   298.32099     -148.39676      0             -148.24252      370.53503    \n",
            "      3539   244.98813     -148.34322      0             -148.21655      1435.2062    \n",
            "      3540   192.09393     -148.3108       0             -148.21148      2417.9873    \n",
            "      3541   145.36677     -148.29745      0             -148.22229      3136.4729    \n",
            "      3542   109.89544     -148.28795      0             -148.23113      3790.2768    \n",
            "      3543   89.013738     -148.29358      0             -148.24756      4137.6646    \n",
            "      3544   83.85663      -148.25258      0             -148.20922      4562.9156    \n",
            "      3545   93.445364     -148.25943      0             -148.21111      4595.663     \n",
            "      3546   114.9508      -148.31093      0             -148.2515       4502.2703    \n",
            "      3547   144.54874     -148.30501      0             -148.23027      4234.772     \n",
            "      3548   177.80999     -148.30853      0             -148.21659      4092.9841    \n",
            "      3549   210.71188     -148.34591      0             -148.23697      3654.5666    \n",
            "      3550   240.41034     -148.36758      0             -148.24328      3202.0104    \n",
            "      3551   265.58425     -148.36219      0             -148.22487      2622.7507    \n",
            "      3552   286.00279     -148.37184      0             -148.22397      1984.5659    \n",
            "      3553   302.45947     -148.36195      0             -148.20557      1421.0019    \n",
            "      3554   316.05097     -148.38299      0             -148.21958      692.24033    \n",
            "      3555   327.32205     -148.37761      0             -148.20838     -67.90242     \n",
            "      3556   335.99355     -148.43671      0             -148.26299     -772.23931    \n",
            "      3557   340.91646     -148.43526      0             -148.25899     -1540.4685    \n",
            "      3558   340.34691     -148.4232       0             -148.24723     -2097.5346    \n",
            "      3559   332.15639     -148.41076      0             -148.23902     -2628.022     \n",
            "      3560   314.82425     -148.39146      0             -148.22869     -2942.195     \n",
            "      3561   288.26647     -148.35991      0             -148.21086     -3176.6028    \n",
            "      3562   253.6459      -148.37121      0             -148.24007     -3315.105     \n",
            "      3563   213.87869     -148.3182       0             -148.20762     -3315.4784    \n",
            "      3564   172.99564     -148.32264      0             -148.2332      -3080.6049    \n",
            "      3565   135.69702     -148.32342      0             -148.25326     -2908.4306    \n",
            "      3566   106.23727     -148.2834       0             -148.22848     -2329.1304    \n",
            "      3567   87.433573     -148.30276      0             -148.25755     -1847.7626    \n",
            "      3568   80.482656     -148.33443      0             -148.29281     -1145.1098    \n",
            "      3569   84.462105     -148.26408      0             -148.22041     -334.78724    \n",
            "      3570   96.800646     -148.29766      0             -148.24761      445.12224    \n",
            "      3571   113.86741     -148.30456      0             -148.24569      1506.3364    \n",
            "      3572   131.72328     -148.30506      0             -148.23695      2456.6156    \n",
            "      3573   147.08525     -148.31453      0             -148.23848      3564.0533    \n",
            "      3574   158.18839     -148.32728      0             -148.24549      4802.6098    \n",
            "      3575   164.57382     -148.33443      0             -148.24934      6090.3249    \n",
            "      3576   167.84564     -148.32897      0             -148.24219      7205.5069    \n",
            "      3577   170.31632     -148.31222      0             -148.22416      8435.2172    \n",
            "      3578   174.15424     -148.35893      0             -148.26888      9472.0277    \n",
            "      3579   181.14953     -148.34103      0             -148.24737      10574.866    \n",
            "      3580   191.73636     -148.34447      0             -148.24533      11524.278    \n",
            "      3581   205.01699     -148.33361      0             -148.22761      12297.295    \n",
            "      3582   218.83331     -148.35353      0             -148.24038      12925.817    \n",
            "      3583   230.57965     -148.35592      0             -148.2367       13375.973    \n",
            "      3584   237.68811     -148.37117      0             -148.24828      13616.756    \n",
            "      3585   238.51743     -148.35707      0             -148.23374      13789.104    \n",
            "      3586   232.82681     -148.38847      0             -148.26809      13746.03     \n",
            "      3587   221.64657     -148.3405       0             -148.2259       13551.782    \n",
            "      3588   207.28123     -148.34031      0             -148.23314      13059.349    \n",
            "      3589   192.74919     -148.35583      0             -148.25617      12477.004    \n",
            "      3590   180.92678     -148.37087      0             -148.27732      11869.26     \n",
            "      3591   173.72297     -148.34579      0             -148.25597      10935.719    \n",
            "      3592   171.5653      -148.31695      0             -148.22825      9926.6363    \n",
            "      3593   173.56507     -148.36951      0             -148.27977      8687.3301    \n",
            "      3594   177.69193     -148.3509       0             -148.25903      7366.7392    \n",
            "      3595   180.99921     -148.34201      0             -148.24843      6047.6493    \n",
            "      3596   180.95413     -148.35108      0             -148.25752      4731.0043    \n",
            "      3597   176.03085     -148.34841      0             -148.2574       3339.0354    \n",
            "      3598   165.91391     -148.34841      0             -148.26263      1940.4358    \n",
            "      3599   151.82895     -148.34047      0             -148.26197      623.66326    \n",
            "      3600   136.40893     -148.32748      0             -148.25695     -537.75954    \n",
            "      3601   122.98925     -148.3083       0             -148.24471     -1633.9947    \n",
            "      3602   114.77673     -148.31438      0             -148.25503     -2598.4162    \n",
            "      3603   114.4115      -148.31484      0             -148.25568     -3464.9835    \n",
            "      3604   123.05938     -148.30352      0             -148.23989     -4281.3216    \n",
            "      3605   140.16002     -148.32436      0             -148.25189     -5053.5621    \n",
            "      3606   163.65331     -148.33069      0             -148.24607     -5570.9172    \n",
            "      3607   190.62567     -148.36034      0             -148.26178     -5956.3022    \n",
            "      3608   217.90837     -148.35184      0             -148.23917     -6133.5826    \n",
            "      3609   242.27695     -148.37187      0             -148.2466      -6219.3476    \n",
            "      3610   261.5952      -148.3795       0             -148.24424     -6038.368     \n",
            "      3611   274.8803      -148.39279      0             -148.25066     -5790.9099    \n",
            "      3612   282.21149     -148.41767      0             -148.27175     -5282.4294    \n",
            "      3613   284.78853     -148.40743      0             -148.26018     -4729.6821    \n",
            "      3614   284.17227     -148.40036      0             -148.25343     -3929.8321    \n",
            "      3615   281.67416     -148.40135      0             -148.25572     -3098.8661    \n",
            "      3616   278.04545     -148.42922      0             -148.28545     -2289.8805    \n",
            "      3617   273.15725     -148.40236      0             -148.26113     -1413.0416    \n",
            "      3618   265.92622     -148.4089       0             -148.2714      -550.83802    \n",
            "      3619   254.74988     -148.4204       0             -148.28869      371.6594     \n",
            "      3620   238.59123     -148.38952      0             -148.26616      1232.4615    \n",
            "      3621   216.79304     -148.35345      0             -148.24136      2248.4903    \n",
            "      3622   189.84195     -148.36585      0             -148.26769      3042.6958    \n",
            "      3623   159.67997     -148.34812      0             -148.26556      3746.1269    \n",
            "      3624   129.37175     -148.32378      0             -148.25689      4582.7487    \n",
            "      3625   102.71129     -148.31837      0             -148.26526      5350.0892    \n",
            "      3626   83.495877     -148.28869      0             -148.24552      5895.1085    \n",
            "      3627   75.078988     -148.2825       0             -148.24368      6068.7882    \n",
            "      3628   79.264534     -148.30243      0             -148.26144      6290.8866    \n",
            "      3629   96.042044     -148.31686      0             -148.2672       6445.8934    \n",
            "      3630   123.69179     -148.31266      0             -148.2487       6391.4142    \n",
            "      3631   158.95578     -148.35374      0             -148.27155      6025.8036    \n",
            "      3632   197.32091     -148.39658      0             -148.29456      5731.5823    \n",
            "      3633   234.32082     -148.36681      0             -148.24566      5309.8779    \n",
            "      3634   266.40174     -148.39634      0             -148.2586       4781.0862    \n",
            "      3635   290.94389     -148.40968      0             -148.25925      4169.632     \n",
            "      3636   306.6104      -148.43411      0             -148.27558      3477.9169    \n",
            "      3637   313.68685     -148.43367      0             -148.27148      2896.0911    \n",
            "      3638   314.00464     -148.41144      0             -148.24908      2254.9043    \n",
            "      3639   309.54377     -148.40454      0             -148.24449      1427.5976    \n",
            "      3640   301.83785     -148.43778      0             -148.28172      650.04899    \n",
            "      3641   291.88397     -148.40559      0             -148.25467      20.816898    \n",
            "      3642   279.80131     -148.41038      0             -148.26571     -861.03296    \n",
            "      3643   264.72107     -148.41562      0             -148.27875     -1507.541     \n",
            "      3644   245.47089     -148.39529      0             -148.26838     -2033.2193    \n",
            "      3645   221.28874     -148.36531      0             -148.2509      -2614.8043    \n",
            "      3646   192.0629      -148.36599      0             -148.26669     -2933.005     \n",
            "      3647   159.07904     -148.32581      0             -148.24356     -3253.4035    \n",
            "      3648   124.70064     -148.33532      0             -148.27084     -3518.5751    \n",
            "      3649   92.450129     -148.31867      0             -148.27087     -3535.3722    \n",
            "      3650   66.22861      -148.30263      0             -148.26838     -3494.4351    \n",
            "      3651   49.41127      -148.26849      0             -148.24294     -3372.2688    \n",
            "      3652   44.154274     -148.29192      0             -148.26909     -3213.9653    \n",
            "      3653   51.018056     -148.31262      0             -148.28624     -3042.9779    \n",
            "      3654   68.752058     -148.31378      0             -148.27823     -2686.3652    \n",
            "      3655   94.453811     -148.31912      0             -148.27028     -2260.5899    \n",
            "      3656   124.2866      -148.31182      0             -148.24756     -1691.5599    \n",
            "      3657   154.2219      -148.34816      0             -148.26842     -1254.3841    \n",
            "      3658   180.72999     -148.34412      0             -148.25068     -715.46801    \n",
            "      3659   201.64786     -148.357        0             -148.25273     -130.8774     \n",
            "      3660   216.37963     -148.3534       0             -148.24152      565.29463    \n",
            "      3661   225.78396     -148.37867      0             -148.26193      1068.7616    \n",
            "      3662   231.4888      -148.38162      0             -148.26194      1668.9525    \n",
            "      3663   235.334       -148.37979      0             -148.25811      2296.4016    \n",
            "      3664   238.89386     -148.40122      0             -148.2777       2780.3413    \n",
            "      3665   242.64407     -148.41133      0             -148.28587      3116.5621    \n",
            "      3666   245.86447     -148.40552      0             -148.2784       3425.8441    \n",
            "      3667   247.0404      -148.39355      0             -148.26582      3630.0953    \n",
            "      3668   244.21006     -148.3962       0             -148.26993      3816.7996    \n",
            "      3669   235.77091     -148.40059      0             -148.27869      3890.4357    \n",
            "      3670   220.90077     -148.36023      0             -148.24601      3740.9982    \n",
            "      3671   200.05494     -148.36114      0             -148.25771      3511.9887    \n",
            "      3672   174.8509      -148.35683      0             -148.26642      3203.0824    \n",
            "      3673   147.96716     -148.33739      0             -148.26088      2811.0063    \n",
            "      3674   123.17658     -148.341        0             -148.27731      2253.1037    \n",
            "      3675   103.68524     -148.30097      0             -148.24736      1728.2053    \n",
            "      3676   91.687852     -148.3246       0             -148.27719      894.12981    \n",
            "      3677   88.219372     -148.31205      0             -148.26644      167.49675    \n",
            "      3678   92.839881     -148.29947      0             -148.25147     -854.76293    \n",
            "      3679   103.78674     -148.30134      0             -148.24768     -1778.7955    \n",
            "      3680   118.20228     -148.33286      0             -148.27174     -2952.4314    \n",
            "      3681   133.01168     -148.36729      0             -148.29852     -3961.099     \n",
            "      3682   145.61758     -148.31422      0             -148.23893     -4848.4686    \n",
            "      3683   154.68939     -148.33332      0             -148.25334     -5770.3011    \n",
            "      3684   159.90337     -148.35538      0             -148.27271     -6872.2009    \n",
            "      3685   162.22783     -148.35139      0             -148.26751     -7696.0409    \n",
            "      3686   163.66626     -148.37592      0             -148.2913      -8613.5947    \n",
            "      3687   166.33663     -148.33594      0             -148.24994     -9285.944     \n",
            "      3688   172.31254     -148.34789      0             -148.2588      -9865.1245    \n",
            "      3689   182.77313     -148.36683      0             -148.27233     -10510.911    \n",
            "      3690   197.56426     -148.3516       0             -148.24945     -11008.886    \n",
            "      3691   215.14017     -148.37684      0             -148.2656      -11387.121    \n",
            "      3692   232.95817     -148.37919      0             -148.25874     -11566.934    \n",
            "      3693   248.38533     -148.3921       0             -148.26368     -11857.984    \n",
            "      3694   259.0219      -148.39309      0             -148.25917     -11644.127    \n",
            "      3695   263.08194     -148.38383      0             -148.24781     -11550.665    \n",
            "      3696   260.19566     -148.38646      0             -148.25193     -11274.032    \n",
            "      3697   251.24037     -148.39908      0             -148.26918     -10830.975    \n",
            "      3698   238.30535     -148.37967      0             -148.25646     -10219.104    \n",
            "      3699   223.92019     -148.37637      0             -148.26059     -9417.6892    \n",
            "      3700   210.65086     -148.36798      0             -148.25907     -8632.4872    \n",
            "      3701   200.32572     -148.36013      0             -148.25655     -7888.3445    \n",
            "      3702   193.62361     -148.37341      0             -148.27329     -6965.9272    \n",
            "      3703   190.02325     -148.33711      0             -148.23886     -6017.6104    \n",
            "      3704   187.91253     -148.34981      0             -148.25265     -5060.9841    \n",
            "      3705   185.24041     -148.34433      0             -148.24856     -4250.1793    \n",
            "      3706   180.33281     -148.33228      0             -148.23904     -3160.4668    \n",
            "      3707   172.18883     -148.33446      0             -148.24544     -2143.4997    \n",
            "      3708   161.235       -148.33554      0             -148.25217     -1112.1232    \n",
            "      3709   149.22183     -148.322        0             -148.24484     -223.53751    \n",
            "      3710   138.43491     -148.31526      0             -148.24369      705.20502    \n",
            "      3711   132.12879     -148.32563      0             -148.25731      1493.9817    \n",
            "      3712   133.27823     -148.31833      0             -148.24942      2225.8907    \n",
            "      3713   143.73657     -148.32089      0             -148.24657      2949.7977    \n",
            "      3714   163.94676     -148.32867      0             -148.24391      3463.1021    \n",
            "      3715   192.64157     -148.32693      0             -148.22733      3849.1011    \n",
            "      3716   226.99758     -148.36737      0             -148.25         4094.6713    \n",
            "      3717   262.88115     -148.3724       0             -148.23648      4307.119     \n",
            "      3718   296.03096     -148.41787      0             -148.26481      4278.7445    \n",
            "      3719   322.64683     -148.40033      0             -148.23351      4182.7349    \n",
            "      3720   340.22683     -148.41367      0             -148.23776      4050.9202    \n",
            "      3721   347.97672     -148.3972       0             -148.21728      3926.3468    \n",
            "      3722   346.76409     -148.44604      0             -148.26675      3531.6238    \n",
            "      3723   338.70808     -148.40907      0             -148.23395      3220.3458    \n",
            "      3724   326.41973     -148.40123      0             -148.23245      2879.531     \n",
            "      3725   312.37652     -148.39869      0             -148.23718      2286.196     \n",
            "      3726   298.04451     -148.40224      0             -148.24813      1694.3361    \n",
            "      3727   283.68091     -148.42442      0             -148.27775      987.30379    \n",
            "      3728   268.45086     -148.36961      0             -148.23081      370.24946    \n",
            "      3729   250.88452     -148.35653      0             -148.22681     -31.122997    \n",
            "      3730   229.5187      -148.35998      0             -148.24131     -701.60288    \n",
            "      3731   203.57528     -148.33454      0             -148.22929     -1268.0587    \n",
            "      3732   174.01947     -148.32176      0             -148.23178     -1610.377     \n",
            "      3733   142.86094     -148.30574      0             -148.23188     -1968.0288    \n",
            "      3734   113.46868     -148.30726      0             -148.24859     -2246.613     \n",
            "      3735   89.997331     -148.28378      0             -148.23725     -2482.6432    \n",
            "      3736   76.491776     -148.26099      0             -148.22144     -2539.1325    \n",
            "      3737   75.739961     -148.29086      0             -148.2517      -2720.6866    \n",
            "      3738   88.846972     -148.30587      0             -148.25993     -2755.7083    \n",
            "      3739   114.84442     -148.27711      0             -148.21773     -2630.0928    \n",
            "      3740   150.83992     -148.31715      0             -148.23916     -2611.7612    \n",
            "      3741   192.4762      -148.32214      0             -148.22263     -2301.7723    \n",
            "      3742   234.72698     -148.36236      0             -148.241       -1992.6674    \n",
            "      3743   273.04331     -148.40596      0             -148.26479     -1584.6337    \n",
            "      3744   304.29536     -148.37447      0             -148.21714     -1069.3782    \n",
            "      3745   326.83638     -148.41227      0             -148.24328     -464.56999    \n",
            "      3746   340.6418      -148.40412      0             -148.22799      301.86263    \n",
            "      3747   347.17836     -148.41537      0             -148.23587      1074.5876    \n",
            "      3748   348.48082     -148.39685      0             -148.21667      1675.253     \n",
            "      3749   346.28369     -148.4079       0             -148.22886      2433.0965    \n",
            "      3750   341.4256      -148.38897      0             -148.21244      3108.8514    \n",
            "      3751   333.67386     -148.38857      0             -148.21604      3728.2095    \n",
            "      3752   321.97016     -148.37755      0             -148.21108      4424.9951    \n",
            "      3753   304.74919     -148.38482      0             -148.22725      4754.9656    \n",
            "      3754   280.54294     -148.39831      0             -148.25326      5217.2858    \n",
            "      3755   248.86044     -148.32811      0             -148.19944      5663.6453    \n",
            "      3756   210.70699     -148.32996      0             -148.22101      5815.8941    \n",
            "      3757   168.56714     -148.30714      0             -148.21998      6126.2636    \n",
            "      3758   126.67427     -148.30488      0             -148.23939      6229.2898    \n",
            "      3759   89.738504     -148.28131      0             -148.23491      6314.2775    \n",
            "      3760   62.181935     -148.24616      0             -148.21401      6144.0247    \n",
            "      3761   47.82212      -148.23222      0             -148.20749      5818.0037    \n",
            "      3762   48.574548     -148.23104      0             -148.20592      5473.4525    \n",
            "      3763   64.240222     -148.26296      0             -148.22975      4915.0551    \n",
            "      3764   92.436187     -148.25449      0             -148.20669      4229.8699    \n",
            "      3765   129.0213      -148.28522      0             -148.21851      3488.9818    \n",
            "      3766   169.20657     -148.32646      0             -148.23898      2676.0656    \n",
            "      3767   208.29262     -148.32572      0             -148.21803      1762.3526    \n",
            "      3768   242.35159     -148.34165      0             -148.21634      1008.0136    \n",
            "      3769   269.23391     -148.33334      0             -148.19414      139.51553    \n",
            "      3770   288.23041     -148.35856      0             -148.20953     -587.19277    \n",
            "      3771   300.21643     -148.37647      0             -148.22125     -1457.2549    \n",
            "      3772   307.18498     -148.34084      0             -148.18202     -2153.049     \n",
            "      3773   311.41518     -148.35868      0             -148.19766     -2897.7826    \n",
            "      3774   314.80793     -148.37094      0             -148.20817     -3521.2091    \n",
            "      3775   318.09778     -148.36969      0             -148.20522     -4110.8846    \n",
            "      3776   320.59651     -148.3728       0             -148.20704     -4581.7301    \n",
            "      3777   320.43386     -148.37751      0             -148.21183     -5020.1327    \n",
            "      3778   315.51609     -148.37866      0             -148.21552     -5193.6118    \n",
            "      3779   304.00791     -148.36564      0             -148.20846     -5249.6854    \n",
            "      3780   284.80016     -148.36958      0             -148.22233     -5121.3792    \n",
            "      3781   258.23599     -148.29972      0             -148.1662      -4816.6303    \n",
            "      3782   226.7693      -148.30031      0             -148.18306     -4422.2175    \n",
            "      3783   194.12566     -148.31533      0             -148.21496     -3801.232     \n",
            "      3784   164.41046     -148.272        0             -148.18699     -3023.5844    \n",
            "      3785   141.86318     -148.27305      0             -148.1997      -2265.9334    \n",
            "      3786   129.69301     -148.27391      0             -148.20685     -1383.9096    \n",
            "      3787   129.33499     -148.27758      0             -148.21071     -396.9515     \n",
            "      3788   140.09798     -148.26742      0             -148.19499      629.6072     \n",
            "      3789   159.54335     -148.28693      0             -148.20444      1645.6599    \n",
            "      3790   183.85932     -148.3046       0             -148.20954      2682.7658    \n",
            "      3791   208.92387     -148.29271      0             -148.18469      3947.8037    \n",
            "      3792   231.16517     -148.29124      0             -148.17172      5144.1883    \n",
            "      3793   248.48593     -148.31642      0             -148.18794      6311.0643    \n",
            "      3794   260.87297     -148.32739      0             -148.19251      7352.2053    \n",
            "      3795   269.88871     -148.30905      0             -148.1695       8463.0697    \n",
            "      3796   278.24026     -148.33737      0             -148.19351      9565.7639    \n",
            "      3797   288.96245     -148.34066      0             -148.19126      10461.194    \n",
            "      3798   304.44877     -148.31463      0             -148.15722      11296.222    \n",
            "      3799   325.53692     -148.34274      0             -148.17443      12000.094    \n",
            "      3800   351.01902     -148.37318      0             -148.19169      12262.62     \n",
            "      3801   377.7323      -148.38452      0             -148.18922      12506.305    \n",
            "      3802   401.27553     -148.37549      0             -148.16801      12695.847    \n",
            "      3803   417.30473     -148.39645      0             -148.18069      12685.502    \n",
            "      3804   422.33639     -148.39194      0             -148.17357      12488.029    \n",
            "      3805   414.60121     -148.39453      0             -148.18016      12205.696    \n",
            "      3806   395.19097     -148.36818      0             -148.16385      11877.485    \n",
            "      3807   367.17473     -148.34666      0             -148.15681      11264.461    \n",
            "      3808   334.77099     -148.34077      0             -148.16768      10682.315    \n",
            "      3809   302.93309     -148.31838      0             -148.16175      9849.7703    \n",
            "      3810   275.96003     -148.31616      0             -148.17347      9021.9792    \n",
            "      3811   256.45914     -148.29342      0             -148.16082      8024.8702    \n",
            "      3812   244.80889     -148.29274      0             -148.16617      6838.8079    \n",
            "      3813   239.45783     -148.27676      0             -148.15295      5630.4192    \n",
            "      3814   237.25919     -148.30849      0             -148.18582      4496.6628    \n",
            "      3815   234.96146     -148.28363      0             -148.16215      3314.5873    \n",
            "      3816   230.37841     -148.28877      0             -148.16965      2145.0162    \n",
            "      3817   222.72087     -148.27999      0             -148.16483      1111.545     \n",
            "      3818   212.91731     -148.26132      0             -148.15124      170.4959     \n",
            "      3819   203.90198     -148.2671       0             -148.16168     -758.02825    \n",
            "      3820   199.9603      -148.24483      0             -148.14144     -1575.917     \n",
            "      3821   205.1952      -148.27558      0             -148.16948     -2363.4613    \n",
            "      3822   222.59748     -148.28824      0             -148.17315     -2971.9508    \n",
            "      3823   253.02607     -148.30385      0             -148.17303     -3730.1861    \n",
            "      3824   294.6631      -148.27838      0             -148.12603     -4076.5417    \n",
            "      3825   343.33077     -148.33299      0             -148.15547     -4565.655     \n",
            "      3826   393.26473     -148.3301       0             -148.12676     -4888.4776    \n",
            "      3827   438.20665     -148.37443      0             -148.14786     -5178.9572    \n",
            "      3828   472.66824     -148.35084      0             -148.10645     -5019.5281    \n",
            "      3829   493.06026     -148.39658      0             -148.14164     -5040.5639    \n",
            "      3830   498.05049     -148.40746      0             -148.14995     -4844.1405    \n",
            "      3831   488.94585     -148.40511      0             -148.15231     -4314.468     \n",
            "      3832   469.19823     -148.36909      0             -148.1265      -3979.8796    \n",
            "      3833   442.72271     -148.35219      0             -148.12329     -3362.4473    \n",
            "      3834   413.20201     -148.33258      0             -148.11894     -2903.0005    \n",
            "      3835   383.50864     -148.32994      0             -148.13165     -2290.4391    \n",
            "      3836   354.77129     -148.32014      0             -148.1367      -1837.559     \n",
            "      3837   326.79645     -148.28396      0             -148.11499     -1184.1314    \n",
            "      3838   298.07076     -148.29031      0             -148.1362      -882.17981    \n",
            "      3839   266.67351     -148.27185      0             -148.13396     -353.10836    \n",
            "      3840   231.95819     -148.23536      0             -148.11543     -44.209675    \n",
            "      3841   194.52045     -148.23063      0             -148.13005      329.48315    \n",
            "      3842   156.69249     -148.21316      0             -148.13215      746.55663    \n",
            "      3843   122.39787     -148.19116      0             -148.12787      981.3039     \n",
            "      3844   96.440245     -148.17795      0             -148.12809      1295.8429    \n",
            "      3845   83.826026     -148.12509      0             -148.08175      1390.1963    \n",
            "      3846   88.689947     -148.17655      0             -148.13069      1258.1887    \n",
            "      3847   112.93998     -148.1882       0             -148.1298       1036.1605    \n",
            "      3848   155.79092     -148.19614      0             -148.11559      752.32423    \n",
            "      3849   213.82363     -148.22785      0             -148.1173       242.07965    \n",
            "      3850   281.15247     -148.2602       0             -148.11483     -196.89646    \n",
            "      3851   350.387       -148.29216      0             -148.11099     -828.71587    \n",
            "      3852   414.09414     -148.33035      0             -148.11625     -1627.7011    \n",
            "      3853   465.87645     -148.34854      0             -148.10767     -2244.4092    \n",
            "      3854   501.45418     -148.3784       0             -148.11913     -3147.0347    \n",
            "      3855   519.22069     -148.4058       0             -148.13734     -3784.8009    \n",
            "      3856   520.23714     -148.3635       0             -148.09452     -4513.7818    \n",
            "      3857   507.86139     -148.35039      0             -148.0878      -5169.4327    \n",
            "      3858   486.62815     -148.3632       0             -148.11159     -5945.1025    \n",
            "      3859   460.74574     -148.32375      0             -148.08552     -6585.3958    \n",
            "      3860   433.42167     -148.31074      0             -148.08664     -7284.8511    \n",
            "      3861   405.94718     -148.30497      0             -148.09508     -7891.1576    \n",
            "      3862   377.78282     -148.29413      0             -148.0988      -8491.1138    \n",
            "      3863   347.32552     -148.26555      0             -148.08597     -8780.1793    \n",
            "      3864   312.38267     -148.25679      0             -148.09527     -9289.0045    \n",
            "      3865   271.48004     -148.23839      0             -148.09803     -9561.8154    \n",
            "      3866   225.30849     -148.1802       0             -148.06371     -9597.2781    \n",
            "      3867   176.61871     -148.1714       0             -148.08008     -9677.4616    \n",
            "      3868   130.23277     -148.18944      0             -148.12211     -9509.1146    \n",
            "      3869   92.030897     -148.15602      0             -148.10844     -9213.8523    \n",
            "      3870   68.074834     -148.12338      0             -148.08818     -8743.5916    \n",
            "      3871   63.349541     -148.11583      0             -148.08308     -8316.5038    \n",
            "      3872   80.233038     -148.11349      0             -148.072       -7898.155     \n",
            "      3873   117.93044     -148.13672      0             -148.07575     -7287.3805    \n",
            "      3874   172.46142     -148.19529      0             -148.10612     -6691.5699    \n",
            "      3875   237.38375     -148.18628      0             -148.06354     -6124.3649    \n",
            "      3876   305.05307     -148.25497      0             -148.09724     -5378.3748    \n",
            "      3877   367.90257     -148.26943      0             -148.07921     -4652.9248    \n",
            "      3878   420.29403     -148.31371      0             -148.0964      -3737.2215    \n",
            "      3879   459.55823     -148.31322      0             -148.07561     -2627.6991    \n",
            "      3880   485.63078     -148.32388      0             -148.07279     -1683.9908    \n",
            "      3881   500.68298     -148.34412      0             -148.08525     -639.55354    \n",
            "      3882   508.69285     -148.327        0             -148.06398      434.54342    \n",
            "      3883   513.42983     -148.29218      0             -148.02672      1435.3331    \n",
            "      3884   517.20504     -148.33459      0             -148.06718      2247.8532    \n",
            "      3885   520.16634     -148.33791      0             -148.06896      2906.777     \n",
            "      3886   520.3991      -148.34102      0             -148.07195      3451.2672    \n",
            "      3887   514.31976     -148.32867      0             -148.06275      3943.0887    \n",
            "      3888   498.02863     -148.3521       0             -148.0946       4249.3243    \n",
            "      3889   469.0111      -148.29281      0             -148.05032      4558.2848    \n",
            "      3890   426.98529     -148.27884      0             -148.05807      4613.6131    \n",
            "      3891   374.13764     -148.26353      0             -148.07009      4860.2273    \n",
            "      3892   315.42808     -148.24127      0             -148.07818      4692.3906    \n",
            "      3893   258.30282     -148.20263      0             -148.06907      4578.7813    \n",
            "      3894   210.57215     -148.15556      0             -148.04668      4270.7561    \n",
            "      3895   178.67966     -148.13521      0             -148.04282      3808.24      \n",
            "      3896   166.88827     -148.1298       0             -148.04351      3242.9635    \n",
            "      3897   176.18542     -148.15831      0             -148.06721      2343.9057    \n",
            "      3898   204.01814     -148.13984      0             -148.03435      1383.8163    \n",
            "      3899   245.08393     -148.16875      0             -148.04203      198.57123    \n",
            "      3900   292.44588     -148.18102      0             -148.02981     -891.8796     \n",
            "      3901   338.89876     -148.19547      0             -148.02025     -2000.6048    \n",
            "      3902   378.88046     -148.22271      0             -148.02681     -3237.1321    \n",
            "      3903   409.47351     -148.2442       0             -148.03248     -4459.0558    \n",
            "      3904   430.49336     -148.27735      0             -148.05477     -5604.1754    \n",
            "      3905   444.55527     -148.25964      0             -148.02979     -6719.509     \n",
            "      3906   455.79253     -148.2778       0             -148.04213     -7776.5549    \n",
            "      3907   468.61329     -148.29571      0             -148.05341     -8839.5831    \n",
            "      3908   486.11537     -148.28724      0             -148.0359      -9858.1029    \n",
            "      3909   509.17643     -148.29195      0             -148.02869     -10804.418    \n",
            "      3910   535.98622     -148.32612      0             -148.04899     -11706.97     \n",
            "      3911   562.25616     -148.35006      0             -148.05935     -12509.82     \n",
            "      3912   582.591       -148.34178      0             -148.04055     -13133.575    \n",
            "      3913   592.00379     -148.33111      0             -148.02502     -13527.708    \n",
            "      3914   586.96944     -148.32767      0             -148.02418     -13815.818    \n",
            "      3915   566.51123     -148.31594      0             -148.02303     -13939.103    \n",
            "      3916   533.00642     -148.29902      0             -148.02343     -13852.046    \n",
            "      3917   490.88376     -148.26178      0             -148.00798     -13419.399    \n",
            "      3918   446.61774     -148.26434      0             -148.03343     -12957.937    \n",
            "      3919   406.54911     -148.25497      0             -148.04477     -12305.969    \n",
            "      3920   375.37614     -148.23098      0             -148.0369      -11508.743    \n",
            "      3921   355.72657     -148.2044       0             -148.02047     -10683.638    \n",
            "      3922   347.32147     -148.19039      0             -148.01081     -9725.9404    \n",
            "      3923   347.4319      -148.19843      0             -148.01879     -8685.3098    \n",
            "      3924   351.81118     -148.20735      0             -148.02545     -7601.0748    \n",
            "      3925   355.47995     -148.22107      0             -148.03727     -6467.5984    \n",
            "      3926   354.59356     -148.19016      0             -148.00682     -5204.5379    \n",
            "      3927   347.02638     -148.16897      0             -147.98955     -3834.4247    \n",
            "      3928   333.87795     -148.15929      0             -147.98666     -2499.0836    \n",
            "      3929   318.62019     -148.17317      0             -148.00843     -1012.1483    \n",
            "      3930   306.28595     -148.15979      0             -148.00143      292.94637    \n",
            "      3931   302.75142     -148.15635      0             -147.99982      1663.3393    \n",
            "      3932   312.74996     -148.16973      0             -148.00803      2861.1599    \n",
            "      3933   338.98409     -148.17731      0             -148.00205      3995.7021    \n",
            "      3934   381.36367     -148.2105       0             -148.01332      4703.3786    \n",
            "      3935   436.31132     -148.24015      0             -148.01456      5387.786     \n",
            "      3936   497.46287     -148.26289      0             -148.00568      6032.9866    \n",
            "      3937   556.96346     -148.30465      0             -148.01667      6249.4528    \n",
            "      3938   606.99939     -148.32279      0             -148.00895      6536.4964    \n",
            "      3939   641.24645     -148.33774      0             -148.00619      6572.903     \n",
            "      3940   656.57099     -148.36771      0             -148.02823      6583.8205    \n",
            "      3941   652.98287     -148.32832      0             -147.9907       6553.4626    \n",
            "      3942   633.68052     -148.30796      0             -147.98032      6023.3086    \n",
            "      3943   603.87474     -148.30944      0             -147.99721      5760.7656    \n",
            "      3944   569.54697     -148.25994      0             -147.96546      5207.5449    \n",
            "      3945   535.81446     -148.26108      0             -147.98405      4582.8668    \n",
            "      3946   505.33364     -148.25407      0             -147.99279      3700.3699    \n",
            "      3947   477.98582     -148.22006      0             -147.97293      2971.2614    \n",
            "      3948   451.13832     -148.23073      0             -147.99747      1909.0842    \n",
            "      3949   421.21252     -148.20554      0             -147.98775      1014.5651    \n",
            "      3950   384.70312     -148.20265      0             -148.00374      97.804008    \n",
            "      3951   339.58567     -148.17668      0             -148.0011      -678.2366     \n",
            "      3952   287.45004     -148.15253      0             -148.00391     -1277.8747    \n",
            "      3953   232.69673     -148.09906      0             -147.97875     -1690.2672    \n",
            "      3954   182.40555     -148.11718      0             -148.02287     -2163.7963    \n",
            "      3955   145.05303     -148.06111      0             -147.98611     -2288.1844    \n",
            "      3956   128.60525     -148.04575      0             -147.97926     -2495.3239    \n",
            "      3957   138.5268      -148.04646      0             -147.97483     -2512.8238    \n",
            "      3958   176.49344     -148.07852      0             -147.98727     -2558.9067    \n",
            "      3959   239.65901     -148.12638      0             -148.00246     -2464.4442    \n",
            "      3960   320.94473     -148.15256      0             -147.98662     -2139.5115    \n",
            "      3961   410.82493     -148.20282      0             -147.9904      -1918.293     \n",
            "      3962   499.0875      -148.24444      0             -147.9864      -1104.4477    \n",
            "      3963   576.49823     -148.2718       0             -147.97372     -301.05884    \n",
            "      3964   636.8361      -148.26578      0             -147.93651      770.19287    \n",
            "      3965   677.15282     -148.34351      0             -147.9934       1868.5142    \n",
            "      3966   698.266       -148.38083      0             -148.0198       3292.1505    \n",
            "      3967   703.80371     -148.34492      0             -147.98103      4657.0288    \n",
            "      3968   697.94308     -148.33709      0             -147.97623      6316.3306    \n",
            "      3969   684.3035      -148.354        0             -148.00019      7643.3423    \n",
            "      3970   664.80267     -148.33352      0             -147.98979      9162.7326    \n",
            "      3971   638.90801     -148.32295      0             -147.99261      10518.937    \n",
            "      3972   604.33453     -148.28657      0             -147.9741       11755.855    \n",
            "      3973   558.45456     -148.27117      0             -147.98242      12828.35     \n",
            "      3974   499.42895     -148.24692      0             -147.9887       13905.432    \n",
            "      3975   428.10299     -148.22932      0             -148.00797      14712.462    \n",
            "      3976   348.6202      -148.17529      0             -147.99504      15500.696    \n",
            "      3977   267.724       -148.1252       0             -147.98678      16037.347    \n",
            "      3978   194.8277      -148.08613      0             -147.9854       16437.033    \n",
            "      3979   139.96091     -148.06565      0             -147.99328      16513.625    \n",
            "      3980   111.79101     -148.04217      0             -147.98437      16277.193    \n",
            "      3981   115.87376     -148.06292      0             -148.00301      16031.724    \n",
            "      3982   153.21342     -148.08816      0             -148.00894      15219.141    \n",
            "      3983   220.11497     -148.08711      0             -147.9733       14230.75     \n",
            "      3984   308.8197      -148.1368       0             -147.97713      13006.402    \n",
            "      3985   408.33859     -148.20047      0             -147.98935      11497.676    \n",
            "      3986   506.98837     -148.25464      0             -147.99251      9890.8772    \n",
            "      3987   594.06119     -148.3065       0             -147.99935      8252.4975    \n",
            "      3988   661.66683     -148.33265      0             -147.99054      6485.0985    \n",
            "      3989   706.16732     -148.37011      0             -148.00499      4538.2895    \n",
            "      3990   727.49238     -148.38152      0             -148.00538      3005.2203    \n",
            "      3991   729.14393     -148.35372      0             -147.97672      1191.392     \n",
            "      3992   716.70045     -148.35752      0             -147.98696     -604.21056    \n",
            "      3993   695.38854     -148.36161      0             -148.00206     -2201.6673    \n",
            "      3994   669.38305     -148.36392      0             -148.01782     -3903.956     \n",
            "      3995   640.84556     -148.31625      0             -147.98491     -5333.1584    \n",
            "      3996   609.85599     -148.3281       0             -148.01278     -6653.4216    \n",
            "      3997   574.53554     -148.30704      0             -148.00998     -7936.0871    \n",
            "      3998   532.62542     -148.28222      0             -148.00683     -8786.7248    \n",
            "      3999   483.39282     -148.25114      0             -148.00121     -9406.0416    \n",
            "      4000   427.75657     -148.20165      0             -147.98048     -9664.6061    \n",
            "      4001   369.01963     -148.17581      0             -147.98501     -9681.7291    \n",
            "      4002   312.66478     -148.1562       0             -147.99454     -9535.313     \n",
            "      4003   265.442       -148.14315      0             -148.00591     -8946.6916    \n",
            "      4004   233.91409     -148.13531      0             -148.01437     -8205.5091    \n",
            "      4005   223.10134     -148.12163      0             -148.00628     -7369.7132    \n",
            "      4006   235.14349     -148.13276      0             -148.01119     -6389.272     \n",
            "      4007   268.77689     -148.15093      0             -148.01196     -5218.8851    \n",
            "      4008   319.19188     -148.15646      0             -147.99142     -3862.898     \n",
            "      4009   378.95348     -148.21925      0             -148.02331     -2500.5192    \n",
            "      4010   439.37403     -148.23247      0             -148.0053      -1013.6909    \n",
            "      4011   492.20005     -148.23754      0             -147.98305      643.40354    \n",
            "      4012   531.06984     -148.29568      0             -148.02109      2384.9699    \n",
            "      4013   552.59791     -148.31139      0             -148.02568      4050.7439    \n",
            "      4014   557.80535     -148.30669      0             -148.01828      5883.7034    \n",
            "      4015   550.68754     -148.29022      0             -148.00549      7517.9618    \n",
            "      4016   536.91184     -148.28053      0             -148.00293      9154.4806    \n",
            "      4017   522.88389     -148.31079      0             -148.04043      10450.957    \n",
            "      4018   513.7994      -148.29067      0             -148.02501      11679.491    \n",
            "      4019   512.26259     -148.27602      0             -148.01116      12672.315    \n",
            "      4020   517.75838     -148.30747      0             -148.03977      13324.741    \n",
            "      4021   526.96353     -148.30961      0             -148.03714      13683.622    \n",
            "      4022   534.9078      -148.29678      0             -148.02021      13847.788    \n",
            "      4023   536.48254     -148.28567      0             -148.00828      13859.677    \n",
            "      4024   527.78286     -148.28437      0             -148.01148      13489.949    \n",
            "      4025   507.49672     -148.29213      0             -148.02974      13020.337    \n",
            "      4026   477.28052     -148.27337      0             -148.0266       12346.073    \n",
            "      4027   441.15784     -148.26343      0             -148.03534      11530.613    \n",
            "      4028   404.93537     -148.22061      0             -148.01124      10480.335    \n",
            "      4029   374.25792     -148.22042      0             -148.02691      9332.2585    \n",
            "      4030   353.32753     -148.22158      0             -148.03889      7982.423     \n",
            "      4031   343.89255     -148.19781      0             -148.02         6593.3367    \n",
            "      4032   344.38192     -148.20254      0             -148.02448      5001.9369    \n",
            "      4033   350.58012     -148.2154       0             -148.03414      3342.6324    \n",
            "      4034   356.83881     -148.23999      0             -148.05549      1660.8301    \n",
            "      4035   357.89506     -148.20496      0             -148.01991      238.49573    \n",
            "      4036   350.28773     -148.19034      0             -148.00922     -1298.3091    \n",
            "      4037   333.29537     -148.22181      0             -148.04949     -2695.284     \n",
            "      4038   309.73783     -148.18685      0             -148.0267      -3806.4016    \n",
            "      4039   284.82394     -148.21346      0             -148.06619     -4850.864     \n",
            "      4040   265.45888     -148.15027      0             -148.01301     -5646.8733    \n",
            "      4041   258.52812     -148.1461       0             -148.01243     -6418.5662    \n",
            "      4042   268.81771     -148.16842      0             -148.02943     -7015.7481    \n",
            "      4043   297.86156     -148.19952      0             -148.04551     -7606.841     \n",
            "      4044   343.39429     -148.22834      0             -148.05079     -7910.2469    \n",
            "      4045   399.74681     -148.28694      0             -148.08025     -8221.338     \n",
            "      4046   459.01339     -148.29499      0             -148.05767     -8289.5411    \n",
            "      4047   512.74769     -148.30937      0             -148.04425     -8094.1768    \n",
            "      4048   553.82958     -148.36121      0             -148.07486     -7692.2101    \n",
            "      4049   577.66724     -148.34272      0             -148.04404     -7012.233     \n",
            "      4050   582.89001     -148.32546      0             -148.02409     -6195.8083    \n",
            "      4051   571.38253     -148.37257      0             -148.07714     -5292.5211    \n",
            "      4052   547.38271     -148.31821      0             -148.03519     -4230.7635    \n",
            "      4053   515.84662     -148.31266      0             -148.04595     -3093.9373    \n",
            "      4054   481.16038     -148.30386      0             -148.05508     -1952.5571    \n",
            "      4055   446.25423     -148.31385      0             -148.08312     -937.23342    \n",
            "      4056   412.02055     -148.28668      0             -148.07364      130.59517    \n",
            "      4057   377.62035     -148.26502      0             -148.06977      1125.5828    \n",
            "      4058   341.30319     -148.25288      0             -148.07642      1944.7275    \n",
            "      4059   301.46555     -148.22588      0             -148.07001      2781.5467    \n",
            "      4060   257.78809     -148.20865      0             -148.07537      3395.4745    \n",
            "      4061   212.38492     -148.18726      0             -148.07745      4032.4489    \n",
            "      4062   169.18518     -148.14918      0             -148.0617       4458.1836    \n",
            "      4063   133.602       -148.13386      0             -148.06478      4912.8599    \n",
            "      4064   111.83623     -148.11607      0             -148.05824      5026.2045    \n",
            "      4065   109.28855     -148.12809      0             -148.07159      4947.2371    \n",
            "      4066   129.19784     -148.13878      0             -148.07198      4595.3174    \n",
            "      4067   172.01187     -148.14651      0             -148.05757      4205.5979    \n",
            "      4068   234.67177     -148.2066       0             -148.08527      3307.802     \n",
            "      4069   311.03906     -148.22136      0             -148.06054      2458.2442    \n",
            "      4070   392.75569     -148.28211      0             -148.07904      1456.5873    \n",
            "      4071   470.09726     -148.33989      0             -148.09683      82.659637    \n",
            "      4072   534.25755     -148.34379      0             -148.06756     -1251.67      \n",
            "      4073   578.91185     -148.3443       0             -148.04498     -2623.047     \n",
            "      4074   600.48632     -148.40198      0             -148.09151     -4160.3899    \n",
            "      4075   598.85619     -148.43003      0             -148.12039     -5361.5006    \n",
            "      4076   577.33821     -148.37863      0             -148.08012     -6824.5184    \n",
            "      4077   541.08735     -148.40743      0             -148.12767     -8149.5229    \n",
            "      4078   496.21292     -148.33816      0             -148.0816      -9362.4479    \n",
            "      4079   448.22787     -148.32981      0             -148.09806     -10547.279    \n",
            "      4080   401.07925     -148.28335      0             -148.07597     -11618.496    \n",
            "      4081   356.67339     -148.2814       0             -148.09698     -12718.785    \n",
            "      4082   315.1418      -148.28312      0             -148.12017     -13504.211    \n",
            "      4083   275.36674     -148.25613      0             -148.11376     -14293.003    \n",
            "      4084   236.1997      -148.23722      0             -148.1151      -14873.29     \n",
            "      4085   197.43085     -148.19716      0             -148.09508     -15061.277    \n",
            "      4086   160.38573     -148.20169      0             -148.11877     -15183.978    \n",
            "      4087   128.09004     -148.16575      0             -148.09952     -14938.788    \n",
            "      4088   104.58295     -148.14153      0             -148.08746     -14570.249    \n",
            "      4089   94.696903     -148.16386      0             -148.11489     -14215.638    \n",
            "      4090   102.30059     -148.15973      0             -148.10684     -13548.531    \n",
            "      4091   128.98651     -148.1944       0             -148.12771     -12703.95     \n",
            "      4092   173.79748     -148.2064       0             -148.11654     -11803.821    \n",
            "      4093   232.89004     -148.2405       0             -148.12009     -10735.482    \n",
            "      4094   299.8606      -148.26187      0             -148.10683     -9570.6988    \n",
            "      4095   366.97305     -148.29287      0             -148.10313     -8218.6943    \n",
            "      4096   426.34106     -148.34118      0             -148.12075     -6840.7488    \n",
            "      4097   471.14447     -148.33442      0             -148.09082     -5277.8094    \n",
            "      4098   497.33628     -148.37631      0             -148.11917     -3690.7155    \n",
            "      4099   503.98601     -148.38628      0             -148.1257      -1980.1002    \n",
            "      4100   493.02983     -148.36609      0             -148.11118     -331.61771    \n",
            "      4101   469.05023     -148.38436      0             -148.14185      1334.7998    \n",
            "      4102   437.55571     -148.35973      0             -148.1335       2966.9764    \n",
            "      4103   403.90616     -148.33838      0             -148.12955      4269.5323    \n",
            "      4104   372.20559     -148.32435      0             -148.13191      5716.4855    \n",
            "      4105   344.5972      -148.31284      0             -148.13467      6674.764     \n",
            "      4106   321.48173     -148.26408      0             -148.09786      7496.4585    \n",
            "      4107   301.38594     -148.27056      0             -148.11473      8083.0385    \n",
            "      4108   282.30624     -148.27416      0             -148.1282       8589.0577    \n",
            "      4109   262.57038     -148.27524      0             -148.13948      8644.0877    \n",
            "      4110   241.57744     -148.27662      0             -148.15172      8780.5969    \n",
            "      4111   220.32177     -148.27698      0             -148.16307      8713.1983    \n",
            "      4112   201.49655     -148.22336      0             -148.11918      8297.4146    \n",
            "      4113   188.52157     -148.20877      0             -148.1113       7775.337     \n",
            "      4114   184.68299     -148.26128      0             -148.16579      7035.2819    \n",
            "      4115   192.1897      -148.25533      0             -148.15596      6145.4607    \n",
            "      4116   211.29363     -148.25281      0             -148.14356      5125.2194    \n",
            "      4117   239.88268     -148.29187      0             -148.16784      3906.5688    \n",
            "      4118   273.68654     -148.27893      0             -148.13743      2578.092     \n",
            "      4119   307.07136     -148.28701      0             -148.12824      1128.2654    \n",
            "      4120   334.35638     -148.32313      0             -148.15025     -189.11185    \n",
            "      4121   350.8231      -148.35462      0             -148.17323     -1694.499     \n",
            "      4122   354.02088     -148.32041      0             -148.13736     -2894.288     \n",
            "      4123   344.34912     -148.32313      0             -148.14509     -4044.383     \n",
            "      4124   324.89141     -148.32209      0             -148.15411     -5324.3873    \n",
            "      4125   300.75599     -148.31756      0             -148.16206     -6210.4563    \n",
            "      4126   277.56003     -148.2942       0             -148.15069     -7115.5242    \n",
            "      4127   260.48813     -148.28086      0             -148.14618     -7850.8088    \n",
            "      4128   253.09044     -148.2779       0             -148.14704     -8455.0145    \n",
            "      4129   256.45082     -148.28401      0             -148.15142     -8897.1708    \n",
            "      4130   269.11275     -148.29246      0             -148.15332     -9181.9049    \n",
            "      4131   287.86253     -148.29795      0             -148.14911     -9301.3353    \n",
            "      4132   308.14353     -148.32392      0             -148.1646      -9275.7516    \n",
            "      4133   325.59073     -148.31793      0             -148.14959     -8978.9652    \n",
            "      4134   337.12064     -148.33918      0             -148.16488     -8390.0401    \n",
            "      4135   341.06958     -148.37263      0             -148.19628     -7707.0147    \n",
            "      4136   337.63866     -148.34268      0             -148.1681      -6783.87      \n",
            "      4137   328.67039     -148.35317      0             -148.18324     -5542.861     \n",
            "      4138   316.99051     -148.33623      0             -148.17233     -4409.7279    \n",
            "      4139   305.07326     -148.3143       0             -148.15656     -2998.3323    \n",
            "      4140   294.57675     -148.33078      0             -148.17847     -1590.3014    \n",
            "      4141   285.86176     -148.30605      0             -148.15824     -228.70005    \n",
            "      4142   277.80567     -148.31965      0             -148.17602      1211.5162    \n",
            "      4143   268.5806      -148.31916      0             -148.18029      2571.3447    \n",
            "      4144   256.38655     -148.33364      0             -148.20108      3788.121     \n",
            "      4145   239.61587     -148.29447      0             -148.17058      5245.2521    \n",
            "      4146   218.10302     -148.28364      0             -148.17087      6375.3699    \n",
            "      4147   193.70065     -148.29961      0             -148.19946      7507.4662    \n",
            "      4148   169.70932     -148.25706      0             -148.16932      8454.1869    \n",
            "      4149   150.36407     -148.2534       0             -148.17566      9391.2485    \n",
            "      4150   140.30595     -148.2718       0             -148.19926      9899.2201    \n",
            "      4151   143.28404     -148.24602      0             -148.17194      10314.318    \n",
            "      4152   161.23489     -148.2329       0             -148.14953      10475.289    \n",
            "      4153   193.75099     -148.27069      0             -148.17052      10285.155    \n",
            "      4154   237.81414     -148.30736      0             -148.18441      9941.7252    \n",
            "      4155   288.3938      -148.3171       0             -148.16799      9335.4043    \n",
            "      4156   339.04958     -148.36285      0             -148.18755      8504.6754    \n",
            "      4157   383.17753     -148.38589      0             -148.18778      7509.8039    \n",
            "      4158   415.27277     -148.39049      0             -148.17578      6481.4424    \n",
            "      4159   431.60353     -148.40132      0             -148.17817      5277.9314    \n",
            "      4160   431.00994     -148.42288      0             -148.20003      3883.1328    \n",
            "      4161   415.0765      -148.41001      0             -148.1954       2820.1297    \n",
            "      4162   387.13128     -148.40818      0             -148.20802      1497.3303    \n",
            "      4163   351.44189     -148.3907       0             -148.20899      171.49241    \n",
            "      4164   312.37889     -148.36959      0             -148.20807     -980.78007    \n",
            "      4165   273.56129     -148.3529       0             -148.21146     -2307.736     \n",
            "      4166   237.34892     -148.3012       0             -148.17849     -3275.6319    \n",
            "      4167   204.79243     -148.29611      0             -148.19022     -4156.0108    \n",
            "      4168   175.7279      -148.30186      0             -148.211       -5205.7236    \n",
            "      4169   149.77377     -148.28299      0             -148.20555     -5754.8448    \n",
            "      4170   126.72868     -148.26849      0             -148.20297     -6155.4738    \n",
            "      4171   107.09802     -148.26756      0             -148.21218     -6451.9442    \n",
            "      4172   92.595812     -148.26603      0             -148.21815     -6428.5895    \n",
            "      4173   85.70377      -148.25049      0             -148.20618     -6190.5904    \n",
            "      4174   89.248875     -148.27342      0             -148.22727     -5700.0403    \n",
            "      4175   105.42772     -148.26831      0             -148.2138      -5134.4215    \n",
            "      4176   135.11334     -148.27292      0             -148.20306     -4339.7851    \n",
            "      4177   177.32574     -148.30538      0             -148.21369     -3418.1472    \n",
            "      4178   228.94032     -148.31329      0             -148.19491     -2336.2377    \n",
            "      4179   284.73905     -148.34971      0             -148.20249     -941.98429    \n",
            "      4180   338.50533     -148.39601      0             -148.22099      158.91452    \n",
            "      4181   383.53202     -148.42425      0             -148.22595      1711.1569    \n",
            "      4182   414.1019      -148.43326      0             -148.21916      3327.2142    \n",
            "      4183   426.82206     -148.41087      0             -148.19019      4870.657     \n",
            "      4184   420.61785     -148.44807      0             -148.2306       6612.3231    \n",
            "      4185   397.22955     -148.40341      0             -148.19803      8271.8899    \n",
            "      4186   360.70093     -148.39445      0             -148.20795      9758.3069    \n",
            "      4187   316.64243     -148.39927      0             -148.23556      11171.913    \n",
            "      4188   270.71627     -148.35862      0             -148.21865      12568.59     \n",
            "      4189   227.71154     -148.35065      0             -148.23291      13524.978    \n",
            "      4190   190.90172     -148.33305      0             -148.23435      14498.834    \n",
            "      4191   161.88519     -148.31876      0             -148.23506      15026.458    \n",
            "      4192   140.55006     -148.31098      0             -148.23831      15472.292    \n",
            "      4193   125.71449     -148.27568      0             -148.21068      15656.398    \n",
            "      4194   116.19832     -148.27856      0             -148.21848      15534.142    \n",
            "      4195   111.1701      -148.30845      0             -148.25097      15202.379    \n",
            "      4196   110.56774     -148.27628      0             -148.21911      14827.018    \n",
            "      4197   114.95608     -148.3079       0             -148.24846      14114.668    \n",
            "      4198   125.36423     -148.27801      0             -148.2132       13085.506    \n",
            "      4199   143.15385     -148.30455      0             -148.23053      12023.013    \n",
            "      4200   168.72976     -148.33189      0             -148.24465      10728.64     \n",
            "      4201   200.93326     -148.36079      0             -148.2569       9267.6536    \n",
            "      4202   237.18615     -148.34571      0             -148.22308      7651.3117    \n",
            "      4203   273.6446      -148.38037      0             -148.23888      6064.172     \n",
            "      4204   305.83661     -148.38733      0             -148.2292       4361.8945    \n",
            "      4205   329.00473     -148.41695      0             -148.24684      2609.1229    \n",
            "      4206   339.63457     -148.42219      0             -148.24658      969.76084    \n",
            "      4207   336.21713     -148.42463      0             -148.25079     -689.49642    \n",
            "      4208   319.63816     -148.40147      0             -148.2362      -2130.4766    \n",
            "      4209   292.7414      -148.38924      0             -148.23789     -3529.3308    \n",
            "      4210   259.92599     -148.37393      0             -148.23953     -4573.127     \n",
            "      4211   226.7248      -148.36016      0             -148.24294     -5558.2638    \n",
            "      4212   197.87166     -148.34797      0             -148.24566     -6550.7447    \n",
            "      4213   176.9111      -148.34302      0             -148.25155     -7158.4707    \n",
            "      4214   165.67855     -148.32389      0             -148.23823     -7505.412     \n",
            "      4215   163.94997     -148.35511      0             -148.27034     -7944.0172    \n",
            "      4216   169.70605     -148.36228      0             -148.27454     -8088.5063    \n",
            "      4217   180.04797     -148.33433      0             -148.24124     -8019.449     \n",
            "      4218   191.65862     -148.33382      0             -148.23473     -7560.9811    \n",
            "      4219   201.57676     -148.33313      0             -148.22891     -7165.382     \n",
            "      4220   208.31064     -148.34991      0             -148.24221     -6652.1512    \n",
            "      4221   211.50218     -148.34811      0             -148.23876     -5739.1866    \n",
            "      4222   211.91013     -148.34231      0             -148.23274     -4638.1295    \n",
            "      4223   211.16461     -148.32949      0             -148.22031     -3579.4272    \n",
            "      4224   211.02799     -148.36055      0             -148.25144     -2515.1449    \n",
            "      4225   212.68716     -148.32239      0             -148.21242     -1358.0156    \n",
            "      4226   216.18733     -148.3637       0             -148.25192     -164.06644    \n",
            "      4227   220.86164     -148.35322      0             -148.23902      1069.1778    \n",
            "      4228   225.01385     -148.36716      0             -148.25082      2185.9749    \n",
            "      4229   226.66065     -148.37336      0             -148.25617      3045.0938    \n",
            "      4230   224.12413     -148.34097      0             -148.22509      4174.3471    \n",
            "      4231   216.54389     -148.35513      0             -148.24316      5018.5572    \n",
            "      4232   204.59761     -148.33843      0             -148.23265      5695.7656    \n",
            "      4233   190.15538     -148.35823      0             -148.25991      6366.8143    \n",
            "      4234   176.09797     -148.33323      0             -148.24218      6831.2153    \n",
            "      4235   165.93198     -148.34516      0             -148.25936      7205.4802    \n",
            "      4236   162.95695     -148.32299      0             -148.23874      7228.3295    \n",
            "      4237   168.97294     -148.33984      0             -148.25248      7012.7632    \n",
            "      4238   184.26576     -148.31982      0             -148.22455      6831.0308    \n",
            "      4239   207.29102     -148.36547      0             -148.25829      6206.0721    \n",
            "      4240   234.91094     -148.35781      0             -148.23635      5559.6494    \n",
            "      4241   262.8383      -148.39375      0             -148.25785      4532.145     \n",
            "      4242   286.27634     -148.39253      0             -148.24451      3532.0966    \n",
            "      4243   301.4592      -148.419        0             -148.26314      2382.9605    \n",
            "      4244   306.08832     -148.40741      0             -148.24915      1311.3975    \n",
            "      4245   299.51268     -148.41986      0             -148.265        165.22231    \n",
            "      4246   282.9112      -148.40227      0             -148.25599     -974.59039    \n",
            "      4247   259.07851     -148.3965       0             -148.26255     -2232.5096    \n",
            "      4248   231.68718     -148.37146      0             -148.25167     -3389.2896    \n",
            "      4249   204.16068     -148.35057      0             -148.24501     -4482.7993    \n",
            "      4250   179.32925     -148.34392      0             -148.2512      -5583.5121    \n",
            "      4251   158.84001     -148.33933      0             -148.2572      -6668.8596    \n",
            "      4252   143.18417     -148.32258      0             -148.24855     -7459.428     \n",
            "      4253   131.76557     -148.31743      0             -148.2493      -8338.2989    \n",
            "      4254   123.20204     -148.3254       0             -148.2617      -8988.8192    \n",
            "      4255   116.53472     -148.32902      0             -148.26877     -9524.238     \n",
            "      4256   111.54329     -148.31673      0             -148.25906     -9803.5027    \n",
            "      4257   108.73698     -148.3205       0             -148.26428     -9969.8074    \n",
            "      4258   109.53352     -148.29486      0             -148.23823     -9922.2525    \n",
            "      4259   115.91171     -148.2994       0             -148.23947     -9679.6198    \n",
            "      4260   129.73081     -148.30943      0             -148.24235     -9291.4052    \n",
            "      4261   151.96014     -148.32474      0             -148.24618     -8830.5299    \n",
            "      4262   182.32243     -148.33602      0             -148.24175     -8198.9982    \n",
            "      4263   218.68066     -148.37437      0             -148.2613      -7373.7233    \n",
            "      4264   257.39315     -148.39119      0             -148.2581      -6651.9675    \n",
            "      4265   293.79248     -148.38882      0             -148.23692     -5617.8053    \n",
            "      4266   322.68132     -148.40207      0             -148.23523     -4598.652     \n",
            "      4267   339.74184     -148.42251      0             -148.24685     -3599.3283    \n",
            "      4268   342.55283     -148.43607      0             -148.25896     -2385.8826    \n",
            "      4269   330.50355     -148.41646      0             -148.24557     -1243.7408    \n",
            "      4270   305.01274     -148.40916      0             -148.25146      57.605625    \n",
            "      4271   269.57821     -148.42442      0             -148.28504      1174.1004    \n",
            "      4272   228.59452     -148.37603      0             -148.25784      2313.6909    \n",
            "      4273   186.79189     -148.36227      0             -148.26569      3418.6901    \n",
            "      4274   148.37557     -148.35928      0             -148.28256      4180.3959    \n",
            "      4275   116.318       -148.31953      0             -148.25939      4948.5742    \n",
            "      4276   92.14481      -148.30265      0             -148.255        5508.0641    \n",
            "      4277   75.890122     -148.28725      0             -148.24801      5929.9827    \n",
            "      4278   66.760872     -148.28293      0             -148.24841      6196.1345    \n",
            "      4279   63.508186     -148.31042      0             -148.27758      6096.3279    \n",
            "      4280   65.109075     -148.28196      0             -148.24829      6096.9453    \n",
            "      4281   71.33813      -148.31231      0             -148.27543      5781.6833    \n",
            "      4282   82.201805     -148.29463      0             -148.25213      5353.2678    \n",
            "      4283   98.355846     -148.31511      0             -148.26426      4681.7889    \n",
            "      4284   120.4951      -148.33094      0             -148.26863      4042.8311    \n",
            "      4285   148.80551     -148.33616      0             -148.25922      3194.9775    \n",
            "      4286   182.53666     -148.37435      0             -148.27998      1972.5384    \n",
            "      4287   219.60701     -148.38283      0             -148.26928      949.76498    \n",
            "      4288   256.70857     -148.40733      0             -148.2746      -428.59432    \n",
            "      4289   289.74103     -148.38989      0             -148.24008     -1709.0548    \n",
            "      4290   314.3938      -148.42634      0             -148.26378     -3191.9539    \n",
            "      4291   326.92907     -148.43633      0             -148.26729     -4393.0866    \n",
            "      4292   325.31558     -148.4346       0             -148.2664      -5827.7446    \n",
            "      4293   309.41791     -148.39376      0             -148.23377     -6886.1094    \n",
            "      4294   281.0614      -148.40669      0             -148.26137     -8139.0142    \n",
            "      4295   243.73315     -148.37443      0             -148.24841     -9196.8743    \n",
            "      4296   202.18947     -148.35228      0             -148.24774     -10035.923    \n",
            "      4297   161.74755     -148.31638      0             -148.23275     -10873.632    \n",
            "      4298   126.74724     -148.3392       0             -148.27367     -11539.977    \n",
            "      4299   100.18779     -148.34518      0             -148.29338     -12019.025    \n",
            "      4300   83.555296     -148.26801      0             -148.2248      -12234.387    \n",
            "      4301   76.574036     -148.33159      0             -148.292       -12556.333    \n",
            "      4302   77.50366      -148.31289      0             -148.27282     -12632.087    \n",
            "      4303   84.240359     -148.30746      0             -148.2639      -12480.584    \n",
            "      4304   94.508909     -148.28426      0             -148.2354      -12118.432    \n",
            "      4305   106.60615     -148.29174      0             -148.23662     -11612.809    \n",
            "      4306   119.78486     -148.31266      0             -148.25072     -10838.804    \n",
            "      4307   133.98717     -148.32218      0             -148.2529      -9925.6028    \n",
            "      4308   149.88082     -148.35999      0             -148.2825      -8954.1589    \n",
            "      4309   168.10356     -148.33446      0             -148.24754     -7832.7959    \n",
            "      4310   188.92377     -148.34244      0             -148.24476     -6711.348     \n",
            "      4311   211.68284     -148.37858      0             -148.26913     -5380.5243    \n",
            "      4312   234.70093     -148.37605      0             -148.2547      -4072.0535    \n",
            "      4313   255.57745     -148.40438      0             -148.27224     -2596.402     \n",
            "      4314   271.2888      -148.40624      0             -148.26598     -1303.8156    \n",
            "      4315   278.84543     -148.4155       0             -148.27133      33.296553    \n",
            "      4316   276.47027     -148.39643      0             -148.25349      1308.2649    \n",
            "      4317   263.91406     -148.40148      0             -148.26503      2643.3361    \n",
            "      4318   242.44879     -148.39254      0             -148.26718      3823.1526    \n",
            "      4319   214.88985     -148.35816      0             -148.24705      4885.0666    \n",
            "      4320   185.33163     -148.34381      0             -148.24799      5927.9681    \n",
            "      4321   158.53946     -148.3373       0             -148.25533      6542.7013    \n",
            "      4322   138.51155     -148.32433      0             -148.25271      7160.9716    \n",
            "      4323   127.99867     -148.31703      0             -148.25085      7598.6162    \n",
            "      4324   128.12602     -148.33595      0             -148.2697       7738.3847    \n",
            "      4325   138.03347     -148.33492      0             -148.26355      7611.4165    \n",
            "      4326   155.3775      -148.32708      0             -148.24675      7372.6458    \n",
            "      4327   176.46212     -148.34963      0             -148.25839      6858.685     \n",
            "      4328   197.30169     -148.40011      0             -148.2981       6142.1374    \n",
            "      4329   214.73626     -148.36895      0             -148.25792      5529.1052    \n",
            "      4330   226.6906      -148.3668       0             -148.24959      4691.7845    \n",
            "      4331   232.546       -148.36802      0             -148.24779      3773.5129    \n",
            "      4332   232.76516     -148.36998      0             -148.24964      2662.2106    \n",
            "      4333   229.42598     -148.35296      0             -148.23434      1676.7711    \n",
            "      4334   224.58601     -148.36544      0             -148.24932      564.32264    \n",
            "      4335   219.87317     -148.35189      0             -148.23821     -546.92841    \n",
            "      4336   216.23147     -148.37578      0             -148.26398     -1739.6587    \n",
            "      4337   213.41372     -148.35975      0             -148.24941     -2833.7925    \n",
            "      4338   210.54417     -148.34336      0             -148.2345      -3815.1452    \n",
            "      4339   206.05632     -148.37247      0             -148.26593     -4843.117     \n",
            "      4340   198.48312     -148.34081      0             -148.23819     -5552.1197    \n",
            "      4341   187.73871     -148.34816      0             -148.25109     -6170.0703    \n",
            "      4342   174.47574     -148.33547      0             -148.24526     -6660.4112    \n",
            "      4343   160.78278     -148.33375      0             -148.25062     -6989.7633    \n",
            "      4344   149.65777     -148.30708      0             -148.2297      -6965.3772    \n",
            "      4345   144.68818     -148.34342      0             -148.26861     -6923.0829    \n",
            "      4346   148.85303     -148.31196      0             -148.23499     -6606.2538    \n",
            "      4347   163.42875     -148.31496      0             -148.23046     -6145.4941    \n",
            "      4348   187.80946     -148.35527      0             -148.25817     -5445.4158    \n",
            "      4349   219.51719     -148.34076      0             -148.22726     -4796.7039    \n",
            "      4350   254.36026     -148.36966      0             -148.23815     -3740.2169    \n",
            "      4351   287.19608     -148.39336      0             -148.24486     -2745.4788    \n",
            "      4352   312.7574      -148.42255      0             -148.26084     -1462.4886    \n",
            "      4353   326.9952      -148.41988      0             -148.25081     -132.00383    \n",
            "      4354   327.6356      -148.40705      0             -148.23765      1284.5408    \n",
            "      4355   314.73291     -148.42098      0             -148.25825      2761.2349    \n",
            "      4356   290.32715     -148.40044      0             -148.25033      4278.0559    \n",
            "      4357   258.14885     -148.36705      0             -148.23358      6015.7476    \n",
            "      4358   222.77546     -148.36065      0             -148.24546      7382.253     \n",
            "      4359   188.49297     -148.3425       0             -148.24504      8712.2374    \n",
            "      4360   158.59067     -148.31407      0             -148.23207      10075.672    \n",
            "      4361   135.10673     -148.31836      0             -148.24851      11083.608    \n",
            "      4362   118.72795     -148.30894      0             -148.24755      11838.392    \n",
            "      4363   108.94837     -148.28277      0             -148.22644      12629.272    \n",
            "      4364   104.58421     -148.30841      0             -148.25434      13024.812    \n",
            "      4365   104.92147     -148.3094       0             -148.25515      13213.076    \n",
            "      4366   109.76041     -148.28453      0             -148.22777      13401.462    \n",
            "      4367   119.26986     -148.29506      0             -148.23339      13263.844    \n",
            "      4368   134.30294     -148.32291      0             -148.25347      12929.153    \n",
            "      4369   156.21683     -148.31683      0             -148.23606      12242.844    \n",
            "      4370   185.61043     -148.31808      0             -148.22211      11446.43     \n",
            "      4371   221.62252     -148.35219      0             -148.2376       10292.915    \n",
            "      4372   262.09968     -148.38116      0             -148.24564      9122.2323    \n",
            "      4373   303.47589     -148.38279      0             -148.22588      7622.934     \n",
            "      4374   341.03232     -148.40061      0             -148.22428      6073.0472    \n",
            "      4375   369.57648     -148.44187      0             -148.25079      4346.8015    \n",
            "      4376   384.17867     -148.43099      0             -148.23235      2737.8788    \n",
            "      4377   381.6988      -148.43164      0             -148.23429      1057.4637    \n",
            "      4378   361.27132     -148.39991      0             -148.21312     -581.15318    \n",
            "      4379   324.43531     -148.38682      0             -148.21907     -2146.0539    \n",
            "      4380   275.30457     -148.38688      0             -148.24454     -3643.4325    \n",
            "      4381   219.61984     -148.33602      0             -148.22247     -4815.3273    \n",
            "      4382   163.89476     -148.31443      0             -148.22969     -5982.3147    \n",
            "      4383   114.32573     -148.27426      0             -148.21514     -6905.9858    \n",
            "      4384   75.83145      -148.26025      0             -148.22104     -7781.4002    \n",
            "      4385   51.289201     -148.2385       0             -148.21198     -8444.5774    \n",
            "      4386   41.261484     -148.2464       0             -148.22507     -8763.0853    \n",
            "      4387   44.495736     -148.23702      0             -148.21401     -9046.1524    \n",
            "      4388   58.698861     -148.23974      0             -148.20939     -8959.3701    \n",
            "      4389   81.033324     -148.23511      0             -148.19321     -8669.6699    \n",
            "      4390   108.61775     -148.28596      0             -148.2298      -8249.5664    \n",
            "      4391   139.26975     -148.28331      0             -148.2113      -7566.2316    \n",
            "      4392   172.03355     -148.30966      0             -148.22071     -6572.2082    \n",
            "      4393   206.31477     -148.32821      0             -148.22154     -5301.3199    \n",
            "      4394   241.71548     -148.36145      0             -148.23647     -3914.8146    \n",
            "      4395   277.96521     -148.33979      0             -148.19607     -2480.4557    \n",
            "      4396   313.8014      -148.37125      0             -148.209       -921.73973    \n",
            "      4397   346.72        -148.41822      0             -148.23895      685.26211    \n",
            "      4398   373.2946      -148.40469      0             -148.21168      2432.961     \n",
            "      4399   389.71034     -148.42075      0             -148.21926      4111.7739    \n",
            "      4400   392.47018     -148.4338       0             -148.23087      5585.2736    \n",
            "      4401   379.29551     -148.4161       0             -148.21998      7214.944     \n",
            "      4402   350.14935     -148.40737      0             -148.22633      8644.2615    \n",
            "      4403   307.372       -148.38097      0             -148.22205      10226.31     \n",
            "      4404   255.25117     -148.33501      0             -148.20303      11363.355    \n",
            "      4405   200.07203     -148.32291      0             -148.21946      12426.513    \n",
            "      4406   149.04345     -148.28004      0             -148.20297      13267.558    \n",
            "      4407   108.65704     -148.2851       0             -148.22892      13681.889    \n",
            "      4408   83.826268     -148.25051      0             -148.20716      14084.402    \n",
            "      4409   77.131541     -148.25086      0             -148.21098      13945.609    \n",
            "      4410   88.193905     -148.23971      0             -148.19411      13691.643    \n",
            "      4411   114.15191     -148.27928      0             -148.22026      13133.095    \n",
            "      4412   150.2639      -148.26975      0             -148.19206      12422.548    \n",
            "      4413   190.89121     -148.29995      0             -148.20125      11352.593    \n",
            "      4414   230.75066     -148.33715      0             -148.21784      10180.73     \n",
            "      4415   265.98523     -148.32077      0             -148.18324      8837.8605    \n",
            "      4416   294.35003     -148.33066      0             -148.17847      7453.1065    \n",
            "      4417   315.2976      -148.38187      0             -148.21885      5836.215     \n",
            "      4418   330.07313     -148.37615      0             -148.20549      4324.1664    \n",
            "      4419   340.20806     -148.36214      0             -148.18624      2626.5442    \n",
            "      4420   347.04766     -148.36255      0             -148.18311      971.80096    \n",
            "      4421   351.36353     -148.38729      0             -148.20562     -767.75355    \n",
            "      4422   352.80534     -148.3978       0             -148.21539     -2417.9752    \n",
            "      4423   349.88515     -148.37325      0             -148.19235     -4127.6942    \n",
            "      4424   340.90761     -148.36765      0             -148.19139     -5454.8415    \n",
            "      4425   324.70345     -148.33841      0             -148.17052     -6645.2676    \n",
            "      4426   301.15702     -148.33372      0             -148.17801     -7817.2611    \n",
            "      4427   271.96283     -148.34105      0             -148.20043     -8540.0108    \n",
            "      4428   240.48494     -148.31264      0             -148.1883      -9181.8305    \n",
            "      4429   211.22782     -148.28754      0             -148.17832     -9581.8324    \n",
            "      4430   189.35598     -148.25594      0             -148.15804     -9669.0154    \n",
            "      4431   179.38688     -148.26789      0             -148.17514     -9660.5468    \n",
            "      4432   184.0475      -148.2815       0             -148.18634     -9543.1252    \n",
            "      4433   203.67175     -148.28989      0             -148.18459     -9196.6738    \n",
            "      4434   235.87394     -148.30603      0             -148.18408     -8615.7308    \n",
            "      4435   275.76052     -148.32805      0             -148.18547     -7835.5687    \n",
            "      4436   317.31197     -148.34144      0             -148.17737     -6931.7574    \n",
            "      4437   353.94674     -148.36466      0             -148.18165     -5875.9016    \n",
            "      4438   380.08453     -148.35858      0             -148.16206     -4763.1721    \n",
            "      4439   392.7574      -148.35884      0             -148.15577     -3467.4537    \n",
            "      4440   391.07703     -148.36937      0             -148.16717     -2045.3675    \n",
            "      4441   376.57339     -148.37568      0             -148.18098     -638.2543     \n",
            "      4442   352.84445     -148.3638       0             -148.18137      755.94519    \n",
            "      4443   324.7136      -148.35375      0             -148.18586      2168.663     \n",
            "      4444   296.85848     -148.32445      0             -148.17096      3430.3039    \n",
            "      4445   273.03875     -148.30944      0             -148.16826      4646.3382    \n",
            "      4446   255.25393     -148.30153      0             -148.16956      5613.7765    \n",
            "      4447   243.64116     -148.3057       0             -148.17972      6332.6465    \n",
            "      4448   237.35586     -148.28251      0             -148.15979      7075.1312    \n",
            "      4449   234.84283     -148.28581      0             -148.16439      7393.1492    \n",
            "      4450   234.50522     -148.30679      0             -148.18554      7649.2056    \n",
            "      4451   235.9064      -148.26868      0             -148.1467       7667.197     \n",
            "      4452   239.42953     -148.28476      0             -148.16096      7445.6766    \n",
            "      4453   246.32719     -148.28582      0             -148.15846      7135.2652    \n",
            "      4454   258.83581     -148.26697      0             -148.13314      6435.4875    \n",
            "      4455   278.88914     -148.29023      0             -148.14603      5527.514     \n",
            "      4456   307.05391     -148.31459      0             -148.15583      4420.7778    \n",
            "      4457   342.25818     -148.31965      0             -148.14269      3068.5384    \n",
            "      4458   381.50108     -148.34634      0             -148.14909      1572.0824    \n",
            "      4459   419.78395     -148.34651      0             -148.12946     -13.307495    \n",
            "      4460   451.20255     -148.35404      0             -148.12075     -1731.5897    \n",
            "      4461   469.7601      -148.38023      0             -148.13735     -3608.0988    \n",
            "      4462   470.57349     -148.38814      0             -148.14483     -5341.1625    \n",
            "      4463   451.67811     -148.37814      0             -148.14461     -6974.4879    \n",
            "      4464   413.57434     -148.36237      0             -148.14853     -8645.683     \n",
            "      4465   360.04303     -148.34639      0             -148.16024     -10061.378    \n",
            "      4466   297.4614      -148.27889      0             -148.12509     -11377.216    \n",
            "      4467   233.34691     -148.27554      0             -148.15489     -12660.184    \n",
            "      4468   175.38165     -148.2319       0             -148.14122     -13814.396    \n",
            "      4469   129.97177     -148.19787      0             -148.13067     -14627.724    \n",
            "      4470   101.19213     -148.1881       0             -148.13578     -15464.575    \n",
            "      4471   90.338695     -148.16983      0             -148.12312     -15987.035    \n",
            "      4472   96.190511     -148.19099      0             -148.14126     -16319.829    \n",
            "      4473   115.71989     -148.18242      0             -148.12259     -16418.228    \n",
            "      4474   144.93057     -148.23795      0             -148.16301     -16488.603    \n",
            "      4475   180.1397      -148.20781      0             -148.11467     -16027.784    \n",
            "      4476   218.36328     -148.22442      0             -148.11152     -15452.793    \n",
            "      4477   257.95722     -148.27122      0             -148.13784     -14614.476    \n",
            "      4478   298.67248     -148.29437      0             -148.13994     -13690.457    \n",
            "      4479   340.6676      -148.29897      0             -148.12283     -12296.998    \n",
            "      4480   384.16128     -148.28562      0             -148.087       -10830.255    \n",
            "      4481   428.38798     -148.33771      0             -148.11621     -9280.1649    \n",
            "      4482   470.95635     -148.355        0             -148.1115      -7642.3342    \n",
            "      4483   507.71954     -148.37606      0             -148.11355     -5896.859     \n",
            "      4484   533.06802     -148.3989       0             -148.12328     -4238.3133    \n",
            "      4485   541.46403     -148.40083      0             -148.12087     -2353.8976    \n",
            "      4486   528.35746     -148.39523      0             -148.12204     -539.78882    \n",
            "      4487   491.75982     -148.40012      0             -148.14586      1250.9233    \n",
            "      4488   433.33397     -148.34473      0             -148.12068      3009.5422    \n",
            "      4489   358.028       -148.30234      0             -148.11723      4796.031     \n",
            "      4490   274.00619     -148.20029      0             -148.05861      6321.5424    \n",
            "      4491   190.91091     -148.1938       0             -148.09509      7569.5432    \n",
            "      4492   118.64045     -148.16657      0             -148.10523      8665.838     \n",
            "      4493   65.660475     -148.1461       0             -148.11215      9492.7395    \n",
            "      4494   37.584123     -148.12628      0             -148.10685      10093.57     \n",
            "      4495   36.517579     -148.09806      0             -148.07918      10361.973    \n",
            "      4496   60.943243     -148.13452      0             -148.10301      10254.512    \n",
            "      4497   106.11825     -148.14792      0             -148.09305      9942.3849    \n",
            "      4498   165.24099     -148.18769      0             -148.10225      9509.3857    \n",
            "      4499   231.08113     -148.22759      0             -148.10812      8509.6289    \n",
            "      4500   297.14792     -148.24508      0             -148.09145      7576.8944    \n",
            "      4501   358.49788     -148.27822      0             -148.09286      6437.4832    \n",
            "      4502   412.22802     -148.31205      0             -148.09891      5042.3501    \n",
            "      4503   457.77095     -148.33749      0             -148.10081      3667.0905    \n",
            "      4504   495.5632      -148.35096      0             -148.09473      1970.4357    \n",
            "      4505   526.11485     -148.36446      0             -148.09244      169.40803    \n",
            "      4506   549.44096     -148.37045      0             -148.08636     -1557.1835    \n",
            "      4507   564.28981     -148.34695      0             -148.05519     -3461.8088    \n",
            "      4508   567.97413     -148.38917      0             -148.09551     -5324.5861    \n",
            "      4509   557.66744     -148.39187      0             -148.10353     -7122.5265    \n",
            "      4510   531.06556     -148.36087      0             -148.08629     -8672.0104    \n",
            "      4511   487.20796     -148.35074      0             -148.09884     -10154.023    \n",
            "      4512   427.85327     -148.3012       0             -148.07998     -11303.975    \n",
            "      4513   358.07457     -148.28175      0             -148.09662     -12229.235    \n",
            "      4514   285.38465     -148.25239      0             -148.10484     -12923.196    \n",
            "      4515   218.82616     -148.19984      0             -148.0867      -13460.077    \n",
            "      4516   167.40098     -148.16978      0             -148.08323     -13648.155    \n",
            "      4517   138.35548     -148.11608      0             -148.04454     -13569.034    \n",
            "      4518   135.8582      -148.13666      0             -148.06641     -13412.65     \n",
            "      4519   159.96514     -148.16931      0             -148.0866      -13077.125    \n",
            "      4520   206.78649     -148.15823      0             -148.05131     -12399.481    \n",
            "      4521   269.61378     -148.22335      0             -148.08395     -11478.137    \n",
            "      4522   339.52487     -148.26062      0             -148.08507     -10567.265    \n",
            "      4523   407.24784     -148.29038      0             -148.07982     -9233.8413    \n",
            "      4524   465.34157     -148.31752      0             -148.07692     -7602.6248    \n",
            "      4525   508.84671     -148.33544      0             -148.07235     -5803.6358    \n",
            "      4526   536.00624     -148.34833      0             -148.07119     -3846.6039    \n",
            "      4527   548.20645     -148.31585      0             -148.0324      -1664.5005    \n",
            "      4528   548.72773     -148.33827      0             -148.05455      357.06397    \n",
            "      4529   541.5784      -148.34447      0             -148.06445      2578.5241    \n",
            "      4530   530.06972     -148.34712      0             -148.07305      4690.0494    \n",
            "      4531   516.30587     -148.30224      0             -148.03529      6611.9921    \n",
            "      4532   500.86411     -148.32262      0             -148.06366      8380.3392    \n",
            "      4533   482.68264     -148.2979       0             -148.04833      10062.098    \n",
            "      4534   460.703       -148.3055       0             -148.0673       11289.732    \n",
            "      4535   434.01643     -148.28437      0             -148.05997      12578.79     \n",
            "      4536   402.99307     -148.2422       0             -148.03384      13488.819    \n",
            "      4537   370.48478     -148.22503      0             -148.03348      14120.398    \n",
            "      4538   340.60746     -148.21165      0             -148.03554      14688.233    \n",
            "      4539   319.01753     -148.2038       0             -148.03885      14624.786    \n",
            "      4540   311.11732     -148.19996      0             -148.0391       14286.145    \n",
            "      4541   320.8168      -148.19136      0             -148.02549      13687.049    \n",
            "      4542   349.5654      -148.22103      0             -148.04029      12704.192    \n",
            "      4543   394.92399     -148.2321       0             -148.02791      11359.48     \n",
            "      4544   451.06801     -148.28964      0             -148.05642      9804.8318    \n",
            "      4545   509.36762     -148.28946      0             -148.0261       7939.6028    \n",
            "      4546   560.14935     -148.34391      0             -148.05429      6107.6219    \n",
            "      4547   594.24837     -148.36098      0             -148.05373      4023.3591    \n",
            "      4548   605.04777     -148.35094      0             -148.0381       1946.4045    \n",
            "      4549   589.95635     -148.31862      0             -148.01359     -104.64892    \n",
            "      4550   550.57921     -148.31333      0             -148.02866     -1933.6713    \n",
            "      4551   492.71489     -148.27278      0             -148.01803     -3736.1325    \n",
            "      4552   425.07        -148.24629      0             -148.02651     -5484.4108    \n",
            "      4553   357.6565      -148.24561      0             -148.06069     -6915.0944    \n",
            "      4554   299.80782     -148.17908      0             -148.02407     -8151.8182    \n",
            "      4555   258.24465     -148.17135      0             -148.03782     -9250.2782    \n",
            "      4556   236.54343     -148.1751       0             -148.05279     -10213.35     \n",
            "      4557   234.94709     -148.12932      0             -148.00784     -10822.791    \n",
            "      4558   250.60822     -148.13129      0             -148.00172     -11141.878    \n",
            "      4559   278.72247     -148.18183      0             -148.03772     -11152.919    \n",
            "      4560   314.4635      -148.20715      0             -148.04456     -10715.441    \n",
            "      4561   353.49492     -148.17977      0             -147.997       -9944.1656    \n",
            "      4562   393.2857      -148.25336      0             -148.05001     -8894.0521    \n",
            "      4563   432.75333     -148.23146      0             -148.00771     -7367.945     \n",
            "      4564   472.16675     -148.25823      0             -148.0141      -5673.4595    \n",
            "      4565   512.09545     -148.2783       0             -148.01352     -3540.2618    \n",
            "      4566   551.94316     -148.29952      0             -148.01414     -1422.3397    \n",
            "      4567   589.33929     -148.35435      0             -148.04964      1028.3134    \n",
            "      4568   619.58451     -148.32755      0             -148.0072       3570.4741    \n",
            "      4569   636.47832     -148.34329      0             -148.01421      6045.4635    \n",
            "      4570   633.41492     -148.35454      0             -148.02704      8514.9764    \n",
            "      4571   605.34779     -148.30739      0             -147.9944       11043.764    \n",
            "      4572   550.16751     -148.28638      0             -148.00192      13578.748    \n",
            "      4573   470.24804     -148.27333      0             -148.03019      15866.122    \n",
            "      4574   372.5015      -148.21803      0             -148.02543      18084.27     \n",
            "      4575   267.78966     -148.13864      0             -148.00018      20002.173    \n",
            "      4576   169.211       -148.10222      0             -148.01473      21714.425    \n",
            "      4577   89.72134      -148.04055      0             -147.99416      23060.864    \n",
            "      4578   40.457071     -148.03826      0             -148.01734      23911.906    \n",
            "      4579   28.216013     -148.03209      0             -148.0175       24405.441    \n",
            "      4580   54.331208     -148.03582      0             -148.00773      24430.279    \n",
            "      4581   114.87438     -148.0825       0             -148.02311      23905.077    \n",
            "      4582   201.30673     -148.12613      0             -148.02204      23047.681    \n",
            "      4583   302.42076     -148.16965      0             -148.01329      21809.9      \n",
            "      4584   405.96757     -148.22295      0             -148.01305      20264.704    \n",
            "      4585   501.17656     -148.2501       0             -147.99097      18440.908    \n",
            "      4586   579.99355     -148.29792      0             -147.99804      16334.483    \n",
            "      4587   638.01151     -148.35346      0             -148.02358      14031.177    \n",
            "      4588   674.79181     -148.323        0             -147.9741       11700.775    \n",
            "      4589   692.10109     -148.35942      0             -148.00158      9025.031     \n",
            "      4590   692.35861     -148.38008      0             -148.02211      6344.4547    \n",
            "      4591   678.1285      -148.33411      0             -147.98349      3630.9654    \n",
            "      4592   651.02509     -148.34384      0             -148.00724      1044.8732    \n",
            "      4593   611.30792     -148.29432      0             -147.97825     -1477.807     \n",
            "      4594   558.60258     -148.29168      0             -148.00286     -3805.4413    \n",
            "      4595   492.96813     -148.24266      0             -147.98777     -5947.632     \n",
            "      4596   416.00762     -148.23674      0             -148.02165     -7807.6481    \n",
            "      4597   332.36765     -148.19139      0             -148.01954     -9335.4401    \n",
            "      4598   248.46469     -148.14037      0             -148.01191     -10343.445    \n",
            "      4599   172.94124     -148.09511      0             -148.00569     -11177.587    \n",
            "      4600   115.35811     -148.07489      0             -148.01524     -11530.361    \n",
            "      4601   84.259068     -148.04009      0             -147.99653     -11613.11     \n",
            "      4602   85.649468     -148.0503       0             -148.00602     -11464.949    \n",
            "      4603   121.57443     -148.05815      0             -147.99529     -11102.564    \n",
            "      4604   189.41938     -148.07585      0             -147.97791     -10472.846    \n",
            "      4605   281.89841     -148.16295      0             -148.01719     -9566.656     \n",
            "      4606   388.53316     -148.19544      0             -147.99455     -8464.3128    \n",
            "      4607   496.78382     -148.29092      0             -148.03406     -7154.5608    \n",
            "      4608   594.01776     -148.32926      0             -148.02213     -5385.6512    \n",
            "      4609   670.24762     -148.34472      0             -147.99818     -3482.5627    \n",
            "      4610   719.03387     -148.36434      0             -147.99257     -1584.7913    \n",
            "      4611   738.63215     -148.37886      0             -147.99695      636.48167    \n",
            "      4612   731.5331      -148.37243      0             -147.9942       2832.2648    \n",
            "      4613   703.28011     -148.37537      0             -148.01175      4955.9328    \n",
            "      4614   661.3704      -148.3446       0             -148.00264      6956.628     \n",
            "      4615   612.90286     -148.31756      0             -148.00067      8959.5743    \n",
            "      4616   563.43852     -148.30947      0             -148.01815      10452.875    \n",
            "      4617   516.65442     -148.23987      0             -147.97273      11894.679    \n",
            "      4618   473.63605     -148.24094      0             -147.99605      12984.14     \n",
            "      4619   434.20996     -148.21009      0             -147.98558      13680.878    \n",
            "      4620   397.85544     -148.20929      0             -148.00358      14002.182    \n",
            "      4621   364.64901     -148.16906      0             -147.98052      14388.641    \n",
            "      4622   335.88423     -148.1846       0             -148.01093      14016.736    \n",
            "      4623   314.0208      -148.17767      0             -148.01531      13703.823    \n",
            "      4624   302.9653      -148.17358      0             -148.01694      12894.705    \n",
            "      4625   306.56629     -148.1695       0             -148.01099      11867.104    \n",
            "      4626   327.421       -148.17937      0             -148.01008      10562.396    \n",
            "      4627   365.88379     -148.19362      0             -148.00444      8935.6274    \n",
            "      4628   419.22361     -148.2452       0             -148.02845      7251.6902    \n",
            "      4629   481.62379     -148.2625       0             -148.01348      5216.0993    \n",
            "      4630   544.64547     -148.30125      0             -148.01965      3039.6179    \n",
            "      4631   598.77486     -148.32077      0             -148.01118      809.46274    \n",
            "      4632   635.0591      -148.34173      0             -148.01338     -1472.7868    \n",
            "      4633   647.01272     -148.34718      0             -148.01265     -3519.2681    \n",
            "      4634   632.81213     -148.34587      0             -148.01868     -5547.0072    \n",
            "      4635   594.35454     -148.32344      0             -148.01614     -7481.8346    \n",
            "      4636   537.99505     -148.28856      0             -148.01039     -9089.4524    \n",
            "      4637   473.36859     -148.26418      0             -148.01943     -10398.661    \n",
            "      4638   410.44937     -148.21594      0             -148.00372     -11585.028    \n",
            "      4639   358.79466     -148.19007      0             -148.00456     -12707.823    \n",
            "      4640   325.02731     -148.18198      0             -148.01393     -13504.018    \n",
            "      4641   311.85558     -148.14496      0             -147.98372     -14118.805    \n",
            "      4642   317.87664     -148.18439      0             -148.02004     -14553.799    \n",
            "      4643   338.47123     -148.21181      0             -148.03681     -14740.325    \n",
            "      4644   367.42741     -148.19383      0             -148.00385     -14627.989    \n",
            "      4645   398.37762     -148.23379      0             -148.02782     -14118.716    \n",
            "      4646   426.20274     -148.23671      0             -148.01634     -13490.448    \n",
            "      4647   448.05441     -148.27091      0             -148.03925     -12428.459    \n",
            "      4648   463.5982      -148.26562      0             -148.02592     -11214.009    \n",
            "      4649   474.6421      -148.2639       0             -148.01849     -9792.0017    \n",
            "      4650   483.74079     -148.2587       0             -148.00859     -8229.0275    \n",
            "      4651   492.7259      -148.2688       0             -148.01404     -6448.7802    \n",
            "      4652   502.06339     -148.27265      0             -148.01306     -4837.7283    \n",
            "      4653   509.93725     -148.26844      0             -148.00478     -3123.4112    \n",
            "      4654   512.71554     -148.30876      0             -148.04367     -1471.2768    \n",
            "      4655   505.57513     -148.31532      0             -148.05391      194.14838    \n",
            "      4656   484.02135     -148.30228      0             -148.05202      1978.08      \n",
            "      4657   445.74211     -148.25514      0             -148.02467      3598.0756    \n",
            "      4658   391.74156     -148.2362       0             -148.03365      5015.1156    \n",
            "      4659   326.48345     -148.20566      0             -148.03685      6511.2312    \n",
            "      4660   257.55063     -148.16476      0             -148.0316       7665.4118    \n",
            "      4661   194.54093     -148.10293      0             -148.00234      8800.5354    \n",
            "      4662   146.94607     -148.09918      0             -148.0232       9485.656     \n",
            "      4663   122.65556     -148.08368      0             -148.02026      9995.0016    \n",
            "      4664   126.09524     -148.0697       0             -148.00451      10155.659    \n",
            "      4665   157.4687      -148.12032      0             -148.03891      9960.9941    \n",
            "      4666   212.71044     -148.14157      0             -148.03159      9489.6705    \n",
            "      4667   284.44125     -148.18718      0             -148.04011      8562.0719    \n",
            "      4668   363.10137     -148.23859      0             -148.05085      7595.6386    \n",
            "      4669   438.72955     -148.28305      0             -148.05621      6312.4467    \n",
            "      4670   502.66642     -148.29993      0             -148.04003      4899.4245    \n",
            "      4671   549.29832     -148.30452      0             -148.02051      3335.7679    \n",
            "      4672   576.2202      -148.36484      0             -148.06691      1480.7853    \n",
            "      4673   584.14331     -148.36595      0             -148.06392     -141.92386    \n",
            "      4674   576.09048     -148.32203      0             -148.02417     -2019.8387    \n",
            "      4675   556.13295     -148.36854      0             -148.08099     -4021.0722    \n",
            "      4676   528.30434     -148.31752      0             -148.04436     -5806.0733    \n",
            "      4677   495.20748     -148.31163      0             -148.05558     -7684.9534    \n",
            "      4678   457.72981     -148.30638      0             -148.06972     -9503.5844    \n",
            "      4679   415.40017     -148.26172      0             -148.04695     -11264.12     \n",
            "      4680   367.37967     -148.25283      0             -148.06288     -12851.344    \n",
            "      4681   313.19812     -148.23233      0             -148.07039     -14213.383    \n",
            "      4682   253.88128     -148.20544      0             -148.07417     -15291.921    \n",
            "      4683   192.76636     -148.14295      0             -148.04328     -16104.511    \n",
            "      4684   135.28836     -148.1446       0             -148.07465     -16739.141    \n",
            "      4685   88.218868     -148.09409      0             -148.04847     -17056.652    \n",
            "      4686   58.626547     -148.11745      0             -148.08714     -17354.943    \n",
            "      4687   52.48759      -148.07373      0             -148.04659     -17198.167    \n",
            "      4688   73.234647     -148.10932      0             -148.07146     -17027.755    \n",
            "      4689   120.77014     -148.10539      0             -148.04295     -16560.141    \n",
            "      4690   191.41518     -148.1397       0             -148.04074     -16015.141    \n",
            "      4691   278.14823     -148.1985       0             -148.05468     -15268.875    \n",
            "      4692   371.376       -148.25568      0             -148.06366     -14232.951    \n",
            "      4693   460.99304     -148.30739      0             -148.06904     -13105.642    \n",
            "      4694   537.50912     -148.32675      0             -148.04884     -11596.884    \n",
            "      4695   593.88914     -148.34983      0             -148.04276     -9965.1956    \n",
            "      4696   626.70289     -148.40709      0             -148.08306     -8330.8342    \n",
            "      4697   635.88645     -148.41585      0             -148.08707     -6368.5475    \n",
            "      4698   624.4937      -148.39809      0             -148.07521     -4503.0834    \n",
            "      4699   597.26334     -148.37064      0             -148.06183     -2459.8097    \n",
            "      4700   559.51882     -148.38252      0             -148.09323     -571.12559    \n",
            "      4701   516.0394      -148.33686      0             -148.07005      1114.2032    \n",
            "      4702   469.93305     -148.30877      0             -148.06579      2948.7303    \n",
            "      4703   422.95452     -148.3103       0             -148.09162      4293.6546    \n",
            "      4704   375.47751     -148.28072      0             -148.08658      5621.1424    \n",
            "      4705   327.58471     -148.23428      0             -148.0649       6751.2348    \n",
            "      4706   280.11185     -148.22536      0             -148.08053      7579.9648    \n",
            "      4707   234.75881     -148.20601      0             -148.08463      8324.3989    \n",
            "      4708   194.88097     -148.20059      0             -148.09983      8718.6903    \n",
            "      4709   164.72914     -148.16402      0             -148.07885      9074.7016    \n",
            "      4710   148.99158     -148.13829      0             -148.06126      9062.3459    \n",
            "      4711   151.84709     -148.15906      0             -148.08055      8835.6299    \n",
            "      4712   175.72709     -148.19998      0             -148.10912      8346.4141    \n",
            "      4713   220.57722     -148.21967      0             -148.10562      7496.8451    \n",
            "      4714   282.94395     -148.22445      0             -148.07815      6588.6499    \n",
            "      4715   356.25767     -148.30125      0             -148.11705      5335.9707    \n",
            "      4716   431.89222     -148.32146      0             -148.09816      4014.8189    \n",
            "      4717   500.52738     -148.32152      0             -148.06272      2530.9772    \n",
            "      4718   553.50062     -148.37888      0             -148.0927       1019.2259    \n",
            "      4719   584.8341      -148.39664      0             -148.09426     -492.09604    \n",
            "      4720   592.20218     -148.42675      0             -148.12056     -2014.7324    \n",
            "      4721   576.67506     -148.41591      0             -148.11775     -3382.5039    \n",
            "      4722   543.17726     -148.40525      0             -148.1244      -4680.5933    \n",
            "      4723   498.60602     -148.38128      0             -148.12348     -5862.6643    \n",
            "      4724   450.24071     -148.37766      0             -148.14486     -7081.396     \n",
            "      4725   404.80334     -148.31827      0             -148.10897     -7860.8123    \n",
            "      4726   366.68111     -148.29058      0             -148.10099     -8737.1866    \n",
            "      4727   337.62353     -148.30091      0             -148.12635     -9505.5027    \n",
            "      4728   317.05363     -148.30109      0             -148.13716     -9929.4903    \n",
            "      4729   302.90223     -148.30291      0             -148.1463      -10135.178    \n",
            "      4730   292.16393     -148.26407      0             -148.113       -10049.999    \n",
            "      4731   282.49479     -148.24361      0             -148.09754     -9727.2256    \n",
            "      4732   273.34367     -148.26517      0             -148.12384     -9229.4823    \n",
            "      4733   265.38275     -148.25548      0             -148.11826     -8475.9498    \n",
            "      4734   260.90991     -148.27144      0             -148.13654     -7487.2612    \n",
            "      4735   262.70831     -148.26257      0             -148.12674     -6328.7988    \n",
            "      4736   272.88028     -148.28796      0             -148.14687     -4894.1499    \n",
            "      4737   291.99531     -148.27236      0             -148.12139     -3347.695     \n",
            "      4738   318.63503     -148.2941       0             -148.12936     -1808.7165    \n",
            "      4739   349.24858     -148.33115      0             -148.15057     -207.0125     \n",
            "      4740   378.40532     -148.32371      0             -148.12806      1391.7679    \n",
            "      4741   400.45006     -148.331        0             -148.12395      3130.4047    \n",
            "      4742   410.18618     -148.32713      0             -148.11505      4844.6183    \n",
            "      4743   404.67082     -148.3641       0             -148.15486      6482.2176    \n",
            "      4744   384.26761     -148.34776      0             -148.14908      8284.084     \n",
            "      4745   351.64893     -148.30737      0             -148.12555      9889.8179    \n",
            "      4746   312.26743     -148.30064      0             -148.13918      11277.914    \n",
            "      4747   273.1997      -148.30576      0             -148.1645       12489.807    \n",
            "      4748   241.04722     -148.25403      0             -148.1294       13524.233    \n",
            "      4749   220.92282     -148.27372      0             -148.15949      14190.434    \n",
            "      4750   215.66042     -148.23866      0             -148.12715      14743.421    \n",
            "      4751   224.92366     -148.26765      0             -148.15136      14860.071    \n",
            "      4752   245.73728     -148.27514      0             -148.14808      14709.753    \n",
            "      4753   273.52124     -148.28849      0             -148.14707      14277.797    \n",
            "      4754   302.42865     -148.32064      0             -148.16427      13609.2      \n",
            "      4755   327.00423     -148.33004      0             -148.16096      12692.909    \n",
            "      4756   344.08249     -148.31537      0             -148.13747      11664.738    \n",
            "      4757   352.11484     -148.37083      0             -148.18877      10339.383    \n",
            "      4758   351.2125      -148.32785      0             -148.14626      9016.1344    \n",
            "      4759   343.52764     -148.33247      0             -148.15485      7394.5122    \n",
            "      4760   331.64386     -148.31923      0             -148.14775      5745.1145    \n",
            "      4761   317.85272     -148.32539      0             -148.16105      4019.3532    \n",
            "      4762   303.64599     -148.31608      0             -148.15908      2292.3488    \n",
            "      4763   289.3737      -148.30298      0             -148.15336      512.04835    \n",
            "      4764   273.92581     -148.30302      0             -148.16139     -1164.8929    \n",
            "      4765   255.49023     -148.27751      0             -148.14541     -2865.4746    \n",
            "      4766   232.66016     -148.29482      0             -148.17452     -4364.6273    \n",
            "      4767   205.11778     -148.26021      0             -148.15416     -5602.6413    \n",
            "      4768   173.9639      -148.26061      0             -148.17067     -6755.037     \n",
            "      4769   142.22303     -148.25569      0             -148.18216     -7542.2627    \n",
            "      4770   114.17902     -148.21997      0             -148.16093     -8131.083     \n",
            "      4771   94.82776      -148.21851      0             -148.16948     -8657.5096    \n",
            "      4772   88.931199     -148.21552      0             -148.16954     -8652.3597    \n",
            "      4773   99.711465     -148.23448      0             -148.18293     -8614.6827    \n",
            "      4774   128.12802     -148.24123      0             -148.17498     -8504.864     \n",
            "      4775   172.36824     -148.26717      0             -148.17805     -7988.0043    \n",
            "      4776   227.86058     -148.29423      0             -148.17642     -7286.7198    \n",
            "      4777   287.8805      -148.34227      0             -148.19342     -6457.4666    \n",
            "      4778   345.01076     -148.3796       0             -148.20122     -5389.8586    \n",
            "      4779   392.36869     -148.37352      0             -148.17065     -4104.2098    \n",
            "      4780   424.62987     -148.40337      0             -148.18382     -2585.9148    \n",
            "      4781   438.80284     -148.41137      0             -148.18449     -1098.4878    \n",
            "      4782   434.68482     -148.40894      0             -148.18419      635.92089    \n",
            "      4783   414.54211     -148.38401      0             -148.16967      2503.9522    \n",
            "      4784   382.60445     -148.37684      0             -148.17901      4210.8875    \n",
            "      4785   343.7233      -148.38805      0             -148.21033      5832.2793    \n",
            "      4786   302.36988     -148.33916      0             -148.18282      7518.0944    \n",
            "      4787   261.90305     -148.32316      0             -148.18774      9072.0527    \n",
            "      4788   224.31132     -148.30422      0             -148.18824      10353.73     \n",
            "      4789   190.4205      -148.31343      0             -148.21497      11568.915    \n",
            "      4790   160.31982     -148.27057      0             -148.18768      12431.056    \n",
            "      4791   133.69637     -148.26855      0             -148.19942      13207.807    \n",
            "      4792   111.19587     -148.25921      0             -148.20172      13714.385    \n",
            "      4793   94.361349     -148.24353      0             -148.19474      14062.819    \n",
            "      4794   85.292524     -148.2368       0             -148.1927       14168.894    \n",
            "      4795   86.95349      -148.24018      0             -148.19522      14097.906    \n",
            "      4796   101.96802     -148.25246      0             -148.19974      13746.588    \n",
            "      4797   131.60324     -148.26213      0             -148.19408      13100.738    \n",
            "      4798   175.36113     -148.27452      0             -148.18385      12301.559    \n",
            "      4799   230.33606     -148.29917      0             -148.18007      11159.918    \n",
            "      4800   291.44514     -148.35714      0             -148.20645      9864.601     \n",
            "      4801   351.9629      -148.36238      0             -148.1804       8388.5366    \n",
            "      4802   404.75372     -148.437        0             -148.22773      7038.6456    \n",
            "      4803   443.29139     -148.4269       0             -148.1977       5372.0647    \n",
            "      4804   463.16806     -148.45748      0             -148.218        3697.7617    \n",
            "      4805   462.89412     -148.45079      0             -148.21146      2340.5606    \n",
            "      4806   443.56212     -148.44638      0             -148.21704      772.24686    \n",
            "      4807   408.9689      -148.41518      0             -148.20373     -698.00499    \n",
            "      4808   364.70746     -148.38743      0             -148.19886     -2029.4633    \n",
            "      4809   316.92741     -148.36086      0             -148.19699     -3157.9776    \n",
            "      4810   271.13617     -148.33998      0             -148.1998      -4343.1703    \n",
            "      4811   231.3171      -148.32777      0             -148.20817     -5329.7971    \n",
            "      4812   199.47968     -148.30388      0             -148.20074     -6138.976     \n",
            "      4813   175.82028     -148.30143      0             -148.21052     -6723.5786    \n",
            "      4814   159.14486     -148.29215      0             -148.20987     -7288.6901    \n",
            "      4815   147.46755     -148.26713      0             -148.19089     -7522.9605    \n",
            "      4816   139.3703      -148.28017      0             -148.20811     -7657.8332    \n",
            "      4817   134.33508     -148.2769       0             -148.20744     -7502.3657    \n",
            "      4818   132.74047     -148.28482      0             -148.21619     -7242.0395    \n",
            "      4819   136.05044     -148.28653      0             -148.21619     -6677.9254    \n",
            "      4820   146.30883     -148.29229      0             -148.21664     -6069.694     \n",
            "      4821   164.97025     -148.3006       0             -148.2153      -5418.6027    \n",
            "      4822   192.1771      -148.30932      0             -148.20996     -4531.1931    \n",
            "      4823   226.47319     -148.3219       0             -148.2048      -3569.4533    \n",
            "      4824   264.76003     -148.36549      0             -148.2286      -2660.193     \n",
            "      4825   302.59815     -148.37825      0             -148.2218      -1620.7049    \n",
            "      4826   335.04301     -148.38638      0             -148.21315     -610.17453    \n",
            "      4827   357.33565     -148.41144      0             -148.22669      454.20641    \n",
            "      4828   366.13678     -148.41004      0             -148.22073      1553.704     \n",
            "      4829   360.59093     -148.39219      0             -148.20575      2549.0822    \n",
            "      4830   342.01157     -148.40598      0             -148.22915      3573.0361    \n",
            "      4831   313.60216     -148.38119      0             -148.21905      4395.4031    \n",
            "      4832   280.23545     -148.36872      0             -148.22382      5223.4138    \n",
            "      4833   246.94402     -148.34957      0             -148.22189      5800.912     \n",
            "      4834   217.92256     -148.36079      0             -148.24812      6180.6568    \n",
            "      4835   196.16686     -148.32181      0             -148.22039      6411.3308    \n",
            "      4836   182.87706     -148.31937      0             -148.22481      6411.7198    \n",
            "      4837   177.37826     -148.33401      0             -148.2423       6248.6763    \n",
            "      4838   177.64784     -148.33647      0             -148.24462      5851.2662    \n",
            "      4839   180.63469     -148.32983      0             -148.23644      5303.1177    \n",
            "      4840   183.65998     -148.34029      0             -148.24533      4688.8121    \n",
            "      4841   185.12585     -148.35251      0             -148.25679      3895.0381    \n",
            "      4842   184.23371     -148.34015      0             -148.2449       2883.3157    \n",
            "      4843   181.47112     -148.3356       0             -148.24177      1979.4068    \n",
            "      4844   178.13268     -148.31864      0             -148.22654      884.67543    \n",
            "      4845   176.25983     -148.32502      0             -148.23388     -389.64159    \n",
            "      4846   177.50634     -148.32241      0             -148.23063     -1604.7366    \n",
            "      4847   182.57861     -148.32978      0             -148.23538     -2966.1805    \n",
            "      4848   190.92092     -148.32423      0             -148.22551     -4214.3906    \n",
            "      4849   200.9131      -148.32594      0             -148.22206     -5409.335     \n",
            "      4850   210.18852     -148.35424      0             -148.24556     -6742.1859    \n",
            "      4851   215.91239     -148.36824      0             -148.2566      -7876.6739    \n",
            "      4852   215.92657     -148.3656       0             -148.25395     -8958.5298    \n",
            "      4853   209.42326     -148.32495      0             -148.21667     -9734.9642    \n",
            "      4854   197.14631     -148.33447      0             -148.23253     -10430.117    \n",
            "      4855   181.19988     -148.32043      0             -148.22674     -11027.801    \n",
            "      4856   164.92763     -148.33034      0             -148.24507     -11406.812    \n",
            "      4857   152.23034     -148.32253      0             -148.24382     -11740.021    \n",
            "      4858   146.34494     -148.3248       0             -148.24914     -11772.685    \n",
            "      4859   149.29102     -148.29608      0             -148.21889     -11743.964    \n",
            "      4860   161.43849     -148.31976      0             -148.23628     -11616.366    \n",
            "      4861   181.11193     -148.34217      0             -148.24853     -11368.524    \n",
            "      4862   205.11387     -148.32913      0             -148.22308     -10811.307    \n",
            "      4863   229.22948     -148.34168      0             -148.22316     -10188.436    \n",
            "      4864   249.1683      -148.36679      0             -148.23796     -9435.1226    \n",
            "      4865   261.53368     -148.38536      0             -148.25013     -8579.2789    \n",
            "      4866   264.34889     -148.37206      0             -148.23538     -7493.5906    \n",
            "      4867   257.57114     -148.38862      0             -148.25545     -6318.2969    \n",
            "      4868   242.70129     -148.37793      0             -148.25244     -5216.6342    \n",
            "      4869   222.79451     -148.37692      0             -148.26173     -4032.1214    \n",
            "      4870   201.26008     -148.37142      0             -148.26736     -2757.547     \n",
            "      4871   180.93334     -148.33003      0             -148.23648     -1542.9835    \n",
            "      4872   163.82857     -148.35682      0             -148.27212     -426.23681    \n",
            "      4873   150.56938     -148.32771      0             -148.24986      781.52589    \n",
            "      4874   140.70821     -148.32834      0             -148.25558      1670.3299    \n",
            "      4875   133.03561     -148.3203       0             -148.25152      2728.5954    \n",
            "      4876   126.12118     -148.30407      0             -148.23886      3494.6711    \n",
            "      4877   119.3158      -148.31006      0             -148.24837      4104.3969    \n",
            "      4878   112.75737     -148.30022      0             -148.24192      4746.0729    \n",
            "      4879   107.69375     -148.28321      0             -148.22753      5200.4974    \n",
            "      4880   106.49636     -148.29376      0             -148.23869      5452.1529    \n",
            "      4881   111.63788     -148.27738      0             -148.21965      5594.5974    \n",
            "      4882   125.24768     -148.33308      0             -148.26832      5500.172     \n",
            "      4883   148.4125      -148.33147      0             -148.25474      5380.8828    \n",
            "      4884   180.60374     -148.32318      0             -148.2298       4856.5222    \n",
            "      4885   219.4132      -148.35925      0             -148.24581      4264.0153    \n",
            "      4886   260.81444     -148.38565      0             -148.2508       3471.9286    \n",
            "      4887   299.85864     -148.39594      0             -148.2409       2676.2082    \n",
            "      4888   331.28561     -148.42066      0             -148.24937      1645.2782    \n",
            "      4889   350.9091      -148.41356      0             -148.23212      661.31555    \n",
            "      4890   356.4319      -148.4176       0             -148.23331     -484.86344    \n",
            "      4891   347.42158     -148.43085      0             -148.25122     -1518.8494    \n",
            "      4892   325.72558     -148.42783      0             -148.25942     -2541.4805    \n",
            "      4893   295.05831     -148.40049      0             -148.24793     -3765.8803    \n",
            "      4894   259.73103     -148.3774       0             -148.2431      -4696.6126    \n",
            "      4895   223.98469     -148.36499      0             -148.24918     -5660.1308    \n",
            "      4896   191.40676     -148.32752      0             -148.22856     -6598.1582    \n",
            "      4897   164.15319     -148.32938      0             -148.2445      -7399.0164    \n",
            "      4898   142.94988     -148.34382      0             -148.26991     -8211.0164    \n",
            "      4899   127.27632     -148.30046      0             -148.23465     -8741.1656    \n",
            "      4900   115.78523     -148.29424      0             -148.23437     -9302.5683    \n",
            "      4901   107.18302     -148.33301      0             -148.27759     -9613.9946    \n",
            "      4902   101.16707     -148.31333      0             -148.26103     -9840.841     \n",
            "      4903   98.334886     -148.29249      0             -148.24164     -9805.3133    \n",
            "      4904   99.905287     -148.29606      0             -148.2444      -9716.7192    \n",
            "      4905   107.89042     -148.30986      0             -148.25407     -9280.5323    \n",
            "      4906   124.47885     -148.31162      0             -148.24726     -8823.2555    \n",
            "      4907   150.4965      -148.2902       0             -148.21239     -8217.3735    \n",
            "      4908   185.50031     -148.32043      0             -148.22451     -7610.1949    \n",
            "      4909   227.32695     -148.34401      0             -148.22647     -6822.7687    \n",
            "      4910   272.15256     -148.40643      0             -148.26571     -5986.8343    \n",
            "      4911   315.22981     -148.40726      0             -148.24427     -4922.9624    \n",
            "      4912   351.28532     -148.44391      0             -148.26228     -3996.1871    \n",
            "      4913   375.71856     -148.44548      0             -148.25122     -2789.4827    \n",
            "      4914   385.70486     -148.434        0             -148.23458     -1634.881     \n",
            "      4915   380.28162     -148.45044      0             -148.25382     -409.62282    \n",
            "      4916   360.63462     -148.42118      0             -148.23472      772.18397    \n",
            "      4917   330.10705     -148.41284      0             -148.24216      2044.7214    \n",
            "      4918   293.23745     -148.39289      0             -148.24128      2994.3873    \n",
            "      4919   254.69528     -148.36344      0             -148.23175      4142.6292    \n",
            "      4920   218.5372      -148.34334      0             -148.23035      5023.9495    \n",
            "      4921   187.65268     -148.34547      0             -148.24845      5702.0586    \n",
            "      4922   163.4912      -148.34237      0             -148.25784      6128.0218    \n",
            "      4923   146.12987     -148.31551      0             -148.23996      6621.3207    \n",
            "      4924   134.56102     -148.31548      0             -148.2459       6794.5099    \n",
            "      4925   127.29441     -148.30243      0             -148.23661      6871.7828    \n",
            "      4926   123.01816     -148.30587      0             -148.24226      6764.7538    \n",
            "      4927   121.19066     -148.31482      0             -148.25216      6481.0395    \n",
            "      4928   122.17627     -148.31294      0             -148.24977      6086.1767    \n",
            "      4929   126.89865     -148.31301      0             -148.24739      5658.9589    \n",
            "      4930   136.70564     -148.31499      0             -148.24431      4992.6202    \n",
            "      4931   152.66073     -148.32865      0             -148.24971      4248.4846    \n",
            "      4932   174.93774     -148.33969      0             -148.24924      3343.978     \n",
            "      4933   202.42196     -148.37393      0             -148.26927      2395.7296    \n",
            "      4934   232.65149     -148.38636      0             -148.26607      1376.0582    \n",
            "      4935   262.26116     -148.4067       0             -148.2711       310.52828    \n",
            "      4936   287.39094     -148.38863      0             -148.24004     -683.40989    \n",
            "      4937   304.1808      -148.40538      0             -148.24811     -1752.9452    \n",
            "      4938   310.08546     -148.39067      0             -148.23034     -2553.1164    \n",
            "      4939   304.2798      -148.42237      0             -148.26504     -3414.5124    \n",
            "      4940   287.92077     -148.38782      0             -148.23896     -4176.6773    \n",
            "      4941   263.9578      -148.38718      0             -148.2507      -4660.0306    \n",
            "      4942   236.32871     -148.39783      0             -148.27563     -5080.8481    \n",
            "      4943   209.78758     -148.33886      0             -148.23039     -5391.686     \n",
            "      4944   188.30756     -148.33093      0             -148.23357     -5455.3708    \n",
            "      4945   174.19307     -148.34205      0             -148.25199     -5465.6043    \n",
            "      4946   168.03529     -148.34198      0             -148.2551      -5251.8098    \n",
            "      4947   168.77005     -148.32687      0             -148.23961     -4830.6004    \n",
            "      4948   174.00591     -148.36533      0             -148.27536     -4496.0382    \n",
            "      4949   180.30489     -148.33788      0             -148.24466     -3731.7858    \n",
            "      4950   184.53424     -148.32577      0             -148.23036     -2956.9555    \n",
            "      4951   184.88884     -148.32581      0             -148.23021     -1844.4825    \n",
            "      4952   180.97971     -148.33765      0             -148.24407     -886.14147    \n",
            "      4953   173.54571     -148.35898      0             -148.26925      355.27746    \n",
            "      4954   164.73097     -148.32344      0             -148.23827      1666.2215    \n",
            "      4955   157.18856     -148.34636      0             -148.26509      2909.7992    \n",
            "      4956   153.14941     -148.32554      0             -148.24636      4336.5501    \n",
            "      4957   154.08096     -148.31443      0             -148.23476      5648.4377    \n",
            "      4958   159.98994     -148.33048      0             -148.24776      6933.311     \n",
            "      4959   169.42194     -148.30379      0             -148.21619      8107.2023    \n",
            "      4960   180.20353     -148.3243       0             -148.23113      9345.1303    \n",
            "      4961   189.7714      -148.34283      0             -148.24471      10278.277    \n",
            "      4962   195.9653      -148.31228      0             -148.21096      11221.408    \n",
            "      4963   197.89887     -148.33708      0             -148.23476      11948.768    \n",
            "      4964   196.16282     -148.32473      0             -148.2233       12562.624    \n",
            "      4965   192.55208     -148.34291      0             -148.24335      12964.789    \n",
            "      4966   189.84433     -148.33749      0             -148.23933      13114.895    \n",
            "      4967   191.19932     -148.34309      0             -148.24424      13256.539    \n",
            "      4968   198.82537     -148.37287      0             -148.27007      13050.791    \n",
            "      4969   213.43307     -148.37998      0             -148.26962      12712.284    \n",
            "      4970   233.99201     -148.37152      0             -148.25053      12047.142    \n",
            "      4971   257.71949     -148.36178      0             -148.22853      11318.334    \n",
            "      4972   280.70104     -148.35962      0             -148.21449      10372.435    \n",
            "      4973   298.50663     -148.39331      0             -148.23897      9195.5266    \n",
            "      4974   307.20434     -148.39393      0             -148.23509      7951.0513    \n",
            "      4975   304.40998     -148.40989      0             -148.2525       6577.4457    \n",
            "      4976   289.96058     -148.38003      0             -148.23011      5326.3618    \n",
            "      4977   265.57324     -148.34044      0             -148.20313      4020.9926    \n",
            "      4978   234.40644     -148.37405      0             -148.25286      2761.2059    \n",
            "      4979   201.14695     -148.33376      0             -148.22976      1445.9292    \n",
            "      4980   170.20295     -148.33749      0             -148.24949      192.0702     \n",
            "      4981   145.07413     -148.28211      0             -148.2071      -980.92171    \n",
            "      4982   127.93679     -148.32736      0             -148.26121     -2120.4656    \n",
            "      4983   119.36086     -148.31556      0             -148.25385     -3125.3055    \n",
            "      4984   118.39452     -148.30964      0             -148.24843     -3999.6542    \n",
            "      4985   123.21541     -148.28625      0             -148.22254     -4806.8353    \n",
            "      4986   131.86033     -148.30236      0             -148.23418     -5325.6771    \n",
            "      4987   142.6825      -148.30384      0             -148.23007     -5727.6613    \n",
            "      4988   155.06707     -148.32742      0             -148.24724     -5989.35      \n",
            "      4989   169.5419      -148.31473      0             -148.22707     -6027.2875    \n",
            "      4990   187.31572     -148.32947      0             -148.23262     -5642.6314    \n",
            "      4991   209.69926     -148.3241       0             -148.21568     -5299.6373    \n",
            "      4992   237.79715     -148.3696       0             -148.24665     -4791.3055    \n",
            "      4993   271.39642     -148.35528      0             -148.21495     -4156.555     \n",
            "      4994   308.6647      -148.40394      0             -148.24435     -3399.2108    \n",
            "      4995   346.29127     -148.42617      0             -148.24713     -2481.008     \n",
            "      4996   379.63629     -148.41978      0             -148.22349     -1414.6983    \n",
            "      4997   403.69989     -148.44776      0             -148.23903     -649.843      \n",
            "      4998   414.19045     -148.44357      0             -148.22941      561.69225    \n",
            "      4999   408.444       -148.4331       0             -148.22192      1718.8569    \n",
            "      5000   386.18249     -148.42391      0             -148.22424      2868.4964    \n",
            "Loop time of 34.9726 on 1 procs for 5000 steps with 5 atoms\n",
            "\n",
            "Performance: 1.235 ns/day, 19.429 hours/ns, 142.969 timesteps/s, 714.845 atom-step/s\n",
            "98.8% CPU use with 1 MPI tasks x 1 OpenMP threads\n",
            "\n",
            "MPI task timing breakdown:\n",
            "Section |  min time  |  avg time  |  max time  |%varavg| %total\n",
            "---------------------------------------------------------------\n",
            "Pair    | 34.75      | 34.75      | 34.75      |   0.0 | 99.36\n",
            "Neigh   | 0.00010908 | 0.00010908 | 0.00010908 |   0.0 |  0.00\n",
            "Comm    | 0.0094096  | 0.0094096  | 0.0094096  |   0.0 |  0.03\n",
            "Output  | 0.14356    | 0.14356    | 0.14356    |   0.0 |  0.41\n",
            "Modify  | 0.062053   | 0.062053   | 0.062053   |   0.0 |  0.18\n",
            "Other   |            | 0.00722    |            |       |  0.02\n",
            "\n",
            "Nlocal:              5 ave           5 max           5 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Nghost:             35 ave          35 max          35 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Neighs:             10 ave          10 max          10 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "FullNghs:           20 ave          20 max          20 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "Total # of neighbors = 20\n",
            "Ave neighs/atom = 4\n",
            "Neighbor list builds = 3\n",
            "Dangerous builds = 0\n",
            "Total wall time: 0:00:37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Radial Function Analysis\n",
        "We will check it for N and H atoms as a test.\n",
        "\n"
      ],
      "metadata": {
        "id": "HqHRJtqZ7qHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['font.size'] = 30\n",
        "\n",
        "def parse_lammps_rdf(rdffile):\n",
        "    \"\"\"\n",
        "    Parse the RDF file written by LAMMPS\n",
        "\n",
        "    copied from Boris' class code: https://github.com/bkoz37/labutil\n",
        "    \"\"\"\n",
        "    with open(rdffile, 'r') as rdfout:\n",
        "        rdfs = []\n",
        "        buffer = []\n",
        "        for line in rdfout:\n",
        "            values = line.split()\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            elif len(values) == 2:\n",
        "                nbins = values[1]\n",
        "            else:\n",
        "                buffer.append([float(values[1]), float(values[2])])\n",
        "                if len(buffer) == int(nbins):\n",
        "                    frame = np.transpose(np.array(buffer))\n",
        "                    rdfs.append(frame)\n",
        "                    buffer = []\n",
        "    return rdfs\n",
        "\n",
        "rdf = parse_lammps_rdf('./lammps_run/des.rdf')\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "plt.plot(rdf[0][0], rdf[0][1], 'b', linewidth=5, label=\"Allegro-DES-SPICE, T=300K\")\n",
        "plt.xlabel('r [$\\AA$]')\n",
        "plt.ylabel('g(r)')\n",
        "plt.title(\"Des bond length: {:.3f}$\\AA$\".format(rdf[0][0][np.argmax(rdf[0][1])]))\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "7aKj3yzhomge",
        "outputId": "978e3925-3c2c-45c0-da7b-ec23f64c02eb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABScAAAMhCAYAAAAaRQT3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAADps0lEQVR4nOzdd3jTVf//8Ve6oYXSMsvespEtQ0GZggwHIMgUBziRW1FcgAu8nXhzO5ANgrJFBBk3IEOZCihT9t50Uuj6/P7gR758mqRN2qZJyfNxXbn0c3JWkoYm7573ORbDMAwBAAAAAAAAQC7z8/QEAAAAAAAAAPgmgpMAAAAAAAAAPILgJAAAAAAAAACPIDgJAAAAAAAAwCMITgIAAAAAAADwCIKTAAAAAAAAADyC4CQAAAAAAAAAjyA4CQAAAAAAAMAjCE4CAAAAAAAA8AiCkwAAAAAAAAA8guAkAAAAAAAAAI8gOAkAAAAAgA+4fv26vvjiCzVv3lwRERHKly+fqlSpoiFDhujAgQOenh4AH2UxDMPw9CQAAAAAAID7HDhwQF26dNH+/fvt3h8QEKB///vfeumll3J5ZgB8HcFJAAAAAABuY6dOnVKjRo105syZTOt++umnBCgB5CrSugEAAAAAuI09++yz1sBkRESEvv32W124cEGJiYn69ddf1axZM2vdV199Vf/884+npgrAB7FyEgAAAACA29Thw4dVqVIlSVJgYKA2b96sevXqmeokJSWpVatW+v333yVJL774oj7//PPcnioAH8XKSQAAAAAAblNr1qyx/v+DDz5oE5iUpKCgIL3zzjvW6/Xr1+fK3ABAIjgJAAAAAMBt6/z589b/r1y5ssN6VatWtf7/pUuX3DonALgVwUkAAAAAAG5ThQoVsv7/wYMHHda7dZ/JEiVKuHNKAGBCcBIAAOSYUaNGyWKxmG7IO9auXWvz+q1du/a2GxPeg38zAPerW7eu9f8XLlyoP//806ZOcnKyRo0aZb1u2LBhbkwNACRJAZ6eAAAAAAAAOe3ChQvaunWrDh06pNjYWAUGBqpw4cKqUaOGGjZsqMDAQLeNnZqaqj179mj37t26fPmyYmJi5O/vr0KFCqlo0aKqV6+eypcv77bxb9W0aVNVqlRJhw4dUnJyslq3bq2PPvpI3bp1U1hYmLZu3aoRI0Zow4YNkiQ/Pz8988wzuTI3AJAITgIAvNDRo0dVoUIFp+r6+/srJCREISEhKly4sIoVK6by5curWrVqqlu3rpo1a6bIyEg3zxgAgJxx+fJlbdu2zXrbvn27jh8/blPPMAwPzM7WgAEDNG3atBzpq3379vrll1+y3c+8efM0btw4bdy40eHzVKBAAfXo0UPDhw837bWYXStWrNDEiRP1888/6+rVqxnWLVq0qHr27KmnnnpKtWvXzrE5pGexWPTNN9+obdu2MgxDV65c0RNPPKEnnnjCbv0xY8aoRo0abpsPAKRHcBIAkKelpqYqISFBCQkJunTpkg4cOGD9y7904wN5vXr11L17d/Xt21elSpXy4GwBIG+bOnWqBg4caCo7cuRIrq0Aux39/fff+vnnn7V9+3Zt27ZNR44c8fSU8qxTp07pscce06+//ppp3bi4OE2aNEkzZszQm2++qTfffDNb2wocPHhQTz31lOlk7MxcuHBB48eP13//+1899dRT+vDDDxUeHp7lOWSkdevW+v7779WvXz9dv37dbp38+fPrww8/1HPPPeeWOQCAI+w5CQC4rRmGoT/++EMjRoxQhQoV1K9fP7srUAAA8ISJEyfqtdde09y5cwlMZsOBAwfUqFEjpwKTt0pKStLbb7+txx57TKmpqVkae9OmTWrQoIFLgclbGYahb775Rs2aNTOdrJ3TMttHcujQoQQmAXgEKycBAHlCaGioKleubPe+uLg4xcTEKCYmRikpKQ77SE5O1owZMzRv3jx98MEHGjp0qJtmCwCAbwoMDMxySnClSpWy1O7SpUtq27atzpw5Y3NfgwYN1LVrV1WoUEGJiYk6cOCAZs2apdOnT5vqzZ49W8WKFdPnn3/u0tiHDx9W+/btFRsba3NfVFSUunTporp166pw4cJKTk7W2bNntWnTJi1ZskTXrl0z1d+zZ4/atWunrVu3umU/zH/9618OV01K0u7du3N8TABwBsFJAECe0LBhQ6dO8D137py2bNmiLVu2aOXKldq8ebNNncTERL300kvasmWLpk+froAAfh0CALxLUFCQatWqpYYNG2rOnDmKjo729JScUrJkSe3YsSNXx3zqqadssiIKFCigmTNnqkuXLjb1P/jgA33wwQem06klady4cWrfvr3uv/9+p8d+9tlnbQKTwcHBGjt2rJ599lmHQcZLly5p6NChmjlzpql8586d+vjjjzVixAin5+CMVatWadGiRaYyi8Vi2pMzt183ALiJtG4AwG2lePHi6ty5s959911t2rRJ27ZtU58+fezuIzV79mz169fPA7MEAOD/BAQEqE6dOnr88cf15ZdfasuWLYqLi9P27dv1zTffuG0fwtvBypUrtWDBAlNZUFCQVq9ebTcwKd1Y3Tly5Ei7qyRfeOGFDLMwbrV37167B/jMmjVLQ4cOzXD1Y+HChTVjxgwNHjzY5r5x48YpLS3NqTk4IyUlRS+++KKprG7duurQoYOp7NixY3kmCA7g9kJwEgBwW2vQoIFmzJihxYsXq3Dhwjb3z549W59++qkHZgYAgPTqq68qNjZWO3fu1KRJkzRkyBA1atRIQUFBnp5anvDuu+/alI0cOTLT/RUl6cUXX1Tbtm1NZQcPHtSsWbOcGnvhwoU2Zd26ddNDDz3kVHtJ+vjjj1W8eHFT2blz57Rp0yan+8jM+PHjtWfPHlPZJ598orp169rUZfUkAE8gOAkA8AkPPPCAtm3bpqJFi9rc98Ybb+jYsWMemBUAwNdFRUUpX758np5GnrR7926tX7/eVFakSBG9/PLLTvcxZswYm7KvvvrKqbb79u2zKevVq5fTY0s39tTu1q2bTfn+/ftd6seRCxcuaPTo0aayBx54QK1bt1atWrVs6hOcBOAJbLIFAPAZ5cuX1/fff6927dqZTuS8du2a3nrrLU2fPj1b/UdHR2vr1q06d+6cLly4oOvXr6tIkSIqVqyYGjVqpKioqOw+BJNz585p586dOnbsmGJjY5WYmKiQkBDlz59fJUqUUPny5VW1alWFhYXl6Lg54dKlS9q8ebMOHTqk2NhYhYeHq2TJkqpbt26WD0TISFJSkrZs2aKTJ0/q/PnzSkhIUOHChVWsWDHVrFlTVapUyfEx7UlMTNSmTZu0b98+XblyRfny5VPRokVVq1Yt1a1b1+72A9lhGIa2b9+uf/75R6dPn1ZycrIiIyNVrVo1NW7cWCEhITk6njfJ7fejPbn9eqelpWnr1q06dOiQzpw5Y3q9mzRpouDg4BwdLzfl9nOJvOH777+3KRs4cKBLq04bNGigBg0aaPv27dayTZs26ciRI6pQoUKGbe2drF29enWnx86ozblz51zux57XX3/dlKodEBCgjz/+WJJUu3Ztm/oEJwF4hAEAgJc5cuSIIcl0a9myZY71//zzz9v0HxQUZJw5c8blvq5evWp88sknRtOmTQ1/f3+bfm+91axZ0xg7dqwRHx+f5blfuXLFeP/9940aNWpkONbNm7+/v1G3bl1j6NChxoYNG4y0tLQsj+2MkSNH2szhVmvXrjXatWuX4XNVv35949tvv82Rua5cudLo0qWLERYWluHzVLFiRWPYsGFZ+hlw5nEfPHjQGDBggJE/f36HcyhevLgxevTobP183BQfH2+88cYbRtmyZR2OFxoaagwaNMg4fPiwtd2aNWts6q1Zsybb88lITo6ZW+9Hb3u9L168aLz44otGiRIlHI4XFhZmPPnkk8aRI0es7Zx97vv37+/UvzcZ3UaOHJknnktvVa5cuQyfJ09K//NRrly5XBu7YcOGNs/L5s2bXe5nzJgxNv18+eWXmbbr3LmzTbt9+/a5PP5XX31l089nn33mcj/pbd++3fDz8zP1+/zzz1vvv379uhEQEGC6v27dutkeFwBc5T2/1QAA+P/cHZw8fPiw3cDFuHHjXOrn22+/NaKiolz+kl68eHFj7ty5Ls97zpw5RtGiRbMVINi7d6/L47rCUaAhJSXFePbZZ12aa4sWLUyBM1ccOnTIaNeuncvPT2hoqPHOO+8YqampOfK4DcMwxo8fb4SEhDg9h/LlyxsHDhzI0uM2DMNYtWpVhkHJ9Lf8+fMbU6dONQwjbwcnc/P96E2v9/fff28ULlzYpZ9xV19vTwUnc+u5tPc759Z5eAOCk7bi4+NtAm/58+c3kpOTXe5rw4YNNs9vr169Mm33wgsv2LRbuXKly+O//vrrNv0sXrzY5X7Sa968uanPQoUKGRcvXjTVSf/HzqCgICMpKSnbYwOAK9hzEgDgcypUqKDOnTvblC9ZssSp9snJyXriiSf05JNP6syZMy6Pf+7cOfXo0cPuJv6OTJw4UT179tSFCxdcHs/TDMPQY489pv/+978utduwYYNatmypw4cPu9Ru+/btatasmVasWOFSO0lKSEjQ22+/rZ49e+r69esut0/v9ddf13PPPadr16453ebo0aNq0aKFTp065fJ4S5YsUceOHXX8+HGn21y9elUDBgzQ119/7fJ43sAT70dHcvv1/uabb9SrVy9dunTJ6TYJCQl54vXO7ecSec+OHTtsTrRu2LChAgJc37msUaNGNidr35rm7ch9991nU2bv9O7MLFu2zHQdFBSkFi1auNzPrb777jtt3LjRVPbWW2/ZHA6YPrU7KSnJ5vAcAHA39pwEAPik1q1ba9GiRaayTZs2yTCMDPcuS0tLU7du3bR06VKb+0qWLKnWrVurXr16KlKkiEJCQnT58mX9+eefWrZsmSlgZBiG3n77bRUpUkRDhgzJcK779+/Xc889J8MwTOX58uXTfffdp0aNGqlcuXIKCwtTSkqKYmNjdebMGe3evVtbt27V0aNHM39C3OiTTz7RDz/8YL0uUKCAunbtqkaNGql48eKKjo7Wvn37NH/+fJ04ccLU9sSJE7rvvvu0Y8cOFSpUKNOx9u7dq1atWik+Pt7mvipVqujBBx9U5cqVVbBgQZ09e1ZbtmzR4sWLberPmzdP165d008//ZS1B60bgaNbD1ooWrSo7r//fjVq1EhFixbVtWvXdPDgQS1cuFC7d+82tT1//ryefvpppwPmkvT777/r4YcfVlJSkqncYrHorrvuUseOHVWmTBkFBATo5MmTWr58udatW2fdf/W5557TBx98kOXH6wmeeD86ktuv98KFCzVkyBCbfxf8/f119913q127dipVqpT8/f11+vRprVmzRv/73/+sPx/PPfec3nvvPafGKlu2rPVU38uXL9u8T6tXr57pHn8lSpRw9qHl+nOJnHX16lWNGTNGGzZs0L59+6x7vkZGRioyMlJVq1bVPffco/vuu8/uadHOsncYTeXKlbPUV1BQkEqXLq0jR45Yyw4dOqSUlJQMg52dOnVSuXLlTIfqff3113r++edVrlw5p8b+/vvv9eeff5rK+vbtq4iICBcfxf9JSEjQq6++aiqrXLmynnvuOZu6tWrVMv2Olm4EfrPz2gCAyzy5bBMAAHvcndZtGIaxbds2u2l8+/fvz7Dd22+/bdOmdOnSxpw5c4yUlBSH7ZKTk41vv/3WZu/DoKAgY/v27RmOOWjQIJsx+/TpY5w/f96px7p7925j9OjRRqlSpTyS1n1rWubAgQONK1eu2G2bmppqfPzxx3bTOAcMGJDp2NeuXTPq1q1r0zYyMtKYMWOGw3aXL182BgwYYPfn4fPPP8/24w4MDDTGjBljJCYm2m2blpZmjBs3ziY9UZLx+++/OzV+YmKiUa1aNZv2VatWNTZu3Oiw3c6dO40GDRpY6+fLl8+mD29O6/bE+9EwPP96X7hwwShWrJhN+0aNGhk7d+502O7w4cNGmzZtsvV6T5kyxabNrftYusrTz6VhkNadXdlJ+2/WrJnx448/ZmncN99806a/0aNHZ/lxtGzZ0qa/Q4cOZdru559/tml3xx13ZPp5wjAMY+HChTbvw+LFizv9+92RESNG2MxpwYIFdusuWrTIpu7QoUOzNT4AuMp7fqsBAPD/5UZwMjk52QgMDLQZZ9myZQ7b/PbbbzZfgps2bWpER0c7Pe6OHTuMggULmvq4//77M2xTpEgRU/377rsvS4fFJCcnG9evX3e5nSvsBRpu3l577TWn+vjpp59sNuiXZPz6668Ztnv//fdt2kRGRmYYrLmVvT2/goODjZMnT2b5cQcHBxurVq1yanx78x80aJBTbUePHm3Ttnr16saFCxcybRsfH280bdrU4evmrcFJT70fDcPzr/fgwYNt2t59991GQkJCpm2Tk5ONhx56KMuvd24EJ3PzuTQMgpPZlRN7knbv3t2IiYlxadwnnnjCpp/Jkydn+XH06dPHpr/ffvvNqbZffPGFYbFYTG3z5ctnPP3008Yvv/xinD171khKSjISEhKMQ4cOGbNmzTLat29vM16RIkWMHTt2ZPkxGMaNPZeDg4NN/Wb0GergwYM282jVqlW25gAArmLPSQCATwoICFBkZKRN+cmTJx22ee+990z7W5UsWVJLly5VeHi40+PWrVtXX375pals2bJl2rlzp936cXFxunjxoqls0KBBGaaeOxIQEJBp6qW7tGrVypSmmZEHHnhAb775pk35F1984bBNcnKyxo8fb1M+depU1alTx6lx33//fXXo0MFUdv36dZf3yrzVRx99pNatWztV95VXXlGpUqVMZcuXL8+0XXJysr766itTWWBgoObPn68iRYpk2j40NFQLFy50Km3em3ji/ZiZ3Hi9Y2NjNWPGDFNZeHi45syZo/z582faPiAgQNOnT1fZsmWdmqen5MZzCfeKjIxUxYoVVaNGDUVFRdns6XiruXPnqkGDBjp79qzT/V++fNmmLCwsLEtzddTW2f1cn3/+eS1btkzly5e3liUmJuqbb75Rhw4dVKJECQUFBSk0NFSVKlVS7969bX5Gu3Tpop07d2Y7nfqll14y7ZlssVj0ySefOKxfsWJFhYaGmsqy+m8gAGQVwUkAgM+yF4yJjY21W/fvv/+22dfugw8+yFJAp3fv3qpSpYqpLP3+lzfFxcXZlKXfzD4vyCiwaM+rr76q0qVLm8p+/PFHhweezJ8/3+a+jh072j34KCPjx4+Xv7+/qWzChAlKTk52qR/pxhe+Z5991un6gYGB6tmzp6ns5MmTOn/+fIbtFi1aZPOF/rnnnlP16tWdHrt48eJ66623nK7vaZ56P2Ykt17v7777TgkJCaay1157zaU9HUNDQ716f9Hcei5vKl++vIwbGWWmG1xTu3ZtjRgxQqtXr9bFixd16dIlHTp0SLt379bp06cVFxen9evX66WXXrIbCDx48KA6d+6sq1evOjVe+veBdGMv5qyy19bZuUhS+/btdeDAAc2cOdPuQTn2+Pn56amnntKOHTv0448/qmTJkk6PZ8+KFSu0ePFiU1nfvn3VoEEDh20sFotq1KhhKrty5YpLB6sBQHYRnAQA+Cx7m80nJibarTtv3jzTdYECBWy+DDvLYrHo/vvvN5WtXbvWbt3IyEibVZIbNmzI0riectddd9mcBpqZkJAQ9e3b11SWkpKiVatW2a2/cuVKm7JnnnnGpTElqVKlSmrfvr2p7NKlS/rjjz9c7uvxxx+Xn59rH7UaN25sU7Z///4M26Q/5VWSnnzySZfGlaQBAwZ4bGWtqzz1fsxIbr3eq1evNl37+/urf//+Lo0rSQ8//LDXrpbNrecSOaNTp07aunWrdu3apQ8++ED33nuv3T+iBQcHq0WLFvr000919OhRu3882rZtm81BLo7Y+6NRSEiI6w/g/7MXnEx/wFhGDMPQL7/8ounTp2v9+vVOtUlLS9PkyZM1fPjwbK/2TU5O1tChQ01l+fPnd+oPEfZ+R+/YsSNb8wEAVxCcBAD4rFtTQm9ylC7966+/mq7r16+frS9BFSpUMF2nP6nzppCQEJu05I8//lgLFizI8ti5rVu3bllq99BDD9mUbdq0yW7djRs3mq5DQ0NtUrSd1aNHj0z7d0bLli1dblOpUiWbspiYmAzbpH9OqlWr5tKqyZsiIyPVqlUrl9t5gqfejxnx1Otdr149RUVFuTx2SEiI02nTuS23nkvkjO7du6thw4YutSlcuLAWL16sxx9/3Oa+b775RocPH87SXLKy5UlGbZ1dQXv48GHdc8896tKli1asWGETOC1SpIiqVaumypUr2/xRICUlRStWrFCHDh3UrVs3m61cnDV+/Hjt3bvXVGZvywN7atWqZVNGcBJAbgrw9AQAAPCU6OhomzJ7KydSU1NtAgK7du3SnXfemeWx0++VFRMTo+TkZLt7cg0cONC0GuLatWt6+OGHddddd6l///564IEHbFKgvUlG6WQZqV27tgICApSSkmIt2759u029hIQEHThwwFRWr149m/RsZzVq1MimLCsrJ9OnCjvD3n6JGQVYrl69qn379pnKsvp832y7YsWKLLfPDZ5+PzqSG693dHS0zb649evXd3ncm+rVq6f58+dnub275MZzCe/wzTffaOfOnaZ/25OTk/XFF1/o888/z7Ctvfeno+wHZ9hr68xq8l27dql169Y2QcXKlStr2LBh6tKli02A8ODBg5o/f77GjRtn2pLkxx9/1IEDB7R69WqXtmq4cOGCRo8ebSorWbKkhg8f7lR7Vk4C8DSCkwAAn3XlyhWbsoIFC9qUXbp0SdeuXbNpa699dly+fFnFixe3KR88eLBmzZqlLVu2mMo3bdqkTZs2aciQIapcubJatGihhg0bqkWLFqpTp062VpDkpDvuuCNL7YKDg1WhQgX9888/1jJ7e8hdunTJZnVLVlYO3lStWjX5+fmZVtZmZSWLvQOXMmPvy3ZG+11euHDB5rFn9fmWbjx2b+fp96MjufF62zuc49YDOFyVfsWot8iN5xLeISAgQGPHjlXbtm1N5cuWLcs0OGnvAKicDk6mPygmvcuXL6tjx442vyMef/xx/fe//3W4orty5cp69dVX9dRTT6lPnz6mPXT37t2rnj17as2aNU5vbzBixAibYPz777/v1CFZEisnAXgead0AAJ+UlJRkN5hRpkwZmzJnT+vMLkdfqoKDg/Xzzz+rTZs2DtsePHhQU6dO1XPPPac777xTRYoUUd++fbV06VK76eu5yZXTkzNra2+1q73XMTt76fn5+dkEqe2dCpsZV1bdZZW95yMnn29v5On3oyO58Xrb+1m/HV/v3Hgu4T1at25t84eAAwcO2KwSTs/evpbx8fFZnoe9tpkdQPfaa6/p1KlTprKHHnpIEydOdGqriYiICM2fP19NmjQxla9bt05Tp07NfNK6kVEwZcoUU1m9evXUr18/p9pLUokSJVSkSBFT2dGjRx0eEggAOY3gJADAJ/3555+mdOGbKlasaFOW0yuysqJIkSJasWKFfvjhB6fSdi9fvqyZM2eqU6dOqlGjhkdTNzNbeeJKW3unl9sry86Yzo7rDXL6sWf3ecsN3vB+9JTr16/blGXnEKPg4ODsTAfIERaLRffcc49NeWanRdtb2ZxZQDMjJ06ccGqMmy5evKjp06ebykJCQvTFF1+4lLkQEhKi8ePH25RntnJUurEn5gsvvGDzR8hPP/3U5UOl0qd2G4bB6kkAuYbgJADAJ/3+++82ZQULFrR7qIK9fSh79uwpwzBy9JZZeqbFYlGPHj20bds27dmzRx9//LE6d+6c6cqO/fv365FHHtGQIUOc3tw/JyUkJORY2wIFCtjUsVeWnTGdHdcb5PRjz+7zlhu85f3oCfZWOmYncM6qKHgLe4c6XbhwIcM29rYlOHbsWJbnkD4Y6u/vr7Jlyzqsv2rVKps/GLRp08apA2jSa9iwoWrWrGkq++uvv0z7Udrz3Xff6bfffrMpv/fee2WxWFy6rVmzxqafnTt3uvxYACArCE4CAHzSqlWrbMqaNWtmd7VD+lQnKWtpvjmpevXq+te//qXFixfr4sWLOnDggCZNmqS+ffvana8kff311/rggw9yeabZO5QifVt76doRERE2ZfbSnZ2VlpZmE7TJyh54ucHe85GTz7c38sb3Y26x97OenTT33EqRBzJjb9V2Zlsr2Ntf9+DBg1kaPykpyWblZKVKlRQQ4PiIhl27dtmU3XXXXVka31Fbe2PcFB8fr1dffTXL4zmDlZMAcgvBSQCAz/nnn3+0bNkym/LOnTvbrV+0aFGboGV2Vme4Q5UqVfT4449r+vTpOnv2rH766Se7p05/8MEHWTrcJTvSn6TtrKSkJB09etRUVqxYMZt6RYoUsXl99u7dm6UxpRsrTdOnyDkK+HqavZ/N/fv3Z7m/9Cd/e6O88H50l2LFitmkYv/1119Z7i+jwAeQm+ytkszs39169erZpC5v27bN7pYtmdm2bZvNAUr169fPsI294H7RokVdHjujthn94eX999/X6dOnszyeMwhOAsgtnNYNAPA5n332mU3wKTg4WN27d7dbPyQkRHXr1jV9SD9w4IDOnTvn0mm+ucXf318PPPCAOnTooC5dupgCsVevXtUvv/yiPn365Np8tm/frtatW7vcbteuXTZfFu3tt5k/f37dcccdpsDajh07lJqaKn9/f5fH3bp1q02ZM/t8ekL+/PlVrVo1UzB2+/btWe4vO21zS157P+akwMBA3Xnnndq8ebO1bMuWLUpLS3N5fzlJ2rRpU05OD8gye39QyizQFxoaqnr16pn+3UpISNCff/5p949zGdmwYYNNmb19MG9l78Cb7JwWfvXqVZsyR6dtHzp0SJ999pmpLCIiIsM0dGfs2bPH9Ht39+7dSklJyXAFKQDkBFZOAgB8ysqVK/XNN9/YlD/22GMZfhFq27atTdmCBQtydG45LSAgQGPGjLEpz+3VUosWLcpSO3vPr6OUuWbNmpmu4+Pj9csvv2Rp3Llz52bavzdJ/5zs27cvSysgr1y5orVr1+bQrNwrL74fc0rTpk1N12fPnrW7V1xmDhw4oG3btrnczl6QIjU11eV+gJsuX75sEygPCQlR1apVM23boUMHm7J58+a5PAd7bez1fSt7nxmOHDni8tg3HT582KkxJOmll16y2e9y2rRp2rFjR7ZudevWNfV5/fr1bGUiAICzCE4CAHzG4cOH1atXL5tVk/nz59fo0aMzbNu1a1ebso8//jhL6WO5qVq1ajZlub2v4O+//67du3e71Ob69euaMWOGqSwgIEBt2rSxW799+/Y2ZV9//bVLY0o3vlimD2oWKVIk0/Q+T7r//vttyr799luX+5k2bZqSkpJyYkpul1ffjzmhZ8+eNmUff/yxy/189NFHWRrf3iFM8fHxWeoLkG78/KYPcLdq1cru4VfpPfroozZlU6ZMcenfsj///NNmxXyTJk3sHrhzqypVqtiU2dsyxhlXr161+eOQxWJR5cqVbeouX75cP/30k6nsvvvuc7g1jStq1KhhU0ZqN4DcQHASAOATbu7BaG+PqA8//FClS5fOsH3z5s3VqlUrU9nhw4f1r3/9KyenmePsnfSZnT2xsurFF190qf6///1vnTx50lTWtWtXuye6StKDDz5oc9+SJUtc/qL4/PPP2wS4nn76aQUGBrrUT27q1q2bTTrz+PHjXdp78sKFC3rnnXdyempuk1ffjznhrrvuUr169Uxlv/zyi2bOnOl0H6tXr9akSZOyNL69Q3nsrfgCnLFt2zZ9/vnnNuWPPPKIU+1r1aqlFi1amMouXLhgk/KckREjRtiUDRkyJNN2bdq0sdlO4cCBA3ZX32fms88+swny16tXz2af5eTkZA0dOtRU5ufnp08++cTlMe2xF5zkxG4AuYHgJADgtrZt2zb17dtXXbp0sbuxfP/+/fXcc8851dd7771ncxDHF198oZEjR8owjCzN7++//1a/fv105coVu/f/9ttvGjdunOLi4rLU/7hx42zK0qdt5Yb//e9/evPNN52qu2zZMr377rs25S+88ILDNoGBgXr++edtyvv166c9e/Y4Ne7bb7+tn3/+2VQWEhKiZ555xqn2nhIYGGgzx6SkJD388MNOncZ89epVPfTQQw5/Br2VJ96P3uKtt96yKRs0aJBTqe3r1q1Tt27dsvwc1apVy6Zs6dKlWerLWxw9elQWi8Xm5isGDBhg89hHjRqVabsjR45owoQJWV5xvXXrVj3wwAM2+zRWrVpV/fv3d7ofe79bRo4cqT/++CPTtuPHj9fy5ctNZRUrVlTv3r0zbVu4cGG7q/kHDx7sUir0ypUr7WZv2FsV+p///Mdm245+/frpzjvvdHq8jLByEoDHGAAAeJkjR44Ykky3li1bOtX2/PnzxpIlS4y33nrLaNKkiU0/t94GDBhgpKSkuDS3UaNG2e2rVatWxrp165zq4+LFi8a3335rtGnTxrBYLIYk48KFC3brLly40JBkFCxY0Hj66aeN5cuXG0lJSZmOERcXZwwfPtxmnuHh4cbVq1ddesyuGDlypM2YISEh1v9/4oknjOjoaLttU1NTjc8++8xU/9bXKjPXr1837rzzTpu2RYsWNb7//nuH7a5cuWIMGjTI7us6bty4LD/urLD3sz9lypRM2yUmJhp33HGHTdtq1aoZv//+u8N2f/31l9GoUSNr/Xz58tn0sWbNmiw9FmetWbMmy2Pm9vvxJk+/3oZhGD169LD72Hv06GGsW7fOSE1NNdXfsmWL8dRTTxl+fn7Wuk2bNs3Sc1+tWjVTG39/f+PTTz91+N7OiDc8l/baetvXpHLlyrltfv3797fpe+TIkZm2+/PPPw1JRsmSJY0333zT2LFjh1PjnT9/3njjjTeMwMBAm3H9/PyMpUuXuvwYunbtatNXwYIFjcWLF9utn5SUZLzzzjt2X/clS5Y4Pe62bdus/26kH3vKlClGcnKyw7YJCQnGBx98YPd5KF26tM3v6nPnzhnh4eGmevnz5zdOnTrl9Hwzc+jQIZu5FClSJMf6BwBHOHYLAJAnbNu2zeHKgISEBEVHRysmJsbmdGd7QkND9eGHH+rZZ591eR5vv/229u3bp++//95UvnbtWt1zzz2qWrWqWrVqpZo1ayoyMlLBwcGKjo7WlStXtGfPHm3fvl179+51+QCJ2NhYffPNN/rmm29UoEAB1a9fX/Xq1VOFChVUqFAhhYaG6tq1azp58qT++OMPLV++3O7ekh9//LFT+3jlpHfeeUfDhw+XJE2cOFFz5sxRt27d1KhRIxUrVkzR0dHat2+f5s+fr+PHj9u0L1eunFMpekFBQZo1a5YaN25sSo+7cOGCHn30UY0aNUrdunVT5cqVVaBAAZ07d06bN2/W4sWL7a5M7dSpk93VmN4oJCREU6ZMUatWrUyrmPbt26dmzZqpWbNm6tixo8qUKSM/Pz+dOnVKK1as0Nq1a60/i/7+/ho5cqRee+01Tz0Ml3nq/egNvvrqK+3du1d//fWXqXzOnDmaM2eO8ufPrxIlSsjf319nzpyxSRmtX7++3n77bZs9S5054f7xxx+3vqelGwfiDBs2TP/6179UunRphYeH2/QzePBgDR482NWH6TM6duyo06dPO7zf3n2ZrZZbunSpSpYsmd2pZer06dN677339N5776l06dKqX7++6tSpo6ioKIWHhys4OFhXrlzRqVOn9Ntvv2n9+vW6du2a3b7GjRtndx/dzHz77bfavn27aTuQ2NhYdenSRQ0bNlTXrl1VoUIFJSYm6p9//tF3332nU6dO2fTz3HPPqVOnTk6P26BBA7366qsaO3asqTw2NlYDBw7U22+/rQ4dOujOO+9U4cKFlZaWpgsXLmjLli1atmyZ3YyOwMBATZo0yeZ39YgRI2x+r7/yyis5+hqXL19e+fLlM61mvXjxok6ePJnp9jcAkC2ejo4CAJCeo1Us2b0FBQUZAwYMME6cOJGt+aWmphpvvPGG3dUSWb1ltnIyJ24vvfRSth63M+ytgkpLS3O4yiuzW+nSpY1Dhw65NIetW7caxYoVy9Zz9dBDDxmJiYnZetxZkZ3VX4ZhGD/99JMRFBSUpcf85ZdfZmsVY1Zld8zcfD/e5C2v94ULF+yuFs7sVqtWLeP06dPGypUrbe5zZvVbQkKCUbNmTZfGdLQSzxueS29YOWlvZWR2b0eOHHFq7OyunMyJW0hIiDF+/PhsPYd79uwxSpQokeU59OjRw+VsipuGDBmSI89DUFCQMXv2bJv+t27davNvXFRUlBEfH5+t58yeevXq2czrp59+yvFxAOBW7DkJALitWSwWNWzYUGPHjtXRo0c1ZcqUbP/138/PT++9956WLl2a7f0bw8PD9cQTTygsLMzh/SEhIdkao2jRopo0aZI+/fTTbPWTVRaLRd99952efvppl9o1b95cv/76qypWrOhSu4YNG+r33393eLJ3RkJDQzV69GjNnTs328+7JzzwwAP6+eefVaZMGafb5MuXT5MmTXLqAAhvlJvvR29TpEgRbdq0SSNHjlRwcHCm9f39/fX0009r48aNioqKsru3Znh4eKb95M+fX8uXL1fbtm2zNG/gVi1atND27duzlM1wq+rVq2vLli02B+RkJjAwUCNHjtTs2bOdWjlsz5dffqnvv/9eRYoUyVJ76cZK2K1bt9rsNWkYhp5//nmbfWLfe+89hYaGZnk8R9h3EoAnkNYNAMjT/Pz8FBwcrJCQEBUuXFjFixdX+fLldccdd+jOO+9U8+bNFRkZ6ZaxO3TooA4dOmjJkiWaMmWK1q5dazdFK72KFSuqdevWat++vTp27JhhmvW9996rS5cuaeXKlVqxYoV+++03/fXXX06loTZo0EC9evXSE0884VTAwZ0CAgL09ddfq3v37vrggw+0du1apaWl2a1br149PfPMMxo0aFCWD6WoWLGiVq5cqZUrV+o///mPVq9erYSEhAzrd+3aVcOHD1eJEiWyNKa3aNOmjfbs2aOxY8dq+vTpOnHihN16+fLlU48ePfTWW2+pUqVKuTzLnJcb70dvFBwcrFGjRmnw4MGaP3++Fi9erMOHD+vMmTNKTk5WZGSkqlWrpnvvvVd9+/ZVhQoVrG3Pnj1r05+z/16WKlVKK1as0B9//KG5c+fqzz//1P79+xUdHa34+HibU+9x+6lVq5bWrVuntWvXasOGDfrzzz914cIFp9qWK1dObdu21ZAhQ1S/fv0cm1OZMmW0bt06zZ07V+PGjdPvv//u8PCnsLAwde/eXcOHD1e1atWyPXbPnj31wAMPaPbs2ZoyZYq2bt2a6VYzBQoUUJs2bfTUU0+pffv2dn/nzZw5U5s2bTKV1alTRwMGDMj2nO2pWbOmTRnBSQDuZjEc/WsNAABcYhiG/vrrLx06dEiXLl3SpUuXlJaWpgIFCqhQoUKqVKmSqlevrkKFCmVrnISEBP3zzz86fPiwzp49q7i4OCUnJyssLEzh4eGqXLmy6tSp4/GAZEYuXryoTZs26dChQ4qPj1fBggUVFRWlevXquSVQlpSUpM2bN+vEiRO6cOGCEhISVLhwYRUtWlS1atVS1apVc3xMb2AYhrZt26YDBw7ozJkzSkpKsgarmjRpkucCca7IrfdjXvbYY49p1qxZ1usyZcrY3fcVcNaZM2d0+PBh67+1V69eVXJysgoUKKCIiAgVLVpU9evXV/HixXNlPufPn9eWLVt0+PBhxcbGKiAgQEWKFFH16tXVqFEjBQUFuW3s69ev688//9ShQ4es+2L7+/urUKFCioiIUK1atVStWjX5+ZHMCAAEJwEAAAAfc/36dZUtW1bnz5+3lj388MOaN2+eB2cFAAB8EX+mAQAAAHzMtGnTTIFJSbrnnns8NBsAAODLWDkJAAAA+JCDBw+qYcOGiomJsZbly5dPJ0+edNsevQAAAI6wchIAAADIg958802X94j8/fff1apVK1NgUrqx/ySBSQAA4AmsnAQAAADyoJCQEKWkpOi+++5T165ddffdd6t69eoKDAw01bt8+bI2btyoyZMna/HixUpLSzPdHxUVpV27dqlIkSK5OX0AAABJBCcBAACAPCkkJETXr183lQUGBqpYsWIKDw9XSkqKrly5oosXL8rRR/6goCAtXbpUrVu3zo0pAwAA2CA4CQAAAORB9oKTrihevLgWLFigZs2a5eCsAAAAXMOekwAAAEAe9OCDDyo0NNTldmFhYRo2bJh27txJYBIAAHgcKycBAACAPCoxMVHr16/Xb7/9pp07d+rIkSM6ffq0EhISdO3aNYWFhSkyMlLFihVTw4YNdc8996hdu3aKiIjw9NQBAAAkEZxEDklLS9Pp06dVoEABWSwWT08HAAAAAAAAHmIYhuLi4lSyZEn5+WWcuB2QS3PCbe706dMqU6aMp6cBAAAAAAAAL3HixAmVLl06wzoEJ5EjChQoIOnGD13BggU9PBsAAAAAAAB4SmxsrMqUKWONF2WE4CRyxM1U7oIFCxKcBAAAAAAAgFNb/3FaNwAAAAAAAACPIDgJAAAAAAAAwCMITgIAAAAAAADwCIKTAAAAAAAAADyC4CQAAAAAAAAAjyA4CQAAAAAAAMAjCE4CAAAAAAAA8AiCkwAAAAAAAAA8guAkAAAAAAAAAI8gOAkAAAAAAADAIwhOAgAAAAAAAPAIgpMAAAAAAAAAPILgJAAAAAAAAACPIDgJAAAAAAAAwCMITgIAAAAAAADwCIKTAAAAAAAAADyC4CQAAAAAAAAAjwjw9AQAAADcKS0tTSkpKUpLS/P0VAAAAACP8fPzk7+/v/z9/T09FROCkwAA4LaTkpKiuLg4xcXFKSEhwdPTAQAAALxGUFCQChQooLCwMOXLl08Wi8Wj8yE4CQAAbisJCQk6ceKEDMNQaGioSpQooaCgIPn5+Xn8gxcAAADgCYZhKC0tTampqUpISFBMTIwuXbqkkJAQlSlTRgEBngsREpwEAAC3jZuBydDQUEVFRXn0QxYAAADgjQoWLCjDMJSYmKhTp07p6NGjKlu2rIKCgjwyHw7EAQAAkqSTJ6UdO6S8ujVjSkqKNTBZqlQpApMAAACAAxaLRfnz51f58uVlsVh07NgxGYbhkbkQnAQAwMddvCg1by6VKSPVqyeVKCGtXu3pWbkuLi5OhmEoKipKfn58xAEAAAAyExgYqFKlSiklJUXx8fEemQOf3AEA8HEPPCD99tv/XV+4cKPswAHPzSkr4uLiFBoayopJAAAAwAUhISEKCQlRTEyMR8YnOAkAgA87ckTavNm2PDFR+umn3J9PVqWlpSkhIUEFChTw9FQAAACAPCc8PFzx8fFK88AeTwQnAQDwYePHO77v5Zdzbx7ZlZKSIkke28QbAAAAyMuCgoJkGIZSU1NzfWyCkwAA+LBLlzw9g5xx8y+87DUJAAAAuM7f31+SCE4CAIDc5aED+dzGYrF4egoAAABAnuPJz9EEJwEAAAAAAAB4BMFJAAB82O22chIAAABA3kJwEgAAH0ZwEgAAAIAnEZwEAAAAAAAA4BEEJwEA8GGsnAQAAADgSQQnAQAAAAAAAHgEwUkAAHwYKycBAAAAeFKApyfgDZKSkrRv3z4dPXpUp06dUlxcnJKTk1WwYEEVLlxYderUUfXq1eXv758j46WkpGjz5s36+++/denSJfn7+ysqKkoNGjRQzZo1c2SMm06dOqXff/9dx44dU2JiogoWLKiqVauqRYsWCgsLy9GxAAB5D8FJAAAAAJ7ks8HJefPmadWqVdq4caP27dunlJSUDOuHh4erV69eevHFF1WtWrUsjRkfH6+xY8fqq6++0uXLl+3WueOOO/Tqq69qwIABslgsWRpHkn799VeNGjVKa9eutXt/UFCQevbsqXfeeUfly5fP8jgAAACwb8CAAZo2bZr1+siRIw4/dx09elQVKlSwXvfv319Tp0518wwBAAA8z2eDk0OHDtWpU6ecrh8TE6Ovv/5akyZN0uuvv66RI0e6FDz866+/1LVrVx05ciTDevv379fjjz+uH374QT/88IPCw8OdHkOSDMPQq6++qo8++ijDeklJSZoxY4YWLFigadOm6eGHH3ZpHAAAAABwlmEYOnDggHbs2KGLFy8qJiZG/v7+Cg0NVVRUlCpWrKiqVasqNDTUo/OMjY3Vtm3bdOTIEUVHRysxMVGhoaEKDw9X+fLlValSJZUrV86jc8wthmFo//792rt3r06ePKn4+HhZLBZFREQoMjJStWrVUrVq1bK1qAZ5W25noUpkot62DB9VqlQpQ5LpFhISYlStWtVo1KiR0aBBA6NcuXKGxWKxqSfJePzxx50ea9++fUaRIkVs+ggLCzPq1KljVKlSxQgMDLS5v2nTpkZiYqJLj+u5556z6cdisRhlypQx6tevb3ce/v7+xoIFC1x9Ck1iYmIMSUZMTEy2+gEA5K5HHzWMG8nd9m95RWJiorFnzx6Xf28CNy1btszmM1KrVq2y1Wf//v1N/R05csRh3SNHjpjq9u/fP1tjwzvZ+16R/nN7cHCwUahQIaNy5cpGixYtjEGDBhmff/65sW3bNiM1NTVb45crVy7TOTh7W7hwoVNj7t+/33jxxReNwoULZ9qnv7+/UatWLePJJ5805s+fn2vfLZKTk43p06cbzZs3N/z8/DKdZ2RkpNG+fXtj7Nixxs6dOzPtf+TIkU49p6GhoUapUqWMJk2aGM8995yxcOFCIzk52anHMGXKFFNfLVu2zPLzsWHDBmPgwIFOvWbh4eFGt27djPnz5xvXr1/PtG9P/AzmpDVr1uTY/DO6TZkyJdcfm7Pmzp1rPP3000atWrWMgIAAp35GBg8ebOzduzdb48bFxRlvvPGGERkZ6XCsO+64w5g8ebKRlpaWrbHWrl1rtGrVyuE4QUFBRt++fTP8vW5P+p+fcuXKuTy3f/75xyhbtqypn5CQEGPJkiUu95VeTn+ediVOlIe+duSsUqVKGSVLljSefPJJY8aMGcbBgwft/rK/fPmyMWHCBKN06dI2P5CTJ0/OdJzk5GSjdu3aNr/Mpk2bZiQlJVnrXbp0yXjjjTdsfhk+//zzTj+mH374wWaODz/8sHHgwAFTvVWrVhl16tQx1StQoIDLb6xbEZwEgLyJ4CRwQ8+ePW0+R1ksFuPo0aNZ7pPgJNLLbsCiRIkSxgsvvGAcPHgwS+PnZmAoJSXFGD16tN1FGM7eunXrlqXH6Ypdu3YZ9erVy9ZzsWPHjgzHcDY4ae9WqlQpY/r06Zk+jpwITv79998ZBmQyuxUvXtz4+uuvjZSUFIdjEJx07ubNwUl7C72cuQUGBhojR47MUuBw165dRoUKFZweq3379kZ0dLTL46SlpRmvvPKK0+OEhoYa8+bNc7r/7AYn9+zZY0RFRdnMYdWqVS4+Uvs8GZz02dO6ly5dqpMnT2rChAnq06ePKlWqJD8/26cjIiJCTz75pHbt2qX69eub7nvjjTeUlpaW4TiTJ0/WX3/9Zepv/fr16tevnwIDA63lkZGReu+99zRjxgxT+6+++kr//PNPpo8nKSlJr776qqls8ODBmjt3rqpUqWIqb926tdatW6eGDRtay+Li4jRy5MhMxwEA3F44EAe4sX3Pjz/+aFNuGIZpz0jA086ePasvvvhCVatW1dNPP60rV654ekp2paWlqX///ho5cqSSk5Nt7i9durTuvPNONW7cWJUrV3aYym24+ZfUtm3b1KJFC/3555829+XPn1/Vq1dXkyZNVLt2bZUsWdJhP+6c56lTp9SvXz8NGjTIreNMnDhRDRo0sHtmgcViUeHChVWtWjU1atRIZcuWVXBwsE29c+fOafDgwXr00UfdNk94p5CQEFWtWlWNGjVSgwYNVK5cOZt0/+TkZI0ePVpPPPGES33v379f9913n80WeWFhYapTp46qVKliiq1I0vLly3X//ffr2rVrLo31wgsv2GyRZ7FYVKZMGdWvX19FihQx3ZeQkKCePXtq4cKFLo2TFTt37lTLli115swZa1nBggW1fPlytW7d2u3ju5vP7jlZp04dl+pHRERo5syZqlmzpvWXwpkzZ7Rx40bdfffddtskJSXpvffeM5V9/PHHqlGjhsNxevfurWXLlmnmzJmSbuynMGrUKH333XcZzm/SpEk6evSo9bpKlSr67LPPHO7/ER4ermnTpqlevXpKSkqSJH333XcaMWJElg/8AQDkPQQnAen77793+AVm+vTpevvtt3N5RvAVM2fOVPHixU1l169f15UrVxQTE6MDBw5o06ZN2rFjh/Uzu3Qj+DdhwgStXLlSP/74o2rXrp2l8T/++GPVrVs3S20zajd27Fib7y81a9bU8OHD1blzZ0VERJjuS0tL0549e7R+/XrNmzdPv/76q1JTU7M0L2dFR0erU6dOio2NtZYFBgbq6aef1sCBA3XnnXfaLF65fPmytm7dqp9//lnz58/X6dOnszR2u3bt9Morr9iUx8bG6tChQ1qyZInWrVtnum/y5MkqWbKk3n333SyNmZGxY8dqxIgRNuX33Xef+vbtq/vvv9/m51SS/vjjDy1evFjff/+99u/fby2/cOGC02O762fQXerWrauVK1c6VXf69OmmxUeOXnd7cnrvxJxWsmRJderUSffcc4+aNm2qChUq2Lxfrly5onnz5umdd97RyZMnreWTJ09WixYtNHDgwEzHSUlJUffu3XXx4kVrWWRkpD777DP16tXLGpS8fPmyPv30U40ZM8a6gOz333/X8OHD9cUXXzj1mObMmaPx48ebyh5++GGNGTPGtODrf//7n4YNG6Zdu3ZJklJTU9W/f3/Vq1fPbYcNb926Ve3btzf9QSoyMlLLly83LTrL03JkraYPadiwoWkJ7TfffOOw7uLFi011y5cv79QS5oMHD5r2ugwMDMx0SXL9+vVNYzmTcm4YhtG3b19Tu+HDhzvVLj3SugEgb+rRg7Ru4K677rJ+FrJYLEbz5s1Nn4/WrVuXpX5J60Z6t77Gmf1M3Co6Otr46KOPjDJlytj0UbhwYWP37t1O9ZM+pXbNmjVZfzAOnDlzxsifP79pnIEDB2aY6pveqVOnjBEjRri0z7+rhg0bZppjZGSksWXLFqfbp6WlGYsXLzZatWrlclq3M+/vZcuWGRERETZpsYcPH7ZbP6tp3T/++KPNz1Tp0qWNn3/+2an2hnEjhX/ixInWdN+Mxs6Nn0FvkZXXPS/YuXOnS6nZly9ftolXREVFObWH7jfffGNqFxERkeG/d999952pfkBAgM02d/Zcv37dKF++vKnt4MGDHT7O6Ohom9hQv379Mh0nK2nd69evNwoWLGhqV6xYMaf2u3UVad15SKVKlUzXt0bw00ufHjRw4ECnTjKrVKmSWrZsab1OTk7W0qVLHdY/efKk/vjjD+t1WFiYevTokek4kjRo0KAM5wwAuL2xchK+7ubKtJuaN2+u1157zVSH1G54Wnh4uF5++WXt3btXvXr1Mt136dIlPfTQQ0pISPDQ7MzmzZunq1evWq9r1aqlCRMmuHRab8mSJfXBBx9o0qRJ7pii0tLSrJlqN/33v/9Vo0aNnO7DYrGoc+fOWrNmjVtW8HXo0EFz5swxlSUnJ+u///1vjo1x/Phxm9VrVapU0caNG9WxY0en+/H399egQYO0Z88ePfDAAzk2P3inOnXquHRC+80s1Fvb3MxCzUhWM1H79Oljvb6ZiZqZrGaiBgUFWcu+++477du3L9OxXLF69Wp16NDBtMK7VKlSWrduncvZwN6O4KSL0qf8FCpUyGHdn3/+2XTdrl07p8dp27at6XrJkiVOj9O8eXOH+7ak17x5c+XPn996vX//fqf2uAQAALgdTJ061XTdp08fdejQwbSv1Ny5c03BFsBTQkNDNWvWLL3wwgum8v3792vMmDEempVZ+pTXxx9/XAEB3rWb2M6dO3X+/HnrdWRkpLp37+7BGdnXpk0bm++FK1asyLH+hw4dqsuXL1uvQ0ND9csvv6hs2bJZ6q9gwYL68ccfNXTo0ByaIW4X1atXV4MGDUxle/fuzbDN8uXLdeLECet1+fLlnUoFHzVqlCmoOHfuXMXExGTYZuLEiabrESNGKCQkJMM2NWrUUM+ePa3XqampmjJlSqbzc9bSpUvVqVMn0x+eypcvr3Xr1umOO+7IsXG8hXf9lvByhmFo69atprL0b7Cbzp07p7Nnz1qvg4ODbQ7UyUjz5s1N1zt27HBYN/19zZo1c3qcgIAANW7c2LTx8Y4dO2wO0QEA3J5YOQlflpaWZtoPLCgoSD169FBAQIB69uxpXaEUGxurhQsX6rHHHvPUVJ129epVbdy4UadOndL58+fl7++vYsWKqUaNGqpfv75Lq13sMQxDW7Zs0d69e3X27FkFBASoXLlyat68eYYHhmRXfHy8Nm7cqNOnT+vs2bMKCQlRy5YtM/18ffz4cW3ZskXnzp1TTEyMIiMjVaJECTVv3lxFixZ123zd7eOPP9amTZu0ZcsWa9lnn32ml19+OcPFE7nh1r3lJHnlfvbp51i5cmWXVnbmpgceeMAU8N29e7fS0tLsHubqigMHDthkzY0ZM0YVK1bMVr9+fn7q1q1btvrA7alSpUratm2b9TqjLFQp+5moN2McNzNR0686vym7mai3fo748ccf9eGHHzrVNiMLFixQr169THsNV61aVf/73/9UunTpbPfvjQhOumDy5MmmTY+rVaumxo0b262b/q8AlStXNi35zUz6pcoHDx5USkqK3b86ph8ro2XOjsa6NTiZ2V8wAAC3D4KT8GX/+9//TEGKjh07Wg/q6NOnjyl9curUqV4dnNy4caPef/99rV69WtevX7dbp1ixYnr66af1yiuvqECBAi71n5aWpvHjx+ujjz6yCexIN1Jc27dvr3//+9/Ww1lu/RJ56xdFe1q1aqVff/3Vem38/3+c9uzZo/fee08//vijzerVF1980W5wMi0tTVOnTtVnn32mv//+2+54fn5+aty4sV5//XV17tzZ8QP3UoGBgfr3v/+tVq1aWcuuXr2q6dOn26yqzG1xcXGma3cfbJMVeWGON6UPFqalpenKlSsqXLhwtvr97LPPrAeHSFLx4sU1ePDgbPUJZMSVLFQp+5mot/7OWbJkicPgZE5kot78/XQzEzU7i71mz56tfv36KSUlxVpWq1YtrVy5UiVKlMhyv96OtG4nTZs2Tc8884z12s/PT+PHj3cYub/1tDJJKlOmjEvjFS1a1LSMOCkpSUeOHHHLWOnrp+8PAADgdmQvpfumu+66y7TX+OrVq+0G5TwtISFBPXr0UIsWLbRs2TKHgUlJOn/+vN59911VrVrVJhsoI9HR0br77rv14osvOnwODMPQL7/8ooYNG+qHH35w+XHY891336levXqaPXu202n1p06dUqNGjTRo0CCHgUnpRoBn06ZN6tKlizp27GgTrMoLWrZsabPX4aJFizwzmVukDzjcuqert0g/x927dys+Pt4zk8nEzdOIb3Vr0CKrFi5caLoeMGCA3bGAnOBKFqqU9zJRnR0rM5MnT1afPn1M7/H69etr7dq1t3VgUiI4aXXgwAGtWrXKelu2bJlmzZql119/XTVr1tSAAQOsS2qDgoI0efJktW7d2mF/t+5hIilLS2/Tp8ak7/OmCxcuZGusUqVKOTUOAOD2w8pJ+Kqbqdo3hYeH2xzkcOtKybS0NE2fPj3X5ueM8+fPq2XLlpo7d67NfaVLl1aDBg105513WleD3nT27Fm1atVKGzZsyHSMhIQEtWvXTr/99pvNfWXLllWjRo1UuXJla3ZPUlKS+vTpY1oFmRVLly5Vv379rJ+//fz8VKlSJTVq1EjlypWzm4J75MgRNWvWzJSed7NtxYoV1bBhQ5UrV86m3bJly3TvvffqypUr2ZqzJzz00EOm640bNyo5OdlDs7mhZs2apuuvvvrKdNCEN0g/x2vXruntt9/20Gwydmvm3k2RkZHZ6vPAgQM6d+6cqaxr167Z6hPIiCtZqJL7MlGdGSsrmagZ9ees//73v3riiSdMK5qbNm2q1atXZ3uldF5AcPL/+/LLL9W2bVvrrWPHjnrsscc0ZswY7dmzR9KN1JT7779fW7ZsUf/+/TPsL/1f3pxdFpxRG3t/zUtMTLRJQ3B1LGfGAQDcnghOwlfNmTNHiYmJ1utHHnlEwcHBpjq3rqSUvOvU7rS0ND366KPavn27taxo0aL66KOPdObMGZ04cULbtm3Tn3/+qYsXL2rDhg267777rHWvXr2qXr166dKlSxmOM2LECNNqF4vFoieeeEIHDx7UsWPHtGXLFv3zzz86e/asxo4dq/z58yslJcWpQwsy8vjjjystLU3h4eH65JNPdO7cOR08eFBbtmzR0aNHderUKfXu3dtaPyUlRb169dLx48etZQEBAXrttdd04sQJHTp0SFu3btXRo0d18OBBPfHEE6bxtm/friFDhmRrzp7QpEkT03VSUpJ2797todnc0KlTJ9P15cuX1bhxY02YMMFrDpYqV66cTYDys88+0yOPPKJdu3Z5aFb2pd8OoXTp0tle4bh+/XrTdUBAgO68885s9Qk44moWquR7magff/yxnnvuOeuWJtKN7U5WrFih8PBwl/vLi9hz0gXdu3fXCy+8YJM+YU/6AF9mJz3Zky9fvgz7dFTm6ljOjJPe9evXTWlDtx5tDwBAXlGggHTLXuNwk6AgyduyZtMHGtMHIiWpSpUqaty4sfXQkQMHDuj3339X06ZNc2WOGfnoo4+0Zs0a63WTJk20ePFiFStWzKaun5+fmjdvrpUrV+rFF1/U+PHjJd04BOCdd97RuHHj7I6xc+dO076bkvT111/rqaeesqlbuHBhvfrqq7r33nvVpk0bh18CnXXu3DmVKFFCa9assXugSvHixVW8eHHr9bhx47R582brdVBQkH788Ud16NDBpm2lSpX07bffqnHjxqbH8sMPP+jRRx/NU4d52EuLPHz4sNOBpu3bt2cpRbhYsWKqU6eO3fsefPBB3XHHHaYv6BcuXNDTTz+tl156Sa1bt1aLFi3UpEkT1a9f3+X9T3PKiBEjbN738+fP1/z581W9enW1adNGd911lxo3bqxKlSpl+zCprDh27JjNNgm3/pEhq/bt22e6vuOOO2y+E+YWd/wM3o6mTp2a7T/6OKN///42W55k5sCBA6Y/DCUnJ+vKlSv6+++/9eOPP1oXe0k3/m2eMGFChlmoUs5loh4+fNjUp729ID2difruu+/arNzu0KGDFixY4LH3pScQnHTBnDlzNGfOHN19992aPHmyKleu7LBu+s1eXVmCfFP6v97f+td9R+NkZSxnxklvzJgxGj16tEvjAAC8j6+vnExKIjjpiw4dOmRKaS5Tpoxatmxpt26fPn1MJyJPmzbN48HJq1ev6t///rf1OioqSkuXLs001dPPz0+ff/65tm3bZt0HcPLkyRo9erTdgwnGjx9vSi8bMGCA3cDkrRo3bqzPPvvMZmViVkydOtWpk55TU1NtAqwffPCB3cDkrZ588kn98ccf+vrrr61ln3zySZ4KTkZGRspisZhW25w5c8bp9i+//HKWxu3atavD/S39/f31ww8/qEWLFjaLHq5evaqffvpJP/30k6QbP5PVq1dXy5Ytdd9996ljx4659mX8scce04oVK+xu17B3717t3btX//nPfyRJERERatq0qVq1aqVOnTq5nPaZFWfPnlW3bt1s9pHt169ftvu+fPmy6dreHzVyizt+BpG7vvzyS4d/5LrJYrGoQ4cOGjNmTJYWe92umajHjx+3CUx269ZNP/zwQ5ZiSHkZad3/3+effy7DMKy3q1ev6sSJE1qyZIkGDRpk+iW5fv16NWrUSNu2bXPYX/rVi0lZ+OaT/heRvRWR9spcHcuZcdIbMWKEYmJirLcTJ064NCYAAICnpF8V0rt3b4eroh599FHrforSjdV19v44nJumT59uCi6MGjXK6T3o/P39NWLECOt1fHy8li9fblMvKSlJ33//vand+++/79QYjz/+uKpWrepUXUdatGih9u3bO1V3+fLlps+i5cqV04svvuhU2/fff1/58+e3Xm/YsCHL+4V5gp+fnwoWLGgq84YtmurWravffvtNd9xxR4b10tLStHv3bn355Zd65JFHFBUVpRdeeCHX9sCfPHmyXn/9dbt7mN7qypUrWrp0qYYPH66aNWvqrrvucktgLD4+Xjt27NB7772nOnXq2Bys0bZt20xXnDkjfXAys1OTgezq3r273njjDacCk9Ltn4l6k5FulUCRIkU0Y8YMnwtMSqycdChfvnwqXbq0SpcurU6dOum1115T9+7drb8goqOj1a1bN/399992/zEPCwszXWflQ2z6FYzp+3RUdu3aNZfeUM6Mk15wcLDNiksAQN7j6ysn4XsMw9CMGTNMZfZSum8qWrSo2rVrp6VLl0q68Rnwxx9/VM+ePd06z4zcnIt0Y6+4Rx991KX2rVu3lp+fn3VV5Pr1620ez86dO01fsFq1amVzWKMjFotFjz32mEaOHOnSvG7Vq1cvp+umP3ynX79+poByRiIjI9WtWzfNmjXLWrZu3TpVr17d6fE9LSwsTDExMdbrrCyKcIfatWvrr7/+0qRJk/Sf//zHlNrpSExMjP7zn/9o2rRp+vrrr136OciKm0H3vn376v3339f8+fOdyiLbvHmzHnzwQbVv316zZs1y+YCaadOmubyHbdWqVTV79myX2jiS/nT6rKxKQ+5q3769Vq5c6fZxnP133lWuZKFKt38mqiMXL15Ujx49tGjRIp8LUBKcdFLlypW1cuVK1a9f3/qX2VOnTumjjz6y+1fk9AG+hIQEl8dM38Ze0DBfvnzy9/c3LUVOSEhw6a9fzowDAABwO1izZo2OHTtmva5Tp45q1aqVYZvHHnvMFBCcOnWqx4KThmFo48aN1uuqVavarJzLTGhoqAoXLmzdZ8veSsH0GULNmjVzaQxX66eX0Smu6d2616Tk+p58rVu3NgUnN23apKefftqlPjwpfaDJlQUEa9asUatWrXJ4Rv8nMDBQgwcP1uDBg7Vjxw4tX75c69at0+bNmzM8jCk2Nla9e/fWxYsX9fzzz7ttfjdVq1ZNM2bM0Jdffqlly5Zp7dq12rBhg/bs2WOT8nmr5cuXq2nTpvr999+zfYJ2Rh555BF9/fXXOXZib/p9PrPyXTWnuPtn8HYRFRWlqKgoT0/Drs8//1yff/659ToxMVGXLl3Szp07tXDhQs2aNcsasLuZhbpy5Uo1bNjQYZ+ezkR1ZbFXVjJRbypVqpQaNGigxYsXW8uWLVum7t27a968edk+/CovITjpgiJFimj06NF6/PHHrWVTp061G5xMv2/HyZMnXR7v9OnTGfZ5U9GiRXX27FnTWOk3Zc3IqVOnnBoHAAAgr0uf0p3RqsmbunXrprCwMOtKwpUrV+rMmTMe+aJ47tw5U0rmnj17sn1QR/oUT8n286GrKwmzu/KwQoUKTte9NdgsyeVDMtKnGd56sMNN165dM+1TmpF8+fKpefPmLs0hq9LS0myCk9660ODOO+/UnXfeqVdffVWSdPToUW3ZskVr167VokWL7O6VOWzYMDVq1Eh33XWXqdxdr0eBAgXUo0cP9ejRQ9KNPTJ37typ9evXa8WKFVqzZo1pH1bpxmEg/fv3t+6jmRMsFosqVaqk1q1b68knn7R78FF2pA+k3rryFsiu7GahSrd/JupNAQEBmjt3rrp166Zly5ZZyxcvXqzevXtr9uzZTmcC5HW+8Shz0IMPPqhBgwZZ9wY4ffq0jh07pnLlypnqpd9fxd6HnIycP3/e9AYMCgpSxYoV7da94447TMHJ48ePq0mTJk6PlX5uzmw8DgC4Pfh6WrePZcx4jLc8z/Hx8VqwYIH12s/PT7179860Xf78+fXggw9a08FTU1M1Y8YMDR8+3G1zdSSj1WZZZS8wER0dbboODw93qc/s7mHnymrQK1euWP/fz8/P5RVsRYoUcdjfTWfPnlXbtm2d6q9cuXI6evSoS3PIqosXL9rsWeautMycVr58eZUvX149evTQ+PHjNW/ePL3yyium7yYpKSl66623bNJZc+v1yJ8/v5o2baqmTZtq+PDhOnHihEaNGqXJkyeb6i1ZskQbN250Ogjarl07vfLKK6Yyi8Wi/PnzKzw8XCVLlnTrPpDp3yO5tccnfJOrWaiSb2WiBgUFacGCBerSpYvp37qbKydnzpwpP7/b/7gYgpMuKlSokCIjI00fDM+ePWsTnEwf4Dt06JCSkpKc3jcgfXpNpUqVHEbMq1WrZtprx5n9XDIai+AkAPgOXw9OpltwhNvc3LlzTV8iqlataj2VNzPp/0g8bdo0jwQn0wcNc0L6lWCSbZpadvfgcpUrqWy37o156+E2zkq/3176lYjebPv27TZllSpV8sBMssfPz089evRQ69atdffdd5vek6tWrdLZs2dVokQJD87whjJlymjSpElq2LChnnnmGdN9M2fOdDo4GRUVpTZt2rhjik5J/31v3759Lq8WA1zhShaq5HuZqCEhIfrxxx/VsWNHrV271lo+e/ZsBQYGasqUKbd9gJLgZA6w9+GpRIkSKlGihPWH/Pr169q+fbuaNm3qVJ+37iUk3UiDcCT9fb/99ptTY0g3/hq5ZcsWp8cCAADIq9IfQLFv3z6nV1+lt2fPHm3dulWNGjXKiak5LX3wrUaNGho3bly2+kx/0qhku1LS1ROgY2NjszUnV9x6IMzVq1ddbp9+1Uv6/fi8WfrP8cHBwXnqMJ/0ChcurG+++Ub33HOPqXzDhg165JFHPDQrW0OGDNG8efO0evVqa9n69es9OCPX3H333abrlJQU7dixwyZ9Ht7jzJkz2r17t9vHKVmypGrUqOGWvp3NQpV8MxM1X758WrJkiTp06GDatmL69OkKCgrShAkTsr2NizcjOOmiuLg4m315ihcvbrdup06dNGnSJOv1ypUrnQ5Opk9d6Ny5s8O6nTp1Ml3/9ttvSkhIcOrUtY0bN5o+xFWtWlVVq1Z1ao4AgLzv3DlPzwDIHUeOHNG6detytM9p06blenAyfQqyYRhuWYGVPu0z/QqUzLhaPzsiIiKswcm0tDRduXJFERERTre/ePGiTX/plS9f3iZ92hvMnz/fdH333Xfn+f3J7r77bkVFRZn2oEy/MskbXo8ePXqYgpPp5+jNqlatqmLFipnSuRcvXkxw0ostX75cAwcOdPs4/fv3t9mbOac4m4Uq+W4mamhoqJYuXap27dpp06ZN1vKJEycqKChI//3vf7Pct7e7vdeFusHPP/9s+kVYtGhRh5uhd+nSxXQ9ZcoUp36JHjp0yPTmCAwMVMeOHR3WL1OmjOrVq2e9jo+P15w5czIdR5IpeCpJXbt2daodACBvO3pUuusuKd2iG+C2NW3atBwPZsyePTtLJ4hmR4kSJUwrHY8dO6bk5OQcHyf9ypk///zTpfY3Dz7IDem/2O7cudOl9unr2/ui7I3Wrl2rv/76y1T24IMPemg2OSv9a5CVFbHuVr58edO1N84xI+l/VqZMmaKUlBQPzQa+ytEWHjczUW+6mYnqrLyciVqgQAH98ssvNqeZf/nllxo6dGi2+vZmBCddkJiYqJEjR5rKHnjgAYe5/+3bt1fp0qWt10ePHtWUKVMyHWfUqFGmD88PP/xwppuQDxo0yHQ9duzYTE+02rt3r3744QfrtZ+fnwYMGJDp/AAAedvVq9J990mbN3t6JkDuMAxD06dPN5WtXbtWhmG4fGvdurW1j8uXL+foCb3OCAwMNO1rd/XqVW12w5u5cePGpuulS5fa3ZvSkcWLF+f0lBxKv9rr1tVszkhfPy+sHktOTraeen1TWFiYU6fP5wXpU+3Trxj2BnlhjhkZNmyYKUX07NmzmjBhggdnhNudK1mokm2GaPrs0ozkRCaqM9yViRoeHq4VK1aYFqFJ0rhx4zyy33Vu8Mng5PDhw7V161aX2ly+fFldunTRgQMHrGX+/v566aWXHLYJDg7WG2+8YSp7+eWXM1wmPGvWLM2cOdM0xujRozOd35NPPqmyZctarw8cOKCXXnrJ4QqB2NhY9evXz/TX/t69e7ttfwkAgPdYt046csTTswByz7p163Tklh/6kiVL2uy55qxevXqZrt2V/paRDh06mK7/85//5PgYJUuWVIMGDazXp0+f1qJFi5xqe/z48VwN2rZs2dJ0PXPmTKdXgF25ckULFy40laXf79AbvfzyyzardV5++WWXTjn3VrGxsabvXJJM33O8Rfrvk944x4xUrVrVJtPv1VdfzZGT5pcuXZrtPmA2YMCALP1BzdWbO3+nuZKFKpGJGhERoZUrV6p27dqm8o8++khvvfVWjo3jLXwyOLlixQo1btxYTZo00aeffqodO3bYTYcxDEP79u3Tu+++qzvuuEOrVq0y3f/SSy/Z/KCkN2jQINWsWdN6feXKFd19992aPn266UPT5cuX9dZbb6lv376m9k8//bRTkfegoCCNHTvWVPb111+rR48e+ueff0zlq1ev1t13361t27ZZy8LCwvTOO+9kOg4AIO/78ENPzwDIXekPwunZs2eWT7186KGHTHte/fLLL6Z923LDE088oUKFClmv582bp59//jnHx3nyySdN18OGDTPtFWZPWlqahgwZkmkGT05q166dKTB05MgRjR8/3qm2b731lmnVy913352t/cLcLSEhQY899pi++OILU3nNmjW9ZjXNzJkzbfbxdMX48eNNp8Xny5cvxwPGBw4c0LJly7Lc/sqVKzYZce3bt8/utHLd559/bvq3JD4+Xvfff3+WTka+2b5Pnz7697//nUMzxO3C1SxUiUxU6cYhYatWrbI56Oy99967/eI3hg+qW7euIcl0CwoKMipUqGDUq1fPaNKkiVGjRg2jQIECNvVu3vr372+kpqY6Nd6ePXuMyMhImz7CwsKMunXrGlWrVjUCAwNt7m/cuLFx9epVlx7bkCFDbPqxWCxG2bJljQYNGhhFihSxud/Pz8+YO3duVp5Kq5iYGEOSERMTk61+AADuV6SIYUjO3fKKxMREY8+ePUZiYqKnpwIvk5CQYPOZbvPmzdnqs3Pnzqb+PvnkE7v1+vfvb6p35MgRh30eOXLE5rNmRsaMGWOqX6BAAePHH3906XFs27bN6NGjh8P7ExMTjSpVqpjGqVevnnHo0CG79WNiYozevXtbP3/e2q5ly5YZzqVly5am+q765JNPTO2Dg4ONVatWZdhm0qRJNvNctGiRy2O7Iv3n8Ix+Jm4VHR1tfPzxx0bZsmVt+ihWrJixf/9+p/opV66cqe2aNWuy/mAcaNmypREWFma89tprxtGjR11qO2vWLJvvRX379s3xOa5Zs8aQZNx1113GokWLjOTkZKfbXr582bj77rtNcwwICMjwNRg5cqRL7++smDJlikvvuZsWLFhg8zNVtmxZ45dffnFp/IULFxoVKlTIdOzc+Bm8Kf2/KyNHjnTbWPbkxuue21555RVjy5YtLrW5dOmS0aZNG9Nz4e/vb+zatSvTtl999ZWpXUREhLF7926H9b/77jubcZz59/H69es2/74OHjzYSEtLs1s/JibGaNiwoal+nz59Mh3n5r89N2/lypXLtI1hGMaZM2eMqlWr2rxXx44d61R7Z+X052lX4kR5+yi3HJSUlGRK93GkYMGCGjt2rAYPHuz0Me7Vq1fX6tWr1bVrVx07dsxaHh8f73DD7jZt2mju3LmmDc+dMX78eIWEhOizzz6zlhmGoePHj9sccS9J+fPn15QpU/TII4+4NA4AIO/Kn9/TMwByz7x58xQXF2e9rlSpks1+iq569NFHTWnL06ZN07Bhw7LVp6uGDx+u9evXW9Mn4+Li1K1bN3Xr1k3PPfecmjdvruDgYFOba9euadeuXVq1apXmz5+vP/74Q5JMKz9uFRISom+//Vb33Xefdb/JP//8U7Vq1VL37t117733qlixYoqJidG2bds0a9YsnT17VtKN7J+vv/7aXQ/fxosvvqg5c+ZY99+8fv267r//fr3yyit67rnnTKmDhw8f1ocffqhvv/3WtLqmZ8+euX445MaNG3Xw4EFT2fXr1xUdHa3o6Gj9888/2rRpk/7880+7hy9VrlxZixcvzvIeZ9u3b8/yISjFihVTnTp17N4XHx+vsWPH6sMPP1SrVq300EMPqUWLFqpTp47NSqnExET9+uuv+vLLL222AwgLC7PJDstJmzZtUrdu3VSsWDH16tVLrVu3VrNmzVS4cGGbuocPH9bcuXP1ySef6MKFC6b7XnjhhRzZZ84THnzwQb377rumNNHjx4+rQ4cOatOmjfr27asOHTqoWLFiNm137dqln376ST/88IPN4UzOctfPINxjxYoV+uijj9S4cWP17NlT9913n2rWrGlzsI1hGNq/f7/mzp2rL774wmY1tTNZqNKNFY3jx4/X7t27Jf1fJupnn32m3r17W0/gvnz5sj777DN98MEHpvauZqL27t3bWvb111/r4sWL+uCDD1SlShVr+erVq/XSSy9p165d1jJ3Z6KWKFFCq1evVsuWLXXo0CFr+WuvvaagoKAMtxvMM3IkHJrH7Nmzx/jwww+NNm3aGAULFrS7MvLWm8ViMerUqWN89NFHxvnz57M8bmxsrDFixAgjIiLC4VhVqlQxvv32W4cRemetXr3a5i96t96CgoKMxx57zDh8+HC2xrmJlZMAkHdUr87KSfiO++67z/QZ6PXXX892n/Hx8Ub+/PlN/f7xxx829dy5ctIwbqyma9Wqld3PesHBwUbVqlWNJk2aGHXr1jXKlStn+Pv7262bmWnTphl+fn6Zfma+eWvSpIlx9epVU1nr1q0zHCO7KycNwzAOHTpkd2Whn5+fUalSJaNRo0ZG+fLl7c65fv36xuXLl7M0riucfQ4zu/n7+xvPPvusy5+9069ay86ta9eudsdI/1reesuXL59Rvnx5o2HDhkbDhg2NSpUqOfy5zJ8/v7F69eoceNZtpV+9lP5WrFgxo0aNGsZdd91l1K5dO8Pvb126dDGSkpIyHM+bV07e9NVXXxlBQUF2H6PFYjGKFi1q1KhRw/o+CgkJcficPProow7HyY2fwZtYOZnzcjsL1TBur0zUrK6cvOnYsWN2f4+NHz/epX4cYeVkLqtevbqqV6+u4cOHKy0tTf/8848OHjyo48ePKzY2VsnJySpQoIDCw8NVvnx51a9fP0c2ly5QoIA++OADjR49Wps3b9bff/+tS5cuyd/fX1FRUapfv75Tfz1wxr333qt7771XJ0+e1G+//abjx4/r2rVrKlCggKpUqaIWLVrcFhtmAwBcx8pJ+Irjx49rzZo1prL0B9pkRWhoqB544AHThvnTpk2zOVXT3cLDw7Vy5Uq9+uqr+uKLL0yrj65fv25zqIg9ZcqUybROv379VLhwYQ0ePDjTvei6d++uSZMmKTU11Wau7laxYkVt3LhRXbt2ta4KlW7sg3no0CHTapNb3X///frhhx9UoEABt88xu6KiovToo4/q+eefV4UKFTw9HbsaNmyozZs3292vLTExUUePHs300JV69eppwoQJatiwoVvmWLx4cVWpUsVmb/6bzp8/n+leskFBQRo+fLjefvttm1VjedHgwYPVvHlzPfvss1q/fr3pPsMwdOHCBZsVo+mVLVtW7777rs05Cp6S/mcwIiLCQzO5vbkzC1UiE/VWZcuWta6gPHHihLX8+eefV1BQkM1e0XlKjoRD4fNYOQkAecc997ByEr7h3XffNa0sqFmzZo71vXDhQlPfRYsWtVk95e6Vk7c6dOiQ8dRTTxnFihXLdLVR+fLljaeeespYsWKFS6tX4uLijAkTJhjt2rUzypQpYwQFBRn58+c3qlWrZjzxxBPGhg0brHWPHTtmGnPgwIEZ9p0TKydvSk1NNSZOnGjUrFnT4XNgsViMJk2auLxPZ3Zl9tro/69CKlSokFGpUiWjRYsWxqBBg4zPP//c2L59u0uvlz25tWotLi7OmDNnjvHEE0/Y3SfN3i04ONh44IEHjDlz5hgpKSnZepzO2rVrl/H+++8bbdu2NcLCwpyaZ4UKFYzXX3/d4d6r9uSFlZO3WrdundGvX78MV4zevBUqVMh45JFHjCVLljj1uuXWz+DVq1eN4OBga92IiAgjOjo6y89JVtyOKyc9lYVqGLdHJmp2V07e9M8//xglS5a0ea6nTJmSpf5u8uTKSYthOHEWO5CJ2NhYhYeHKyYmhhWZAODlOnaUnD2kNK98Srh27ZqOHDmiChUqKCQkxNPTATzGMAzt3r1bu3fv1sWLFxUdHa3g4GCFh4erQoUKqlGjhkqWLOn2eSxZskSdO3e2Xo8aNcrmpNbccPz4cW3evFnnzp1TbGysIiIiFBUVpWbNmtndQw/uER0drf379+vgwYO6dOmS4uLi5O/vr4IFCyoyMlI1atRQ9erVPboCMTU1VUeOHNGBAwd08uRJxcTE6Pr16woNDVXBggVVtmxZ1a1b16d+bgzD0L59+7Rnzx6dOnVKcXFx8vPzU0REhIoUKaLatWuratWqLq2Cyy2rVq1S27Ztrdfvvvuu3nzzTQ/O6PaTm1mot0pOTnZ7JupNvpaJmtOfp12JE/lkWjcAAL6MtG7g9mWxWFSrVi3VqlXLo/P43//+Z7p2V3puZsqWLauyZct6ZGz8n0KFCqlJkyZq0qSJp6fikL+/vypXrqzKlSt7eipew2KxWLdEy2tWr15t/f/ChQvrxRdf9OBsbk9+fn664447dMcdd+TquIGBgWrRooVatGjh9rFKly6tHj16uH0cSH6ZVwEAALeT0FBPzwDA7Sw2NlbTpk2zXgcEBHh1UArA7efW4OTLL7+cJ/aUBXwZwUkAAHwMKycBuMKVXaAMw9CQIUN05coVa1nnzp1VpEgRd0wNAGzExcVp+/btkqSiRYvq+eef9/CMAGSG4CQAAD6GlZMAXNG2bVtNnDhRCQkJGdY7e/asevTooVmzZlnLLBaLhg4d6uYZAsD/WbdunVJSUiRJw4cPVygffACvx56TAAD4GM6LAeCKgwcP6sknn9SLL76odu3aqXHjxqpYsaLCw8OVkJCg06dPa/369frpp5907do1U9tnnnlG99xzj4dmDsAXderUyaUV3wA8j+AkAAAAgExdvXpVixYt0qJFi5yq/8gjj+jjjz9276QAAECeR1o3AAAAAIdKly7tUv3ChQvro48+0pw5cxTCUm0AAJAJVk4CAOBjyHQC4IoNGzZox44dWrVqlTZv3qx//vlHJ0+eVHx8vNLS0hQREaEiRYqoYcOGuvfee/XII48oLCzM09MGAAB5BMFJAAB8DMFJAK668847deedd3p6GgAA4DZEWjcAAAAAAAAAjyA4CQCAj2HlJAAAAABvQXASAAAfQ3ASAAAAgLcgOAkAAAAAAADAIwhOAgDgY1g5CQAAAMBbEJwEAAAAAAAA4BEEJwEA8DG388pJ43Z+cAAAAICbePJzNMFJAAB8zO0Yv/Pzu/GRJi0tzcMzAQAAAPKe1NRUSZK/v3+uj01wEgAA5HkBAQGSpKSkJA/PBAAAAMh7kpKSZLFYCE4CAAD3u11XToaGhiouLs7TUwEAAADynJiYGIWFhVkzknITwUkAAHzM7RiclKQCBQooISFBKSkpnp4KAAAAkGdcu3ZN165dU3h4uEfGJzgJAABuCwUKFJDFYtGZM2fYexIAAABwQnJysk6dOqWAgACFhYV5ZA4EJwEA8DG368rJgIAAlSlTRgkJCTp16hQrKAEAAAAHDMPQ1atXdfToURmGoXLlyslisXhkLgEeGRUAAHjM7RqclKTQ0FCVKVNGJ06c0D///KPQ0FAVKFBAQUFB8vPz89gHLgAAAMCTDMNQWlqaUlJSlJCQoPj4eKWmpiokJERlypSxHjDpCQQnAQCAQ4Yh5bV4XmhoqCpXrqy4uDjFxcXp7Nmznp4SAAAA4DWCg4NVqFAhhYWFKV++fB7/Az7BSQAAfIwrKyfzYnBSupHiHRERoYiICOtfiNmHEgAAAL7Mz89P/v7+8vf39/RUTAhOAgDgY1wNTuZ1fn5+CgoK8vQ0AAAAANjBgTgAAMCh2yE4CQAAAMB7EZwEAMDHuBJwJBMaAAAAgDsRnAQAAA6xchIAAACAOxGcBADAx7ByEgAAAIC3IDgJAICP8bUDcQAAAAB4L4KTAADAIYKTAAAAANyJ4CQAAD6GtG4AAAAA3oLgJAAAPoa0bgAAAADeguAkAABwiOAkAAAAAHciOAkAgI8hrRsAAACAtyA4CQCAjyGtGwAAAIC3IDgJAAAcIjgJAAAAwJ0ITgIA4GNI6wYAAADgLQhOAgDgY0jrBgAAAOAtCE4CAACHWDkJAAAAwJ0ITgIA4GNYOQkAAADAWxCcBAAADhGcBAAAAOBOBCcBAPAxHIgDAAAAwFsQnAQAwMeQ1g0AAADAWxCcBAAADhGcBAAAAOBOBCcBAPAxpHUDAAAA8BYEJwEA8DGkdQMAAADwFgQnAQCAQ6ycBAAAAOBOBCcBAPAxrJwEAAAA4C0ITgIA4GMITgIAAADwFgQnAQCAQ6R1AwAAAHAngpMAAPgYVk4CAAAA8BYEJwEA8DEEJwEAAAB4C4KTAADAIdK6AQAAALhTgKcn4A0Mw9DRo0f1119/6eTJk4qOjlZwcLAiIiJUpUoVNWrUSCEhIZ6eZpbt3r1b27dv15kzZ5SamqrChQurVq1aatKkiQIC+BEAAF/DykkAAAAA3sJnI1NXrlzRokWL9Msvv2j16tW6ePGiw7qBgYHq1KmThg4dqpYtW7o0ztGjR1WhQoVszdXIwjdDwzA0ZcoUffjhhzpw4IDdOoULF9aQIUP02muvKTQ0NFtzBADcnghOAgAAAHAnn0zrfvbZZ1WiRAk9/vjjmjNnToaBSUlKTk7WokWL1KpVK/Xv31+xsbG5NNOsiY6OVvv27TVo0CCHgUlJunTpkt577z3VqVNHu3fvzsUZAgA8yZWAI2ndAAAAANzJJ4OTmzdvVlJSkk25v7+/SpcurQYNGqhOnToKDw+3qTN9+nS1bdtW8fHxuTFVlyUmJqp9+/ZauXKlqTwoKEhVq1ZV7dq1bVZJHj58WPfee68OHjyYm1MFAHgIad0AAAAAvIXPpnXfVKhQIfXu3VudOnXS3XffrQIFCljvS01N1fr16/X2229r/fr11vItW7ZowIABmjdvnsvjtWvXTq+88kqOzN2eYcOGacuWLdZrPz8/vfHGG3rppZcUEREhSUpKStKsWbM0bNgwXblyRZJ04cIF9ejRQ1u3bpW/v7/b5gcAyFtYOQkAAADAnXw2OFm+fHm9+eab6t27t/Lly2e3jr+/v1q1aqU1a9bomWee0YQJE6z3zZ8/X2vWrNG9997r0rhRUVFq06ZNtubuyL59+/Ttt9+aymbOnKlevXqZyoKCgjRgwAA1atRILVq0UHR0tCTpzz//1PTp0zVw4EC3zA8A4B1YOQkAAADAW/hkWvfo0aO1f/9+DRo0yGFg8lb+/v768ssv1bBhQ1P5xIkT3TXFLBk5cqRSU1Ot13379rUJTN6qZs2a+vjjj01lo0ePVnJystvmCADwPIKTAAAAALyFTwYnO3XqpKCgIJfa+Pv7a/jw4aay5cuX5+S0suXKlStasGCB9dpisWjUqFGZths4cKDKlStnvT527JhWrVrljikCAPIg0roBAAAAuJNPBiez6u677zZdX7p0SVevXvXQbMx+/vlnpaSkWK9btWqlihUrZtrOz8/PJo170aJFOT09AIAXYeUkAAAAAG9BcNIFNw+UuVVMTIwHZmLr559/Nl23a9fO6bZt27Y1XS9ZsiRH5gQA8E4EJwEAAAB4C4KTLjh16pRNWeHChT0wE1s7duwwXTdr1szptg0aNFBwcLD1+vTp07pw4UJOTQ0AkIeR1g0AAADAnXz2tO6sWL9+vem6XLlyLu9dedOJEyd09uxZXbt2TZGRkSpWrJiKFi2apb6Sk5N18OBBU1mNGjWcbh8cHKxKlSppz5491rK9e/dmeT4AAO/GykkAAAAA3oLgpAsmT55suu7YsaPLfaxYsUIlS5bUmTNnbO4rX768WrVqpaeeekpNmzZ1us/Dhw+b9pvMly+fihQp4tK8ypQpYwpO7t+/X/fcc49LfQAA8gZXAo6snAQAAADgTqR1O2np0qVat26dqWzAgAEu93PmzBm7gUlJOnr0qKZOnapmzZqpdevWOn78uFN9nj9/3nRdqlQpl+eVvk36PgEAvomVkwAAAADcieCkEy5fvqynn37aVNatWzc1btzYbWOuXr1a9erVswmI2hMfH2+6Dg0NdXm89G3S95ne9evXFRsba7oBAPIG0roBAAAAeAuCk5lIS0tTnz59dPLkSWtZeHi4vvjiC5f6KV26tIYMGaK5c+dq7969io6OVnJysi5evKitW7fq3//+typWrGhqc/nyZXXt2lX79u3LsO/0gcSQkBCX5ibdSAXPqM/0xowZo/DwcOutTJkyLo8JAPB+pHUDAAAAcCeCk5l45ZVXtGzZMlPZN99843QwLjw8XIsXL9axY8f05Zdf6pFHHlG1atUUHh6ugIAAFS5cWA0bNtQrr7yiAwcOaOTIkfLz+7+XJTo6Wn369JGRwdKVa9euma6zckjPrad1S1JiYmKG9UeMGKGYmBjr7cSJEy6PCQDwDFZOAgAAAPAWBCcz8MUXX+jTTz81lQ0fPlw9e/Z0uo+IiAh17tzZFHB0xN/fX6NGjbIZc/v27VqwYIHDdulXSiYlJTk9v5uuX7+eYZ/pBQcHq2DBgqYbACBvIDgJAAAAwFsQnHRg1qxZGjp0qKlswIABGjt2rNvHfvHFF9WyZUtT2YwZMxzWDwsLM12nX0npjPQrJdP3CQDwTaR1AwAAAHAngpN2LFmyRP379zelUj/00EOaOHGiLBZLrszhX//6l+l69erVSklJsVs3fSAxISHB5fHStyE4CQC3L1ZOAgAAAPAWBCfTWbNmjbp3724KBLZt21azZ8+Wv79/rs3jvvvuMwVC4+LidObMGbt1ixUrZro+deqUy+Olb5O+TwDA7YPgJAAAAABvQXDyFps3b1aXLl1MadHNmjXTwoULs3TITHaEhoYqIiLCVHbhwgW7dStWrKiAgADrdWJiosO6jhw/ftx0Xa1aNZfaAwBuT6R1AwAAAHAngpP/365du3T//fcrPj7eWlavXj0tXbpUoaGhHplTYGCg6To5OdlhvUqVKpnK9uzZ4/Q4169f1+HDh01lBCcB4PbFykkAAAAA3oLgpKT9+/erbdu2unLlirWsevXqWr58ucLDwz0yp5SUFF26dMlUVrRoUYf177zzTtP1b7/95vRY27dvN53WHRUVRVo3ANzGXAk4snISAAAAgDv5fHDy2LFjatOmjc6fP28tq1ChglauXJlhMNDdNm3aZNr3MiAgQCVKlHBY/4EHHjBdr1y50umx0tft3Lmz020BALc3Vk4CAAAAcCefDk6eOXNGrVu31smTJ61lpUqV0v/+9z+VKlXKgzOTJk2aZLpu2rSp8ufP77B+x44dTftOrl271iZV2x7DMDR16lRTWdeuXV2bLAAgTyGtGwAAAIC38Nng5OXLl9W2bVsdOnTIWla0aFGtXLlSFSpU8ODMbgQWZ8yYYSrr1q1bhm0iIyNNdQzD0KhRozIda/LkyTp69Kj1uly5cmrTpo0LswUA5DWkdQMAAADwFj4ZnIyLi1OHDh20e/dua1mhQoW0YsUKVa9ePcfGWblypaZMmWJKz87M6tWr9dBDDyk1NdVaFhUVpcGDB2fadvTo0fLz+7+XdMaMGZo9e7bD+nv27NHLL79sKnvrrbdy/WRyAID3YuUkAAAAAHcKyLzK7adLly7aunWrqWzYsGG6ePGiVq1a5VJfDRo0UEREhN37Tp06pccff1xvvfWWunfvri5duqh+/fo2h+ykpqZq27Zt+vLLLzVz5kyl3bJMxc/PT//9738zTOm+qUaNGnriiSc0YcIEa1mfPn20d+9evfTSS9Z5Jicn67vvvtOwYcMUHR1trVunTh3179/flYcPAMiDSOsGAAAA4C0shuF7XzssFkuO9bVmzRq1atXK7n1Tp07VwIEDbcpLlSqlyMhIhYaGKjY2VsePH1d8fLzdeY4bN07PP/+80/O5evWqWrZsqW3btpnKg4KCVKFCBQUHB+vw4cM24xUpUkQbN25U1apVnR7rVrGxsQoPD1dMTIwKFiyYpT4AALmjY0dp2TLn6s6dKz3yiHvnAwAAAOD24kqcyCdXTnraqVOndOrUqQzrREVFadq0aWrbtq1LfefPn1/Lly9X9+7dtXr1amt5UlKS9u/fb7dN+fLltXjx4iwHJgEAeQsrJwEAAAB4C5/cczK33HfffRo9erRatWqlAgUKZFrfz89P9evX19dff62DBw+6HJi8KTIyUitXrtSECRNUuXLlDOu9/vrr+uuvv1S7du0sjQUAyHs4EAcAAACAt/DJtG5PMAxDhw4d0sGDB3XixAlFR0fr2rVrCg0NVUREhMqUKaPGjRu7JSX6r7/+0h9//KEzZ84oNTVVhQsXVq1atdSkSRMFBgbmyBikdQNA3tGhg7R8uXN1Z8+WHn3UvfMBAAAAcHshrdsLWSwWVa5cOcOVjO5Su3ZtVkYCAKxI6wYAAADgLUjrBgDAx5DWDQAAAMBbEJwEAAAOsXISAAAAgDsRnAQAwMeQ1g0AAADAWxCcBADAx5DWDQAAAMBbEJwEAAAOsXISAAAAgDsRnAQAwMeQ1g0AAADAWxCcBADAx5DWDQAAAMBbEJwEAAAOsXISAAAAgDsRnAQAwMewchIAAACAtyA4CQAAHGLlJAAAAAB3IjgJAICP4UAcAAAAAN6C4CQAAD6GtG4AAAAA3oLgJAAAcIiVkwAAAADcieAkAAA+hrRuAAAAAN6C4CQAAD6GtG4AAAAA3oLgJAAAcIiVkwAAAADcieAkAAA+hpWTAAAAALwFwUkAAHwMe04CAAAA8BYEJwEAgEMEJwEAAAC4E8FJAAB8DGndAAAAALwFwUkAAHwMad0AAAAAvAXBSQAA4BDBSQAAAADuRHASAAAfQ1o3AAAAAG9BcBIAADjEykkAAAAA7kRwEgAAH8OekwAAAAC8BcFJAAB8DGndAAAAALwFwUkAAOAQKycBAAAAuBPBSQAAfAwrJwEAAAB4C4KTAAD4GPacBAAAAOAtCE4CAACHCE4CAAAAcCeCkwAA+BjSugEAAAB4C4KTAAD4GNK6AQAAAHgLgpMAAMAhgpMAAAAA3IngJAAAPoa0bgAAAADeguAkAAA+hrRuAAAAAN6C4CQAAHCIlZMAAAAA3IngJAAAPoaVkwAAAAC8BcFJAADgEMFJAAAAAO5EcBIAAB/DgTgAAAAAvAXBSQAAfAxp3QAAAAC8BcFJAADgEMFJAAAAAO5EcBIAAB9DWjcAAAAAb0FwEgAAH0NaNwAAAABvQXASAAA4RHASAAAAgDsRnAQAwMeQ1g0AAADAWxCcBADAx5DWDQAAAMBbEJwEAAAOsXISAAAAgDsRnAQAwMewchIAAACAtyA4CQCAjyE4CQAAAMBbEJwEAAAOkdYNAAAAwJ0ITgIA4GNYOQkAAADAWxCcBADAxxCcBAAAAOAtCE4CAACHSOsGAAAA4E4EJwEA8DGsnAQAAADgLQhOAgAAh1g5CQAAAMCdAjw9AW9gGIaOHj2qv/76SydPnlR0dLSCg4MVERGhKlWqqFGjRgoJCcnRMePi4rRx40YdOHBAsbGxypcvn8qVK6dmzZqpZMmSOTrW7t27tX37dp05c0apqakqXLiwatWqpSZNmigggB8BAPA1rJwEAAAA4C18NjJ15coVLVq0SL/88otWr16tixcvOqwbGBioTp06aejQoWrZsmW2xj1y5IjefvttzZkzR0lJSTb3WywWtWzZUqNHj9Y999yT5XEMw9CUKVP04Ycf6sCBA3brFC5cWEOGDNFrr72m0NDQLI8FAMhbCE4CAAAA8BYWw/C9rx3PPvusJk6caDc4mJl+/frpP//5jwoWLOhy2zlz5mjgwIG6evVqpnUtFouGDx+uMWPGyGKxuDROdHS0evTooZUrVzpVv2LFilq8eLFq1qzp0ji3io2NVXh4uGJiYrL03AAAck/FitKRI87V7dFD+uEH984HAAAAwO3FlTiRT+45uXnzZruBSX9/f5UuXVoNGjRQnTp1FB4eblNn+vTpatu2reLj410ac+7cuerVq5dNYLJo0aKqX7++SpcubQpCGoahDz/8UMOGDXNpnMTERLVv394mMBkUFKSqVauqdu3aNqskDx8+rHvvvVcHDx50aSwAQN7EykkAAAAA3sIng5O3KlSokJ555hn9/PPPunLlik6cOKFt27Zp586dunTpktasWaO7777b1GbLli0aMGCA02McOnRIAwcOVNotpwrUrVtXq1ev1vnz57V9+3adOHFCe/fu1UMPPWRq+/nnn2vBggVOjzVs2DBt2bLFeu3n56e33npLZ8+e1f79+7Vr1y5dvnxZU6ZMUUREhLXehQsX1KNHD6Wmpjo9FgAgbyI4CQAAAMBb+GRad8OGDXXp0iW9+eab6t27t/Lly5dh/dTUVD3zzDOaMGGCqXz16tW69957Mx2vd+/emj17tvW6UaNGWrVqld1lrYZhaPDgwaaxKlWqpH379mV6eM2+fftUq1YtU4Bx1qxZ6tWrl936u3fvVosWLRQdHW0tmzx5sgYOHJjpY0qPtG4AyDvKl5eOHXOu7kMPSfPnu3U6AAAAAG4zpHVnYvTo0dq/f78GDRqUaWBSupHu/eWXX6phw4am8okTJ2badvfu3frhls26goKCNG3aNIcvjMVi0bhx41SlShVr2aFDhzRlypRMxxo5cqQpMNm3b1+HgUlJqlmzpj7++GNT2ejRo5WcnJzpWACAvIuVkwAAAAC8hU8GJzt16qSgoCCX2vj7+2v48OGmsuXLl2fabvLkyaZ07kcffVTVq1fPsE1ISIhee+01U1lmgdArV66Y0r8tFotGjRqV6fwGDhyocuXKWa+PHTumVatWZdoOAJB3EZwEAAAA4C18MjiZVen3nrx06VKmJ28vXrzYdD1o0CCnxurZs6fp4JqtW7fq9OnTDuv//PPPSklJsV63atVKFStWzHQcPz8/mzTuRYsWOTVHAMDt75a/rwEAAABAjiM46YJbD5C5KSYmxmH9/fv3m07ADg0NVbNmzZwaK31dwzD0888/O6yf/r527do5NY4ktW3b1nS9ZMkSp9sCAPIeVk4CAAAA8BYEJ11w6tQpm7LChQs7rL9jxw7TdePGjTM91OZWzZs3z7C/jO5zNggqSQ0aNFBwcLD1+vTp07pw4YLT7QEAeQvBSQAAAADeguCkC9avX2+6LleuXIZ7V+7du9d0XaNGDZfGS18/fX83JScnm1ZoujpWcHCwKlWq5NRYAADfQnASAAAAgDsRnHTB5MmTTdcdO3bMsP7+/ftN12XKlHFpvPT10/d30+HDh037TebLl09FihRxy1gAgLyPlZMAAAAAvAXBSSctXbpU69atM5UNGDAgwzbnz583XZcuXdqlMUuVKmW6dpRqnX6c9O2yMlb6PgEAvongJAAAAAB3cn4DRB92+fJlPf3006aybt26qXHjxhm2i4+PN13fevq2M9LXT05O1vXr1037Q+bEOPbapO8zvevXr+v69evW69jYWJfHBAB4BisnAQAAAHgLVk5mIi0tTX369NHJkyetZeHh4friiy8ybZs+wBcSEuLS2Pny5cu0z5wYx95YmQUnx4wZo/DwcOvN1ZR1AIDnEHAEAAAA4C0ITmbilVde0bJly0xl33zzjVPBuGvXrpmuMzo8x570KyQlKTExMcfHsTeWvXFuNWLECMXExFhvJ06ccHlMAID3I5AJAAAAwJ1I687AF198oU8//dRUNnz4cPXs2dOp9ulXMCYlJbk0/q1p0476zIlx7I2V2erL4OBgu8FTAID3I60bAAAAgLdg5aQDs2bN0tChQ01lAwYM0NixY53uIywszHSdfoVjZuytXkzfZ06MY28se+MAAG4PBCcBAAAAeAuCk3YsWbJE/fv3l3HLN7KHHnpIEydOlMVicbqf9AG+hIQEl+aRvn5AQIDdFY3ZHcdeG4KTAACJ4CQAAAAA9yI4mc6aNWvUvXt3paSkWMvatm2r2bNny9/f36W+ihUrZrq+9VAdZ5w6dcp0XbRoUafGSd8uK2Ol7xMAcPtg5SQAAAAAb0Fw8habN29Wly5dTGnRzZo108KFC7N0yMwdd9xhuj5+/LhL7dPXr1atmt16FStWVEDA/20fmpiYqAsXLrhlLABA3kdwEgAAAIC3IDj5/+3atUv333+/4uPjrWX16tXT0qVLFRoamqU+0wf49uzZ41L7vXv3ZtjfTYGBgapUqVKWx7p+/boOHz7s1FgAAN9CcBIAAACAOxGclLR//361bdtWV65csZZVr15dy5cvV3h4eJb7vfPOO03XW7duNaWLZ2bjxo0Z9pfRfb/99pvT42zfvt10WndUVBRp3QBwGyPgCAAAAMBb+Hxw8tixY2rTpo3Onz9vLatQoYJWrlzpcI9HZ1WrVs20ojEhIcHpoGFCQoJ+//1367XFYtEDDzzgsH76+1auXOn0PNPX7dy5s9NtAQB5D2ndAAAAALyFTwcnz5w5o9atW5sOqilVqpT+97//qVSpUjkyRpcuXUzXkyZNcqrdDz/8YEoxb9iwoUqWLOmwfseOHU37Tq5du9YmVdsewzA0depUU1nXrl2dmiMA4PZHcBIAAACAO/lscPLy5ctq27atDh06ZC0rWrSoVq5cqQoVKuTYOI8//rgsFov1+vvvv7fZSzK9a9euaezYsaayQYMGZdgmMjJS3bp1s14bhqFRo0ZlOr/Jkyfr6NGj1uty5cqpTZs2mbYDAORdrJwEAAAA4C18MjgZFxenDh06aPfu3dayQoUKacWKFapevXqOjlWrVi316NHDep2UlKT+/fsrNjbWbn3DMDR06FD9888/1rKKFSvq8ccfz3Ss0aNHy8/v/17SGTNmaPbs2Q7r79mzRy+//LKp7K233srSyeQAgNsTwUkAAAAA7mQxDN/72nHvvfdq7dq1prJ33nlHTZs2dbmvBg0aKCIiIsM6Bw8eVN26dXX16lVrWd26dfX555+rVatW1rIDBw5oxIgRWrBggan9nDlz1L17d6fm8/TTT2vChAnWaz8/P73xxht66aWXrPNMTk7Wd999p2HDhpkOAapTp462b99uSg93VmxsrMLDwxUTE6OCBQu63B4AkHsiIqToaOfqtmghrV/v1ukAAAAAuM24EifyyeDkrWnW2bVmzRpTgNGR77//Xr1791b6p7to0aIqW7aszp8/r5MnT9rc//zzz+uLL75wej5Xr15Vy5YttW3bNlN5UFCQKlSooODgYB0+fNi0n6UkFSlSRBs3blTVqlWdHutWBCcBIO8oVEiKiXGubvPm0oYNbp0OAAAAgNuMK3Ei15fIIUseffRRGYahQYMGKTEx0Vp+4cIFXbhwwW6bl19+Wf/+979dGid//vxavny5unfvrtWrV1vLk5KStH//frttypcvr8WLF2c5MAkAAAAAAABkhU/uOekpvXr10t9//63evXsrMDDQYb177rlHa9eu1UcffZSlVZ6RkZFauXKlJkyYoMqVK2dY7/XXX9dff/2l2rVruzwOACBv4kAcAAAAAN7CJ9O6vUFsbKw2bNigf/75R3FxcQoJCVHZsmXVvHlzlSpVKkfH+uuvv/THH3/ozJkzSk1NVeHChVWrVi01adIkwyCpK0jrBoC8o2BBKS7OubpNm0q//ebe+QAAAAC4vZDWnQcULFhQHTt2zJWxateuzcpIAECW8CdMAAAAAO5EWjcAAD6GtG4AAAAA3oLgJAAAPobgJAAAAABvQXASAAA4RHASAAAAgDsRnAQAwMewchIAAACAtyA4CQCAjyHgCAAAAMBbEJwEAAAOEcgEAAAA4E4EJwEA8DGkdQMAAADwFgQnAQCAQwQnAQAAALgTwUkAAHwMKycBAAAAeAuCkwAA+BiCkwAAAAC8BcFJAADgEMFJAAAAAO5EcBIAAB/DykkAAAAA3oLgJAAAPoaAIwAAAABvQXASAAA4RCATAAAAgDsRnAQAwMeQ1g0AAADAWxCcBADAxxCcBAAAAOAtCE4CAACHCE4CAAAAcCeCkwAA+BhWTgIAAADwFgQnAQDwMQQnAQAAAHgLgpMAAMAhgpMAAAAA3IngJAAAAAAAAACPIDgJAAAcYuUkAAAAAHciOAkAgA9xNdhIcBIAAACAOxGcBADAhxCcBAAAAOBNCE4CAACHCE4CAAAAcCeCkwAA+BBWTgIAAADwJgQnAQDwIQQnAQAAAHiTgNwY5MqVKzp69KhOnDihmJgYJSQkSJJCQ0MVHh6usmXLqnz58ipUqFBuTAcAADiJ4CQAAAAAd3JLcHLfvn365ZdftG7dOm3btk2nTp1yql2pUqXUsGFD3XPPPWrfvr2qV6/ujukBAOCzCDYCAAAA8CYWw8iZryn79+/Xd999p1mzZunIkSPWcle7t1gs1v8vX768evfurccee0zVqlXLiWnCTWJjYxUeHq6YmBgVLFjQ09MBADiQlCQFBztfv2xZ6dgx980HAAAAwO3HlThRtoOTixcv1rhx47R27VpJ/xeMvDXIeGu5w4k4qH+zvFWrVnrxxRfVuXNnm7rwPIKTAJA3uBqcLFNGOn7cffMBAAAAcPtxJU6U5bTuOXPmaPTo0dq3b5+kG8FEi8Uii8UiwzCswcWwsDBVrVpVJUuWVFRUlMLCwpQ/f34ZhqHExETFx8fr9OnTOn36tA4cOGDdj1L6v8CkYRhau3at1q5dq6pVq2r06NHq0aNHVqcOAIDP4kAcAAAAAN7E5ZWTa9eu1dChQ/XXX39Jsl0RWaNGDd17771q2bKlGjRooAoVKrg0ocOHD2v79u369ddftXbtWu3Zs8d20haLateurc8//1ytWrVyqX+4BysnASBvuHZNypfP+fqlSkknT7pvPgAAAABuP25L637kkUe0cOFCSeagZO3atdW3b189+OCDqlSpUhanbd+hQ4e0YMECzZw50xoQvbk602Kx6KGHHtLcuXNzdEy4juAkAOQNrgYnS5aUnDzXDgAAAAAkuTE46efnZw0MBgUFqXfv3ho8eLAaN26c7Uk7Y+vWrfrqq680a9YsJSUlSboRqExNTc2V8eEYwUkAyBsSE6X8+Z2vHxUlnT7tvvkAAAAAuP24Eifyc7XzkJAQ/etf/9Lhw4c1efLkXAtMSlKjRo00efJkHTlyRP/617+Uz5WlHwAAwGXsOQkAAADAnVwKTj799NM6ePCgPvroI5UsWdJdc8pUVFSUPvroIx08eFBPPfWUx+YBAEBeQ7ARAAAAgDdx+UAcwB7SugEgb0hIkMLCnK9fvLh09qz75gMAAADg9uPWtG4AAOA7+BMmAAAAAHciOAkAgA9xNdhIcBIAAACAOwXkdIfr1q3T559/br0eNmyYWrRokdPDAACALCA4CQAAAMCb5HhwcuvWrVq0aJEsFosCAwM1derUnB4CAADkEoKTAAAAANwpx9O6U1NTJUmGYahs2bIcjgIAgBdh5SQAAAAAb5LjwcmoqChJksViUURERE53DwAAsoFgIwAAAABvkuPBydKlS1v//8KFCzndPQAAyEUEMwEAAAC4U44HJ5s3b65ChQrJMAwdO3ZMZ86cyekhAABAFpHWDQAAAMCb5HhwMigoSN27d7def/PNNzk9BAAAyCKCkwAAAAC8SY4HJyXp7bffVnh4uCTpo48+0u+//+6OYQAAgJsRnAQAAADgTm4JTpYqVUpz585Vvnz5lJiYqA4dOmjixInuGAoAALiAlZMAAAAAvInFMHL+a8fx48clSX/88YeGDBmic+fOyWKxqGzZsurZs6caN26sChUqqGDBggoMDHSp77Jly+b0dJEDYmNjFR4erpiYGBUsWNDT0wEAOHD5slS4sPP1w8KkuDj3zQcAAADA7ceVOJFbgpN+fn6yWCymspvDpC93hcViUUpKSrbmBvcgOAkAecOlS1KRIs7XDw2V4uPdNx8AAAAAtx9X4kQB7pyIYRjWYOTN/7ohFgoAAJzEr2EAAAAA3sStwUmJYCQAAHkZv8YBAAAAuJNbgpP9+/d3R7cAACCbOBAHAAAAgDdxS3ByypQp7ugWAABkE8FJAAAAAN7E7Wnd8KxDhw5py5YtOnnypJKSkhQREaFq1aqpWbNmCgkJ8fT0AABejuAkAAAAAHciOCnp1KlT2rJlizZv3qwtW7Zo27ZtiouLs95frlw5HT16NEt9Z+d0ckk6cuSIypcv73K7RYsW6d1339Uff/xh9/6wsLD/1969h0dR3v0f/2zO4ZQEwsEQCKdWiBVRBB6DUhCwohykeK4tWPQR0FarIgIqovJQUdFatR5Qqa3FUxUUbCsgESQVUAGVBBRICAko4ZBAyBGyvz/yY2WS7GY32d2Z3Xm/rivXwz1zz8w3PE0kn3zvuTVp0iTNmTNHyb5s2woACGl0TgIAAACwEtuGk+vXr9cTTzyhDRs2aN++fWaX4zeVlZWaPHmyXn/9dY/zSktL9cwzz+jNN9/UO++8oyFDhgSpQgCAmQgnAQAAAFhJhNkFmGXTpk167733wiqYrKmp0TXXXFMvmIyMjFT37t3Vr18/JSQkGM4VFRVp1KhR+u9//xvMUgEAIYJwEgAAAEAg2bZz0pNWrVqptLTU7/ft27evnnjiCZ+u6dSpk9dzH3vsMS1btsxwbMqUKbr//vuVkpIiqTbAXLZsme644w7l5+dLksrKynT11Vfrm2++qRdeAgDCC2EjAAAAACvxKZwsKipS+/btA1VLkzS3ptatW6t///4aMGCABg4cqAEDBig3N1fDhg3zY5W1kpKSNGLECL/fV5IOHTqkefPmGY7Nnz9f9957r+FYRESExo8fr4EDB+rCCy90vUuzoKBACxcu1Ny5cwNSHwDAGljWDQAAAMBKfFrW3bNnT82dOzcgXYW+Ki0t1Zw5c9SzZ88mXT9mzBht27ZNxcXFWrNmjRYsWKArr7xSaWlpfq40OBYsWGDYxGfIkCGaMWOG2/mdO3fWokWLDMeefPJJHTp0KGA1AgBCD+EkAAAAgEDyKZwsLS3VQw89pLS0NN1///0qKioKVF1uHThwQPfdd5/S0tL0yCOP6Pjx4026T8+ePZWenq6IiNB/7WZNTY1effVVw7EHH3yw0Z3Chw8frosuusg1PnbsmN56662A1AgAsAY6JwEAAABYSZOSuSNHjuj//u//lJaWphtuuEGZmZl+Lqu+NWvW6Fe/+pW6deum+fPnq7i4WE5+YpIkZWVlGYLiHj16aOjQoV5dO3nyZMN46dKlfqwMABDq+E8tAAAAgEDyKZxcu3at+vbtK0lyOp2qqKjQkiVLNHz4cHXt2lW33367Pv74Y1VWVja7sMrKSn388cf6/e9/r65du2rEiBF64403VFFRIafTKafTqX79+mnt2rXNflaoW7FihWE8cuTIRrsmT597uszMzCZ3owIArI+wEQAAAICV+LQhzoUXXqgvv/xSixYt0iOPPKK9e/dKqg0qCwoK9Mwzz+iZZ55RTEyMBgwYoPPPP18/+9nPdOaZZyo1NVVnnHGGYmJiDPesrKzU/v37VVBQoB07dujrr7/WF198oc8//1xVVVWu+58uNTVVDzzwgCZPnux1CBfOtmzZYhhnZGR4fW1KSoq6devm2hinqqpK2dnZGjBggB8rBABYRVPCSadT4j+3AAAAAALBp3BSkhwOh26++WZNnDhRL7zwghYuXKg9e/a4zjudTlVWVmr9+vVav359vesjIyMVHx/v6rw8efJkg885FUieHj6mpaXp7rvv1s0331wv5Awl+/fv1759+3T8+HElJSUpOTlZZ5xxRpPvl5OTYxinp6f7dH16erornDx1P8JJAMAphJMAAAAAAsXncPKUmJgY/e53v9Ott96q9957T3/5y1+UmZlZr8ux7vjEiROGXaXraqgT8uKLL9bUqVM1fvz4kN7A5uuvv1aPHj2Um5tb71ynTp3085//XJMmTdKll17q9T3Ly8uVn59vONalSxef6qo7f8eOHT5dDwAIHSzrBgAAAGAlzU76IiIiNGHCBK1atUr5+fl6/PHHNXToUEVHR9cLJh0OR4Mfp3M6nYqKitLQoUP12GOPKT8/X6tWrdKECRNCOpiUpMOHDzcYTErS999/rzfffFOjRo3Seeedp6+//tqrex48eNDw9xwdHa0OHTr4VFfnzp0N4wMHDvh0PQAgdDR1WTcAAAAABEKTOycbkpKSojvvvFN33nmnjh8/rg0bNuiLL77QV199pdzcXO3du1clJSUqKyuTJLVo0UKJiYnq0qWLunXrpr59+6p///4aNGiQWrZs6c/SQsrmzZs1aNAg/fWvf9VVV13lcW5paalh3KJFC5/fw1n377ruPRtSWVlp2Pjo6NGjPj0TABA6CCcBAAAABIpfw8nTtWzZUhdffLEuvvjiQD0iZCQnJ2v06NEaMWKE+vbtq9TUVLVu3VqlpaXKz8/XunXr9NJLL2nr1q2ua8rLy3XDDTeoY8eOGjJkiNt71w0S4+LifK4vPj7e4z0bMn/+fM2dO9fnZwEAzEXnJAAAAAArCe110iHg73//uwoLC/Xqq6/qV7/6lc4++2wlJSUpKipKiYmJ6tu3r2699VZt2bJFzz//vGJjY13XVlVV6frrr1dFRYXb+9c915SNgk5/plQbjDZm5syZKikpcX2c2rkdAGBthJMAAAAArIRwMsB+9atfeR0Y3nLLLfrHP/5heLdmYWGhnn32WbfX1O2UrKqq8rnG05dnN3TPhsTGxqpNmzaGDwBAeCKcBAAAABAohJMW88tf/lK//vWvDcf+9re/uZ3fqlUrw9hTl6U7dTsl694TABA+6JwEAAAAYCWEkxZ01113GcZfffWVfvjhhwbn1g0Sy8rK6u2S3pjjx497vCcAIHwQTgIAAACwEsJJCzr77LPVoUMH19jpdOrbb79tcG5ycrJhd+7q6modOHDAp+cVFhYaxqc/GwAAAAAAAAiUgOzW/dvf/tav94uKilJCQoISEhKUlpam888/X3369PHrM6wmNTXVEDIWFRU1OC8+Pl5du3bVnj17XMfy8/PVsWNHr5+Vn59vGPfu3dvHagEAoYLOSQAAAABWEpBwcvHixYZuvkBo27atJk2apKlTp6pHjx4BfZYZoqOjDePq6mq3c3v37m0IJ7OzszVgwACvn5WTk1PvfgAAnEI4CQAAACBQAr6s2+l0Gj6aO//U8UOHDmnhwoX62c9+pieffDIQpZvq+++/N4zbt2/vdm6/fv0M46ysLK+fs3//fuXl5bnG0dHRSk9P9/p6AEBooXMSAAAAgJUELJw8PVx0OByuj7rhY0NhpKf5p5w6V1FRobvvvlt33nlnoD6VoCsoKDB0QkpSly5d3M4fPXq0Ybxq1SqvN8X56KOPDONhw4axIQ4AhDHCSQAAAABWEpBl3bm5uZKk7du3a9q0acrLy5PT6VRCQoJ++ctfKiMjQ3369FFiYqJiY2N19OhR7du3T1u2bNH777+vTZs2SZIiIiI0ZcoU3X333SorK9Phw4f11VdfafXq1frggw908uRJV0j5pz/9Seeee65+/etfB+JTCqqXX37ZMO7SpYt+8pOfuJ2fkZGh5ORkHTx4UJK0e/duZWZmatiwYT4/a9y4cU2oGAAQzggnAQAAAASKw+lti52P1q1bpzFjxujYsWOKjIzUvffeq5kzZyo+Pr7Razdu3KgpU6Zoy5YtcjgcGj9+vN58801FRka65uTl5WnixIlat26dK6Ds2rWrdu7cqaio5mWudYO9tLQ0w9LnQMrJydGgQYN07Ngx17Hf/e53evrppz1eN336dD3++OOu8c9//nOtWbPG47s/V69erREjRrjGrVu31u7du5WcnOxz3UePHlVCQoJKSkrUpk0bn68HAATHjh2Sr68WLi6WEhICUg4AAACAMORLThSQZd179+7V2LFjdfToUUVFRendd9/VQw895FUwKUkDBw5UVlaWLr74YjmdTr333nv1lm1369ZNq1ev1kUXXeRawrx3714tXbrU359Ok2zZskVPPvmkysrKfLrm0ksvNQST8fHxuvfeexu9dsaMGYbl2J988okeffRRt/MLCwt10003GY7dfvvtTQomAQChg2XdAAAAAKwkIMu677nnHpWUlMjhcOjOO++s905Eb8TFxen1119X7969VVJSomeffVb/+7//q7POOss1JyoqSq+++qrOPPNM1dTUSKrtBrzyyiu9esb69etVXl5e7/jWrVsN44qKCq1atarBe6SkpDS4gUxxcbHuvPNOzZs3T7/85S81fvx4DRgwoF7453Q69c033+ill17Siy++qMrKSsP5+fPnKyUlpdHPJTk5WbNmzdKsWbNcx2bOnKn8/Hzdd999rnvU1NTo/fff1+233678/HzD53HXXXc1+hwAAAAAAADAX/y+rPvIkSPq0KGDTp48qYiICO3du1dnnHFGk+83depUvfDCC3I4HPr973/f4M7cY8eO1fLly+VwONS3b19t3rzZq3t369at3sYzvpo4caIWL15c77i7dz527NhRycnJat26tUpLS1VYWKgjR440eO+77rrLsFS7MTU1NRo3bpyWL19uOB4ZGam0tDQlJCQoNzdXxcXFhvPx8fFauXKlBg8e7PWz6mJZNwCEhpwcqYHfqXl0+LCUlBSYegAAAACEH1OXda9bt861UU337t2bFUxK0oUXXuj6c2ZmZoNzhg4dKqm2C/HAgQPNel6g/fDDD9q2bZs+++wzffPNNw0Gk23atNHf//53n4JJqXYDobffflvXXnut4fjJkye1e/dubd68uV4w2a5dO3344YfNCiYBAKGDZd0AAAAArMTv4eSuXbtcf27Xrl2z73fqHk6nU7t3725wTteuXV1/dteFGGxnn322Hn30UV166aVq27atV9f07t1bCxYsUF5enn71q1816blxcXFasmSJ3nnnHfXr18/tvJYtW2ratGnKzs52hbsAADSEcBIAAABAoPj9nZMVFRWuPx86dKjZ9zt8+LDrz3Xfx3hK69atXX8+9e5JbwRyB+527drpnnvu0T333CNJ2rNnj7777jvl5+fryJEjKi8vV1xcnJKSknTGGWdo0KBBfglzT5kwYYImTJignTt3asOGDSosLFRVVZUSExPVp08fDR48WHFxcX57HgAgNNA5CQAAAMBK/B5OduzYUVJtp2Nubq6+//57derUqcn3+/TTT11/bt++fYNzSktLXX8+fcdqK0lLS1NaWlrQn9urVy/16tUr6M8FAFgT4SQAAAAAK/H7su5TQZjD4ZDT6dQzzzzT5HsVFRXpjTfekMPhkMPhcBuyndrUxuFweLWzNQAA8B7hJAAAAIBA8Xs4OXjwYCUnJ0uq7Z587LHH9O9//9vn+1RVVenXv/61iouLdWpD8fHjxzc4d9OmTa4/9+zZswlVAwBgDwSNAAAAAKzE7+FkZGSkpk6dKqfTKYfDoerqal1xxRV65JFHVFVV5dU9Nm/erCFDhmjlypVyOBySpKSkJN1www315lZXV+s///mPa97AgQP998kAAAACTQAAAAAB4/d3TkrS7Nmz9dZbb+nbb7+Vw+FQVVWV5syZoyeffFITJkxQRkaG+vTpo4SEBMXExKi0tFT79u3T5s2btXz5cn322Weue50KORcuXNjgrtdLly5VcXGxK5wcNmxYID4lAADCAu+cBAAAAGAlDqczMD9y5Ofna9iwYcrNzXW9f1KSK0T05FQgeeqahx9+WLNnz25w7nnnnactW7ZIklJTU5Wfn++fTwA+OXr0qBISElRSUqI2bdqYXQ4AwI2tW6V+/Xy7Zt8+6YwzAlIOAAAAgDDkS07k92Xdp3Tt2lVZWVkaM2ZMvWDS6XS6/Tg1z+l0qm3btvrb3/7mNpiUpC+//FI1NTWqqakhmAQAIADonAQAAAAQKAELJyWpY8eOWrZsmZYtW6bhw4dLkhpr1HQ6nerQoYPuvfde5eTk6Fe/+lUgSwQAwFZY1g0AAADASgLyzsm6xowZozFjxmjfvn3KysrS559/rsLCQhUXF6uyslIJCQlq27at0tPTNXDgQA0YMEBRUUEpDQAAWyGcBAAAAGAlQU0AU1JSdOWVV+rKK68M5mMBAEAzEE4CAAAACJSALusGAADWQtAIAAAAwEoIJwEAsBGWdQMAAACwEsJJAADgEeEkAAAAgEAhnAQAwEbonAQAAABgJYSTAADYCOEkAAAAACshnAQAAB4RTgIAAAAIFMJJAABshM5JAAAAAFZCOAkAADwinAQAAAAQKISTAADYCEEjAAAAACshnAQAwEZY1g0AAADASggnAQCAR4STAAAAAAKFcBIAABuhcxIAAACAlRBOAgBgI4STAAAAAKyEcBIAAHhEOAkAAAAgUAgnAQCwETonAQAAAFgJ4SQAADZCOAkAAADASggnAQAAAAAAAJiCcBIAABuhcxIAAACAlRBOAgBgI4STAAAAAKyEcBIAAHhEOAkAAAAgUAgnAQCwETonAQAAAFgJ4SQAAPCIcBIAAABAoBBOAgBgI3ROAgAAALASwkkAAGyEcBIAAACAlRBOAgAAAAAAADAF4SQAADZC5yQAAAAAKyGcBADARggnAQAAAFgJ4SQAAPCIcBIAAABAoBBOAgBgI3ROAgAAALASwkkAAGyEcBIAAACAlRBOAgAAjwgnAQAAAAQK4SQAADZC0AgAAADASggnAQCwEZZ1AwAAALASwkkAAOAR4SQAAACAQCGcBADARuicBAAAAGAlhJMAAMAjwkkAAAAAgUI4CQCAjdA5CQAAAMBKCCcBALARwkkAAAAAVkI4CQAAPCKcBAAAABAohJMAANgIQSMAAAAAKyGcBADARljWDQAAAMBKCCcBAIBHhJMAAAAAAoVwEgAAG6FzEgAAAICVEE4CAGAjhJMAAAAArIRwEgAAeEQ4CQAAACBQCCcBALAROicBAAAAWEmU2QXYUUVFhbKysrR9+3YdOXJEMTExSk1N1aBBg9SjRw+/PmvXrl3auHGjCgoKVFVVpaSkJPXu3VsZGRmKi4vz67MAANZHOAkAAADASggnJRUWFmrjxo3asGGDNm7cqM8//1zHjh1znU9LS1NeXl6zn1NUVKS5c+dq8eLFOn78eINz+vfvr/vvv1/jxo1r1rOWLl2qhx9+WF9++WWD51u1aqVJkyZpzpw5Sk5ObtazAAAAAAAAgKawbTi5fv16PfHEE9qwYYP27dsX8OdlZmbqqquu0sGDBz3O++KLL3TFFVfoN7/5jV566SXFxMT49JzKykpNnjxZr7/+usd5paWleuaZZ/Tmm2/qnXfe0ZAhQ3x6DgAgNNE5CQAAAMBKbPvOyU2bNum9994LSjD56aef6rLLLqsXTCYmJurcc89Vt27dFBkZaTj32muv6brrrpPTh58Ia2pqdM0119QLJiMjI9W9e3f169dPCQkJhnNFRUUaNWqU/vvf//r4WQEA7IJwEgAAAECg2Dac9KRVq1Z+u9eRI0d0zTXXqLy83HUsLS1NS5cu1eHDh/Xll18qNzdXeXl5uuWWWwzXvvvuu3ryySe9ftZjjz2mZcuWGY5NmTJF+fn52r17tzZv3qzDhw/r3XffVdeuXV1zysrKdPXVV6ukpKSJnyUAIFTQOQkAAADASmwfTrZu3VpDhw7V9OnT9fbbbysvL08ffPCB3+7/2GOPGbozu3fvrqysLI0bN04Oh8N1PDU1Vc8//7zmzZtnuP6hhx7SkSNHGn3OoUOH6l07f/58/eUvf1FKSorrWEREhMaPH6+srCx169bNdbygoEALFy709dMDAIQYwkkAAAAAVmLbcHLMmDHatm2biouLtWbNGi1YsEBXXnml0tLS/PaMoqIi/fnPfzYce+mllwxhYV0zZ840vP+xpKREjz/+eKPPWrBggWETnyFDhmjGjBlu53fu3FmLFi0yHHvyySd16NChRp8FALAXwkkAAAAAgWLbcLJnz55KT09XRETg/greeOMNlZaWusZDhgzR8OHDPV7jcDg0Z84cw7FXXnnF47sna2pq9OqrrxqOPfjgg4bOzIYMHz5cF110kWt87NgxvfXWWx6vAQCENjonAQAAAFiJbcPJYKj7/sfJkyd7dd2wYcPUvXt31/j777/XZ5995nZ+VlaWioqKXOMePXpo6NChXj2rbk1Lly716joAQGginAQAAABgJYSTAVJaWqq1a9cajl1yySVeXetwODRixAjDseXLl7udv2LFCsN45MiRjXZNnj73dJmZmTp+/LhX1wIAAAAAAADNQTgZINu2bVN1dbVr3L17d3Xq1Mnr6wcPHmwYb9myxe3cuucyMjK8fk5KSophY5yqqiplZ2d7fT0AILTQOQkAAADASggnAyQnJ8cwTk9P9+n6uvPr3s+sZwEAQhvhJAAAAAArIZwMkB07dhjGXbp08en6uvP37NmjioqKevPKy8uVn5/v12fVrR0AYG+EkwAAAAAChXAyQA4cOGAYp6am+nR9x44dFRUV5RrX1NTo0KFD9eYdPHjQsJN3dHS0OnTo4NOzOnfubBjXrR0AED7onAQAAABgJVGNT0FTlJaWGsYtW7b06XqHw6H4+HgdO3bM7T0bOtaiRQuvN8NxV1tDz6mrsrJSlZWVrvHRo0d9eiYAwByEkwAAAACshM7JAKkb8MXFxfl8j/j4eI/3DOZz6po/f74SEhJcH74uJQcAhA7CSQAAAACBQjgZIHXfDxkTE+PzPWJjYw3j8vJy055T18yZM1VSUuL62Lt3r8/PBQAEH52TAAAAAKyEZd0BUreDsaqqyud7nL5suqF7BvM5dcXGxtYLNQEAAAAAAABf0DkZIK1atTKMG9ppuzF1Oxjr3jOYzwEAhAc6JwEAAABYCeFkgNQN+I4fP+7T9U6ns0nhZFlZmWH3bm/UrY1wEgDCF+EkAAAAACshnAyQDh06GMYFBQU+Xf/DDz/oxIkTrnFERISSk5PrzUtOTjbszl1dXa0DBw749KzCwkLDuG7tAAB7I5wEAAAAECiEkwFy5plnGsb5+fk+XV93flpaWoPvgoyPj1fXrl39+qzevXv7dD0AIHTQOQkAAADASggnA6RuwJedne3T9Tk5OR7vZ9azAAChjXASAAAAgJUQTgbIWWedpejoaNc4Ly9P+/fv9/r69evXG8b9+vVzO7fuuaysLK+fs3//fuXl5bnG0dHRSk9P9/p6AED4I5wEAAAAECiEkwHSunVrDRkyxHBs5cqVXl3rdDq1atUqw7ExY8a4nT969GjDeNWqVV5vivPRRx8ZxsOGDWNDHAAIY3ROAgAAALASwskAGjt2rGH88ssve3XdmjVrlJub6xp37NhRgwYNcjs/IyPDsFnO7t27lZmZ6dWz6tY0btw4r64DAIQmgkYAAAAAVkI4GUDXXnutWrZs6RqvXbtWH3/8scdrnE6n5s6dazh24403KiLC/f+rIiIiNGnSJMOxuXPnNto9uXr1aq1bt841bt26ta6++mqP1wAA7IdAEwAAAECgEE4GUIcOHXTbbbcZjt10003at2+f22vmz5+vtWvXusYJCQmaPn16o8+aMWOGYTn2J598okcffdTt/MLCQt10002GY7fffruhAxMAEH5Y1g0AAADASqLMLsBM69evV3l5eb3jW7duNYwrKirqvQPylJSUFI8byNxzzz3661//qu+//16SlJubq4yMDD399NMaM2aMHA6HJKmgoECPPPKIXnjhBcP1s2fPVtu2bRv9XJKTkzVr1izNmjXLdWzmzJnKz8/Xfffdp5SUFElSTU2N3n//fd1+++3Kz883fB533XVXo88BAIQ2wkkAAAAAVuJwertzShjq1q2b9uzZ06x7TJw4UYsXL/Y4Z+3atfrFL36hiooKw/HExER1795dxcXFys/P18mTJw3nx40bp/fee88VYDampqZG48aN0/Llyw3HIyMjlZaWpoSEBOXm5qq4uNhwPj4+XitXrtTgwYO9ek5Djh49qoSEBJWUlKhNmzZNvg8AILBeeUWaPNm3axYvliZODEg5AAAAAMKQLzkRy7qDYMiQIVqxYkW9Dsji4mJt3rxZubm59YLJ66+/Xm+++abXwaRU++7Jt99+W9dee63h+MmTJ7V7925t3ry5XjDZrl07ffjhh80KJgEAoYPOSQAAAABWQjgZJBdffLGys7M1depUtWjRwu28c889V//85z/1+uuvKzY21ufnxMXFacmSJXrnnXfUr18/t/NatmypadOmKTs7W0OHDvX5OQAA+yCcBAAAABAotl7WbZby8nJlZWUpJydHxcXFiomJUefOnTVo0CD16tXLr8/auXOnNmzYoMLCQlVVVSkxMVF9+vTR4MGDFRcX57fnsKwbAELDokXSzTf7ds0rr0g33hiYegAAAACEH19yIltviGOW+Ph4DR8+XMOHDw/4s3r16uX3wBMAELpY1g0AAADASljWDQAAPCKcBAAAABAohJMAANgInZMAAAAArIRwEgAAGyGcBADry8+X3nhD+uILqabG7GoAAAgswkkAAOAR4SQABIfTKd13n5SWJl13nXT++dL//I90+LDZlQEAEDiEkwAA2AidkwBgXW+8Ic2bZzy2aZP029+aUw8AAMFAOAkAgI0QTgJALSt+b3vrrYaP//vf0vHjwa0FAIBgIZwEAAAAYBsffCD9/OdSYqI0cqS0apXZFf1o6dKGj1dW1r5/EgCAcBRldgEAACB46JwEYGfvvy/98pfSyZO141WrpHXrpH/9Sxo2zNzaGsPGOACAcEXnJAAANkI4CcDOnnzyx2DylMpK6emnzanHF9HRZlcAAEBgEE4CAACPCCcBhIOaGikzs+Fz7pZTB1N1dfPOAwAQqggnAQCwETonAdhVZaXZFXh29Kjn8xUVwakDAIBgI5wEAAAeEU4CCAdWDydLSjyfJ5wEAIQrwkkAAGyEzkkAdkU4CQCANRFOAgBgI4STAOwq1MPJ8vLg1AEAQLARTgIAAAAIe1YPJ4uLPZ+ncxIAEK4IJwEAsBE6JwHYVWPhpNnf61jWDQCwK8JJAABshHASgF01Fk7W1ASnDncIJwEAdkU4CQAAPCKcBBAOCCcBALAmwkkAAGyEzkkAdtVYOHnyZHDqaEhNjfTAA57nEE4CAMIV4SQAADZCOAnArqwcTj74YONzCCcBAOGKcBIAAHhEOAkgHFh1Wfd330mPPtr4PMJJAEC4IpwEAMBG6JwEYFdW7Zx8/HGpqqrxeYSTAIBwRTgJAICNEDQCsCurdk7+61/ezSOcBACEK8JJAADgEYEmgHBgxc5Jp1Pau9e7uYSTAIBwRTgJAICNsKwbgF1ZsXOyutr7uYSTAIBwRTgJAAA8IpwEEA6s2DnZWE2nKy8PXB0AAJiJcBIAABuhcxKAXYV6OEnnJAAgXBFOAgBgI4STAOzKisu6CScBACCcBAAAjSCcBBAOrNg5WVXl/VzCSQBAuCKcBADARuicBGBXdE4CAGBNhJMAANgIQSMAu7Ji5yThJAAAhJMAAKARBJoAwgGdkwAAWBPhJAAANsKybgB2ReckAADWRDgJAICNEE4CsCsrhpO+bIhTXW1OjQAABBrhJAAA8IhwEkA4CPVl3U2ZDwBAKCCcBADARuicBGBXVuyc9DVsZGk3ACAcEU4CAGAjhJMA7CocOicJJwEA4YhwEgAAeEQ4CSAcWLFz0pd3TkpSeXlg6gAAwEyEkwAA2AhBIwC7onMSAABrIpwEAAAeEWgCCAeNdSmGwjsn6ZwEAIQjwkkAAGyEd04CsCsrLuv2NZw8fjwwdQAAYCbCSQAAbIRwEoBdhcOy7tLSwNQBAICZCCcBAIBHhJMAwoEVOyd93RCHcBIAEI4IJwEAsBE6JwHYVTh0Th47Fpg6AAAwE+EkAAA2QjgJwK6s2DnJsm4AAAgnAQAAANhAOHROEk4CAMIR4SQAADZC5yQAOzpxovHwkc5JAADMQTgJAICNEE4CsCNvQkA2xAEAwByEkwAAwCPCSQChzpsQkGXdAACYg3ASAAAboXMSgB15EzyyrBsAAHMQTgIAYCOEkwDsyJvgkc5JAADMQTgJAAA8IpwEEOq8CSfpnAQAwByEkwAA2AidkwDsyKqdk2yIAwAA4SQAAACAMMc7JwEAsC7CSQAAbITOSQB2ZNXOScJJAACkKLMLQOBUVFQoKytL27dv15EjRxQTE6PU1FQNGjRIPXr0MLs8AIAJCCcB2BHvnAQAwLoIJwPswQcf1Ny5c5t8/cSJE7V48WKfrikqKtLcuXO1ePFiHT9+vME5/fv31/33369x48Y1uTYAgD0QTgIIdVZd1u3rOyePH6/9XCJY/wYACCP8Zy3MZGZmKj09Xc8++6zbYFKSvvjiC11xxRWaOHGiqnz9VxEAIGTROQnAjsJlWbcklZX5vw4AAMxEOBlGPv30U1122WU6ePCg4XhiYqLOPfdcdevWTZGRkYZzr732mq677jo5+ckTAGyBcBKAHYXLsm6Jpd0AgPDDsu4ge/zxx3XOOed4PT8lJcWreUeOHNE111yj8vJy17G0tDT96U9/0tixY+VwOCRJBQUFeuSRR/TCCy+45r377rt68skndeedd3pdFwAgNDWlM4hwEkCo8+Z7X6h0ThJOAgDCDeFkkPXv319Dhw71+30fe+wx7du3zzXu3r27Pv3003rhZmpqqp5//nl17dpVs2fPdh1/6KGHdOONNyopKcnvtQEArKMpnUGEkwBCHZ2TAABYF8u6w0BRUZH+/Oc/G4699NJLHrsuZ86cqSFDhrjGJSUlevzxxwNWIwDAGsz44RsAzGbFd07W1EgnTvh+3bFj/q8FAAAzEU6GgTfeeEOlp/0KdciQIRo+fLjHaxwOh+bMmWM49sorr/DuSQAIcyzrBmBHVtytu6l7UrIhDgAg3BBOhoFly5YZxpMnT/bqumHDhql79+6u8ffff6/PPvvMr7UBAKyFZd0A7MiKy7qPH2/adae9Yh4AgLBAOBniSktLtXbtWsOxSy65xKtrHQ6HRowYYTi2fPlyv9UGALAewkkAdmTFZd1NDScrKvxbBwAAZiOcDHHbtm1TdXW1a9y9e3d16tTJ6+sHDx5sGG/ZssVfpQEALIhl3QDsyIrLuumcBACgFrt1m6CyslK7d+/WoUOHFB0drXbt2iklJUUtWrTw+V45OTmGcXp6uk/X151f934AgPBC5yQAOwqnzknCSQBAuCGcDLJbb71Vu3fvVkWd9RhRUVHq37+/Ro0apWnTpql9+/Ze3W/Hjh2GcZcuXXyqp+78PXv2qKKiQnFxcT7dBwAQGggnAdiRFd852dSNbQgnAQDhhmXdQZadnV0vmJSkEydOaMOGDXrwwQeVlpamBx54QCe9+BfSgQMHDOPU1FSf6unYsaOion7MqGtqanTo0CGf7gEACB2EkwDsiM5JAACsi3DSgsrLy/Xwww9rxIgRKi0t9Ti37vmWLVv69CyHw6H4+HiP92xIZWWljh49avgAAFhfsH/4BgArCKd3TrIhDgAg3BBOBoHD4VBGRobmzZunlStXqqCgQGVlZaqoqFBhYaE++OAD3XLLLfWWUmdmZuraa6/12EFZN0hsynLspoST8+fPV0JCguvD1+XkAABz0DkJwI6suKzbUzgZE+P+HJ2TAIBwQzgZYJdccom2b9+u9evXa9asWRoxYoQ6d+6s+Ph4xcbGKiUlRaNHj9bzzz+v7777rt7u2StWrNBzzz3n9v51l4jHePqXjBuxsbGGcbkX/+KZOXOmSkpKXB979+71+bkAgOAjnARgR6G2rLtdO/fnCCcBAOGGcDLAMjIy9NOf/tSruampqVq1apUuuOACw/FHHnlEZW7emF23U7KqqsrnGisrKz3esyGxsbFq06aN4QMAYH1N+eGbcBJAqAu1Zd3Jye7PEU4CAMIN4aTFxMXF6bXXXjNsUnPgwAF99NFHDc5v1aqVYdzQZjuNqdspWfeeAIDwQeckADuicxIAAOsinLSgXr16aezYsYZj3oaTx318s7bT6SScBAAbIZwEYEeh9s5JOicBAHZCOGlRw4cPN4x37NjR4LwOHToYxgUFBT4954cfftCJEydc44iICCV7+tcQACCkEU4CsCNvuiJDpXOS3boBAOGGcNKi6u5+XVRU1OC8M8880zDOz8/36Tl156elpTVpx28AQGjgnZMA7IjOSQAArItw0qKio6MN4+rq6gbn9e7d2zDOzs726Tk5OTke7wcACC/B/uEbAKyAcBIAAOsinLSo77//3jBu3759g/POOussQ5CZl5en/fv3e/2c9evXG8b9+vXzvkgAQMhhWTcAOwq1Zd2EkwAAOyGctKhPP/3UMK67zPuU1q1ba8iQIYZjK1eu9OoZTqdTq1atMhwbM2aMD1UCAEIN4SQAO6JzEgAA6yKctKDi4mL985//NByru0HO6eru7P3yyy979Zw1a9YoNzfXNe7YsaMGDRrkQ6UAgFDDOycB2JE3waOVOifZEAcAYCeEkxZ09913q7i42DWOiYnRqFGj3M6/9tpr1bJlS9d47dq1+vjjjz0+w+l0au7cuYZjN954oyIi+J8EAIQzOicB2JE3wSOdkwAAmIMkKoD++Mc/6osvvvB6/okTJ3TXXXfV63ycMmWKzjjjDLfXdejQQbfddpvh2E033aR9+/a5vWb+/Plau3ata5yQkKDp06d7XSsAIDQRTgKwIyt2TpaVuT+XmCi56xkgnAQAhBvCyQD697//rfPPP1+DBw/Wn/70J33zzTc6ceJEvXklJSVasmSJBgwYoIULFxrO9ezZUw888ECjz7rnnnvUqVMn1zg3N1cZGRl6//335Tztp8qCggJNmTJFs2fPNlw/e/ZstW3b1tdPEQAQYljWDcCOQu2dky1bSvHxDZ+rqOD7MgAgvESZXYAdZGVlKSsrS5IUGxur1NRUJSQkKDIyUocOHVJeXp5qGvhpsVOnTvrXv/6ldp5eOvP/tW3bVm+++aZ+8YtfqOL/v4hmz549GjdunBITE9W9e3cVFxcrPz9fJ+v8y2vcuHG6++67/fCZAgCsjs5JAHYUSuFkVJQUE1MbTrqbU1HhPrwEACDUEE4GWWVlpXbt2tXovMsuu0yvvvqqOnTo4PW9hwwZohUrVuiqq67S4cOHXceLi4u1efPmBq+5/vrr9corr8jhcHj9HABA6Ar2D98AYAXedI0Hc1n3yZNSZWXD5069St5T+Eg4CQAIJyzrDqDZs2drypQpOuussxQZGdno/FatWumqq67SJ598ohUrVvgUTJ5y8cUXKzs7W1OnTlWLFi3czjv33HP1z3/+U6+//rpiY2N9fg4AIDTROQnAjqzWOdnYkm5JiotzP4f3TgIAwgmdkwE0cuRIjRw5UpJUVlam7Oxs5eXlaf/+/SotLVVNTY0SExOVlJSk9PR0nX322V6FmI3p2LGjnnvuOT3xxBPKyspSTk6OiouLFRMTo86dO2vQoEHq1atXs58DAAg9vHMSgB1ZbUMcT5vhnOov8NQZSTgJAAgnhJNB0qJFC51//vk6//zzg/bM+Ph4DR8+XMOHDw/aMwEA1kbnJAA78iZ4DGbn5P9/RXyDTnVMEk4CAOyCZd0AANgI4SQAO7Ja56S7901K0qk3LhFOAgDsgnASAAAbIZwEYEdWe+ck4SQAAD8inAQAwEZ45yQAO7Labt3ehJOeNsTxtCwcAIBQQzgJAICNBLMzCACsItw6Jz1tqAMAQKghnAQAwEZY1g3AjkIxnDy1a3dDWNYNAAgnhJMAANgIy7oB2FEoLuv2FE7SOQkACCeEkwAA2AidkwDsyGqdk1VV7s+dCidbtnQ/h3ASABBOCCcBALARwkkAduTN975Q6pw8fty/9QAAYKYoswsAAADBQzgJwExOp7R5s/TZZ1JamjRsmOcQzl+8CR6t8s7JmJja/8uybgCAXRBOAgBgI7xzEoBZTpyQbrtNeuGFH4+ddZa0bJnUs2dgnx1unZOEkwCAcMKybgAAbITOSQBmWbLEGExK0rZt0u9/H/hnW+2dk4STAAD8iHASAAAbCeYP3wBwupdfbvj4hx9KubmBfXYohpNsiAMAsAvCSQAAbMLpZFk3AHPU1EiffOL+/KpVgX++P+b4C52TAAD8iHASAACbaGrISDgJoLl27vR8/tNPA/v8UOycZLduAIBdEE4CAGATTf3Bm3ASQHNt3uz5vKeuSn9gQxwAAKyLcBIAAJsgnARglsbCyT17pPz8wD3fm+AxlDonCScBAOGEcBIAAJtoalcQ4SSA5mosnJSk7dsD9/xQ7JxkQxwAgF0QTgIAYBN0TgIwy9dfNz5n9+7APT/c3jlJOAkACCeEkwAA2AThJAAzHDki7d/f+LxduwJXQ7gt62ZDHABAOCGcBADAJpr6g3cwf2AHEH62bfNuXiDDyVBc1h0f734OnZMAgHBCOAkAgE009Qfv6mr/1gHAXrwNJ1nWXetUOBkZ+eOf6yoro6sdABA+CCcBALCJpv7gTTgJoDl86ZwMVODmzS9nrNY5KbnfFOfkSb43AwDCB+EkAAA24SmcjItzf66qyv+1ALAPb8PJ0lLp4MHA1BCKnZMSm+IAAOyBcBIAAJtoajhJdw6A5sjJ8X7u2rWBqSEU3zkpsSkOAMAeCCcBALAJTz94E04CCITjx73bqfuU554LTB2htFt3TMyPf6ZzEgBgB4STAADYBMu6AQSbr5vcfPyxlJfn/zpY1g0AgHURTgIAYBMs6wYQbDt3+n7Nf//r/zq8DR6DFVA2d0MciXASABA+CCcBALAJlnUDCLamhJM7dvi/Dm9Dx2B1itM5CQDAjwgnAQCwCU8/nJ/+w3BdhJMAmmrXLt+vCUQ46e1mN4STAAAEH+EkAAA24SmcjIyUoqMbPsc7JwE0VVM6J7dv938d4dg5yW7dAIBwQTgJAIBNNDWcpHMSQFN9953v13z7rfedjt4KlXAyMrL24xRP75wsKfFvTQAAmIVwEgAAm/D0w35EhPtw0ukM7i62AMLDvn1Sfn7D5yIjpYSEhs+VlUl79/q3Fist63Y63YeTdV+xkZzs/j4HD/qvJgAAzEQ4CQCATTS1c1KiexKA79ascX/urLOkn/3M/fmFC/1bi7e/YPG03NpfPH0/JZwEANgR4SQAADbRWDgZE+P+PO+dBOCrlSvdn7v4Yql3b/fnX3xR+uEH/9VipWXd3r5vUpLat3c/t6jIP/UAAGA2wkkAAGzC07JGOicB+NP330tLlrg/P2yYNHKk+/MVFdLrr/uvHist6yacBADAiHASAACb8NQ55OmdkxLhJADfzJ7tPuiLiJCGDJHGj5fS0tzfY9ky/9UTqp2TLOsGANgB4SQAADbBsm4AwVBWJv3jH+7PZ2RIiYm133Oeesr9vE8/9V8AF6rhJJ2TAAA7IJwEAMAm2BAHQDBs2VK7LNud6dN//PPYsVJKSsPzamqkf//bPzVZaVm3p7+buuFk27bu5zYW3JaUSI8/Lk2YIM2cKW3b5n2NAAAEU5TZBQAAgODw9MM5y7oB+MsXX7g/d+65tYHkKRERtePnn294/ldf+acmK3VOHjvm/lzr1sZxVFRtQHn4cP255eXS8eNSy5b1zx08WPtOzy1bfjz27LPS8uW1S+oBALASOicBALAJOicBBIOncPLaa+sfGzrU/fxdu5pdjiRrhZNHj7o/16ZN/WNNWdr9/PPGYFKqDUVP71oFAMAqCCcBALAJ3jkJIBg++sj9uf796x/7yU/cz9+5s/n1SN4v6/b0Pkh/8aVzUmrapjhLlzZ8fONG6Ycf3N8PAAAzEE4CAGATdE4CCLS33pL273d//rzz6h/r2dP9/K++8s/GL+HaOTlrljR3rrRypfG4p+7Vb77xXB8AAMFGOAkAgE3wzkkAgXTypHTHHe7P9+ghJSXVP56Q4Lk7sEeP2ncrNrc2b1jtnZOS57+blSulBx+ULrlEuu222u/zTqfn53v7dwEAQLAQTgIAYBPNWdZNOAmgMV9/7blr8vLL3Z/z1D1ZWiq9/XbT65KsFU76s3PydM8+K61ZIxUXe57X0OY6AACYiXASAACbaM6ybt45CaAxjW1ec+ed7s/16uX52rrLln3l7Tsnrdg52bGj9/eeMUO67DLPcw4d8v5+AAAEQ5TZBQAAgODw9MM575wE0Fx5ee7PjRwpdevm/rynzklJys1tSkU/CuXOyU6dvL+3p3dNnkI4CQCwGjonAQCwCU8/nEdEsKwbQPN4ChDHjPF87c9+1vR7e8NK4aSnzsmGwklfOie94W6HbwAAzEI4CQCATbCsG0AgeQoQu3f3fO3o0bUb47jz/fdSWVnT6pK8X9ZdWdn0Z3jLU+dkQ8u6femc9AadkwAAqyGcBADAJpoTTtI5CaAxnpZ1e1rSLUnx8dJbbzX9/p44naHzzslgdE4STgIArIZwEgAAm/D0w3lEBOEkgKZzOpsXTkrSJZdI/fq5P9/Upd3eBpOS+e+cbKhzMjHR82s3fMWybgCA1RBOAgBgE411TvLOSQBNdeCA+2XX7dtLrVp5d5+MDPfnwiWc9LVz0uHw79JuOicBAFZDOAkAgE3wzkkAgdLcrslTPL2bsqnhpLeb4UjW7JyU/Lu0m85JAIDVEE4CAGATvHMSQKA0ZzOc0/Xo4f7c7t3e3+d0VgsnPXVOuusw9WfnZGkpv3ACAFgL4SQAADbR2DsnWdYNoKn8FU4GonPSSsu6T56Ujh9v+Fzr1rXfixvCjt0AgHBGOAkAgE3QOQkgUDwt6/ZnOOl0en+vU3zpnKys9P3+vigtdX/O3ZJuyf87dh844N/7AQDQHFFmF4DA27VrlzZu3KiCggJVVVUpKSlJvXv3VkZGhuLi4swuDwAQJLxzEkCgeOpq9OWdk4mJUlKSdORI/XNHj0qHD0vt2vlWm5WWdXt632RDm+Gc0quXf+vYt0865xz/3hMAgKYinAxjS5cu1cMPP6wvv/yywfOtWrXSpEmTNGfOHCUnJwe5OgBAsHla2kjnJIDm8Ney7lPzGwonTz3H13DSSsu6Pb1v0lPn5GWXSQkJUkmJf+ooKPDPfQAA8AeWdYehyspK3XDDDRo/frzbYFKSSktL9cwzzyg9PV1r164NYoUAADN46h7inZMAmqqmRtq50/35tDTf7ufv905aqXOyuNj9OU+dk+3bSy++6L86Cgv9dy8AAJqLcDLM1NTU6JprrtHrr79uOB4ZGanu3burX79+SkhIMJwrKirSqFGj9N///jeYpQIAgoxl3QACYelS9+dSUqTYWN/u5+8du60UThYVuT/XWEfo1VdLzz7r+Xu1t+icBABYCeFkmHnssce0bNkyw7EpU6YoPz9fu3fv1ubNm3X48GG9++676tq1q2tOWVmZrr76apX4a60IAMBy2BAHgL9VVkoTJrg/7+uS7sauef993+9npXDS00Y0HTo0fv20adKmTdL//Z80blzT66BzEgBgJYSTYeTQoUOaN2+e4dj8+fP1l7/8RSkpKa5jERERGj9+vLKystTttDeUFxQUaOHChcEqFwAQZJ7eu8aybgBN8Z//eD7vqQvSHU/hZFaW5OtiH19+925m52T79t7d45xzpJkzpffek4YMaVod//639M03TbsWAAB/I5wMIwsWLNCx096yPWTIEM2YMcPt/M6dO2vRokWGY08++aQOHToUsBoBAOahcxKAv23e7Pn8pZf6fs/zzqv9nuROnd/FN8qXLsHKSt/u7St/hJOnOBzSSy9JPXs2fD493fP1550n/fWvvj0TAIBAIJwMEzU1NXr11VcNxx588EE5HA6P1w0fPlwXXXSRa3zs2DG99dZbAakRAGAu3jkJwN+ysz2fHz/e93t26CCNHu3+/IcfSnl53t/Pl3DS6su66/rpT6WtW2u7KB94QJo6VbrtttrNcz7/3PMO4NXV0h13SIcP+/5cAAD8iXAyTGRlZanotF/F9ujRQ0OHDvXq2smTJxvGSz291RwAELLonATgbzk57s999JEUH9+0+z7xhPtrnc7ajkFvWSmc9Gfn5CktW0pXXCHNnSs995z05z9LN99c+/eXmur52uJiqW9ffgEFADAX4WSYWLFihWE8cuTIRrsmT597uszMTB0/ftxvtQEArIF3TgLwpxMnpB073J8fOLDp9+7ZU/rtb92ff/PN2pDSG+EeTnrSWDgp1f79XHYZASUAwDyEk2Fiy5YthnFGRobX16akpBg2xqmqqlJ2Y2t0AAAhh85JAP6Uk+M+0OrcWUpIaN79p051f27XLmn7du/uY6Vw0tOy7kCEk+ed59281aulu+/2//MBAPBGlNkFwD9y6qypSW/sDdh1pKenK++0l/fk5ORowIAB/igtpGzaJLEfEIBwVafJ3qCxcHL//trdXQFAqu1avOwy9+d9/Kdog846S+rfX/rii4bPjxkjPfNM4/f59lvvn1lREdjvde46JyMipLZt/f+8226rXep92p6Zbv35z7U1/M//+L8OAIDvRo70vEFcOCGcDAPl5eXKz883HOvSpYtP96g7f4enNTphbNYsadUqs6sAgOCLjPS8rHvLFmnUqKCVAyDE9enjn/uMHes+nNy1y//fl6qrzflel5xcG1D6W2qq9Mor0lVXeTd/7lz/1wAAaJqysqa/uznUsKw7DBw8eFDO0166Ex0drQ4+bvfXuXNnw/iApzUnAICwExHhuXMSAHzhr+67sWP9cx+rC8SS7lOuvFJ6/fXA3R8AgOYinAwDpaWlhnGLFi283gznlJYtW3q8Z12VlZU6evSo4QMAELratSOcBOAfP/1p7e7R/nDOObX3C3eBDCcl6frrfVveDgBAMBFOhoG6QWJcXJzP94iv0yvcWDg5f/58JSQkuD58XUYOALCOli1rd9Vt317y8XdbAGCQnCx98IH/lqE5HNKjj/rnXlbWtWvgn/GTn0hZWRL/bAcAWA3hZBioqKgwjGM8vTTMjdjYWMO4vLzc4/yZM2eqpKTE9bF3716fnwkAsIZTmyC0bVsbUgJAUzgc0pIl/u90vOIKaeZM/97TasaPD85zLrhAysurDZABALAKwskwULdTsqqqyud7VFZWerxnXbGxsWrTpo3hAwAQWqKiane6vfHGH489/3ztEm8A8EVERO0vOkaMCMz9582TnnrKfx2ZAwf6Z0dxf7juOunyy4P3vIgIafTo2u/3dMsDAKyA3brDQKtWrQzjup2U3qjbKVn3nnYxZIiUlGR2FQAQeD171v5A3Lev8Xi/ftLnn0vvvSdt3CidPGlKeQBCSK9e0pgxtV15geJwSLffLl1zjfTuu9KGDVIjC30alJBQ+++9a66RKipq7/XJJ9Lx4/6vuTHJydKwYdKECYHZqbsxt9winXuu9NZbUn5+8J8PAPAsMtLsCoKHcDIM1A0Sy8rK5HQ6fdoU53idf5HZNZy8/36zKwAA83XrJv3hD2ZXAQD1deokTZtW+9FcMTHSpEm1H3Y1cCCv8wAAmI9l3WEgOTnZEERWV1frwIEDPt2jsLDQMO7QoYNfagMAAAAAAADcIZwMA/Hx8epaZ4u/fB/XZtSd37t372bXBQAAAAAAAHhCOBkm6oaJ2dnZPl2fk5Pj8X4AAAAAAACAvxFOhol+/foZxllZWV5fu3//fuXl5bnG0dHRSrfK9oUAAAAAAAAIW4STYWL06NGG8apVq+R0Or269qOPPjKMhw0bZtsNcQAAAAAAABA8hJNhIiMjQ8nJya7x7t27lZmZ6dW1L7/8smE8btw4f5YGAAAAAAAANIhwMkxERERo0qRJhmNz585ttHty9erVWrdunWvcunVrXX311YEoEQAAAAAAADAgnAwjM2bMMCzH/uSTT/Too4+6nV9YWKibbrrJcOz22283dGACAAAAAAAAgUI4GUaSk5M1a9Ysw7GZM2dq2rRp2rdvn+tYTU2Nli5dqoyMDMNGOCkpKbrrrruCVS4AAAAAAABszuH0dtcUhISamhqNGzdOy5cvNxyPjIxUWlqaEhISlJubq+LiYsP5+Ph4rVy5UoMHD27Sc48ePaqEhASVlJSoTZs2TS0fAAAAAAAAIc6XnIjOyTATERGht99+W9dee63h+MmTJ7V7925t3ry5XjDZrl07ffjhh00OJgEAAAAAAICmIJwMQ3FxcVqyZIneeecd9evXz+28li1batq0acrOztbQoUODVh8AAAAAAAAgsazbFnbu3KkNGzaosLBQVVVVSkxMVJ8+fTR48GDFxcX55Rks6wYAAAAAAIDkW04UFaSaYKJevXqpV69eZpcBAAAAAAAAGLCsGwAAAAAAAIApCCcBAAAAAAAAmIJwEgAAAAAAAIApCCcBAAAAAAAAmIJwEgAAAAAAAIApCCcBAAAAAAAAmIJwEgAAAAAAAIApCCcBAAAAAAAAmCLK7AIQHpxOpyTp6NGjJlcCAAAAAAAAM53Kh07lRZ4QTsIvjh07Jknq0qWLyZUAAAAAAADACo4dO6aEhASPcxxObyJMoBE1NTXat2+fWrduLYfDYXY5zXb06FF16dJFe/fuVZs2bcwuBwhpfD0B/sHXEuA/fD0B/sPXE+Af4fa15HQ6dezYMaWkpCgiwvNbJemchF9EREQoNTXV7DL8rk2bNmHxTQGwAr6eAP/gawnwH76eAP/h6wnwj3D6WmqsY/IUNsQBAAAAAAAAYArCSQAAAAAAAACmIJwEGhAbG6s5c+YoNjbW7FKAkMfXE+AffC0B/sPXE+A/fD0B/mHnryU2xAEAAAAAAABgCjonAQAAAAAAAJiCcBIAAAAAAACAKQgnAQAAAAAAAJiCcBIAAAAAAACAKaLMLgCwml27dmnjxo0qKChQVVWVkpKS1Lt3b2VkZCguLs7s8gAAAOCDqqoqbd++XXl5eSosLNSxY8dUXV2tNm3aqF27durbt6/69OmjyMhIs0sFAMCWCCeB/2/p0qV6+OGH9eWXXzZ4vlWrVpo0aZLmzJmj5OTkIFcHALATp9OpvLw8ff311yooKFBxcbFiY2OVlJSkn/zkJxowYAC/MAM8eOedd7Rq1SqtX79e27dv14kTJzzOT0hI0HXXXafbb79dvXv3DlKVAABAkhxOp9NpdhGAmSorKzV58mS9/vrrXs1v37693nnnHQ0ZMiTAlQGhp7CwUBs3btSGDRu0ceNGff755zp27JjrfFpamvLy8swrELCwI0eOaOnSpfr3v/+tjz/+WAcPHnQ7Nzo6WpdffrnuuOMO/fznPw9ilUBoSE1NVWFhoc/XRUdHa9asWZozZ44cDkcAKgPC03XXXac33njDcIx/9wFGDz74oObOndvk6ydOnKjFixf7ryALoXMStlZTU6NrrrlGy5YtMxyPjIxU165dlZCQoNzcXJWUlLjOFRUVadSoUVq1apUuuOCCYJcMWM769ev1xBNPaMOGDdq3b5/Z5QAh6dZbb9WiRYtUVVXl1fzq6motXbpUS5cu1W9+8xv9+c9/Vps2bQJcJRDa4uLiXP++q6mp0cGDB5Wfn6/TezWqq6s1d+5c7d27Vy+//LKJ1QKh44MPPqgXTAKAL9gQB7b22GOP1Qsmp0yZovz8fO3evVubN2/W4cOH9e6776pr166uOWVlZbr66qsNoSVgV5s2bdJ7771HMAk0w4YNGxoMJiMjI5Wamqr+/furb9++SkhIqDfntdde08iRI1VaWhqMUoGQkZKSoptvvll/+9vftHPnTh0/flw7duxwdfbn5eXp0KFDevHFF5Wammq49pVXXtGrr75qUuVA6CgpKdHUqVPNLgNAiKNzErZ16NAhzZs3z3Bs/vz5uvfeew3HIiIiNH78eA0cOFAXXniha2lCQUGBFi5c2Ky2bCDctWrVisAE8FFiYqKuv/56XX755brooovUunVr17mTJ09q3bp1euCBB7Ru3TrX8Y0bN2rSpEl65513zCgZsJwPP/xQZ599dqNLs5OSknTzzTfryiuv1IgRIwzvHp89e7YmTpyoiAj6OQB3pk+f7nqFQsuWLXX8+HGTKwJCx+OPP65zzjnH6/kpKSkBrMZchJOwrQULFhjehTdkyBDNmDHD7fzOnTtr0aJFGjFihOvYk08+qd///vdq165dQGsFQkHr1q3Vv39/DRgwQAMHDtSAAQOUm5urYcOGmV0aEBK6deum++67T9dff73i4+MbnBMZGamhQ4dqzZo1mjZtml588UXXuX/+859as2YNX3OApL59+/o0PykpSX//+9911llnuZZ579+/X+vXr9dFF10UiBKBkJeZmalFixZJqm3omDNnju655x6TqwJCR//+/TV06FCzy7AEfg0IW6qpqam3VOfBBx9s9Lfrw4cPN/wD9dixY3rrrbcCUiMQKsaMGaNt27apuLhYa9as0YIFC3TllVcqLS3N7NKAkDF37lzt2LFDkydPdhtMni4yMlLPPfeczj//fMPxUz8kAvBdnz591L9/f8OxnJwck6oBrK28vFw33XSTK8z/3e9+pwEDBphcFYBQRTgJW8rKylJRUZFr3KNHD69/YzF58mTDeOnSpX6sDAg9PXv2VHp6OsvegGa4/PLLFRMT49M1kZGR9TpU/vOf//izLMB2evbsaRgfPHjQpEoAa7v//vu1a9cuSVLXrl31yCOPmFwRgFDGT5KwpRUrVhjGI0eObLRr8vS5p8vMzOTdKgAAU9Rdbnro0CGVlZWZVA0Q+ioqKgzjxMREcwoBLGzTpk166qmnXONnn31WrVq1Mq8gACGPcBK2tGXLFsM4IyPD62tTUlLUrVs317iqqkrZ2dl+qgwAAO8lJSXVO1ZSUmJCJUDoczqd2rRpk+FY3WXegN1VV1dr8uTJOnnypCTpqquu0ujRo02uCkCoI5yELdV9f1B6erpP19edz/uIAABmOLVD6unYpA1omldeeUX79u1zjXv37q2BAweaWBFgPfPnz9fXX38tqbaz+Omnnza5IgDhgN26YTvl5eXKz883HOvSpYtP96g7f8eOHc2uCwAAX61bt84wTktL8/ndlQCkv/71r5o2bZprHBERoWeeecbr1/4AdpCdna158+a5xo8++qg6depkYkVA6KusrNTu3bt16NAhRUdHq127dkpJSVGLFi3MLi2oCCdhOwcPHnTtKidJ0dHR6tChg0/36Ny5s2F84MABv9QGAIAvXnnlFcP4sssuM6kSwNq+/fZbwy+nq6urdeTIEX3zzTdatmyZ4RU9MTExevHFFzV8+HAzSgUsqaamRpMnT1ZVVZWk2nce33zzzSZXBYS2W2+9Vbt37673vuOoqCj1799fo0aN0rRp09S+fXuTKgwewknYTmlpqWHcokULn38r3rJlS4/3BAAg0D788EOtXbvWcGzSpEnmFANY3HPPPac//elPHuc4HA5deumlmj9/vs4555wgVQaEhqefflqfffaZpB8DfDqLgeZxt3fFiRMntGHDBm3YsEGPPvqo7r77bs2ZM0eRkZFBrjB4eOckbKdukBgXF+fzPeLj4z3eEwCAQDp8+LBuueUWw7ErrriC9+MBzXDVVVdp9uzZBJNAHbm5ubrvvvtc45kzZ6p3794mVgTYR3l5uR5++GGNGDEirHMHwknYTt2W6aa8mys2NtYwLi8vb1ZNAAB4q6amRjfccIMKCgpcxxISEtiUAGimt956SxdeeKGGDBminTt3ml0OYBn/+7//q+PHj0uq3Shq1qxZJlcEhC6Hw6GMjAzNmzdPK1euVEFBgcrKylRRUaHCwkJ98MEHuuWWW+o1UWVmZuraa6/VyZMnTao8sAgnYTt1v8hPvTfFF5WVlR7vCQBAoEyfPl3/+te/DMdeeOEFnzd3A+zkqaeektPpdH2UlZVp7969Wr58uSZPnmxYFbNu3ToNGDBAn3/+uYkVA9bw8ssva9WqVZJqQ5UXX3yRjdeAJrrkkku0fft2rV+/XrNmzdKIESPUuXNnxcfHKzY2VikpKRo9erSef/55fffddxo8eLDh+hUrVui5554zqfrAIpyE7bRq1cowrttJ6Y26nZJ17wkAQCA8/fTTWrhwoeHYPffco2uuucakioDQFB8fr9TUVF1++eVatGiRvvrqK/Xr1891vri4WFdccYWKi4tNqxEw2/79+3X33Xe7xjfddJMuuugiEysCQltGRoZ++tOfejU3NTVVq1at0gUXXGA4/sgjj6isrCwQ5ZmKcBK2UzdILCsrM+ze7Y1Tyxrc3RMAAH/7xz/+oTvuuMNwbNKkSfrjH/9oTkFAGOnVq5dWrlxp6EAuLCzUY489ZmJVgLluvfVWV0DfqVMnLViwwNyCAJuJi4vTa6+9pqioH/eyPnDggD766CMTqwoMwknYTnJysmFnuerqah04cMCnexQWFhrGHTp08EttAAA0ZPny5Zo4caLhl2m//OUvtWjRInZLBfwkOTlZc+fONRxbvHixOcUAJnv77bf13nvvucZ/+tOflJiYaF5BgE316tVLY8eONRwjnATCQHx8vLp27Wo4lp+f79M96s5ntzoAQKCsWbNGV111lU6cOOE6NnLkSC1ZskSRkZEmVgaEn/HjxxsC/3379mnPnj0mVgSYY/r06a4/X3755br66qtNrAawt+HDhxvGO3bsMKmSwCGchC3VDROzs7N9uj4nJ8fj/QAA8IcNGzZo7NixhvcjZ2Rk6L333mNDAiAAEhMT1bZtW8Ox77//3qRqAPOc/r7VFStWyOFwNPoxbNgwwz327NlTb86WLVuC+4kAYaDupodFRUUmVRI4hJOwpdNfeC5JWVlZXl+7f/9+5eXlucbR0dFKT0/3U2UAANT66quvNGrUKJWWlrqOnXvuufrwww/VsmVLEysD7CU6OtrsEgAANlb3v0PV1dUmVRI4hJOwpdGjRxvGq1at8npTnLrvdxg2bBgb4gAA/GrHjh0aOXKkjhw54jrWp08f/ec//1FCQoKJlQHh7dixYzp8+LDhWMeOHU2qBgCA+h387du3N6mSwIlqfAoQfjIyMpScnKyDBw9Kknbv3q3MzMx6SxEa8vLLLxvG48aNC0iNAAB72rNnj0aMGGHYrK179+5auXJlWP5jFLCSFStWGH5h3b59e51xxhkmVgSYY9myZT53Z23dulV33323a9yxY0f9/e9/N8zp1auXX+oD7OTTTz81jOsu8w4HhJOwpYiICE2aNEmPP/6469jcuXM1dOhQj7uerl69WuvWrXONW7duzcuhAQB+s3//fg0fPlwFBQWuY507d9bq1avVuXNnEysDwl95ebnmzJljODZ69GhFRLDYDPbz85//3OdroqKM8UJcXJxGjBjhr5IAWyouLtY///lPw7G6G+SEA/5LC9uaMWOGYTn2J598okcffdTt/MLCQt10002GY7fffruSk5MDViMAwD4OHz6skSNHateuXa5j7du318qVK9W9e3cTKwNCyz333KNNmzb5dM3hw4c1duxYffvtt65jkZGR+sMf/uDv8gAA8Nrdd99t2KAqJiZGo0aNMq+gACGchG0lJydr1qxZhmMzZ87UtGnTtG/fPtexmpoaLV26VBkZGYaNcFJSUnTXXXcFq1wAQBg7duyYLr30Um3bts11LDExUR999JH69OljYmVA6Pnoo480cOBADRo0SAsXLtSWLVsaXJ7qdDq1fft2PfzwwzrzzDO1atUqw/k//OEPOvvss4NVNgAgjP3xj3/UF1984fX8EydO6K677qr3WrkpU6aE5etGHE5vdwEBwlBNTY3GjRun5cuXG45HRkYqLS1NCQkJys3NNfymQpLi4+O1cuVKDR48OIjVAta1fv16lZeX1zvuzbuHTklJSWHne9jWsGHDlJmZaTj20EMP6YILLvD5Xv3791dSUpKfKgNCT79+/bR161bDsZiYGHXu3FmJiYmKiYnRsWPHtHfvXh07dqzBe0ycOFGvvPIKS7oBH9R9h39aWpqhuQOws6FDh+qTTz5RRkaGrr76ag0fPly9e/eu9zqEkpISffjhh1qwYIG2bNliONezZ09t2LBB7dq1C2LlwUE4CdurqKjQjTfeqDfeeMOr+e3atdM777yjoUOHBrYwIIR069ZNe/bsadY9Jk6cqMWLF/unICDEeHrfsa/WrFnDf6Ngaw2Fk95q06aN/vjHP2rKlCl+/boE7IBwEnDvVDh5utjYWKWmpiohIUGRkZE6dOiQ8vLyVFNTU+/6Tp06ae3atfrJT34SrJKDil8Fwvbi4uK0ZMkSvfPOO+rXr5/beS1bttS0adOUnZ3ND30AAAAWtWTJEj366KMaMWKE2rRp0+h8h8Ohvn376rHHHtPOnTs1depUgkkAQMBVVlZq165d+vLLL7Vp0ybt3r27wWDysssu09atW8M2mJTYrRtwmTBhgiZMmKCdO3dqw4YNKiwsVFVVlRITE9WnTx8NHjxYcXFxZpcJAAAAD/r06aM+ffronnvuUU1Njb777jvt3LlT+fn5Onr0qKqrq9W6dWslJCSoW7duOu+887wKMQEAaKrZs2erT58+WrdunbZv366TJ096nN+qVSuNGjVKt912m4YMGRKkKs3Dsm4AAAAAAAAgCMrKypSdna28vDzt379fpaWlqqmpUWJiopKSkpSenq6zzz5bkZGRZpcaNISTAAAAAAAAAEzBOycBAAAAAAAAmIJwEgAAAAAAAIApCCcBAAAAAAAAmIJwEgAAAAAAAIApCCcBAAAAAAAAmIJwEgAAAAAAAIApCCcBAAAAAAAAmIJwEgAAAAAAAIApCCcBAAAAAAAAmIJwEgAAAAAAAIApCCcBAAAAAAAAmIJwEgAAAAAAAIApCCcBAAAACzl48KCmT5+uXr16KTY2Vp06ddJ1112nr7/+2uzSAAAA/M7hdDqdZhcBAAAAQNq2bZt+8YtfqLCwsN656OhovfLKK7rhhhtMqAwAACAwCCcBAAAACygtLVXfvn2Vm5vrOpaSkqKioiJVV1dLkqKiorR27VpdcMEFZpUJAADgVyzrBgAAACzg2WefdQWTI0aMUEFBgQoLC1VSUqLp06dLkk6cOKF7773XzDIBAAD8is5JAAAAwAIGDx6srKwsdezYUd9++63atGljOD9hwgS9++67cjgcKioqUrt27UyqFAAAwH/onAQAAAAsYM+ePZKkoUOH1gsmJenKK6+UJDmdTuXl5QWzNAAAgIAhnAQAAAAsoGXLlpKkqqqqBs9XVla6/tyqVaug1AQAABBohJMAAABAIxYvXiyHw+HxIzMzs1nP6NevnyRp9erV9XbrdjqdWrx4saTaEDMtLc2rezZW86RJk5pVMwAAQHMRTgIAAAAWcPPNN0uSjh49qksuuUQff/yxSktLlZ2drauuukqffPKJJOmGG25QXFycmaUCAAD4TZTZBQAAAACo3aF76tSp+stf/qLs7GwNHz683pwePXpo/vz5JlQHAAAQGISTAAAAgI+mT5+uSy65xHDsnHPOafZ9n332Wa1cuVI7d+6sd27YsGH6xz/+oaSkJK/vt3LlSsP4hx9+0A033NDsOgEAAPyFcBIAAADwUXp6ukaMGOH3+3744YcNBpOSdOmll6pTp04+3a9ujezyDQAArIZ3TgIAAAAWcPLkSU2fPt3t+a+++iqI1QAAAAQH4SQAAABgAS+++KJycnJc4wsvvNBwnnASAACEI8JJAAAAwGRHjx7VnDlzXOPY2Fi99tpratGihevY9u3bVVVVZUZ5AAAAAUM4CQAAAJhs/vz5Kioqco1/97vfqXv37urTp4/rWHV1taGzEgAAIBywIQ4AAADCyjfffKOcnBzt379fpaWl6tixo37zm98oOjra7NIalJ+fr6eeeso1TkpK0qxZsyRJZ511lr744gvXua+++sovu4IDAABYBeEkAAAAQkZmZqaGDRvmGs+ZM0cPPvigTpw4ob/85S964YUXtG3btnrXTZgwQYmJiUGs1HszZ85URUWFazx79mwlJSVJqg0nT8d7JwEAQLghnAQAAEBIO3LkiMaOHatPP/3U7FJ8tmnTJi1ZssQ17tatm2677TbX+Gc/+5lhPuEkAAAIN4STAAAACFknTpyoF0wmJSWpS5cukqQ9e/aopKTErPIaddddd8npdLrG8+bNU2xsrGtct3Ny69atQasNAAAgGAgnAQAAELIWLVqkH374QZI0YsQIzZ07V//zP/+jiIjafR+dTqdWr16t+Ph4M8ts0Lvvvqt169a5xv3799d1111nmNO1a1e1atVKpaWlkqQffvhBRUVFat++fVBrBQAACBR26wYAAEDIOhVM3nHHHVq5cqUyMjJcwaQkORwOjRgxwtCNaAXV1dWaMWOG4diCBQvkcDgMxxwOh9LT0w3H6J4EAADhhHASAAAAIS0jI0MLFy40uwyfPPvss9q5c6drPGrUKF188cUNzmVTHAAAEM4IJwEAABDSHnrooXodh1Z25MgRPfzww65xRESEFixY4HZ+3U1x6JwEAADhhHASAAAAIatjx45uOw6t6qGHHtLhw4dd40mTJtULIE9H5yQAAAhnhJMAAAAIWeeff35IdU3u3LlTzz77rGscHx+vhx56yOM1dcPJnJwcnThxIiD1AQAABBvhJAAAAEJW9+7dzS7BJzNmzFB1dbVr/Ic//EGdO3f2eE1qaqoSEhJc48rKSu3YsSNgNQIAAAQT4SQAAABCVps2bcwuwWuffvqp3n33Xde4ffv29Xbsdoel3QAAIFxFmV0AAAAA0FTR0dFml+AVp9OpO++803Bs1KhR2rhxo1fXt2rVyjDeunWrrrvuOr/VBwAAYBbCSQAAACDAlixZok2bNhmOvfbaa3rttdeadD86JwEAQLhgWTcAAAAQQBUVFZo1a5Zf70k4CQAAwgXhJAAAABBATz31lPbs2ePXexYWFurw4cN+vScAAIAZCCcBAACAACkqKtL8+fNd46ioKG3fvl1Op9Pnj549exruvXXr1mB/OgAAAH5HOAkAAAAEyJw5c3T06FHX+Le//a3OPPPMJt2rT58+hjFLuwEAQDggnAQAAAACICcnRy+99JJr3KJFC82ZM6fJ9+vdu7dhTDgJAADCAeEkAAAAEADTp0/XiRMnXOPbb79dKSkpTb4fnZMAACAcEU4CAAAAfrZ69WqtWLHCNW7btq1mzJjRrHvW7Zzctm2bTp482ax7AgAAmI1wEgAAAPCjmpoa3XXXXYZjs2bNUkJCQrPuW7dzsry8XN99912z7gkAAGA2wkkAAADAj/76178adtLu0qWLbr311mbfNykpSR06dDAcY2k3AAAIdYSTAAAAgJ+UlZXpvvvuMxx76KGHFBcX55f7895JAAAQbqLMLgAAAADw1tChQ+V0Os0uw60WLVqosLAwYPfPzMwM2L0BAADMQOckAAAAAAAAAFMQTgIAAAA+uvHGG+VwOAwfVuxqrFtj9+7dzS4JAADAgHASAAAAAAAAgCkIJwEAAAAAAACYwuG08hvFAQAAAAvYv3+/tm3b5nFO//79lZSUFKSKvLNq1SqP51NSUpSenh6kagAAAOojnAQAAAAAAABgCpZ1AwAAAAAAADAF4SQAAAAAAAAAUxBOAgAAAAAAADAF4SQAAAAAAAAAUxBOAgAAAAAAADAF4SQAAAAAAAAAUxBOAgAAAAAAADAF4SQAAAAAAAAAUxBOAgAAAAAAADAF4SQAAAAAAAAAUxBOAgAAAAAAADDF/wNjSdxhvU8SrAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensure cutoff_r (receptive field) is 4 Angstrom for SPICE dataset.\n",
        "Let us do radial function analysis. ngl can be used for viewing molecular trajectories."
      ],
      "metadata": {
        "id": "6bs71WeFor0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Molecular Dynamics analysis using Radial Distribution function."
      ],
      "metadata": {
        "id": "wkn7Nn-Yo3pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Confirmation of Molecular dynamics run.\n",
        "We can see how particles are interacting for SPICE dataset."
      ],
      "metadata": {
        "id": "Gl63qiyw33hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhance modal:\n",
        "from ase.visualize import view"
      ],
      "metadata": {
        "id": "XFJ-li4cb6VA"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#si_traj = read('./Si_data/sitraj.xyz', index=\"::\")\n",
        "si_traj = read('./Si_data/DES_L.xyz', index=\"::\")"
      ],
      "metadata": {
        "id": "rzbb9J6WmR9u"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "si_cell_prod = np.array([10.0, 10.0, 10.0])\n",
        "for i in range(len(si_traj)):\n",
        "  si_traj[i].set_cell(si_cell_prod)\n",
        "  si_traj[i].set_pbc([True, True, True])\n",
        "  si_traj[i].wrap()"
      ],
      "metadata": {
        "id": "F7bJ5b7BnQ1h"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nglview\n",
        "!pip install ipywidgets==7.7.2 nglview\n",
        "#!pip install pytraj\n",
        "#!jupyter nbextension enable --py widgetsnbextension\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3kdzwhamhIk",
        "outputId": "d482f7bb-87a3-456f-c6d0-0bfe5fecd797"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets==7.7.2 in /usr/local/lib/python3.10/dist-packages (7.7.2)\n",
            "Requirement already satisfied: nglview in /usr/local/lib/python3.10/dist-packages (3.0.8)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.2) (1.1.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nglview) (1.26.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.2) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.2) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.7.2) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (7.16.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets==7.7.2) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.7.2) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.7.2) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets==7.7.2) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (0.22.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.2) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart Run-time for Nglview to work"
      ],
      "metadata": {
        "id": "Vs8gZotd92Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q condacolab\n",
        "#import condacolab\n",
        "#condacolab.install()\n"
      ],
      "metadata": {
        "id": "5VmvwWCusUHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import condacolab\n",
        "#condacolab.check()"
      ],
      "metadata": {
        "id": "_EgtW7t5sX18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USE last slider (after ball size) to step through MD simulation\n",
        "\n",
        "From https://www.quora.com/How-did-nitrogen-bond-with-4-hydrogen-in-NH4-when-its-valency-is-only-3\n",
        "\n",
        "One can see how NH4 bonding happens and nnot all N-H bonds are of equal size.\n",
        "\n",
        "We will now coarsened 3 H atoms into one and see how it impacts bonding outcome."
      ],
      "metadata": {
        "id": "Y03EAp224-mD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2024-12-11 at 5.32.33 AM.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "pHHAciu47TLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "import nglview as nv\n",
        "view(si_traj, viewer='ngl')\n",
        "# Animation sometimes does not autostart, so use lider to step through\n",
        "# SLIDER after ball size is next-step in simulation...\n",
        "# View below will change as one steps through.\n",
        "\n",
        "# See below for how NH4 happens. (One bond is ionic)\n",
        "# https://www.quora.com/How-did-nitrogen-bond-with-4-hydrogen-in-NH4-when-its-valency-is-only-3\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517,
          "referenced_widgets": [
            "994c35d93f0041ed99aff4e12a524337",
            "a99c252d856a48d989d99e81742dacd5",
            "d1526029201349b589f342e5783fc96d",
            "699a8050e045453291c43987fd8e7c62",
            "f245accbecbb46bdb89635c7f73eaeb5",
            "e9ad173779214d28b8e41f86f5ae5994",
            "3fa251860a264a76b8fcbbe604b9f1c4",
            "b4599d7d2ca040acb6f2766fb51dd280",
            "35bcf35338bc4b33896a6d663bedd189",
            "de04b796a6734366a186f40036009bea",
            "3ce9ef48cdd4407d939ba258638a4151",
            "57a4bd6ba6454a01b661b8b91d76be7f",
            "b7d49dd491a04aad94c4de9ea1c665ff",
            "4bb33f5cb18c4e19859aaf803ea7bcc3",
            "528e5c482c0f48a697d845ebc1fe8691",
            "6cf7e7a469c64fac809744d34f534236",
            "03b537e3395e4241a10e473b3ba7cd27",
            "93b369f2408d48d1af62b64c274292f4",
            "5abf5dc620354b99a6a36128e404d099",
            "ccab8246681d4684bda585c5b372a1a9",
            "9ea8bbe8cd354bfb89ec65a8673a675a",
            "0d4c0d6613e34153ab1aca37e6ad4bb4",
            "4dc431bb637b4802b6a89b05b6e2a31c",
            "2b259bb2cd9043a6a016fdd4f68c902d",
            "07324f62c49240ec8efdb97de57b316b",
            "63a33f95bc98446eb9813bb2d7d5b75b",
            "1e6c4927b0a64677a28eacaa85010e90",
            "19718e8a0e4041d5a08d2d58bb46584b",
            "42d1ea613ed644d49f89b1011640810f"
          ]
        },
        "id": "DoH2ueMa4NzW",
        "outputId": "1583833d-0fe0-4dd9-ffcb-9625190c65a1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(NGLWidget(max_frame=18699), VBox(children=(Dropdown(description='Show', options=('All', 'N', 'H…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4599d7d2ca040acb6f2766fb51dd280"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "Vovoan395GyL"
      }
    }
  ]
}